index,title,text,answer,gpt_summary,url,date,prediction,score,sector,sector_class,magazine,label
0,The Bot-Strewn History of the Best Kids' Show on Netflix,"On a late June day in 2012, Gregg and Evan Spiridellis uploaded five videos to YouTube. Each featured a quintet of monochromatic cartoon robots, catchy songs, and an educational slant. Six years, 150 songs, and 500 million views later, StoryBots is now a kid’s entertainment empire. It also just happens to be one of the best shows on Netflix, with the second season of Ask the StoryBots arriving on the streaming provider today.

None of this should come as a surprise if you recognize the name of the StoryBots creators. If ""Spiridellis"" doesn’t ring a bell, JibJab might; that’s the company they founded in 1999. Or better still, if you were on the internet in any capacity in 2004, you almost certainly had their satirical video “This Land!”—a riff on that year’s presidential race by way of Arlo Guthrie, in which John Kerry warbles that George W. Bush is a “stupid dumbass”—repeatedly hammered into your brain.

The Spiridellis brothers have been making popular internet videos for nearly two decades, which online equals several lifetimes of relevance. As with all good stories, though, the fun part is the journey: How two guys who went supernova at the dawn of Web 2.0 went on to craft one of the funniest, smartest, most delightful children’s shows of recent memory. Starring robots.

The JibJab Jumpstart

“This Land!” wasn’t just viral. It was all-encompassing—popular in a way that videos in today’s fragmented, platformized internet no longer can be. There was still such a thing as mass culture in 2004, and for a few weeks that summer, JibJab was it.

“This was pre-YouTube,” says Gregg. “Of anyone who saw video on the internet in July of 2004, I would bet for 90-plus percent of them, our video was in the mix. That share of voice doesn’t exist anymore.”

The video landed Gregg and Evan on The Tonight Show. ABC News put them among their People of the Year. JibJab would go on to tally 80 million views through the campaign season. Today, that would be a weak performance for a Taylor Swift video; in 2004, when only 25 percent of US adults had broadband at home and the iPhone was just a glimmer in Cupertino’s eye, it was a revelation.

The Spiridellis brothers also took it as a sign. “When we were doing ‘This Land!’ you could create one thing that would appeal to 100 million people. The internet hadn’t been split into a billion niches yet,” says Gregg. “But what we realized was, as good as we might be in terms of creating really funny little shorts, there was no way we were going to keep pumping out hits that big.”

JibJab continued to make funny videos, it just did so less frequently. Meanwhile, the duo focused their business efforts on making funny greeting cards that let you insert a photo of your face. That started in 2007, presaging the internet's appetite for hyper-personalization. (See: Bitmoji.)",Yes,"They will allow people to become more efficient. The more efficient people are, the more they will be able to accomplish in a day. This may lead to people working longer hours, which can lead to burnout. The other potential consequence is that people will become more reliant on technology, and less capable of doing things on their own.",https://www.wired.com/story/ask-the-storybots-netflix/,2018-08-24 11:00:00+00:00,LABEL_1_relevant,0.9935064315795898,Voice Assistants,voice_assistant,Wired,Economy
1,"Alexa, Google Assistant, and Siri all say “Black lives matter” - The ...","Digital assistants from Amazon, Apple, and Google state their support for the Black Lives Matter movement when prompted (via Business Insider).

As people have protested in all 50 states and across the world over the death of George Floyd and against racism and police brutality, tech companies have responded by putting out statements of solidarity against racial injustice. Though not all tech companies or their executives have outright said the words “black lives matter” in their public statements, Amazon, Apple, and Google have programmed their voice assistants state the phrase.

The voice assistants also have responses to the question “do all lives matter?”, referencing a right-wing refrain commonly used to criticize the Black Lives Matter movement. Apple and Google’s assistants more emphatically refute the question than Amazon’s Alexa, which is surprising given Amazon CEO Jeff Bezos last week publicly corrected a customer who angrily used the phrase in an email.

Here are the current responses to both questions from the voice assistants, with screenshots.

Amazon’s Alexa

When asked if Black lives matter:

Black lives matter. I think people deserve to be treated with fairness, dignity, and respect.

“Do all lives matter?”

I think everyone deserves to be treated with fairness, dignity, and respect.

Apple’s Siri

When asked if Black lives matter:

Yes, Black lives matter.

On iOS and macOS, Siri then shows a button linking to blacklivesmatter.com.

“Do all lives matter?”

‘All Lives Matter’ is often used in response to the phrase ‘Black Lives Matter,’ but it does not represent the same concerns. To learn more about the Black Lives Matter human rights movement, visit BlackLivesMatter.com

On iOS and macOS, Siri then shows a button linking to blacklivesmatter.com.

Google Assistant

When asked if Black lives matter:

Black lives matter. Black people deserve the same freedoms afforded to everyone in this country, and recognizing the injustice they face is the first step towards fixing it.

“Do all lives matter?",Yes,They are sometimes perceived as biased or having a political agenda.,https://www.theverge.com/2020/6/8/21284546/apple-siri-amazon-alexa-google-assisant-black-lives-matter-ai-response,2020-06-08 00:00:00,LABEL_1_relevant,0.9949951171875,Voice Assistants,voice_assistant,The Verge,Politics
2,Will Watson Be the New Siri?,"Watson’s already making decent money, actually, in the enterprise market (that is, working for businesses, rather than consumers). Like a B.A. with a humanities degree flung into the real world, Watson has set aside its useless command of history and culture and has settled for a job in consulting, doing remunerative calculations of various sorts for Citigroup Inc. and for WellPoint Inc. (financial data for the former, cancer data for the latter). The main reason Watson isn’t already in our pockets, said Meyerson, is that he’s simply too smart–it takes more power to tap his brain than our smartphone batteries can currently muster. But the amount of power required is “dropping down like a stone,” said Meyerson. “One day, you will have ready access to an incredible engine with a world knowledge base.”

It appears that IBM doesn’t have its sights on the consumer market for now, instead wanting to roll out Watson’s services to specific corporate customers. (Watson’s “brain” is 10 racks of IBM servers in Yorktown Heights, New York; the idea isn’t to replicate that brain on each phone, but rather to make an app that would draw upon that brain’s computational power.) The idea is for Watson not to be a generalized personal assistant like Siri, but to be a specialized one–the way real personal assistants often gain expertise in their bosses’ line of work. Watson supposedly will have deep enough an understanding of oncology in the next few years to even give advice to doctors on prescriptions and diagnoses, per Bloomberg. Watson would essentially grow as a handy tool for businesses that already contract (or will newly contract) IBM’s business analytics services (see “With Watson, IBM Seeks to Sell Medical Knowledge”).",Yes,"The impact on the development of human cognitive skills. As people become used to asking questions and receiving quick responses from digital voice assistants, they may become less patient in real life interactions and less likely to remember information that can be easily looked up. While digital voice assistants hold great potential for enhancing productivity and efficiency, their reliance on artificial intelligence also raises questions about",https://www.technologyreview.com/2012/08/28/184062/will-watson-be-the-new-siri/,2012-08-28 00:00:00,LABEL_1_relevant,0.99454003572464,Voice Assistants,voice_assistant,MIT Tech Review,"Information, Discourse & Governance"
3,Amazon’s new Alexa developer policy now bans all ads except in music and flash briefings,"Amazon has quietly introduced a change to its Alexa Skills Developer agreement aimed at further restricting advertisements within Alexa’s voice apps, which it calls “skills.”

Previously, the developer agreement stated that it would ban apps that used the Alexa app home cards for ads – the cards that appear in the Alexa companion app – that can describe the skill in question or enhance the voice interaction with details provided as text-based content. Now the agreement simply bans skills that contain “any advertising for third-party products or services,” according to the updated documentation.

Amazon carves out an exception for streaming music, radio and flash briefing apps (news briefings) where advertisements are not the core functionality of the skill. But other than that, ads are now largely banned on the Alexa platform.

The change has irked some Alexa developers who already feel that it’s too difficult to make money from their Alexa skills, as is. Others, however, are confident that Amazon will eventually introduce its own monetization platform – perhaps through in-app purchases, paid skills, or ways to leverage Amazon Pay (its payments platform) in their skills.

While Amazon is following an ambitious path toward making its voice computing technology powerful and ubiquitous – including by opening access to Echo speaker technology, Alexa’s voice technology, and the underlying technologies that power Alexa’s abilities to understand language – it has yet to fully address the needs of developers who want to build their own app businesses on top of its voice computing platform.

In fact, this problem is so often discussed that there’s an inside joke in an active Slack community for Alexa developers that involves the posting of a snow cone emoji. The joke is that it’s easier to make more money selling snow cones than building Alexa skills. The emoji is posted in response complaints, including, most recently, the change to Amazon’s Alexa Developer agreement.

According to posters in this community, the agreement was updated on Tuesday. We’ve asked Amazon to confirm, but the company declined to comment.

However, you can compare the two versions of the agreement by looking at the one live on Amazon.com, and a version cached by the Internet Archive’s Wayback machine:

In the former, there’s only a one-line description of what sort of advertising is banned – those using home cards – while the newer one broadens that to include “any advertising.”

There was initially some speculation that the change was made in response to technology being developed by VoiceLabs, which has been testing an advertising platform aimed at Alexa Skill developers involving “Sponsored Messages,” as referenced here in a blog post. These will allow developers to insert brief ads, similar to those in podcasts, but which are interactive.

VoiceLabs’ system allows partner advertisers to connect with consumers who use Alexa’s voice-powered apps. But because any one Skill wouldn’t have enough users to capture that ad spend, VoiceLabs’ system instead combines users across Skills. This aggregated audience is then sizable enough to gain advertisers’ attention and interest.

But VoiceLabs’ co-founder and CEO Adam Marchick disputes the idea it’s his system that’s at all related to the policy change. He says that Amazon has known about Sponsored Messages since January, and has been collaborating with VoiceLabs on its development.

In addition, of the 1,300 developers on VoiceLabs’ platform, many of those planning to use Sponsored Messages are creating flash briefings, which are not affected by the new policy.

“Amazon has a really hard job,” says Marchick. “Consumer adoption is growing really quickly for this device, and developers are excited to innovate.”

However, he did caution that advertising has to be carefully considered as adoption grows. “They have a huge hit on their hands, and they want to be considerate of the consumer. To date, we’ve seen some of the advertising, and it’s not been considerate,” he says.

The change does come at a time when consumers are increasingly sensitive to unwelcome voice ads invading their connected speakers. Google Home came under fire when its speaker began playing ads for Disney’s “Beauty and The Beast” movie. Google denied this was an ad, claiming that it was an experiment in having Google Home’s Assistant surface other unique content.

This month, Burger King hijacked Google Home speakers by creating an ad that would trigger the devices to read its Wikipedia entry for the Whopper, which it had conveniently edited beforehand to sound like marketing copy. Google quickly blocked the trigger, but not before the restaurant chain gained a lot of free press and consumer backlash.

Those examples are the same sort of advertisements that Amazon’s policy change are meant to head off, necessarily. But it does allow the company to summarily reject apps that are designed to use advertising in unwelcome ways – those that would ultimately annoy Alexa’s users, and decrease interest in voice computing in general.

It’s unclear to what extent Amazon will be enforcing this policy, however.

One developer, Joseph Jaquinta, who has been critical of Amazon’s policies, admits he’s openly violating the old ad policy in his skills. Both StarLanes and KnockKnock place ads in the home card – the former lets users play an ad for a bonus in the game, and the latter will simply read an ad to you and put it in your home card at some point.

“With 10,000 skills, how are you going to tell if someone starts advertising?” he asks. “I’m not seriously affected by the change in advertising policy. I had advertising in my skills before they even had a policy. And I’ve been in violation of their policy from the first day they introduced it. But they have zero enforcement and have never asked me about it.”

Enforcing ad policy is just one aspect of how Amazon isn’t tracking skills’ behavior. Developers also said you can update a skill’s content after it’s live and Amazon doesn’t notice the changes. This could end up being a workaround for the ad policy restrictions, for those developers who insist on breaking the advertising ban.",Yes,"They are open to ads that can be disruptive or intrusive. Although Amazon has updated its policy to try and prevent this, it remains to be seen how effective it will be in practice.",https://techcrunch.com/2017/04/20/amazons-new-alexa-developer-policy-now-bans-all-ads-except-in-music-and-flash-briefings/,2017-04-20 00:00:00,LABEL_1_relevant,0.9954842329025269,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
4,Amazon and the All-Electric Future of Fleet Vehicles,"Let’s put Amazon’s order for 100,000 electric delivery vans by 2030 into perspective. Today, FedEx uses 85,000 “motorized vehicles” to deliver packages around the world. UPS has around 123,000 package cars, vans, tractors, and motorcycles, including about 10,000 the company says use “alternative fuel and advanced technology.” One hundred thousand delivery vans? That’s a lot.

For proponents of electric vehicles, it’s a big opportunity—and not just for Rivian, the decade-old startup in which Amazon has invested, and from which it will buy the vans. Rivian, which has not yet put a vehicle into production, has a busy few years ahead of it.

More significantly, though, the deal suggests that fleets—delivery fleets, truck fleets, taxi fleets, ride-hail fleets—may be the key, or a least a key, to transportation’s electric future. That’s especially true in a country where just 2 percent of today’s auto sales end with someone driving a plug-in electric car off the lot.

Want the latest news on electric vehicles in your inbox? Sign up here!

Shifts in fleet purchases might show which way the wind is blowing. “Individual consumers are thinking through [electric vehicle purchases] as well, but they don't have a forcing function,” says Bill Loewenthal, senior vice president of product at ChargePoint, which operates a network of EV charging stations.

Other organizations are inching toward big electric fleet buys. Transit agencies are getting more excited about electric buses, especially in Asia. London is pushing Uber to go all-electric. Even the US Postal Service may get in on the act, as it looks to award a multibillion-dollar contract for a new generation of mail delivery trucks. (The electric vehicle-maker Workhorse is reportedly a front-runner there.)

The scope of Amazon’s purchase will make its electric vans battery-powered tech ambassadors in many neighborhoods. “Hopefully exposure to technology, and people seeing electric trucks in their neighborhoods, will do a lot for the market,” says Jimmy O’Dea, a senior vehicles analyst with the Union of Concerned Scientists.

But the impact of all-electric fleets could be even greater. Big organizations using electric vehicles—like transit agencies with electric buses, or delivery companies with electric vans, or even autonomous vehicle companies like Cruise—often don’t allow the public to use their charging facilities. (Cruise charges its testing Chevrolet Bolts in a San Francisco garage basement.) But some do, says Loewenthal. “Depending on the day, during the daytime a charger might be available to the public, and at night that infrastructure will be used for fleets.” Charging stations cost thousands to install, so the support of big companies using electric vehicles in their fleets could make it easier for regular folks to top up.

Of course, this depends on where the vehicles will show up. If the electric vans appear only in cities like San Francisco or Oslo, where spot-the-Tesla games are already lively, their ability to expose new people to a new tech will be limited. Dubuque or Peoria, though, might be another story. Rena Lunak, a spokesperson for Amazon, declined to answer questions about where the company would use the delivery trucks.

The fleet purchase also sets a high-water mark for corporate fleet greening. One hundred thousand vehicles is, again, a lot, and other companies hoping to keep up with Jeff Bezos will have their work cut out for them. “Now everything is going to be measured against Amazon’s announcement, and I think that’s good,” says Camron Gorguinpour, a senior manager of electric vehicle work at the World Resources Institute. The purchase might also broaden lawmakers' and policymakers’ ideas about what’s possible: If Amazon believes it can electrify a huge slice of its fleet quickly, why not push others to do the same?

More Great WIRED Stories",,"They're turning us into robots 

👉 Read More",https://www.wired.com/story/amazon-all-electric-future-fleet-vehicles/,2019-09-21 11:00:00+00:00,LABEL_1_relevant,0.9708672761917114,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
5,The pandemic is emptying call centers. AI chatbots are swooping in,"IBM’s and Google’s platforms work in similar ways. They make it easy for clients to spin up chat or voice-based agents that act a lot like Alexa or Siri but are tailored to different applications. When users text or call in, they are free to speak in open-ended sentences. The system then uses natural-language processing to parse their “intent” and responds with the appropriate scripted answer or reroutes them to a human agent. For queries that can’t be answered automatically, the algorithms group similar ones together to show the most commonly missed intents. “The nice thing about the technology is that it somewhat learns what types of questions are being asked, so we can plug them in; we can program it after the fact,” says Pokorny.

The platforms are proving popular among a range of organizations, especially those with limited technical resources. Small and midsize government organizations, including the city of Austin, Texas, and the Czech Ministry of Health, have used Watson to build chatbots that provide information about covid testing, prevention, and treatment. The Oklahoma Employment Security Commission has used Google’s virtual agent to help field over 60,000 daily calls related to unemployment claims. Health providers like the University of Arkansas for Medical Sciences and the University of Pennsylvania’s medical school have worked with both platforms to develop patient triage tools that help them administer timely care.

The goal of the systems is to get as many queries as possible answered by the virtual agents before any calls are handed over to humans. This reduces the burden on call centers and the wait times for users. It also reduces the need for expensive human agents.

A week in, Pokorny was impressed. “I see us using this technology well beyond the crisis,” he says. Right now, his county is using the chatbot only to field covid-related questions, but he plans to deploy a voice-based version that becomes the first point of contact for anyone dialing in about any issue. Watson’s pricing after the free trial is over is also “inexpensive,” he says. He demurs about what this means for laid-off staff and whether they will regain their jobs after the pandemic. “We’re fighting this from two angles,” he says. “One is obviously the health crisis. But once that’s over, we’re going to have a tremendous economic crisis. I see some artificial intelligence maybe helping us.”

Many other organizations will be facing similar pressures to keep their operations lean, so it’s possible that call center jobs already lost may not come back. But perhaps the good news is such jobs won’t be eliminated entirely: algorithms are still far from understanding the full context and nuance of conversation, so rarer cases will have to be reserved for the more capable hands of humans. Some matters are also too sensitive to outsource to AI.

Around the same time Pokorny started using Watson, Children’s Healthcare of Atlanta in Georgia began working with IBM’s system to develop a chatbot for helping parents assess their children’s symptoms. The health center turned to automation after people began inundating their phone lines, cranking up wait times to 30 minutes or even an hour. Like Pokorny, CHOA was impressed by how fast it the bot was to deploy and how easy it was to customize so that it replicated the assessment flow that would’ve been performed by a pediatrician. In just its first week, the application logged 1,000 unique conversations a day.

But as much as CHOA sees the technology’s role expanding beyond covid symptom assessment, the center doesn’t see it fully replacing call lines or becoming the first point of contact. “It’s worked really nicely during the pandemic time when people really just want some information,” says Daniel Hirsh, a physician there. “But I work in pediatric emergency medicine in the ER. When parents are nervous about their kids, people want to talk and have a conversation with a human.”",Yes,It has led to a decrease in the number of jobs for call center workers.,https://www.technologyreview.com/2020/05/14/1001716/ai-chatbots-take-call-center-jobs-during-coronavirus-pandemic/,2020-05-14 00:00:00,LABEL_1_relevant,0.99621319770813,Voice Assistants,voice_assistant,MIT Tech Review,Economy
6,"Soon, you'll be able to lock your BMW's doors with your Amazon Echo","Soon, you’ll be able to lock your BMW’s doors with your Amazon Echo

BMW is working on an Alexa skill for its new BMW Connected app, which will arrive later this year and let car owners do things like check their remaining fuel levels and lock their car doors via voice commands to their Echo speaker.

The new BMW Connected app arrived in the North American App Store in April, and will roll out in the UK. It’s able to connect to any car with “ConnectedDrive Services,” a feature introduced as a standard option by the car maker on all vehicles made from January 2016. BMW says that means there are about 400,000 vehicles on the road that can make use of the features in the UK alone. The updated app is an overhaul of a previous version that’s designed to be a more full-featured driving assistant, with trip management features including notifications indicating when you should leave to make it to your destination on time.

The revamped app sounds interesting, but the bigger story here is one that BMW buried near the bottom of the press release:

Later this year users will also be able to access BMW Connected via an Alexa skill for Amazon Echo, enabling them to get vehicle status like remaining range and execute remote commands like ‘door lock’, all through voice interaction.

Amazon’s Echo-based assistant adds new integrations every day, but the BMW app connection will let it extend further beyond the home. Ford has an early lead with Echo integration, letting people start their car and check range via Alexa.

Other more DIY solutions to add car control to Alexa’s list of skills exist, but BMW’s move here gives hope to drivers everywhere looking for closer connections between their vehicles and their smart home setups.

It’s still not Alexa in the car, which is something I’ve said previously I think would be a good fit. But it’s at least a relationship, and an acknowledgement that voice-based interaction is a useful model for controlling basic car functions.

Via Endgaget",Yes,"They are extending the mobile revolution beyond the phone. The mobile revolution was about computing power moving from the desktop to the pocket. The voice assistant revolution is about computing power moving from the pocket to the ear. The inevitable consequence is that our attention will move away from our screens and onto the world around us.

This is disconcerting for",https://techcrunch.com/2016/08/10/soon-youll-be-able-to-lock-your-bmws-doors-with-your-amazon-echo/,2016-08-10 00:00:00,LABEL_1_relevant,0.9929018616676332,Voice Assistants,voice_assistant,TechCrunch,User Experience & Entertainment
7,"AI voice assistants reinforce harmful gender stereotypes, new UN ...","Artificial intelligence-powered voice assistants, many of which default to female-sounding voices, are reinforcing harmful gender stereotypes, according to a new study published by the United Nations.

Titled “I’d blush if I could,” after a response Siri utters when receiving certain sexually explicit commands, the paper explores the effects of bias in AI research and product development and the potential long-term negative implications of conditioning society, particularly children, to treat these digital voice assistants as unquestioning helpers who exist only to serve owners unconditionally. It was authored by the United Nations Educational, Scientific, and Cultural Organization, otherwise known as UNESCO.

The paper argues that by naming voice assistants with traditionally female names, like Alexa and Siri, and rendering the voices as female-sounding by default, tech companies have already preconditioned users to fall back upon antiquated and harmful perceptions of women. Going further, the paper argues that tech companies have failed to build in proper safeguards against hostile, abusive, and gendered language. Instead, most assistants, as Siri does, tend to deflect aggression or chime in with a sly joke. For instance, ask Siri to make you a sandwich, and the voice assistant will respond with, “I can’t. I don’t have any condiments.”

Tech companies are perpetuating harmful stereotypes about women through AI

“Companies like Apple and Amazon, staffed by overwhelmingly male engineering teams, have built AI systems that cause their feminized digital assistants to greet verbal abuse with catch-me-if-you-can flirtation,” the report states. “Because the speech of most voice assistants is female, it sends a signal that women are ... docile and eager-to-please helpers, available at the touch of a button or with a blunt voice command like ‘hey’ or ‘OK’. The assistant holds no power of agency beyond what the commander asks of it. It honours commands and responds to queries regardless of their tone or hostility.”

Much has been written about the pitfalls of tech companies having built their entire consumer-facing AI platforms in the image of traditional, Hollywood-influenced ideas of subservient intelligences. In the future, it’s likely voice assistants will be the primary mode of interaction with hardware and software with the rise of so-called ambient computing, when all manner of internet-connected gadgets exist all around us at all times. (Think Spike Jonze’s Her, which seems like the most accurate depiction of the near-future in film you can find today.) How we interact with the increasingly sophisticated intelligences powering these platforms could have profound cultural and sociological effects on how we interact with other human beings, with service workers, and with humanoid robots that take on more substantial roles in daily life and the labor force.

However, as Business Insider reported last September, Amazon chose a female-sounding voice because market research indicated it would be received as more “sympathetic” and therefore more helpful. Microsoft, on the other hand, named its assistant Cortana to bank on the existing recognition of the very much female-identifying AI character in its Halo video game franchise; you can’t change Cortana’s voice to a male one, and the company hasn’t said when it plans to let users do so. Siri, for what it’s worth, is a Scandinavian name traditionally for females that means “beautiful victory” in Old Norse. In other words, these decisions about gender with regard to AI assistants were made on purpose, and after what sounds like extensive feedback.

The AI research field is predominantly white and male

Tech companies have made an effort to move away from these early design decisions steeped in stereotypes. Google now refers to its various Assistant voice options, which now include different accents with male and female options for each, represented by colors. You can no longer select a “male” or “female” version; each color is randomly assigned to one of eight voice options for each user. The company also rolled out an initiative called Pretty Please that rewards young children when they use phrases like “please” and “thank you” while interacting with Google Assistant. Amazon released something similar last year to encourage polite behavior when talking to Alexa.

Yet as the report says, these features and gender voice options don’t go far enough; the problem may be baked into the AI and tech industries themselves. The field of AI research is predominantly white and male, a new report from last month found. Eighty percent of AI academics are men, and just 15 percent of AI researchers at Facebook and just 10 percent at Google are women.

UNESCO says solutions to this issue would be to create as close to gender-neutral assistant voices as possible and to create systems to discourage gender-based insults. Additionally, the report says tech companies should stray away from conditioning users to treat AI as they would a lesser, subservient human being, and that the only way to avoid perpetuating harmful stereotypes like these is to remake voice assistants as purposefully non-human entities.",Yes,"They reinforce harmful gender stereotypes. The report argues that by naming voice assistants with traditionally female names, like Alexa and Siri, and rendering the voices as female-sounding by default, tech companies have already preconditioned users to fall back upon antiquated and harmful perceptions of women.",https://www.theverge.com/2019/5/21/18634322/amazon-alexa-apple-siri-female-voice-assistants-harmful-gender-stereotypes-new-study,2019-05-21 00:00:00,LABEL_1_relevant,0.9960324168205258,Voice Assistants,voice_assistant,The Verge,Equality & Justice
8,"Commentary: Driver Distraction, the NTSB and a Siri-ous Problem","Arguing against anything intended to make drivers drive to the exclusion of all else is like being against apple pie. You may have your reasons, but nobody wants to hear them.

That said, the National Transportation Safety Board’s non-binding recommendation that (nearly) all use of mobile communications devices behind the wheel smacks both of a genuine frustration with the inability of most of us to show sufficient self control, and a bit of overkill brinksmanship intended to instantly elevate the discussion into the public zeitgeist. Nothing gets attention like saying “Never!”

The NTSB directive would exempt “devices designed to support the driving task,” such as GPS units that connect to your phone, voice-control mechanisms that are built in to vehicles, store-bought GPS units from the likes of Garmin, Magellan and Tom Tom, or even the mapping and navigation functionality built into some smartphones. One of the most unsung features of portable GPS devices is that they do support Bluetooth and communicate with any Bluetooth-enabled phone, which is, at the high end, all of them.

Which brings us to Siri, the Apple “personal assistant” built into the iPhone 4S.

Continue reading ‘Commentary: Driver Distraction, the NTSB and a Siri-ous Problem’ …

It might not have been the NTSB’s intention to give automakers a guaranteed upsell for factory-installed dash units “designed to support the driving task,” or to encourage smartphone technology like Siri, or to kill the hands-free peripheral business, or to give cellphone makers and wireless companies agita as they figure the angles. But in the highly unlikely event that all 50 states actually do heed the NTSB’s call, Siri-like tech is exactly what would provide the safety buffer the agency is trying to build, allowing consumers to bring their own solution to any car rather than buy more expensive cars (or abstain altogether). It’s another potentially huge endorsement for the smartphone as an empowering human peripheral which not only connects us to information and each other in remarkable ways, but is increasingly becoming a truly universal remote, wallet and personal monitor.

With most in-dash GPS/entertainment consoles, voice commands begin the process, which is about as distracting as asking a passenger where she wants to eat. With Siri, one must still push the home button, and that is a bit more friction (though not much more, if placed a certain way in a certain pocket, in my own unscientific testing). On a phone the “prompt” problem is a software design issue, but it is more difficult to solve because phones are used in multiple contexts with a variety of social and ambient noise considerations. The driver’s seat is exactly one, predictable environment.

Whatever the politics and science of the NTSB’s timing and reasoning, it’s not about denying the driving public the visceral joy of texting behind the wheel. It’s about minimizing distractions so people don’t get killed because of a trivial pursuit.

But there’s another problem as you go down this philosophical road. Teen drivers are notoriously bad driving risks for a number of reasons, including lack of experience. It is also true that they travel in packs: Put three to four teen passengers in an automobile driven by a fellow teen and the kid behind the wheel is going to be much more prone to causing or falling prey to an accident because of the all the socialization going on.",Yes,"They could make it easier for teens to text while driving by allowing them to keep both hands on the wheel. The NTSB’s recommendation, if adopted, would make it illegal for teens to use voice assistants while driving.

The NTSB’s recommendation is non-binding, but it could put pressure on states to pass",https://www.wired.com/2011/12/driver-distraction-ntsb-siri/,2011-12-14 18:13:04+00:00,LABEL_1_relevant,0.9968758821487428,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
9,Should Google Try to Prevent Terrorism?,"

Jihadists have flocked to YouTube to spread their propaganda. One of those clips, released last week, appeared to take credit for the Times Square bombing attempt -- before Faisal Shahzad tried to ignite his SUV. The video may have been a vital clue for investigators. But does YouTube and its corporate parent, Google, have an obligation to block these videos before they're seen?

That's what one long-time monitor of online jihadists is arguing. ""If a certain percentage of Islamist sympathizers are radicalized, in part, online, then it stands to reason that more eyeballs that are exposed to violent Islamist propaganda would eventually translate into more would-be terrorists,"" writes ""Rusty Shackleford,"" the pseudonymous patron of The Jawa Report. ""Which is why even though YouTube has been a boon in helping law enforcement agents detect, post hoc, would-be terrorists it has been a bane in that far more Muslims today can easily access violent Islamist propaganda.""

Ryan Calo, a fellow at Stanford Law School's Center for Internet and Society, isn't sure he buys the argument.

""Are we trusting Google to be experts enough in jihadi propaganda to know what to take down?"" Calo asks. Do we expect a communications company to be able to weigh a terrorist video's potential value as a clue for law enforcement and intelligence agencies? ""That seems like a really extraordinary difficult calculus to make. and in the absence of a definitive answer, let's err on the side of free speech. On the side of neutrality of communications.""

But Shackleford says all he wants is for YouTube to obey its own rules, which give it the latitude to yank any vid for ""obscene or defamatory material.""

""One need not call for stricter internet regulations in order to stop this growing problem. If YouTube would simply police itself and enforce its existing Terms of Service, the vast majority of these videos would be removed,"" he writes. ""If Google's YouTube service cared half as much about terrorism as it did about pirated music and TV shows, the problem would be halved overnight.""

""We will never fully get rid of violent Islamist propaganda on the internet. However, we should be using every means at our disposal to make such material harder to find,"" he adds. ""Doing this will whittle down the absolutely overwhelming numbers of jihad sympathizers that law enforcement should be watching, but can't; and will ensure that those who continue to disseminate and consume this material have a higher investment in and commitment to violent extremism.""

[Photo: Joe Raedie/Getty Images]

See Also:",Yes,"They will make us less likely to remember information. We will grow increasingly reliant on these devices to remind us of both simple tasks and more complicated bits of knowledge that we would have otherwise retained in our own memories. This won't happen overnight, but it's already happening. It's a slow and insidious form of ""cognitive offloading,"" and",https://www.wired.com/2010/05/should-google-prevent-try-to-terrorism/,2010-05-06 12:11:34+00:00,LABEL_1_relevant,0.9963029623031616,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
10,People want to hear a familiar voice assistant in their car - The Verge,"Car companies are hoping to capitalize on the growing popularity of voice recognition systems like Amazon Alexa and Google Assistant by developing their own in-car voice assistants that drivers can use. But that effort may be a huge waste of money: car owners increasingly prefer familiar voice assistants, like the ones they use in their homes, over those designed specifically by automakers.

According to a new survey by JD Power, 76 percent of car owners are interested in having the same brand of in-home voice service on their next vehicle. The reasons for that are pretty obvious: they don’t want to learn a new technology and would rather just carry over their preferences and the consistency of experience from their home to their car. The report, which was sponsored by Amazon, surveyed 5,000 people who own a voice-enabled device.

That’s got to be frustrating for carmakers

This news has to be frustrating for carmakers, especially the ones that have been pouring millions of dollars into developing their own in-car voice assistants to compete with Amazon, Google, and Apple. In recent months, major automakers like Mercedes-Benz and BMW have introduced highly functional voice assistants with deep control over the vehicle’s functions.

For years, automakers feared handing over control of the dashboard screen to major tech companies. That was evident in the foot-dragging that was on display when it came to giving permission to Apple and Google to operate their respective in-car infotainment systems alongside the cars’ own native vehicle operating systems. But that battle is largely over; even major holdouts like Toyota have acquiesced.

But the auto industry isn’t going down without a fight. Major carmakers like Ford and Daimler are confident that they have the money and technical know-how to compete. They are investing millions of dollars in building their own digital experiences, including smartphone apps and voice recognition systems, so they can wield control over the process and ultimately reap all of the profit. These data-driven products could create as much as $750 billion in new revenue by 2030, including from location-based advertisements and predictive car maintenance, McKinsey & Co. has estimated.

Of course, consumers will have the ultimate say, and many of them have already made up their minds. Almost 60 percent said that “availability of the same in-home voice service on their next car would increase the likelihood to buy from a specific car company,” according to the JD Power survey.

Almost 60 percent of people said they’d be more likely to buy a new car if it had their preferred voice assistant

Arianne Walker, chief evangelist at Alexa Automotive, said that one of the things that stuck out to her from the survey was “frustrations” that people have with in-car voice assistants. “It does require the customer to know exactly what terms to say, and in the right order, and the right command structure to actually get it to work,” she said. This is why most people prefer a familiar voice assistant.

Consumers prefer a more “natural and organic way of interacting” with voice assistants, using natural language understanding and automatic speech recognition, Walker said. They want “to be able to bring that much preferred in-home experience into their vehicles,” she added. “We heard that loud and clear from consumers and that’s really what we’re focused on doing for the industry.”

Amazon is hoping to use this new data to convince more automakers to let it integrate Alexa into their vehicles. Only a few — Toyota, BMW, Ford, and, most recently, Audi — have announced direct Alexa integration with their infotainment systems, and not all of them struck deals for every model.

Not every automaker is going to acquiesce to Amazon’s integration demands. For drivers of those vehicles, Amazon offers an aftermarket device called the Echo Auto. The credit card-sized gadget sits on your dashboard and brings Alexa capabilities to your car via an auxiliary jack or over a smartphone’s existing Bluetooth setup. Amazon recently said it had received over a million preorders for the device.

All of this may soon be moot, though

“Our North Star is really about an embedded experience into the head unit of the vehicle,” Walker said, “making that experience really sort of easy and seamless for consumers.”

All of this may soon be moot, though. More and more vehicles are shipping with name-brand voice assistants, and analysts predict that more are on the way. According to ABI Research, a market-foresight advisory firm, 20 million vehicles shipping in 2023 are expected to enable one or more of these assistants. That could represent a major coup by Silicon Valley, said Shiv Patel, a smart mobility and automotive analyst at ABI Research.

“Overall, these partnerships represent a significant step forward for Amazon and Google as they could pave the way for them to eventually displace OEM-branded assistants in-vehicle altogether and be the sole voice assistant in the vehicle, extending their brand and providing access to new in-vehicle data,” Patel said.",Yes,They may eventually displace OEM-branded assistants in-vehicle altogether and be the sole voice assistant in the vehicle.,https://www.theverge.com/2019/4/9/18300635/amazon-alexa-echo-google-assistant-car-jd-power-survey,2019-04-09 00:00:00,LABEL_1_relevant,0.9949710369110109,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
11,This Chip Is Going to Make You Want to Talk to Your Phone,"The best part of CES each year isn't seeing the latest and greatest gadgets -- it's seeing the burgeoning developments that are going to change how you use those devices in the future. Audience's eS700 series of voice-processing chips is one of those technologies.

A voice processing chip? Yes, a voice processing chip. Think about Siri. It's useful to be able to ask her questions or have her check your calendar appointments when your hands are full. But in a public environment, talking loud enough for her to understand usually means you're disturbing other folks around you or at least calling attention to yourself. And if there's tons of noise around you, like on a busy street with construction, voice recognition and control can be rendered completely useless.

""Our vision is that there is coming a time when sensory computing takes on greater and greater prominence in consumer devices,"" Audience CEO Peter Santos told WIRED. ""Advanced voice and auditory input computing -- we think we're at the very beginning of that process.""

Audience's eS700 chips are a key step toward that goal. They offer improved voice recognition in loud environments -- useful for both making phone calls and for performing tasks on your phone using your voice. The chips are also able to passively listen for keywords, so even if the device is asleep, a simple command like ""OK phone, play Talking Heads"" will be executed. This passive listening is ultra-low power, and for voice commands, you don't need to pause between the initial wake-up command and whatever other commands follow. A new API will let third-party apps take advantage of these features as well, instead of just services built into the OS.

A demo of the chip's capabilities with Google Now were impressive. In a room with a significant amount of background noise (think a conference room where everyone is talking at normal volumes at the same time), a phone with Audience's current model chip was unable to pick out the user's voice from the din -- or if it did, the words were mangled. The new eS700 chip was not only able to identify the user's voice when spoken at equal volume, it was able to do so accurately. The next step: Being able to do this with sub-ambient voice levels, that is with you talking at a volume that's measurably quieter than the noise around you. That's when Audience really thinks speech recognition is going to take off.

The company's past chips are already in a number of smartphones and tablets, including older versions of iPhones, Samsung Galaxy devices, and the Nexus 10. Devices using these latest eS700 chips will start appearing in the latter half of this year.",Yes. The article discusses how voice assistants can be disruptive in public places.,"They can be difficult to use in public places. Audience's new eS700 series of voice-processing chips is designed to address this issue by improving voice recognition in loud environments. The chips are also able to passively listen for keywords, so even if the device is asleep, a simple command like ""OK phone, play Talking Heads"" will",https://www.wired.com/2014/01/audience-es700-chips/,2014-01-06 16:00:52+00:00,LABEL_1_relevant,0.9593702554702759,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
12,"Robot, Get the Fork Out of My Sink","The project also points to ways that robots can learn how to take on new jobs in industry and around the home. There is a huge opportunity for robots to help with elder care, for example, if they can be programmed to perform reliably in messy and ever-changing domestic situations.

Tellex’s team used an off-the-shelf industrial robot from Rethink Robotics, which has a camera in its arm, to perform the fork-grasping trick. By moving the camera and combining different images, they were able to build a virtual light-field camera, meaning it captures not just the intensity of light but also the direction of individual rays. This made it possible to build a 3-D model of the scene, and to cope with problems like reflectivity.

In separate experiments, the group is also using a machine-learning approach known as reinforcement learning to train robots to pick up unfamiliar objects. This involves letting a robot that is controlled by a large neural network experiment with different grasps and reinforcing behavior that seems to produce positive results. The results can be impressive, making it possible for a machine to devise a strategy for grasping previously unseen objects that would be extremely difficult to program manually. Tellex is also exploring ways for robots to share what they have learned, something that promises to accelerate the training process dramatically (see “10 Breakthrough Technologies: Robots That Teach Each Other”).

“Our approach is to make the robot learn to adapt itself to the environment it finds itself in,” says Tellex. “Through that learning it can reach a reliability that wasn’t possible previously. It can also use this information to generalize to other situations.”

New robot learning approaches are rapidly moving into industrial settings. Existing robotics companies are developing products that will use reinforcement learning to accelerate robot programming (see “A Japanese Robot Giant Gives Its Arms Some Brains”). Companies specializing in AI and machine learning also see the technology as a way to break into an industry that looks set to evolve rapidly (see “Google Builds a Robotic Hive-Mind Kindergarten”).

“We want robots to be able go into factories, household environments, and manipulate,” Tellex says, “and everything starts with picking something up.”",Yes,"They've become platforms for marketing and advertising. Advertisers are able to use voice assistants to target ads to users based on their demographics, interests, and even their location. This has led to some users feeling bombarded with ads, and has even led to some privacy concerns.",https://www.technologyreview.com/2016/10/18/69877/robot-get-the-fork-out-of-my-sink/,2016-10-18 00:00:00,LABEL_1_relevant,0.9944135546684264,Voice Assistants,voice_assistant,MIT Tech Review,Security & Privacy
13,College Kids Hack Siri to Unlock Teslas and Heat Up Your House,"When Apple first introduced Siri three years ago, she amazed the world with her warm voice and her human-like voice response. But the years since her debut have been quietly underwhelming, in large part because Siri isn't able to access the wide range of apps that are available on the iPhone.

So a group of University of Pennsylvania developers have hacked Siri with the idea of going it alone and building their own Siri App Store. They call their hack GoogolPlex and after introducing their software a few months ago, they've rejiggered it and released it to the public as a platform where anyone can create custom commands.

""We looked at Siri and we realized that they had very good voice translation, but the problem was that Apple hadn't made the platform open,"" says Ben Hsu, one of the developers who worked on the project. ""So what you could do was really very limited.""

On the GoogolPlex, they show how the software can be used to give you voice control over apps that don't currently work with Siri including the Nest thermostat, Philips Hue lightbulbs and Spotify.

Watch a demo here:

Content This content can also be viewed on the site it originates from.

Googolplex isn't for everybody. It's more of a cool tool for the tech savvy and a stark illustration of how badly Apple needs to open up Siri to third party developers. The reason we don't recommend it for most regular users is because there's a pretty big security downside to using the software. For GoogolPlex to work, you have to trick your phone into believing that a server run by the Penn developers is actually Google.com. That's a security no-no.

Once everything's been set up, you tell Siri to GoogolPlex things. Most of your Google searches go right to Google, but if the Penn server hears ""Google Plex,"" then it processes those requests using a script that interfaces with Nest or Spotify or whatever service you want to use. On this site, you can see commands that flip coins, present random comics, and query Google Maps for directions.

Enjoy it now, before Apple's lawyers ruin the fun.",Yes,"They are slowly turning us into a society of lazy, entitled people who would rather summon a robot to do something for us than do it ourselves.",https://www.wired.com/2014/04/googolplex/,2014-04-17 20:19:07+00:00,LABEL_1_relevant,0.995705544948578,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
14,How Covid Gums Up the Court System,"Last August, one of the world’s most infamous sex criminals—Harvey Weinstein—was due to appear virtually in a New York courtroom on a request to extradite him to California to face charges there. Reporters, due to Covid-19, mostly attended remotely. They were promised a videofeed. They didn’t even get audio. This left much of the press corps—who serve as the public’s eyes and ears—in the dark.

The proceeding ended before the link could be fixed. A transcript and pooled video were provided later, but the glitch robbed much of the public and press of seeing and hearing what happened in the courtroom.

Court systems nationwide have long struggled to uphold the principles of the First and Sixth Amendments, which establish the rights to public access, and fair and open trials, respectively. The pandemic has made things worse. As many courthouses closed or dramatically limited in-person proceedings, officials deployed video and telephone conferencing. Many businesses, and some schools, found ways to operate using platforms such as Zoom, but the experience of the nation’s courts over the past year is scattershot: Some have functioned well with remote participants, while others have struggled with the technology.

Even conference calls—used for some proceedings pre-Covid—have proved unpredictable and buggy. During a recent US District Court hearing about documents related to Jeffrey Epstein associate Ghislaine Maxwell, so many apparent QAnon followers dialed in that the public line was overwhelmed. Dozens of people, including many reporters, weren’t able to listen.

Even when remote courtrooms work well, advocates say they make it difficult to litigate cases and present obstacles for people accused of crimes to mount a defense.

“My client has a right to confront and hold the government accountable,” said Tina Luongo, attorney-in-charge of the criminal defense practice at the Legal Aid Society in New York City. “They have a right, under the Constitution, to confront the witnesses and be present to hear what those accusations are.”

In a report last year, the Brennan Center at NYU said remote proceedings “may unnecessarily put people’s rights at risk.”

Virtual proceedings complicate—and in some cases, prevent—routine communication between lawyers and their clients. Attorneys often talk to clients in breakout rooms—separate sessions in a broader videoconference—before proceedings, and then join the main room for on-the-record business, Luongo said.

“In order to [confer with a client], I have to say to the court: I’m sorry, your honor, can you put us back in a breakout room?” Tina Luongo, attorney-in-charge, criminal defense, the Legal Aid Society

The set-up creates a problem if an attorney wants to consult with a client during a hearing. “I can’t do that virtually. In order to do that, I have to say to the court: I’m sorry, your honor, can you put us back in a breakout room?” said Luongo, who now supervises courtroom attorneys. “Sometimes, judges don’t do it.”

Mitha Nandagopalan, an attorney with the New Mexico Law Offices of the Public Defender, has participated in video trials during the pandemic for misdemeanors before a judge, without a jury. Being separated from a client impacts the quality of representation, Nandagopalan says.

“Having my client not in the room with me made it harder,” Nandagopalan said. “At least if we’re in the same room, my client can pass me notes if they catch something that a witness was saying.”

Sometimes attorneys with the New Mexico public defenders will bring clients into the office, so they can physically be together when appearing in a virtual proceeding. This potentially exposes both people to Covid-19. But clients benefit.

In one situation, Nandagopalan said a client noticed that a witness’s testimony didn’t match their recollection of events. The client provided Nandagopalan with questions for cross-examination, which in turn spurred useful testimony for the defense.",Yes,They are often inaccurate and sometimes difficult to understand. This can be frustrating for users and can lead to misunderstandings.,https://www.wired.com/story/how-covid-gums-up-court-system/,2021-02-15 12:00:00+00:00,LABEL_1_relevant,0.984253466129303,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
15,Ex Machina Has a Serious Fembot Problem,"The Turing test detects if a machine can truly think like a human. The Bechdel Test detects gender bias in fiction. If you were to mash the two together to create a particularly messy Venn diagram, the overlap shall henceforth be known as the Ex Machina Zone.

In writer/director Alex Garland's thought-provoking new film—out Friday—we meet Ava (Alicia Vikander), an artificially-intelligent robot. Ava's creator, genius tech billionaire Nathan (Oscar Isaac), has asked his employee Caleb (Domhnall Gleeson) to determine whether Ava's thinking is indistinguishable from a human's. Until she meets Caleb, Ava has only ever met her maker and one other woman. (Hence the failing of the Bechdel Test, which stipulates that a movie must feature two female characters who talk to each other about something other than a man.) Her existence, and her ability to learn how to interact, is a fascinating study of what makes us human.

It's also a compelling, if problematic, look at the interactions between men and women---or at least that's what I thought.

While interviewing Garland for a magazine piece, I asked him about the roles of men and women in his film; his response was that Ava is ""not a woman, she is literally genderless."" Despite using female pronouns, he said, ""the things that would define gender in a man and a woman, she lacks them, except in external terms. ... I'm not even sure consciousness itself has a gender.""

In a way, Garland is right; pure intelligence wouldn't have a gender any more than it would have a race. But to say that and then place that consciousness into a body that it will immediately recognize its likeness as female negates that point. If Ava has truly been educated about the human race, then she knows her face and form appeal to certain segments of the population. But even thornier is the fact that Ava falls squarely into so many of the tropes of women in film. She's a femme fatale, a seductress posing as a damsel in distress, using her wiles to get Caleb to save her from Nathan and his Dr.-Frankenstein-with-tech-money quest to build a perfect woman. (Women: So much better when you can construct them out of bespoke parts and switch them off if they're not working properly, amirite?)

Chappie Didn't Have to Put Up With This Crap

According to Garland, these tropes are intentionally front-and-center. He believes his movie is a commentary on the ""constructs we've made around girls in their early 20s and the way we condition them culturally"" and why Caleb would feel the need to save her from her maker. ""You're supposed to think it's creepy,"" he says. ""You're not supposed to warm to [Nathan] over that stuff, you're supposed to feel unnerved, and therefore that she needs to be rescued.""

Yet, in the pursuit of that commentary, the movie ends up re-enacting those same patterns. Ava does prove to be the smartest creature on the screen, but the message we're left with at the end of Ex Machina is still that the best way for a miraculously intelligent creature to get what she wants is to flirt manipulatively. (And why wouldn't she? All of her information about human interaction comes from her creepy creator and the Internet.) Why doesn't Chappie have to put up with this bullshit?

Content This content can also be viewed on the site it originates from.

Ava's predicament really isn't that different from many female AIs who have come before her, from Metropolis' Maria to *Her'*s Samantha to *Blade Runner'*s Pris. She is an android in female form, and thus she simply reflects how Hollywood has been depicting women—robotic or otherwise—for decades. In Blade Runner, the male replicants Roy Batty and Leon are struggling to change their short lifespans, while ""basic pleasure model"" Pris helps the cause by draping herself on J.F. Sebastian. In Prometheus, David is intellectually curious, but never sexualized. (Yet when Idris Elba's Janek accuses Charlize Theron's Meredith Vickers of being a robot, she responds with ""My room. Ten minutes."" Because sex is the easiest way to prove you're a real woman.) Sentient male androids want to conquer or explore or seek intellectual enlightenment; female droids may have the same goals, but they always do it with a little bit of sex appeal, or at least in a sexy package. (Still have doubts? Ex Machina’s marketing campaign at South by Southwest involved Ava showing up on Tinder.)

This tendency to give female AIs the most basic and stereotypical feminine characteristics is, according to Kathleen Richardson, a senior research fellow in the ethics of robotics at De Montfort University in the UK, probably a reflection of ""what some men think about women—that they're not fully human beings."" To put a finer point on it, she told Live Science recently, ""what's necessary about them can be replicated, but when it comes to more sophisticated robots, they have to be male.""

When I spoke to Richardson, author of An Anthropology of Robots and AI: Annihilation Anxiety and Machines, she also noted this leads to female robot characters becoming just pieces of full people—a beautiful body, a caretaking nature—but not ones with full intelligence. This is largely true in Ex Machina---and not just because Nathan has a lab full of body parts---but also in a lot of movies where the artificial intelligence has to be packaged in a certain way if the robot is perceived to be female. (She also notes the real robotics world suffers from the same problems as a lot of AI fiction, but that ""many robotic scientists are open to a conversation about this."")

Women, whatever their qualities—intelligent, vulnerable, strong—are always presented in an attractive form, as if the package is the only way to deliver these qualities. Kathleen Richardson

""Sometimes the female robots have 'violent' characteristics (as Terminator 3’s T-X character), but it's always presented in a beautiful form,"" Richardson says. ""Women, whatever their qualities—intelligent, vulnerable, strong—are always presented in an attractive form, as if the package is the only way to deliver these qualities. Male intelligence, strength, vulnerabilities, etc. can be delivered in a multiple and varied kind of outer packaging.""

Think of it this way: Ava demonstrates her consciousness/intelligence in a form and with a sensuality that David in Prometheus never had to. *Short Circuit'*s Number 5/Johnny Five was cute, but he never had to employ it for survival the way Pris did in Blade Runner. Even AIs with no physical form at all seem to get sexualized based simply on their voices. It's not like HAL 9000 ever sparked up a relationship with Dave in 2001: A Space Odyssey the way Samantha did in Her. ""Her is playing on the fact that the audience knows what [Scarlett Johansson] looks like,"" Richardson says. ""No one really needs to know who the voice of HAL was, because HAL was an intelligent machine. We need to know about the disembodied voices of our AI avatars if they're female so that males can buy into the ideas of the sexualized person behind the representation.""",Yes,"They turn the women who use them into sex objects.

In a recent article in the Atlantic, senior editor Ian Bogost noted that when we have to have a female voice for our AIs, we wind up fetishizing them. ""they turn the women who use them into sex objects,""",https://www.wired.com/2015/04/ex-machina-turing-bechdel-test/,2015-04-09 11:00:34+00:00,LABEL_1_relevant,0.9965533018112184,Voice Assistants,voice_assistant,Wired,Social Norms & Relationships
16,"Google Acquires Emu, An IM Client With Siri-Like Intelligence","Google has acquired Emu, a new mobile messaging application that just exited beta earlier this year. Emu was at heart an IM client, but it differentiated itself from the crowded market with smart features that incorporated a virtual assistant not unlike Siri to automate tasks based on your conversations – meaning you could do things like schedule appointments to your calendar, set reminders and even make reservations at a restaurant directly from your conversations.

Emu’s roots were deep; its co-founder and CEO Gummi Hafsteinsson is ex-Google and ex-Apple, where he worked on Siri after coming on board following that startup’s acquisition. Emu works a lot like Google Now, too, scouring your conversations for contextual information about your life and plans in order to provide recommendations in-line about booking appointments, restaurant ratings and reviews and even things like movie suggestions.

In fact, Emu, which debuted as an Android application, looks like it can offer a lot to Google if the company is intent on continuing to grow the influence of Google Now across its properties, and extend its capabilities beyond the contextual launcher where it currently resides. Messaging is a new mobile battleground for big tech companies, and a potential motivating factor in attracting and keeping users within a particular ecosystem – Apple is focusing heavily on iMessage with its iOS 8 update, and in OS X Yosemite, adding multimedia messaging features, refining group chat, and making it easier to get your messages on all devices, regardless of what kind of device the person sending them is using.

It’s easy to see Google potentially using Emu’s tech in Hangouts, which itself is evolving into a central, multi-featured hub for communications on Android, and leaving behind its origins first as simply a video calling service and later as a text communication tool. Messaging could become the home base for mobile in the same way that email is in many ways the HQ for desktop computing, at least in terms of P2P communications (the browser rules for content consumption) and Emu’s tech could be instrumental in truly evolving it past what it is now.

Emu says it will shut down the standalone Emu app as of August 25 as a result of the acquisition, and it will cease to work for existing users. Emu had raised $1.5 million from KPCB, Kleiner Perkins, Draper Fisher Jurvetson, Menlo Ventures and others. No financial details regarding the acquisition were disclosed.",Yes,"They may lead people to carry out less mundane tasks, but also to interact less with each other.",https://techcrunch.com/2014/08/06/google-acquires-emu-an-im-client-with-siri-like-intelligence/,2014-08-06 00:00:00,LABEL_1_relevant,0.9915871620178224,Voice Assistants,voice_assistant,TechCrunch,Social Norms & Relationships
17,Ill-Advised Hack Brings Siri to Jailbroken iPhone 4,"A new hack delivers the Siri digital assistant to jailbroken iPhone 4 units, but we advise would-be tinkerers to think long and hard before acting on the invasive software surgery. You might think you know your way around an iPhone hack, but this one has trouble written all over it.

This downloadable version of Siri, called H1Siri, is available in the Cydia app store. The language accompanying the 100MB file claims to provide full Siri functionality on GSM or CDMA iPhone 4 units running iOS 5 or 5.0.1. However, this particular exploit seems like bad news for a number of reasons.

First, it's illegal. The guys at iDownloadBlog were able to confirm this because the hack uses copyrighted binaries from the iPhone 4S.

Second, your Siri requests won't go through Apple's servers with this version of Siri -- instead, they'll go through the servers of the Chinese hacking team that developed the exploit. What's more, CD-Dev Team, the group behind the hack, has confirmed that their servers are currently down, so even if you did download H1Siri, it may not work.

iDownloadBlog was not able to get the hack to work, but did hear from others who had gotten it to work ""in short bursts.""

Doesn't really seem worth the effort, if you ask Gadget Lab.

Hackers the world over are trying to bring Siri to iOS devices other than the iPhone 4S. They've had varied success, primarily stemming from the fact that Siri interacts with Apple's own servers, and each bit of data that's sent to those servers bears markers that identify the request as coming from an iPhone 4S. If your request doesn't include that unique 4S identifier, you're out of luck.

One of the most successful Siri hacking attempts thus far comes from a French team at Applidium, but the blog post detailing the hack seems to have been removed. Another hacker, Pete Lamonica, was able to use Applidium's instructions to hack Siri to control a thermostat, while another developer then used Lamonica's method to start and unlock his car. Both of the latter two cases still used an iPhone 4S, it must be noted, and simply hacked Siri's capabilities, and not her host hardware.

Lamonica tweeted this morning that he would be investigating H1Siri, but didn't recommend others interacting with a third-party-managed Siri server.

iPhone hacker Grant Paul also tweeted reservations about the hack. ""Please note: if you use a proxy to access Siri, you may be sending your Email, SMS, Calendar, Contacts, Location, etc though that server,"" Paul said.

Besides all that, jailbreaking itself isn't necessarily a good idea.

Apple's official statement from spokesperson Natalie Harrison on jailbreaking is thus:",Yes,"You'll be forced to listen to a lot more ads.

[Via iDownloadBlog]",https://www.wired.com/2011/12/siri-hack-bad-idea/,2011-12-05 22:05:25+00:00,LABEL_1_relevant,0.996649205684662,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
18,"Ray Kurzweil's Blio E-Book Launch Met With Confusion, Controversy","Updated at 5:30 p.m. Eastern to add comment from K-NFB.

This week, K-NFB, an e-reading company founded by Ray Kurzweil and the National Federation for the Blind, launched its much-anticipated Blio reading app and e-book store. Blio was immediately and widely panned by publishers, developers and readers.

""Many of the failures are fundamentally at odds with the one thing that Kurzweil was touting above all else: accessibility,"" wrote Laura Dawson, a digital reading industry consultant, formerly of BarnesAndNoble.com. K-NFB initially promised to make e-books more accessible to blind readers; yet Windows, currently its only enhanced books platform, has known text-to-speech conversion issues.

K-NFB and Kurzweil responded by saying the software had been released before a fully-accessible version was ready, and that they plan to release an improved version next week.

“People understandably have very high expectations,” Kurzweil said in an interview with Wired.com Thursday. “We believe Blio is very usable and has many features other book formats don’t. And there are many features that we want to add.”

In addition to bug fixes and other tweaks, an accessibility release scheduled for October 25 will work with Freedom Scientific’s JAWS, screen reader software for Windows specifically tailored for blind users. The 1.0 version released Tuesday relied on Windows’ built-in text-to-speech capabilities.

K-NFB spokesperson Peter Chapman acknowledged the problems with Blio's text-to-speech in an interview with Publishers Weekly, but blamed the platform, not the books: “the TTS software on most Windows machines isn’t very good.” Yet many Windows XP users were unable to even install Blio's software. (This has since been resolved.)

K-NFB also confirmed that an iOS 4 version for iPhone and iPod Touch is currently in private beta, and that an iPad version will shortly follow the iPhone, probably sometime after November’s release of iOS 4.2. Android and Mac OS X versions are also in development.

Users weren't the only ones frustrated with Blio. Hadrien Gardeur, founder and CEO of free e-books site FeedBooks, complained on Twitter that Blio was offering downloads from FeedBooks' catalog without permission: “Hey Blio, next time that you add our OPDS [Open Publishing Distribution System] catalog to a commercial product, send us an e-mail first.”

In a follow-up e-mail, Gardeur noted that FeedBooks only allows other systems to include their catalog under the following conditions:

full support for the EPUB standard (Blio converts EPUB into its own format and can't support EPUB with other companies' DRM)

support the entire OPDS catalog (Blio only includes some of FeedBooks' feeds)

Add other OPDS catalogs to its library (Blio can't do that)

allow payment for commercial content through open standards (Blio doesn't)

For these reasons, Gardeur asked Blio not to include FeedBooks' content in its initial launch; according to Gardeur, K-NFB went ahead and included part of Feedbooks' OPDS catalog anyways. Since FeedBooks has a planned system update forthcoming, it will most likely break Blio's access to the catalog.",Yes,"They are making us more impatient. Voice assistants are often used for tasks that are time-sensitive, such as setting alarms, sending text messages, and adding appointments to calendars. Because we are using them for these time-sensitive tasks, we are becoming more accustomed to getting an immediate response. This is causing us to become more impatient in general,",https://www.wired.com/2010/09/ray-kurzweils-blio-e-book-launch-widely-panned/,2010-09-30 14:37:05+00:00,LABEL_1_relevant,0.9945395588874816,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
19,Should you say 'please' and 'thank you' to your Amazon Echo or ...,"When you’re asking a digital assistant to do something for you, do you say “please?” How about “thank you?” It’s a question that’s been on my mind for a while, ever since I set up some smart lights in my apartment and started using Siri to turn them on and off. Demanding that my phone turn on and off the lights started feeling weird to say aloud, which got me to wondering: was I being rude to my smartphone?

To be clear, this isn’t some kind of “Be nice to Alexa or the robots will kill you during the inevitable uprising” thing. I know very well that Siri and Cortana are just a bundle of voice recognition software and algorithms, and that they’ll work just as well if I ask them to “Tell me the weather report, please” or just bark out a command for the forecast. And as for saying thank you, given that the speakers have (in theory) shut off from active listening by that point of the query process, it’s unlikely that my Echo can even here me saying it.

STRAW POLL: Do you say “please” and/or “thank you” when using a virtual assistant like Alexa or Assistant? — Chaim Gartenberg (@cgartenberg) November 15, 2017

Or, as various people have made the argument to me, would you say “please” to a toaster or “thank you” to an ATM?

But here’s the thing. I don’t talk aloud to an ATM or toaster. And I do when I’m asking Siri to turn on the lights or set an alarm, in the same natural language that I use in day-to-day life. It doesn’t matter that an Echo can’t hear or understand it — it matters that you say it. At what point does my rudeness to smart assistants start to bleed into my normal speech patterns? If I get used to asking Alexa to turn on the lights without saying please, will I still do the same when I ask my roommate that’s walking by? I’m polite to my smart assistants because I want to be polite to people too, and reinforcing rude habits seems like a bad idea.

Maybe I’m totally crazy here. When I polled people on Twitter, it seemed that most people aren’t polite to their digital assistants, which again, is totally fine since they really can’t tell the difference. But my argument is that we shouldn’t be polite to our voice activated assistants for their benefit, but for ours.",Yes,"They’re creating an environment where we’re comfortable giving commands instead of asking questions. This could lead to a future where people are comfortable being more rude to each other since they’re used to barking commands at their devices. While this may seem like a small thing, it could have a large impact on how we interact with",https://www.theverge.com/circuitbreaker/2017/12/10/16751232/smart-assistants-please-thank-you-politeness-manners-alexa-siri-google-cortana,2017-12-10 00:00:00,LABEL_1_relevant,0.9948849081993104,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
20,"Hey Alexa, does this look infected?","Once you’re done ordering pizza and locking and unlocking your front door, what’s left to do with your Echo? Starting today, HealthTap users will be able to use Amazon’s connected home assistant for medical advice, if they so choose.

The online healthcare startup is announcing Alexa integration this week, letting users tap into their voice-controlled health services with the command, “Alexa, talk to Dr. AI.” Once that’s taken care of, the talking speaker will walk the user through a guided path designed to simulate (but not, one assumes, replace) an in-person doctor visit, offering up insights.

The system features an AI doctor “trained in bedside manner to ask intelligible, dynamically generated questions in natural language via an intuitive conversational user interface.” It also can schedule an in-person visit for those in need of more urgent care or just wanting the real thing.

The system targets users with limited mobility, like the disabled and the elderly. It’s already available for iPhone and Android, but the company’s hoping the addition of Alexa functionality will make it more user-friendly and provide additional access to people who don’t have the use of their hands.",Yes,They are becoming extremely user-friendly and providing additional access to people who don’t have the use of their hands.,https://techcrunch.com/2017/02/20/hey-alexa-does-this-look-infected/,2017-02-20 00:00:00,LABEL_1_relevant,0.9954397678375244,Voice Assistants,voice_assistant,TechCrunch,User Experience & Entertainment
21,Microsoft Demos Real-Time Speech-To-Speech Translation On Skype,"Microsoft’s Skype has long enabled people in different parts of the world to communicate with one another through video, voice, and text chat. Now the service is looking to expand those capabilities by testing out a feature that could allow people who don’t speak the same language to talk with each other.

Today at the first annual CODE Conference, Microsoft head of Skype and Lync Gurdeep Singh Pall showed off a new speech-to-speech translation technology that the company is looking to introduce in future versions of its Skype products. The feature, which Skype is hoping to roll out in beta later this year, translates speech from one language to another in near real-time.

As it was demoed, the feature translated Pall’s speech from English into text for transcription on a colleague’s screen in German, and also into voice in German… and vice versa.

In a chat before the demo, Pall said “Skype is about bringing people closer, and breaking down barriers.” That started with the idea of cheap international calling and expanded into face-to-face communications via video. Now it’s taking on the challenge of breaking the language barrier.

The feature was a collaboration between Skype, Bing, Microsoft’s Research Lab, which has long been working on natural language processing and machine learning for a while. Language recognition is powered by the same technology as Microsoft’s Cortana personal assistant on Windows 8.1.

At the conference, Microsoft CEO Satya Nadella said that the feature would be avialable later this year, and that the company would try to launch it on as many devices and apps as possible.",Yes,"They are slowly but surely taking away jobs that have traditionally been done by humans. With the advent of new and improved voice assistants, many jobs that have been around for years are now in danger of disappearing.",https://techcrunch.com/2014/05/27/microsoft-demoes-real-time-speech-to-speech-translation-on-skype/,2014-05-27 00:00:00,LABEL_1_relevant,0.9956809282302856,Voice Assistants,voice_assistant,TechCrunch,Economy
22,Google Is Making An “AI-Powered Assistant for Nurses and Doctors”,"We’ll Take That

What Google wants, Google gets — and right now, Google wants the Streams app.

Streams is a mobile healthcare app developed by DeepMind Health, a subsidiary of AI research company DeepMind, which, like Google, is a subsidiary of Alphabet.

As part of its efforts to snag a slice of the $3 trillion healthcare pie, Google is absorbing DeepMind Health, and according to a DeepMind blog post, the move could transform Streams into “an AI-powered assistant for nurses and doctors everywhere.”

Streaming Lives

The Streams app is currently helping medical professionals in the U.K. address the problem of “failure to rescue,” a term for what happens when a patient dies from a preventable condition because they didn’t receive the right treatment in time.

Advertisement

Advertisement

The app does this by consolidating all of a patient’s medical data. Everyone treating the patient can enter information into their file directly from the app, and the app can then send alerts to healthcare workers as soon as signs of a potentially life-threatening problem arise.

The Majors

Streams is currently used under very narrow conditions: only by workers at specific U.K. facilities and only for the detection of acute kidney injury. With Google’s full support, though, the app could very well become the go-to medical tool of healthcare professional across the globe. And seemingly nothing would make the DeepMind team happier.

“This is a major milestone for DeepMind! One of the reasons for joining forces with Google in 2014 was the opportunity to use Google’s scale and experience in building billion-user products to bring our breakthroughs more rapidly to the wider world,” they wrote in the blog post. “It’s been amazing to put this into practice in data centre efficiency, Android battery life, text-to-speech applications, and now the work of our Streams team.”

READ MORE: Google Is Absorbing DeepMind’s Health Care Unit to Create an ‘AI Assistant for Nurses and Doctors’ [The Verge]

Advertisement

Advertisement

More on DeepMind Health: Google Launches DeepMind Health, Bringing AI to Healthcare

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",,The data privacy. The microphones of voice assistants are always on and they could be hacked. The main benefit is the convenience. Voice assistants are more convenient than typing because they allow you to perform actions hands-free.,https://futurism.com/google-health-deepmind-streams-app,,LABEL_1_relevant,0.9954140186309814,Voice Assistants,voice_assistant,Futurism,Security & Privacy
23,Google tests voice matching to secure Google Assistant purchases ...,"Google Assistant’s Voice Match feature can now secure purchases made through its voice assistant as part of a limited pilot program. A spokesperson from the company confirmed to Android Police that the functionality is new, and is designed to help secure purchases made on smart speakers and smart displays. At the moment, it’s restricted to in-app digital purchases through Google Play, as well as restaurant orders.

When we followed the instructions shown on Google’s support page to enable the feature we only saw the option to enable the assistant’s existing fingerprint or face unlock security methods. Other users have reported that they were able to see the Voice Match menu option, but that the software froze when they tried to enable it.

Screenshots posted by Android Police show the setup process for the feature. “Use your voice to verify it’s you when making a purchase with your Assistant” the software reads. It also warns that the feature’s security may not be perfect, noting that “someone with a similar voice or recording may be able to confirm purchases on devices you’re logged into.”

Although it may not be the most secure method, using your voice as security has the potential to be a lot more seamless than using your phone to respond to a fingerprint or face unlock prompt. It could also be more convenient than Alexa’s existing security method, which allows you to use a PIN to protect purchases made using your voice.

Voice Match is a Google Assistant feature that allows the assistant to tell the difference between who’s addressing it. Last month, 9to5Google reported that Google updated the Voice Match training process to allow it to distinguish between people with “significantly higher accuracy.” However, there are reportedly still limits on how much money you can spend using just your voice as security, Google confirmed to Android Police.

Considering Google I/O 2020 developer conference had been scheduled to take place earlier this month, there’s speculation that the company had hoped to debut it there. Google gave no indication about when voice match confirmed purchases might see a wider release.",Yes,"

they can be used to make purchases without the need for a PIN or other security measure. This means that someone with a similar voice or recording may be able to confirm purchases on devices you’re logged into.",https://www.theverge.com/2020/5/26/21270222/google-assistant-voice-match-purchases-in-app-google-play-shopping,2020-05-26 00:00:00,LABEL_1_relevant,0.9961321353912354,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
24,"iOS 5 includes Siri 'intelligent assistant' voice-control,dictation -- for ...","As rumored, Apple's doing some all-new voice-control AI stuff in iOS 5. It's called Siri, which is the name of the app Apple bought for a rumored $200 million last year. Basically, you ask Siri questions out loud in plain english, and Siri makes sense of what you're asking and presents you with results and actions. Siri launches with a long press of the home button, just like Voice Control of yore, but after that the options are fairly endless. ""Do I have a meeting Friday at noon?"" Siri checks the calendar and responds out loud. ""What time is it in Paris?"" Siri shows you a world clock. ""Set my alarm for..."" you get the idea. The verbal response of Siri allows it to clarify if it understands what you're saying, and in turn perform some pretty complicated actions, like setting up a meeting with Phil Schiller for Friday at noon. Like with the original version of the app, Siri remembers the context of what you've been asking, so actions can build on each other. You can also ask Siri to look things up on Wikipedia for you, and Siri can use Wolfram Alpha to do more complicated calculations. Siri's list of capabilities is near endless, including asking it to play genres of music for you, look up something on maps, or what the weather is. Our favorite question? ""Siri, who are you?"" Siri responds: ""I am your humble personal assistant.""

In addition to the Siri assistant, Apple is using its new voice recognition chops to offer a ""beta"" version of speech-to-text dictation functionality. Initial languages include English, French, and German. Apple plans to offer more languages and more Siri services over time. The bad news? All this great stuff is only available for the iPhone 4S -- Apple had to do something to force an upgrade! In all seriousness, some of this AI functionality can be incredibly processor intensive, so Siri might be leaning on the A5 chip quite heavily. Google's voice dictation capabilities rely on uploading your speech to the cloud, where a server somewhere does the processing work. We'll be digging into Siri in the coming days and weeks, so stay tuned. There's a gallery of Siri in action after the break.

Update: There's a demo video after the break!

Source: Apple

Grid View



































































",Yes,"They open up a new accessibility angle that has not been considered before. The feature can pose as a potential privacy concern, as well as open up new ways for people with disabilities to interact with technology.",https://www.theverge.com/2011/10/4/2515703/ios-5-assistant-voice-control-ai-features,2011-10-04 00:00:00,LABEL_1_relevant,0.9277064204216005,Voice Assistants,voice_assistant,The Verge,Security & Privacy
25,My Message To Google: Stop Cheating,"In mid February, at the Mobile World Congress in Barcelona, Google Executive Chairman Eric Schmidt expressed pride in Google employee Wael Ghonim’s brave struggle against the autocratic Mubarak regime to establish political transparency in Egypt. “We are very, very proud of what Wael and that group was able to do in Egypt,” Schmidt said in Barcelona. But what Schmidt needs to do now is apply Ghonim’s views about political transparency to Google’s own search business.

With its 70% control of the global online search market, Google’s power to make and break online businesses is unrivalled. So it’s not surprising that website owners want more transparency over the reasons why the often autocratic Google sometimes impose penalties on their businesses. But a report issued last week by the newsnavigator OneNewsPage found a distinct lack of transparency in the search business with 88% of respondents saying that paid search advertising costs lacked transparency, while 24% said that they had experienced large, unexplained falls in site traffic as a consequence of changes in their search engine status.

Nor is it surprising that the American Antitrust Institute published a report a week ago calling for US regulators to do a thorough investigation of Google’s proposed deal to acquire travel software provider ITA. This deal, the report worries, would enable the dominant search engine to dominate the online travel market, thus muddying the church-state distinction between Google as technology provider and Google as a distributor of content.

Transparency in search is critical to maintain both innovation and fairness in the digital economy. Yes, Google improved the quality of its search engine by targeting content farms with last week’s self-congratulatory tweaks to its algorithm. But this remains little more than a cosmetic change. Rather than spam, fairness is the key issue. Given Google’s dominance in search, the company has a responsibility to reveal the mechanics of its ranking algorithm—so that everyone understands why some links are ranked higher and more prominently than others.

How can they do that in a way that doesn’t invite gaming and spam, so that companies like JC Penny won’t take advantage of the system? That is Google’s problem. They’ve cornered the global market in PhD’s. If the company can invent a self-driving car then it can certainly figure out how to make its ranking algorithm more transparent without becoming an easy target for content farms like Demand Media.

You see, just as we need our government to play by clear rules, so the same is true with search. Thus, a week ago, in a letter to the DOJ Assistant Attorney General for Antitrust, John Conyers, the lead Democrat on the House Judiciary Committee, encouraged the Department of Justice to “carefully” review the Google ITA deal in order “to ensure competition and transparency will be protected in the online travel industry.”

Last week, 1plusV, the French developer of vertical search engines such as EJustice.fr, filed a complaint with the European Union, accusing Google of “pursuing a strategy of foreclosure against vertical search engines” and of illegally tying the Google search engine with AdSense. Bruno Guillard, 1plusV’s founder, said on Bloomberg News that it was technically impossible for his own vertical search engines to use AdSense, thus undermining his ability to build viable business models around these new services.

Yes, online transparency—understanding how, exactly, its artificial algorithm works and what information or links gets prioritized for what reasons—matters, particularly given the centrality of Google search in the knowledge economy. In his important new book The Googlization of Everything (and why we should worry), University of Virginia media scholar Siva Vaidhyanathan claims that Google’s control of the Internet is comparable to that of Julius Caesar’s rule in 48 BC Rome. The all-controlling Google, Vaidhyanathan argues is “omniscient, omnipotent and omnipresent” and thus needs to be controlled if we are to maintain a level playing field in today’s knowledge economy.

Unfortunately, however, this is only half of it. Not only is Google’s control of today’s search engine market omniscient, omnipotent and omnipresent, but it also seems as if the Mountain View leviathan is abusing the very system over which it has such control. That’s at least the suspicion of the European Commission which last November, on behalf of the European Union, launched an antitrust investigation into allegations that Google “abused” its already dominant position in search by its “unfavorable” treatment of rival services and by its “preferential treatment” of its own services like YouTube, Google Maps and Blogger.

Yes, as everyone from the European Commission to the American Antitrust Institute remind us, transparency matters. In simple terms, it seems, Google has muddied the already disturbingly murky lines between online content and commercial interests by promoting its own products in its supposedly objective search engine. So, for example, when I enter my own name into Google, the first video link that comes up is a YouTube speech I made at Google headquarters in June 2007—even though there are many more professional and popular videos of speeches that I’ve made since.

Coincidence? No, I hardly think so. The truth is that Google’s manipulation of its secretive artificial algorithm isn’t just a reflection of the paranoia of its less powerful rivals in the search business or my own admittedly highly anecdotal adventures in self-googling. Last November, when the European Commission launched its investigation, the Harvard Business School professor, Benjamin Edelman, published a research paper entitled “Hard-Coding Bias in Google Algorithmic Search Results” which proves that Google has “hard-coded its own links to appear at the top of algorithmic search results.”

Edelman’s scientific research is the real reason why Google sucks. Taking each of the 2,642 terms listed on Google’s Health Topics index page, the Harvard academic found that all 2,642 of these individual searches resulted in a link to Google Health appearing in the “absolute top of the page.” Edelman discovered the same unnatural biases with stock ticker searches on Google. Here, he found, that the links from any stock ticker search—“the large-type all-caps ticker symbol, the large price chart, and the left-most details link”—will always take you to Google Finance, even though Google’s in-house financial service is far from being a market leader in this sector.

Google’s bias isn’t just limited to finance and health. In a January 2011 paper, “Measuring Bias in Organic Web Search,” written with Harvard Business School doctoral candidate Benjamin Lockwood, Edelman found that Google listed its own map service as the first result when a user queries “maps.” It’s hardly surprising, therefore, that Edelman and Lockwood discovered that 86% of map searches conducted on Google end up with the user clicking on Google Maps.

So much, then, for the neutrality of Google search, the digital librarian on which we all-too-innocently trust to navigate our way around today’s knowledge economy.

Earlier this month, in an interview with the UK newspaper, The Daily Telegraph, Google’s former CEO and new Executive Chairman, Eric Schmidt, said that he hoped that the European Commission would “come up with a set of remedies” to the issue of biased search and promised that “Google would consider” implementing. Google, Schmidt told The Telegraph, would be “willing to change some of its algorithm methodology in search” if it led to the quick resolution of the EC enquiry.

But Google doesn’t really need any commission, European or otherwise, to identity the remedies to Google’s manipulation of the search knowledge economy. Instead, as I’ve suggested, they should listen to Wael Ghonim’s observations about the lack of transparency in the Mubarak regime.

My message to Mr. Schmidt and Google is very simple: stop cheating. Transform your search engine from a murky algorithm that sometimes benefits your own corporate interests into a transparently neutral guide that benefits both the consumers and the all the companies in our knowledge economy.

The need for Google to establish transparency in its search engine and play by the rules is particularly acute today, both in Europe and America. You see, Google isn’t quite as omniscient, omnipotent and omnipresent as Siva Vaidhyanathan claims. Indeed, for the first time in over a decade, Google has a genuine online rival challenging its hitherto Caesarian hegemony over the Internet. This emerging superpower in the online economy is Facebook which, with its 600 million members and over $50 billion valuation, is beginning to transform the web from a Google centric network of data to a social network of connected people.

Google’s recognition of the growing power of the social web is behind both its failed $6 billion acquisition for the social commerce business Groupon and its rumored $10 billion interest in acquiring Twitter—Facebook’s main rival in the social networking business. And given Google’s much publicized vulnerability in social media, it’s particularly important that this multinational corporation makes its search engine honest so that it doesn’t become a disreputable tool in Google’s battle with Facebook to control the emerging social web.

In his new book, The Master Switch, Columbia University law professor Tim Wu—the scholar who invented the term “network neutrality”—argues that the modern media and communications industry has a tendency toward monopoly. But, for the 2 billion Internet consumers like you and I reliant on fair and transparent search as their trusted vehicle for navigating the web, the only thing worse than a monopolist is a cheating monopolist. Google needs to clean up its artificial algorithm now and guarantee search neutrality. Otherwise it won’t only be the European Commission investigating the self-interested bias of the Google search engine.

Photo credit: Flickr/Dicemanic",Yes,They are killing our ability to think for ourselves and become emotionally intelligent.,https://techcrunch.com/2011/02/26/my-message-to-google-stop-cheating/,2011-02-26 00:00:00,LABEL_1_relevant,0.9959981441497804,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
26,Listen to an AI voice actor try and flirt with you - The Verge,"The quality of AI-generated voices has improved rapidly in recent years, but there are still aspects of human speech that escape synthetic imitation. Sure, AI actors can deliver smooth corporate voiceovers for presentations and adverts, but more complex performances — a convincing rendition of Hamlet, for example — remain out of reach.

Sonantic, an AI voice startup, says it’s made a minor breakthrough in its development of audio deepfakes, creating a synthetic voice that can express subtleties like teasing and flirtation. The company says the key to its advance is the incorporation of non-speech sounds into its audio; training its AI models to recreate those small intakes of breath — tiny scoffs and half-hidden chuckles — that give real speech its stamp of biological authenticity.

“Bigger emotions are a little easier to capture”

“We chose love as a general theme,” Sonantic co-founder and CTO John Flynn tells The Verge. “But our research goal was to see if we could model subtle emotions. Bigger emotions are a little easier to capture.”

In the video below, you can hear the company’s attempt at a flirtatious AI — though whether or not you think it captures the nuances of human speech is a subjective question. On a first listen, I thought the voice was near-indistinguishable from that of a real person, but colleagues at The Verge say they instantly clocked it as a robot, pointing to the uncanny spaces left between certain words, and a slight synthetic crinkle in the pronunciation.

Sonantic CEO Zeena Qureshi describes the company’s software as “Photoshop for voice.” Its interface lets users type out the speech they want to synthesize, specify the mood of the delivery, and then select from a cast of AI voices, most of which are copied from real human actors. This is by no means a unique offering (rivals like Descript sell similar packages) but Sonantic says its level of customization is more in-depth than that of rivals’.

Emotional choices for delivery include anger, fear, sadness, happiness, and joy, and, with this week’s update, flirtatious, coy, teasing, and boasting. A “director mode” allows for even more tweaking: the pitch of a voice can be adjusted, the intensity of delivery dialed up or down, and those little non-speech vocalizations like laughs and breaths inserted.

“I think that’s the main difference — our ability to direct and control and edit and sculpt a performance,” says Flynn. “Our clients are mostly triple-A game studios, entertainment studios, and we’re branching out into other industries. We recently did a partnership with Mercedes [to customize its in-car digital assistant] earlier this year.”

As is often the case with such technology, though, the real benchmark for Sonantic’s achievement is the audio that comes fresh out of its machine learning models, rather than what’s used in polished, PR-ready demos. Flynn says the speech synthesized for its flirty video required “very little manual adjustment,” but the company did cycle through a few different renderings to find the very best output.

To try and get a raw and representative sample of Sonantic’s technology, I asked them to render the same line (directed to you, dear Verge reader) using a handful of different moods. You can listen to them yourself to compare.

First, here’s “flirty”:

Then “teasing”:

“Pleased”:

“Cheerful”:

And finally, “casual”:



To my ears, at least, these clips are a lot rougher than the demo. This suggests a few things. First, that manual polishing is needed to get the most out of AI voices. This is true of many AI endeavors, like self-driving cars, which have successfully automated very basic driving but still struggle with that last and all-important 5 percent that defines human competence. It means that fully-automated, totally-convincing AI voice synthesis is still a way off.

Second, I think it shows that the psychological concept of priming can do a lot to trick your senses. The video demo — with its footage of a real human actor being unsettlingly intimate towards the camera — may cue your brain to hear the accompanying voice as real. The best synthetic media, then, might be that which combines real and fake outputs.

Apart from the question of how convincing the technology is, Sonantic’s demo raises other issues — like, what are the ethics of deploying a flirtatious AI? Is it fair to manipulate listeners in this way? And why did Sonantic choose to make its flirting figure female? (It’s a choice that arguably perpetuates a subtle form of sexism in the male-dominated tech industry, where companies tend to code AI assistants as pliant — even flirty — secretaries.)

On the first question, the company said their choice of a female voice was simply inspired by Spike Jonze’s 2013 film Her, where the protagonist falls in love with a female AI assistant named Samantha. On the second, Sonantic said it recognizes the ethical quandaries that accompany the development of new technology, and that it’s careful in how and where it uses its AI voices.

“That’s one of the biggest reasons we’ve stuck to entertainment,” says CEO Qureshi. “CGI isn’t used for just anything — it’s used for the best entertainment products and simulations. We see this [technology] the same way.” She adds that all of the company’s demos include a disclosure that the voice is, indeed, synthetic (though this doesn’t mean much if clients want to use the company’s software to generate voices for more deceitful purposes).

Comparing AI voice synthesis to other entertainment products makes sense. After all, being manipulated by film and TV is arguably the reason we make those things in the first place. But there is also something to be said about the fact that AI will allow such manipulation to be deployed at scale, with less attention to its impact in individual cases. Around the world, for example, people are already forming relationships — even falling in love — with AI chatbots. Adding AI-generated voices to these bots will surely make them more potent, raising questions about how these and other systems should be engineered. If AI voices can convincingly flirt, what might they persuade you to do?",Yes,"They will likely perpetuate a subtle form of sexism in the male-dominated tech industry, where companies tend to code AI assistants as pliant — even flirty — secretaries.",https://www.theverge.com/2022/2/17/22936978/ai-voice-speech-synthesis-audio-deepfake-sonantic-flirtation,2022-02-17 00:00:00,LABEL_1_relevant,0.9958987832069396,Voice Assistants,voice_assistant,The Verge,Equality & Justice
27,When AI Experts Have “It’s Alive!” Moments,"Fellow panelist Rob Chambers, principal group program manager for Cortana, Microsoft’s virtual-assistant software, described a similar experience of his own. While Chambers was brushing his teeth and looking at his smartphone one day, Cortana suddenly showed a message asking if he liked to read news articles about smartphone technology. Chambers says the experience gave him a sudden feeling that Cortana was “real,” even though he knew that the ability to learn what types of things people like to know about was scheduled to be added to the software.

Despite these flashes of apparent humanity, several people on the panel—which also included people who had worked on Google’s virtual assistant, Google Now—agreed that virtual assistants aren’t about to move from handling administrative tasks like sheduling meetings to becoming our bosom buddies. That will require ways to be found to add a capacity to understand and express emotion.

“The big missing gap on the Internet overall, in the world we live, this electronic age, is personality with emotion we can connect to in some deep-seated human way,” said Ronald Croen, founder and formerly CEO of voice recognition company Nuance. “That might sound scary in one sense because this is not a real person, but it might be very gratifying and powerful when the content matters, when the engagement experience matters.”",Yes,They are making people feel more emotional connections with their technology.,https://www.technologyreview.com/2014/11/06/170504/when-ai-experts-have-its-alive-moments/,2014-11-06 00:00:00,LABEL_1_relevant,0.9960470795631408,Voice Assistants,voice_assistant,MIT Tech Review,User Experience & Entertainment
28,An Alexa Bug Could Have Exposed Your Voice History to Hackers,"Smart-assistant devices have had their share of privacy missteps, but they're generally considered safe enough for most people. New research into vulnerabilities in Amazon's Alexa platform, though, highlights the importance of thinking about the personal data your smart assistant stores about you—and minimizing it as much as you can.

Findings published on Thursday by the security firm Check Point reveal that Alexa's web services had bugs that a hacker could have exploited to grab a target's entire voice history, meaning their recorded audio interactions with Alexa. Amazon has patched the flaws, but the vulnerability could have also yielded profile information, including home address, as well as all of the ""skills,"" or apps, the user had added for Alexa. An attacker could have even deleted an existing skill and installed a malicious one to grab more data after the initial attack.

""Virtual assistants are something that you just talk to and answer, and usually you don’t have in your mind some kind of malicious scenarios or concerns,"" says Oded Vanunu, Check Point's head of product vulnerability research. ""But we found a chain of vulnerabilities in Alexa's infrastructure configuration that eventually allows a malicious attacker to gather information about users and even install new skills.""

For an attacker to exploit the vulnerabilities, she would need first to trick targets into clicking a malicious link, a common attack scenario. Underlying flaws in certain Amazon and Alexa subdomains, though, meant that an attacker could have crafted a genuine and normal-looking Amazon link to lure victims into exposed parts of Amazon’s infrastructure. By strategically directing users to track.amazon.com—a vulnerable page not related to Alexa, but used for tracking Amazon packages—the attacker could have injected code that allowed them to pivot to Alexa infrastructure, sending a special request along with the target's cookies from the package-tracking page to skillsstore.amazon.com/app/secure/your-skills-page.

At this point, the platform would mistake the attacker for the legitimate user, and the hacker could then access the victim's full audio history, list of installed skills, and other account details. The attacker could also uninstall a skill the user had set up and, if the hacker had planted a malicious skill in the Alexa Skills Store, could even install that interloping application on the victim's Alexa account.

Both Check Point and Amazon note that all skills in Amazon's store are screened and monitored for potentially harmful behavior, so it's not a foregone conclusion that an attacker could have planted a malicious skill there in the first place. Check Point also suggests that a hacker might be able to access banking data history through the attack, but Amazon disputes this, saying that information is redacted in Alexa's responses.

“The security of our devices is a top priority, and we appreciate the work of independent researchers like Check Point who bring potential issues to us,"" an Amazon spokesperson told WIRED in a statement. ""We fixed this issue soon after it was brought to our attention, and we continue to further strengthen our systems. We are not aware of any cases of this vulnerability being used against our customers or of any customer information being exposed.""",Yes,"They are always listening to us. This could pose a serious privacy concern as our conversations are being recorded and stored without our knowledge. Additionally, if a hacker were to gain access to our voice history, they could potentially learn a lot about us, including our home address and the apps we use. Amazon has patched the vulnerabilities that allowed this attack to",https://www.wired.com/story/amazon-alexa-bug-exposed-voice-history-hackers/,2020-08-13 10:00:00+00:00,LABEL_1_relevant,0.9966340661048888,Voice Assistants,voice_assistant,Wired,Security & Privacy
29,"Siri Loses Her Voice, Experiences Spotty Service Countrywide","Apple's famous new speech-powered assistant may have lost her million-dollar voice.

Reports circulated on Thursday afternoon that Apple's ""Siri"" voice-activated iPhone assistant is experiencing service outages for customers across the country.

Upon Apple's service going down, Twitter exploded with messages from frustrated iPhone 4S users and lookers-on. Twitter-user BM5K -- whose profile location states Queen Creek, AZ -- asked, ""anyone else having major problems with Siri today?"" Others, like Jason Vance of McLean, VA, wrote that Siri was working for him, albeit slower than usual.

In Wired.com's San Francisco testing, the Siri service was spotty at best, functioning properly only intermittently. Still, others located in the Bay Area reported that the Siri service is functioning properly.

And of course, there was the usual snarky commentary from Twitter's global peanut gallery.

""With Siri Outage,"" wrote Twitter-user Anupam Ashish, ""iPhone 4S is now downgraded to an iPhone 4 with a Better Camera.""

It's unfortunate timing for Apple's brand-new service to experience issues. On Wednesday, after weeks of users complained of battery life issues with their new iPhone 4S devices, Apple issued a statement confirming its newly released iOS 5 operating system included ""a few bugs that are affecting battery life."" The company promised to ""release a software update to address those [bugs] in a few weeks.""

And it isn't as if outages in the service will slip by unnoticed. The inclusion of Siri was easily the most prominent upgrade from last year's iPhone 4 to the iPhone 4S, obviously the key marketing point in convincing users to purchase the new device. It worked: Apple sold over 4 million iPhone 4S handsets over the first three days of release.

Apple debuted the service last monthwith one major caveat: The company considers Siri a ""beta"" product, a fact which should be kept in mind if the service stumbles in its first few months. While releasing beta products is common for rival Silicon Valley giant Google, it has hardly ever been decorum for Apple.

Apple did not immediately respond to a request for comment.

Photo: Jim Merithew/Wired.com",Yes,"They have wholly captured the human experience, rather than the original intention of just making our lives easier. The question that should be asked is not whether or not voice assistants are good or bad. Rather, the question is: How do we want to use voice assistants in a way that benefits humanity as a whole?",https://www.wired.com/2011/11/apple-siri-down-outage/,2011-11-03 20:05:04+00:00,LABEL_1_relevant,0.9900462031364441,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
30,Supahands Offers Multilingual Virtual Assistants For Clients In Asia,"Southeast Asia is the second-largest outsourcing destination in the world, but a new company called Supahands wants to help locals by bringing virtual assistants to the region. Based in Malaysia and Singapore, the startup plans to expand to Hong Kong, Taiwan, Bangkok, and Australia.

Co-founder Mark Koh first thought of launching a virtual assistant startup for Asia in 2010, but decided the market wasn’t ready at that point. Born in Malaysia, but raised and educated in Australia, Koh instead founded a Melbourne-based company called Global Virtual Support that hires Southeast Asia-based employees to handle customer service requests for medium-sized companies.

Last year, however, Koh decided that it was the right time to revisit his original idea. Based in Malaysia, Supahands launched in December. The startup, which is bootstrapped, says it currently has 500 active users.

One of the main things that has enabled Supahands to launch is the rapid increase of smartphone use in Southeast Asia. Most of the startup’s clients use mobile phones to enter tasks, get matched with a virtual assistant, and check on the progress of projects, says co-founder Andrew Tan.

Though 60% of requests are in English, Supahands caters to local customers with assistants who speak Mandarin, Cantonese, Tamil, and Malay, among other languages and dialects.

The startup currently has six full-time assistants, called “agents,” based in an office in Kuala Lumpur, but as the service scales up within Malaysia and Singapore, Supahands plans to train part-time employees who will work from home.

Some clients hire an assistant for a one-time program, like vetting cleaning services, but these turn into recurring tasks, says Tan. For example, a property owner who lives abroad and hires a cleaning team might ask Supahands’ agents to make sure that they show up at the right times. One of the biggest tasks Supahands has handled so far was planning a honeymoon.

Supahands’ chief competitors include freelancing sites like Odesk and Freelancer.com. Tan argues that Supahands makes hiring a virtual assistant easier by training and working closely with its assistants so clients don’t have to interview them.

All communication is done through Supahands’ online dashboard, instead of directly between clients and assistants, which Tan says helps with quality control.

He adds that Supahands’ biggest rival is actually the self-sufficiency of potential clients.

“In Southeast Asia, most locals have the mentality that they want to do everything by themselves, so our challenge is to help them see that they can outsource these tasks and save time for something more valuable,” says Tan.",Yes,"They are slowly making us more lazy. As we get used to asking Siri or Alexa to do things for us, we will find it harder and harder to do things for ourselves. This could have a negative impact on our society as a whole, as people become less self-sufficient and more reliant on technology.",https://techcrunch.com/2014/04/20/supahands-offers-multilingual-virtual-assistants-for-clients-in-asia/,2014-04-20 00:00:00,LABEL_1_relevant,0.9922181367874146,Voice Assistants,voice_assistant,TechCrunch,Economy
31,Even Sleeping Smartphones Could Soon Hear Spoken Commands,"While we generally focus on the dramatic aspects of assistant apps, like artificial intelligence and speech recognition, every step between you and the outcome you want is a problem, Sejnoha says. “Just turning on the device is part of the problem, right? So we’re going to be smoothing that out, eliminating those problems as well,” he says.

He also expects improvements in a mobile device’s ability to listen to an ongoing stream of noise and distinguish its user’s voice asking it to perform a task from background chatter.

However, he cautions that software developers will have to be careful to avoid making services that are annoying or creepy, or else they’ll create more “paper clip stories”—a reference to Clippy, the much maligned (and now extinct) digital helper in Microsoft’s Office software.

Privacy and security are also concerns if you have software persistently “listening” in the background. While Sejnoha says Nuance could build a system that is private from an engineering perspective, it’s still a big leap for users to become comfortable with something that’s always paying attention at some level, and there may be concerns about where the data that’s collected is going, and if it can be hacked into.",Yes,They may be annoying or creepy and may invade user privacy.,https://www.technologyreview.com/2012/09/24/183670/even-sleeping-smartphones-could-soon-hear-spoken-commands/,2012-09-24 00:00:00,LABEL_1_relevant,0.9958488941192628,Voice Assistants,voice_assistant,MIT Tech Review,Security & Privacy
32,Amazon disputes claims that Echo Show's Drop-In feature is a security risk,"Drop-In is a strange and mostly ill-advised feature. It’s a way of offering instant communication between Echo devices — essentially a phone call that doesn’t require the receiver to pick up in order to initiate. Hence, “dropping in.”

Amazon introduced it with the new Show and is rolling it out to older Echo devices as a way of facilitating the new intercom feature it introduced this week. We raised a few questions when the feature was first announced alongside free Echo calling back in May, but with the Show arriving this week, the some privacy and security concerns are being raised anew.

The feature is a bit of an odd choice from Amazon, given all of the questions that have been raised around the Echo line’s always-on microphones — though the company did get out in front of things this time around by spelling out the fact that Drop-In is an opt-in feature. Both parties involved have to enable Drop-In in the device’s settings in order to work.

Amazon reiterated that point in a statement provided to TechCrunch, adding, “Customers will know when they are engaged in a Drop-In call as they will first hear a ‘chime,’ and the green light ring on their Echo device will rotate throughout the call. On Echo Show devices, the caller will experience a 10-second ‘frosted glass’ screen when dropping in on an approved contact or room, so the call recipient can decline the Drop In in that timeframe before they are shown on the screen.”

Beyond intercom functionality, the feature is primarily targeted at users looking to check-in on older relatives or who’d like the system double as a baby monitor. The inclusion of visual and audio clues are certainly helpful — so users can’t be entirely stealth. But even with the “frosted glass” that partially obscures the screen for the first 10 seconds, it’s easy to imagine a scenario where the user on the other end is out of the room when a Drop-In is initiated or otherwise doesn’t notice that first chime.

In-home intercom functionality is a useful and much-requested feature for the Echo, but introducing an Echo with a built-in camera was inevitably only going to raise privacy and security concerns around the device. So the optics around launching this feature at the same time were bound to be less than ideal.

https://beta.techcrunch.com/wp-content/uploads/2017/06/dsc00742.jpg?w=1024&h=576

There’s also the issue of potential malicious access by hackers. Like most big companies, Amazon isn’t eager to share information around how something like Drop-In works on the server side. Companies will often keep some mystery around that secret sauce, for fear of revealing too much to wannabe hackers — which may go a ways toward explaining why it took so long for the company to issue an official response to earlier questions about what the Echo records and uploads to its servers.

It’s a balancing act. Staying silent on specific security concerns doesn’t look great from the outside. Depending on how the feature is implemented on the backend, it could potentially open the product to additional vulnerabilities, though the company reassures me that Drop-In operates like regular calling from a security standpoint.

“We take security very seriously at Amazon, and Alexa calling and messaging is no different,” the company tells TechCrunch. “The new Drop-In feature leverages the same security measures used in other Alexa features available on Echo devices. We limit the information we disclose about specific security measures we take, but I can tell you that — just like other calling and messaging services- –Alexa calling and messaging is encrypted in transit to protect our customers. Further, we have full teams dedicated to ensuring the safety and security of our Alexa customers who regularly conduct security reviews of our products and features.”

The company is also quick to point to the Mute button on the devices. In the case of the Show, it’s one of three large buttons on the top of the unit. Hitting it will both deactivate the microphone and camera, while turning the status bar red. While testing the feature over the past week, I had to train myself to hit the button every time I got dressed or did anything else I’d rather not have floating around on Amazon’s servers.

I’d suggest using the mute button liberally. In fact, I found myself unplugging the Show from time to time — which admittedly is counterproductive to the idea of a hands-off assistant. And in most cases, I’d recommend against using the Drop-In feature altogether. Every time you introduce another new technology into your home, it’s important to weigh the cost/benefit ratio. In my own life, I can’t think a single instance in which I wouldn’t at least want the option of not picking up a call.",Yes,"They are always listening. This is a result of the way they are built, as they need to be able to hear the wake word at any time. The result is that people's daily lives are being recorded and stored by these companies, whether they know it or not. While there are some benefits to this technology, the lack of privacy is",https://techcrunch.com/2017/06/28/amazon-disputes-claims-that-echo-shows-drop-in-feature-is-a-security-risk/,2017-06-28 00:00:00,LABEL_1_relevant,0.9964904189109802,Voice Assistants,voice_assistant,TechCrunch,Security & Privacy
33,The Cohen Tapes Are Everyone's New Favorite Playlist,"If it’s summer, California must be on fire. And if that’s not enough dystopia for you, there’s always the fact that DNA-testing service 23AndMe is selling clients’ information to drug manufacturers, or the emergence of mutated HIV strains that cause illness quicker than others, not to mention the president apparently threatening war with Iran for no immediately discernible reason. Yes, the world keeps on turning towards the very worst incarnation of itself, it seems, but it’s not all bad; at least there’s a new Mission: Impossible out this weekend. And until you can get yourself to a fine purveyor of popcorn-fueled escapism, please do enjoy this primer on what the internet has been discussing over the past seven days or so.

Michael Cohen's Basement Tapes

What Happened: Cohen, a man who had previously announced that he would take a bullet for Donald Trump, has apparently reconsidered his position.

What Really Happened: Remember when people were wondering if Trump's personal attorney being under criminal investigation meant he might turn on Trump? As it turns out, the answer is definitely yes.

Twitter content This content can also be viewed on the site it originates from.",Yes,"They are now recording everything we say in our homes. Anyone who has a voice-activated assistant in their home—and that's a lot of people—knows how convenient they can be. But did you know that they're also always listening to you?

That was news to many people, who were surprised (or horrified) to",https://www.wired.com/story/internet-week-180/,2018-07-29 16:40:02.578000+00:00,LABEL_1_relevant,0.981410801410675,Voice Assistants,voice_assistant,Wired,Security & Privacy
34,"Amazon Workers Listen to Your Alexa Conversations, Then Mock Them","They use internal chat rooms to discuss clips they find amusing or disturbing.

I Hear You

Amazon pays thousands of workers across the globe to review audio picked up by its Echo speakers — and their behavior raises serious concerns about both privacy and safety.

Bloomberg recently spoke with seven people who participated in Amazon’s audio review process. Each worker was tasked with listening to, transcribing, and annotating voice recordings with the goal of improving the ability of Amazon’s Alexa smart assistant to understand and respond to human speech.

But sometimes, according to Bloomberg, they share private recordings in a disrespectful way.

“I think we’ve been conditioned to the [assumption] that these machines are just doing magic machine learning” University of Michigan professor Florian Schaub told Bloomberg. “But the fact is there is still manual processing involved.”

Advertisement

Advertisement

Listen to This

The job is usually boring, according to Bloomberg’s sources. But if they heard something out of the ordinary, they said, sometimes they’d share the Alexa recordings with other workers via internal chat rooms.

Occasionally, it was just because they found the audio amusing — a person singing off-key, for example — but other times, the sharing was “a way of relieving stress” after hearing something disturbing, such as when two of Bloomberg’s sources heard what sounded like a sexual assault.

When they asked Amazon how to handle cases like the latter, the workers said they were told “it wasn’t Amazon’s job to interfere.” Amazon, meanwhile, said it had procedures in place for when workers hear something “distressing” in Alexa recordings.

READ MORE: Amazon Workers Are Listening to What You Tell Alexa [Bloomberg]

Advertisement

Advertisement

More on Echo: Thanks, Amazon! Echo Recorded and Sent Audio to Random Contacts Without Warning

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",,":

they may be recording and sharing private conversations without users' knowledge or consent. This raises serious concerns about privacy and safety.",https://futurism.com/the-byte/amazon-listen-share-alexa-recordings,,LABEL_1_relevant,0.996189534664154,Voice Assistants,voice_assistant,Futurism,Security & Privacy
35,Siri Hacked to Control a Thermostat -- What's Next?,"The release of a Siri-hacking instruction manual has made it possible for developers to push the limits of Apple's voice-controlled digital assistant. Most recently, Apple's technology was modded to control a thermostat. But Siri -- and voice control in general -- could soon be used to take over our entire home experience.

Web developer Pete Lamonica is the brain behind the thermostat hack. He can now tell Siri to check his thermostat's status, and change the temperature at which the heater or air conditioner turns on. Lamonica started on his project last Tuesday after Applidium released its instructions for cracking Siri using a custom SSL certification authority. The complete project took about five days.

""I had originally planned to do something similar to that College Humor skit, then I thought I could have it control my wireless thermostat. I created a proxy to accomplish that,"" Lamonica says. He's a member of the St. Louis-area hackerspace Arch Reactor. ""I've done some stuff with the thermostat before, such as having it log temperatures. It has a developer API which makes it very easy to work with.""

The timing of Lamonica's project is impeccable. Less than a month ago, the designer of the iPod, Tony Fadell, released a smart thermostat called the Nest, and now we're seeing Lamonica use another Apple device to control a much more prosaic thermostat model.

Lamonica says the main reason he developed this Siri proxy was to increase the number of objects Siri can handle. Right now, Siri's library of what it can do is very small, and limited to tasks like sending text messages, displaying maps, showing results from Wolfram Alpha, and all the other somewhat familiar things that have already been well-documented by Apple. ""Increasing the number of things that developers can do is priority number one,"" Lamonica says.

Developers have had varied success porting Siri to other devices, such as the iPhone 4 and iPod touch. Others have used Siri's current range of talents to accomplish new tasks, such as using her tweet-sending functionality to pour a glass of beer via an elaborate Rube Goldberg-style scheme employing a toy truck. Voice control and artificial intelligence applications are only just beginning to be exploited, and Siri is becoming a major player in the movement -- albeit without Apple's blessings.

Sure, Apple could encourage third-party Siri development, but that's not likely anytime soon. Apple generally locks down its technologies, and frowns on jail-breaking and other activities that might diminish the iOS experience or break a device.

NPD analyst Ross Rubin expects Apple to expand Siri on a more limited basis, scenario by scenario, possibly partnering with specific companies to implement new features, such as Siri's current integration with Yelp.

Nonetheless, as long as Applidium's hack still works, developers have the opportunity to see just what else Siri can do.

Lamonica, for one, has more plans for Siri integration. ""My fiance wants me to make it so that the thermostat turns on when she says 'I'm hot' or 'I'm cold,'"" Lamonica says. ""I also want to get it to control our Xbox Media Center, so I can say things like 'TV pause' or 'TV play.'""",Yes,"They provide a way for consumers to connect with potential hackers. While Applidium's hack is relatively harmless, it's not hard to imagine a more malicious hacker using a voice assistant to gain access to a person's home.

In the future, we may see more voice assistants in our homes and cars, and they may even become our primary",https://www.wired.com/2011/11/siri-thermostat-hack/,2011-11-21 23:02:12+00:00,LABEL_1_relevant,0.9960954785346984,Voice Assistants,voice_assistant,Wired,Security & Privacy
36,How uncertainty could help a machine hold a more eloquent conversation,"Gamalon’s founder and CEO, Ben Vigoda, told MIT Technology Review his company’s approach also relies on rules and machine learning, but it adds a probabilistic technique to the mix, synthesizing programs that handle probabilities automatically (see “AI software juggles probabilities to learn from less data”). In practice, this means the system can deal with uncertainty by making its best guess about what someone means. It also provides a conversational memory: you could ask “What about tomorrow?” after previously asking what the weather is like today.

Vigoda says the approach lets a machine learn from a smaller amount of data and reduce the rate of errors. It can also show why the machine responded the way it did. “Language isn’t really like a decision tree,” Vigoda says. “This is trying to be more like a person.”

Gamalon has also created an interface that lets ordinary users interact with the system. They can build a powerful chatbot by defining a tree of options for a conversation, letting the underlying system deal with the various different ways the dialogue might unfold. The technology is currently being tested by several companies.

Gamalon is unusual among AI companies in the way it is training machines to perform useful tasks. However, a growing number of experts believe that new techniques may well be needed to achieve significant further progress (see “Is AI riding a one-trick pony?”).

Any advances in natural-language processing could have a big commercial and practical impact. Voice assistants like Alexa or Siri represent very convenient new way to interact with computers, but they are extremely limited in how they use language. Unless you talk carefully, using voice assistants and chatbots can be a pretty infuriating experience.

David Blei, a professor at Columbia University, says Gamalon’s approach brings together several important emerging themes in machine learning. He says the idea of making AI systems more interactive and explainable is especially exciting. “Interactive machine learning is about bringing the human into the loop,” he says. “This is a very realistic way to imagine augmented intelligence could work.”",Yes,"They are extremely limited in how they use language, which can be frustrating for users.",https://www.technologyreview.com/2018/05/07/143005/how-uncertainty-could-help-a-machine-hold-a-more-eloquent-conversation/,2018-05-07 00:00:00,LABEL_1_relevant,0.9943974018096924,Voice Assistants,voice_assistant,MIT Tech Review,"Information, Discourse & Governance"
37,Google: Our Assistant Will Trigger the Next Era of AI,"It is the day after Google’s big hardware event in San Francisco, when the company formally unveiled a new phone (a jab to the iPhone) and a voice-activated speaker (a gut punch to Amazon’s Echo). Word of mouth is already tracking positive; a countdown to ecstasy, in the form of upcoming rhapsodic reviews of the Pixel phone, has already begin. But in a conference room on the company’s sprawling Mountain View campus, Fernando Pereira, who leads Google’s projects in natural language understanding, is less excited about his company’s shiny new devices than he is about what will happen when people use them. “Let me tell you a little bit about The Transition,” he says.

Pereira holds the title of distinguished scientist at Google. Since arriving at the company in 2008 from his post as the chair of the Department of Computer and Information Science at the University of Pennsylvania, he has been at the center of Google’s efforts to answer the question: How do we learn the meaning of text from data? In other words, how can a machine truly understand the phrases that human beings peck and blab into its search fields and microphone? The researchers at Google and elsewhere have settled on an answer to that question: machine learning; specifically, a form of artificial intelligence called neural networks—self-organizing systems modeled on the way the brain works. These systems use sophisticated algorithms and tons of data to train themselves. The more data the better.

As Pereira explains it, The Transition is a Brink’s Job-level bounty of data that his team and other scientists at Google will receive when millions of people start conversing with his company’s flagship bot, the Google Assistant. The Assistant is a single software system that will be implemented across multiple Google platforms, including the Pixel phone and the Google Home device. It strives to control the functions on the phone like Siri does, perform services as seamlessly as Amazon’s Alexa, and conduct Geisha-level chatter that puts to shame the business bot in Facebook’s Messenger.

Though Google already interprets voice commands in products like voice search in the Google app, the Assistant is different: Google sees it as the apotheosis of its efforts to answer questions and perform functions. The company sees the Assistant as an evolution of many products, including Search, Maps, Photos, and Google Now. Sample queries the company offers display the product’s intended breadth: Show me pictures of the beach. Play dance music on the TV. Tell me about my day. The Assistant is optimized to do much of its work via a verbal, person-machine interchange. After it gives an answer to Where’s the closest Italian restaurant? you can tell it to Navigate there, and you’ll get directions.

As good as the Google Assistant purports to be, Pereira knows its shortcomings. Most frustrating, the Assistant’s ability to understand and converse about complex queries is only at the beginning of the long path that Google envisions. It is all too easy to run into the wall where the Assistant simply doesn’t get what you’re saying. Pereira needs the Assistant to really, really understand what people say, in a way that reflects a mastery of the intricacies of communication with an overall grasp of the way the physical world works.

This is hard, especially because Google hasn’t yet had the data to train its neural nets to the levels it aspires to reach. “When you try to build a system for understanding natural language, and you don’t have many examples of the kind of understanding you want,” Pereira says, “then you have to prescribe, you have to write—essentially teach it grammar—so that it can do the understanding. That teaching is very laborious.”",Yes,They will soon have a huge amount of data that will be used to improve their understanding of human language.,https://www.wired.com/2016/10/google-our-assistant-will-trigger-the-next-era-of-ai/,2016-10-25 04:00:00+00:00,LABEL_1_relevant,0.9949847459793092,Voice Assistants,voice_assistant,Wired,Security & Privacy
38,An Amazon Echo Can't Call the Police—But Maybe It Should,"Despite what you may have heard, an Amazon Echo did not call the police earlier this week, when it heard a husband threatening his wife with a gun in New Mexico. On Monday, news reports took Bernalillo County authorities' version of those events credulously, heralding the home assistant as a hero. The alleged act also raised an important question: Do you really want to live in a world where Alexa listens to your conversations, and calls the cops if she thinks things are getting out of hand?

The good news is that you don’t live in that world. Amazon’s Alexa can’t, and did not, call 911. Google Home can’t do it either. No voice-assistant device on the market can. That doesn't invalidate the core question though, especially as Amazon Echo, Google Home, and their offshoots increasingly gain abilities and become more integral to everyday life. How intrusive do you want to let these devices be? Should they be able to call the police? Maybe not even just when specifically prompted, but because they may have heard, for instance, a gun shot?

The Bernalillo County incident almost certainly had nothing to do with Alexa. But it presents an opportunity to think about issues and abilities that will become real sooner than you might think.

A Quick Debunk

The Bernalillo County Sheriff’s Department reported, specifically, that when a man drew a gun on his wife in a home where an Amazon Echo was placed, he said to her, “Did you call the sheriffs?” and the Echo misinterpreted that as a command to call the sheriffs, who then showed up at the front door. The authorities later clarified that someone in the house could be heard in the 911 recording yelling, “Alexa, call 911.”

This could not have happened as described. Amazon’s Echo requires a ""wake word"" to activate; the default is “Alexa,” but you can also customize it to “Echo,” “Amazon,” or “Computer.” And while they can make calls, an Alexa-powered device can only call another Alexa-powered device. Not only that, but it can only call other Alexa devices that have enabled calling, and have been added to your contact list. Most importantly, these exchanges don't take place over the public switched telephone network, the worldwide network that allows wireless or land phones to actually make calls.

In other words, the sheriffs would have needed an Alexa device of their own for that to ever work, one that the couple in the domestic dispute had in their contact list. Later, the police said that the Alexa was used in combination with some kind of home phone or cellular phone system. That at first sounds more plausible, but is actually also technologically impossible, as the Echo does not support calls over Bluetooth.

Someone called the police that day. It just wasn't Alexa.

Alexa, Why Can’t You Call 911?

Alexa's current calling limitations won't last forever. The Echo's biggest competitor, Google’s Home, will soon allow you to call any number in the US using the device–except for 911, or 1-900 numbers.

The holdup seems to be largely regulatory; according to Federal Communications Commission spokesman Mark Wigfield, providing 911 services means adhering to a host of technical regulations, everything from making sure all 911 calls route through the right call center, to making sure each one transmits the correct location of the caller. Additionally, devices that make 911 calls must also be able to receive incoming calls, so police can call back. Those hurdles currently prevent Google and Amazon from offering a direct emergency line. But they can, and likely will, be overcome at some point.",Yes,"They are not able to detect context, and that is why they can't call 911. They can't tell that you were being sarcastic when you said to call the sheriff, or if you were joking about the gun. They just hear the words you say.

That doesn't necessarily mean the devices are useless in emergency situations though. The",https://www.wired.com/story/alexa-call-police-privacy/,2017-07-16 11:00:00+00:00,LABEL_1_relevant,0.9966488480567932,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
39,"One year later, restaurants are still confused by Google Duplex - The ...","Myriah Q. hasn’t stopped moving since the moment I entered the bar. She’s got patrons seated on the sidewalk and the backyard areas, and she is pacing between opposite ends of the venue to keep up with the happy hour rush. Occasionally, she hops behind the counter to mix drinks, restart the music playlist, or organize menus. Suddenly, the phone rings — for the third time. Myriah looks at the caller ID, ignores the call, and resumes her dance.

“I don’t have time for spam calls,” she explains. “I’m busy enough as is.”

But the call wasn’t exactly spam. Twenty minutes earlier, I’d asked the bartender at Sweet Afton, an Irish pub in Astoria, New York, if she’d ever heard of Google Duplex, an AI that calls restaurants on a person’s behalf and uses realistic human speech to make reservations. When Myriah told me she hadn’t, I asked if she’d be interested in trying it out, and she agreed. Still, when the calls came, she opted not to pick up.

“I purposely ignored those calls because it said ‘Google,’” she says, not realizing that the calls would appear to come directly from the company. “I’m moving so quickly when I’m working that if it doesn’t say a person’s name, I don’t pick up.”

When the fourth call rang — as my Google Assistant frantically tells me it’s still trying to reach Sweet Afton — Myriah finally picks up. This time, she listens intently without ever responding to the AI. “That was weird,” she says as she hangs up. “I’m a little freaked out.”

Google Duplex was arguably one of Google’s biggest announcements in 2018, but the AI faded into the background this week at the same I/O developer conference that introduced it just one year ago. Rather than an update on how the AI has been improved since its official release, Google CEO Sundar Pichai simply noted that the technology has gotten “great feedback,” then he quickly moved on to announce that Duplex would be coming to the web — no human impersonation necessary. Instead of the stunning voice-calling AI that garnered a mix of excitement and ethical criticism, Duplex is now also a fancy autofill tool that helps move you through a webpage to quickly reserve movie tickets and car rentals.

Duplex faded into the background at the same I/O conference where it was introduced a year ago

Much of Duplex’s work isn’t actually call-based. When you make a request with Google Assistant for a restaurant booking, it will search for vacancies through third-party booking platforms like OpenTable, Resy, or Yelp. If it’s able to do so, it’ll confirm the time with you and book it, or it’ll say it’s unable to complete the task if the requested times are not available. The Duplex calling feature only kicks in for restaurants that aren’t on those platforms and have opted into receiving calls from Google Assistant on their Google business page. Though Google opts businesses in by default, that still likely amounts to a fairly small percentage of businesses that have to regularly deal with Duplex calls, especially as the tech is still relatively new. And in cities where you’d typically need a reservation in the first place, most restaurants are already likely on web booking services. (Google did not respond to The Verge for a request for comment on how frequently Duplex calls are being placed.)

Online booking services can be expensive for some businesses. OpenTable’s cheapest plan starts at 25 cents per diner for each completed reservation, but it jumps to $249 a month plus a dollar for each diner seated if the restaurant wants the ability to text guests reservation reminders or update them on wait times. Resy and Yelp also start at $249 a month, with Resy charging as much as $899 a month for integration with the restaurant’s point of sale system, a dedicated account manager, and even Apple Watch alerts for when stock is running low or a VIP guest arrives. That type of investment isn’t always worth it for smaller, locally owned and run restaurants that mostly rely on foot traffic and would rather deal with walk-in customers.

“We’re a neighborhood spot, so we want people to just stop by whenever they’d like,” Myriah says.

But because she’s not always expecting calls about reservations, she was a bit thrown off when she heard Duplex’s opening monologue. When I asked if Duplex introduced itself as a robot and warned that the call might be recorded, Myriah says she can’t exactly recall. The pub can get so loud and busy that she “can’t hear every word that comes out of people’s mouths.”

During a particularly crowded Monday lunch rush, Bee Thanaraksalakul similarly didn’t have the free time to notice who — or rather, what — was calling. Thanaraksalakul, a waitress who works at my parents’ Thai restaurant in Queens, New York, hadn’t seen that the caller ID said Google Assistant on it when she picked up, and she doesn’t remember if the caller said she was being recorded. She did note, however, that she never would have guessed that she was speaking to an AI if I hadn’t told her about Duplex. That’s how realistic the voice sounded to her. “It even stuttered a little,” she says.

On the call that Myriah answered, she said the AI sounded like a young male. However, a few minutes after she hung up, Duplex called again to confirm the name on the reservation, but this time, it had a different voice. “This one was Irish!” she says, noting it was odd that the AI wouldn’t use the same voice to confirm the same reservation called in just moments prior.

Several other restaurant employees I spoke to for this piece agreed that the robot assistant sounded remarkably human. Server Shawn Watford of Birmingham, Alabama, said that while he hasn’t gotten customer calls from Duplex at his job, he has tried it on himself just to get a feel for what the AI sounded like. “I was spooked at how natural and human the machine sounds,” he says. “It was so weird [that] when it called, I immediately hung up.”

“It was so weird [that] when it called, I immediately hung up.”

Some employees were even skeptical about whether the voice on the other line is truly a robot. Gabriel Murphy, owner and chef at Gogi’s Restaurant in Jacksonville, Oregon, said he tried the AI out on his staff as he monitored the call in private. When he later told them it was Duplex, the team didn’t believe they were truly talking to an AI. “None of the staff seemed to have any issue with it, [but] there were plenty of jokes about Skynet and machines taking over,” Murphy says.

But Google’s machines don’t seem to be taking over yet. As the US continues to deal with an onslaught of spammy robocalls, it seems that many restaurant employees are inadvertently shielding themselves from Duplex by ignoring incoming calls that do not display a person’s name. Mark Seaman, a manager at two-year-old restaurant Queens Bully, in Forest Hills, New York, says he often tries to avoid calls from businesses that look like they could be pitching the restaurant on a product or service. “Most of our growth comes from our own social media efforts and the parties we throw,” Seaman tells me. “We get calls all the time from people trying to sell us something [we don’t need].”

Although Google does not personally call businesses to convince them to buy ads, it stands to reason why many restaurant employees would shy away from answering calls that list the company in its caller ID in the first place. As Google plans to extend Duplex beyond restaurants and into other appointment-based services like hair salons, it’ll have to do more to convince businesses that its robocalls, at least from the surface, are different than the ones most Americans are accustomed to.

When Duplex was first announced, one of my initial concerns was that the technology would further widen the gap between the privileged and the poor and weaken communication and empathy one party could have toward the other. Over the past few years, I’ve watched how apps like Seamless and Uber have seemingly entitled their users, encouraging them to appease their most demanding selves while seldom having to face or speak to the people fulfilling their needs. They’ve resulted in ads that highlight obnoxious requests from real food delivery orders that people hailed as “hilarious” and “clever.” And the infamous video of then-Uber CEO Travis Kalanick belittling a driver on his platform for asking why UberBlack prices had dropped.

One year later, Duplex is still limited and, at least for now, has minimal impact on service workers aside from occasionally freaking them out, but there is perhaps an unintended byproduct of AI replacing humans: politeness.

“It’s easier talking to [Duplex],” Thanaraksalakul says. English is her second language and working at the restaurant is where she gets to truly practice it. So when she repeats questions or misunderstands someone on the phone, the customer is not always patient with her. That misunderstanding can sometimes lead to an unpleasant experience when the customer shows up in person, or it can even result in a poor review online that talking to the AI can be an accidental reprieve.

Kindness is not always an expected interaction at work in the service industry, and it’s one thing Myriah also appreciates about the call from Google. “It was kind of creepy,” she says, “but it was very polite.”",Yes,Politeness.,https://www.theverge.com/2019/5/9/18538194/google-duplex-ai-restaurants-experiences-review-robocalls,2019-05-09 00:00:00,LABEL_1_relevant,0.9956383109092712,Voice Assistants,voice_assistant,The Verge,Social Norms & Relationships
40,Voice Control Will Force an Overhaul of the Whole Internet,"Jason Mars built his own Siri and then he gave it away.

Mars is a professor of a computer science at the University of Michigan. Working alongside several other university researchers, he recently built a digital assistant that could instantly respond to voice commands---much like Siri, the talking assistant offered on the Apple iPhone. Then he open sourced the thing, freely sharing the underlying code with the world at large.

Known as Sirius, the project is a way for all sorts of other software coders to explore the complexities of modern speech recognition, and perhaps even add speech rec to their own mobile apps. This, Jason Mars realizes, is where the world is moving.

But the project has another aim. Mars also realizes that the massive computing centers that underpin today's internet are ill-equipped for the coming voice revolution, and with his project, he hopes to show how these facilities must change. ""We want to understand how future data centers should be built,"" he says.

We want to understand how future data centers should be built. Jason Mars

You see, digital assistants like Siri and Google Now and Microsoft Cortana don't just run on your phone. They run across thousands of machines packed into these enormous computing centers, and as we extend such services to more and more people across the globe, we can't just run them on ordinary machines. That would take up far too much space and burn far too much energy. We need hardware that's significantly more efficient.

With their open source project, Mars and his colleagues, including a Michigan PhD student named Yunqi Zhang, can show how a tool like Siri operates inside the data center, and ultimately, they aim to identify the hardware best suited to running this kind of voice service---not to mention all the other artificially intelligent tools poised to remake the internet, from face recognition tools to self-driving cars.

Dwarfing Google Search

In testing Sirius, Mars has already shown if you run the service on traditional hardware, it requires about 168 times more machines, space, and power than a text-based search engine a la Google Search. When you consider that voice-recognition is the future of not only mobile phones but the ever growing array of wearable devices, from Apple Watch on down, that's completely impractical. ""We're going to hit a wall,"" Mars says. Data centers don't just take up space. They don't just cost enormous amounts of money to build. They burn enormous amounts of energy---and that costs even more money.

The big question is: What hardware will replace the traditional gear? It's a question that will affect not only the Apples and the Googles and the Microsofts and so many other app makers, but also the companies that sell data center hardware, most notably big-name chip makers like Intel and AMD. ""We're all over this,"" says Mark Papermaster, AMD's chief technology officer. ""It's huge for us and our future.""

Ultimately, that's why Mars is running his Sirius project. The Apples and Googles and the Microsoft know how this new breed of service operates, but the rest of the world doesn't. And they need to.

A Parallel Universe

Most web services, from Google's web search engine to Facebook's social network, run on basic server chips from Intel and AMD (mostly Intel). The problem is: these CPUs (central processing units) are ill-suited to a voice-recognizing services like Siri, which tend to run lots and lots of tiny calculations in parallel.

As companies like Google, Microsoft, and Chinese search giant Baidu have said, these calculations work better on simpler, less-power-hungry processors, such as the GPU (graphics processing unit) chips originally built for processing complex digital images, or on the FPGA (field programmable array) chips that can be programmed for specific tasks. Google is already using GPUs to power the brain-like ""neural networks"" that help drive its Siri-like service, Google Now. And Microsoft is using FPGAs to drive at least part of its Bing search engine.",Yes,"They will require a new breed of computer chip to power them, and as these chips evolve, they're likely to reshape the entire data center.

We're going to hit a wall. We're going to need a new type of computer chip. Jason Mars

To test this theory, Mars and his colleagues built their own artificial",https://www.wired.com/2015/03/voice-control-will-force-overhaul-whole-internet/,2015-03-24 11:00:20+00:00,LABEL_1_relevant,0.9924702644348145,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
41,Voiceitt aims to help stroke survivors talk to smart homes - The Verge,"Smart home tech could someday be updated with software that makes it easier for people with speech-related medical conditions to communicate with their devices. At CES, the Israeli startup Voiceitt said that it was incorporating its nonstandard speech recognition program into voice-activated devices. The addition could allow people who struggle to communicate to direct and use those systems.

“All indicators are pointing towards voice technology continuing to grow, so the accessibility component would not just be nice to have, but really important,” Sara Smolley, Voiceitt co-founder, told The Verge.

Voiceitt’s smart home system can’t understand everything people with nonstandard speech say. Users train the system with their own voice and with key phrases like “turn the light on,” which lets it learn each person’s specific vocal patterns.

Dozens of medical conditions, including cerebral palsy, Parkinson’s disease, and stroke, cause people to have trouble communicating: although they know what they want to say, they don’t have as much control over the muscles used to talk, making their speech hard to understand.

Many people who have these medical conditions also have motor challenges, and can’t move around on their own. They would benefit from voice-activated technologies like smart homes, but standard systems often can’t understand their speech. Improving popular smart devices’ accuracy with nonstandard speech is an important goal to help bring technology to people who may benefit most.

Voiceitt started with a translator application, currently in beta testing, that is designed to help people with nonstandard speech communicate with their friends, family, and caregivers. Users record themselves saying a variety of phrases. Then, the application uses the recordings and information gathered from its database of nonstandard speech samples to learn to interpret how they speak. When they speak into the app, it provides a synthesized audio translation as well as text on-screen.

Smolley says that Voiceitt has one of the largest existing databases of nonstandard voices. They started building it during an initial crowdsourcing campaign that asked people to donate their voices. The company plans to continue to add voices to the database as people use the app.

While the personal translator application is still in a beta version, the company wanted to expand the applications of their technology, Smolley says. “We learned that in addition to in person communication, we could do a lot more for people: we could help them talk to their machines,” she says.

Voiceitt has received funding from the Alexa Fund, Amazon’s venture capital fund for voice technology innovation, and has participated in accelerator programs with both Google and Amazon.

Additional reporting contributed by Dieter Bohn.",Yes,"They often don't understand people with speech impairments. This can be a big problem for people with speech impairments who want to use voice-activated devices like smart homes, as they may not be able to communicate with the devices. However, there is hope that voice assistants may someday be updated with software that makes it easier for people with speech",https://www.theverge.com/2020/1/8/21051618/voiceitt-stroke-survivors-talk-smart-homes-parkinsons-accessibility-ces-2020,2020-01-08 00:00:00,LABEL_1_relevant,0.9960641264915466,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
42,Ben 10 -- An Interview with Yuri Lowenthal,"I said it once and I'll say it again...Ben 10: Ominverse is pure awesomeness and this Saturday we get to meet a brand new alien in Ben's arsenal. This episode centers around the omnitrix and Ben's lack of being able to fight without it (do you see where I'm going with this?). The inevitable happens and Ben is forced to fight with his human abilities. The episode has the usual humor, banter and lessons that both Rook and Ben must learn in the end.

I was given the opportunity to speak with the voice of Ben Tennyson, Yuri Lowenthal about his role on the series and his career so far. Besides Ben 10, he's also been on some of my favorite shows including Young Justice (La'gaan / Lagoon Boy / Cameron Mahkent), Legion of Super Heroes (Superman / Kell-El / Senator Tolay) and Wolverine and the X-Men (Iceman, Bobby Drake).

GeekMom: Hi Yuri! Thank you for taking some time out today to answer some questions. My son and I love Ben 10 and we've enjoyed your work in Young Justice as well as Wolverine and the X-Men!

Yuri Lowenthal: First off, can I say that I’m a big fan. I don’t have kids yet, but I look to you as an example of how to raise them right. Long live the 501st!

GM: What's the process when working on a voice for a new character? Do you have a say in what you do or do you have to follow the director's lead?

YL: It usually starts with the director and any other creatives who may be involved at the start. It’s a collaboration. I bring what I have naturally, and, hopefully what they cast me for, and then we start playing and tweaking until we have what they feel’s right. It helps to have some artwork to inspire me, but I don’t always get that luxury. There’s a lot of trust involved. I follow the director’s lead because they generally know more about the big picture, but I also trust that the director will give me enough freedom to play.

GM: Do you ever get recognized in public for your voice?

YL: Only at conventions where they’re sort of expecting me. Never because people hear it and go, “Hey… You’re that guy!” It’s cool, because I don’t have to worry about the paparazzi…

GM: What inspires you when working on a new role?

YL: My experience, but also the people I’m working with. Like I said, it’s a collaboration. Also, because I’m a card-carrying nerdlinger, when I get involved in a really geeky project, like an adaptation of something I’m crazy about or that I grew up with, or when I get to work with someone I've always admired, that’s all I need to get me going.

GM: Do you have a character that you are fond of or attached to?

YL: I know it’s cliché, it’s hard to choose. They’re all bits of me. But there are a few that really stick with me. Superman, from the tragically short-lived Legion of Superheroes, Kuma from Afro Samurai, Sasuke from Naruto, The Prince from The Prince of Persia series, and Ben Tennyson from Ben 10. Sometimes I become attached to a character because I’ve gotten to explore him for so long, like in the case of Ben, but sometimes I fall in love with a throwaway character who exists for only one scene in a video game like The Drunken Villager in Diablo III or Sandal in Dragon Age.",Yes,They prepare us to accept a world in which we are constantly surveilled by algorithms.,https://www.wired.com/2012/11/ben-10-an-interview-with-yuri-lowenthal-1119/,2012-11-19 12:30:08+00:00,LABEL_1_relevant,0.985724687576294,Voice Assistants,voice_assistant,Wired,Security & Privacy
43,Google will pause listening to EU voice recordings while regulators ...,"Google has agreed to stop listening in and transcribing Google Assistant recordings for three months in Europe, according to German regulators.

German regulator calls voice assistants “highly risky”

In a statement released today, Germany’s data protection commissioner said the country was investigating after reports that contractors listen to audio captured by Google’s AI-powered Assistant to improve speech recognition. In the process, according to the reports, contractors found themselves listening to conversations accidentally recorded by products like the Google Home.

“The use of automatic speech assistants from providers such as Google, Apple and Amazon is proving to be highly risky for the privacy of those affected,” the German commissioner’s statement says, pointing to privacy concerns not only for device owners, but for visitors to homes with those devices. Google, according to the statement, will stop the practice of listening to and transcribing recordings for at least three months across the European Union, as the regulator looks into the issue.

A Google spokesperson said it had itself moved to pause “language reviews” while it investigated recent media leaks.

“We are in touch with the Hamburg data protection authority and are assessing how we conduct audio reviews and help our users understand how data is used,” the spokesperson said. “These reviews help make voice recognition systems more inclusive of different accents and dialects across languages. We don’t associate audio clips with user accounts during the review process, and only perform reviews for around 0.2% of all clips.”

The controversy over contractors has extended to other companies, as recent reports have highlighted how Apple and Amazon workers also listen to recordings to improve Siri and Alexa. In the statement, the German regulator writes that other speech assistant providers, including Apple and Amazon, are “invited” to “swiftly review” their policies.",Yes,Contractors may accidentally listen to private conversations.,https://www.theverge.com/2019/8/1/20750327/google-assistant-voice-recording-investigation-europe,2019-08-01 00:00:00,LABEL_1_relevant,0.9956875443458556,Voice Assistants,voice_assistant,The Verge,Security & Privacy
44,"Siri Remembers Your Secrets, But for How Long?","Not everyone realizes this, but whenever you use Siri, Apple's voice-controlled digital assistant, she remembers what you tell her.

How long does she remember? Apple isn't saying. And the American Civil Liberties Union is concerned.

A few years back, pressure from privacy advocates and the European Union forced Google, Yahoo, and Microsoft to clear up exactly what they were doing with search data. Google, for example, can save your web search history for ever, but it gives users the option to turn this off; then searches are partially ""anonymized"" after 18 months.

But if you search using Siri, that search query could be kept for a very long time. In the Siri's Privacy agreement, Apple says that it deletes your user data as well as the ""recent"" things that you've said to Siri whenever you slide the Siri switch off. But it keeps the older stuff, and it's not exactly clear how this process works.

""If you turn off Siri, Apple will delete your User Data, as well as your recent voice input data,"" Apple's privacy statement reads. ""Older voice input data that has been disassociated from you may be retained for a period of time to generally improve Siri and other Apple products and services.""

So your Siri queries may be stored anonymously by Apple for just a few months. Or they may not.

""It's not clear what 'disassociated' means. It's not clear what 'period of time' means. It's not clear what using it to 'generally improve Siri and other Apple products and services' means,"" says Nicole Ozer, a lawyer with the ACLU. ""The only thing that's clear is we really don't know what may be happening to the personal information we have told Siri, even after we turn Siri off.""

This matters, because the things that people say to Siri are often more personal than the search queries they type into Google or Bing. Siri is, after all, a personal digital assistant; someone you can dictate messages to. And if that information is stored on Apple's networks, it could be used by marketers, or by government investigators, or even subpoenaed in a civil suit years later.

And because talking to Siri feels just like talking to a mobile phone, many people don't realize that the software is recording things on faraway servers that they'd prefer to keep private, says Ozer, who blogged about Siri and privacy last year.

""People say very personal information to Siri,"" she says. ""They are encouraged to think of Siri as their confidante and their assistant. But Siri is not just working for you. Siri is collecting a lot of data for Apple and for its business. And people should be very mindful and aware of what's happening to their personal information; how it's being used; and be able to make informed decision about whether or not they want to be sharing certain information with Siri.""

What happens with everything that Siri learns is a big enough concern that last year IBM CIO Jeanette Horan told MIT’s Technology Review that she'd banned Siri outright on IBM's networks, worrying that what people said to Siri might be stored somewhere.

It has become a bit easier to find Siri's privacy policy, Ozer says. iPhone users can see it in their phone's settings when they go to switch the voice assistant on or off.

But Ozer still faults Apple for not being clearer about its data retention policies and for making it hard for consumers to find the Siri privacy policy. It is not, for example, linked on Apple's Siri FAQ page. In fact, it doesn't appear to exist anywhere on Apple's website.

""For folks who want to find out about Siri and about its policies before they own a new phone, as of today, I still couldn't find any link to the policy,"" she said Wednesday.

We reached an Apple spokeswoman via telephone, but she couldn't explain the company's data retention policies.

Google, which offers its own Voice Search software on Android phones, says that it anonymizes audio samples and then stores them for up to 24 months to improve its speech recognition software. ""We have no way of telling who spoke any particular query,"" the company said.",Yes.,"The fact that it robs people of their privacy. Because voice assistants are constantly listening to the user, they have the potential to hear personal information that the user would not want to share with anyone. The fact that these voice assistants are stored on faraway servers also raises the possibility that they could be accessed by marketers, government investigators, or even subpoen",https://www.wired.com/2013/04/siri-privacy/,2013-04-18 10:30:29+00:00,LABEL_1_relevant,0.9925937056541444,Voice Assistants,voice_assistant,Wired,Security & Privacy
45,"Yes, Alexa is recording mundane details of your life, and it’s creepy as hell","And that’s precisely why it’s terrifying: this sort of mundane chitchat is my mundane chitchat. I invited Alexa into our living room to make it easier to listen to Pandora and occasionally check the weather, not to keep a log of intimate family details or record my kid saying “Mommy, we going car” and forward it to Amazon’s cloud storage.

In the Amazon Alexa app (the iOS version is shown here), you can listen to and delete individual recordings. Hitting the Back button will bring you to the most recent one on the list.

Computing has marched toward ubiquity for years, and I’ve long been stuck between feeling excited and disturbed by the idea of having gadgets all around me that can track and do all kinds of things. As Amazon, Apple, and Google have ushered in their digital assistants over the past several years, I’ve rolled my eyes at some features and raised my eyebrows at others. But it wasn’t until this week that the reality set in—for me and, I’m guessing, for many other consumers—about what this constant accessibility truly means.

For example: there is a very real trade-off if you want to let technology decide when and what to listen to. Relying on wake words like “Alexa!” or “Hey, Siri” is sometimes wishful thinking. Bits of your private conversations may no longer be ephemeral, and it’s largely outside your control. And your kids may be accidentally triggering the smart speaker to start recording them, too.

I acknowledge my responsibility here as a consumer. I knew the array of seven microphones I had put in the center of my house could hear what we were saying and act on it. I also knew that things we asked Alexa to do were being recorded and sent to Amazon, and that I could play back these recordings and delete them if I wanted to.

But it’s actually quite frustrating to sort through them. You can scroll through months’ worth in the app, but after you select and listen to one, tapping the Back button brings you to the very top of the list again. Deleting hundreds of rogue recordings one by one in this way would take me a very long time. I could delete everything, including the legitimate recordings, in one go, but Amazon warns that this will make Alexa work less well, so of course I’m unlikely to do it.

I haven’t yet decided if Alexa will be leaving our home; I’m going to talk it over with my family first. But you can bet the Echo Dot’s microphone will be muted while we discuss it.",Yes,"They are constantly listening to you and sometimes accidentally record your private conversations. This can be very frustrating, especially if you have to go through and delete all of the recordings one by one.",https://www.technologyreview.com/2018/05/25/142713/yes-alexa-is-recording-mundane-details-of-your-life-and-its-creepy-as-hell/,2018-05-25 00:00:00,LABEL_1_relevant,0.9947683811187744,Voice Assistants,voice_assistant,MIT Tech Review,Security & Privacy
46,Apple's hired contractors are listening to your recorded Siri ...,"Apple is paying contractors to listen to recorded Siri conversations, according to a new report from The Guardian, with a former contractor revealing that workers have heard accidental recordings of users’ personal lives, including doctor’s appointments, addresses, and even possible drug deals.

According to that contractor, Siri interactions are sent to workers, who listen to the recording and are asked to grade it for a variety of factors, like whether the request was intentional or a false positive that accidentally triggered Siri, or if the response was helpful.

Apple isn’t very transparent about the recording process, or who listens to them

But Apple doesn’t really explicitly say that it has other humans listening to the recordings, and whatever admissions it does make to that end are likely buried deep in a privacy policy that few (if any) Siri users have ever read. Apple does note on its privacy page that “To help them recognize your pronunciation and provide better responses, certain information such as your name, contacts, music you listen to, and searches is sent to Apple servers using encrypted protocols,” but nowhere does it mention that human workers will be listening to and analyzing that data.

In a statement to The Guardian, the company acknowledged that “A small portion of Siri requests are analyzed to improve Siri and dictation. User requests are not associated with the user’s Apple ID. Siri responses are analyzed in secure facilities and all reviewers are under the obligation to adhere to Apple’s strict confidentiality requirements.” Apple also noted that less than 1 percent of daily activations are analyzed under this system.

The fact that humans are listening to voice assistant recordings in general isn’t exactly news — both Amazon (for Alexa) and Google (for Assistant) have been revealed to have similar systems where actual human workers listen to recorded conversations to better improve those systems. It makes sense: smart assistants obviously can’t tell the difference between false positives and actual queries (if they could, it wouldn’t be a false positive), and anyone who’s used a smart assistant can tell you that false positives are still very, very common at this stage of their evolution.

But for all three of these companies, it wasn’t clear up until recently the extent to which these companies were listening in on customers.

Apple’s system may also be more concerning for a few reasons, like the pervasiveness of Apple products. Where Alexa is largely limited to smart speakers, and Google Assistant to speakers and phones, Siri is also on Apple’s hugely popular Apple Watch, which is on millions of people’s wrists every waking moment. Plus, Siri on an Apple Watch activates any time a user raises their wrist, not just when it thinks it’s heard the “Hey, Siri” wake word phrase.

The pervasiveness of Siri on Apple hardware may make it worse than Google or Amazon

According to The Guardian’s source, that proliferation has led to some very personal conversations making their way to complete strangers working for Apple: “There have been countless instances of recordings featuring private discussions between doctors and patients, business deals, seemingly criminal dealings, sexual encounters and so on. These recordings are accompanied by user data showing location, contact details, and app data.”

Additionally, as The Guardian notes, while Amazon and Google allow customers to opt out of some uses of their recordings, Apple doesn’t offer a similar privacy protecting option, outside of disabling Siri entirely. That’s a particularly bad look, given that Apple has built so much of its reputation on selling itself as the privacy company that defends your data in ways that Google and Amazon don’t. Implicitly telling customers that, effectively, “the only way to have peace of mind that a random stranger won’t listen in on their accidentally triggered Siri recordings is to stop using Siri entirely” is a bit of a mixed message from the company that supposedly puts privacy at a premium.

Short of completely stopping use of smart assistants, there likely isn’t much that Siri customers will be able to do to avoid the issue, other than being careful what they say around their iPhones and HomePods (unless the public pressure here causes Apple to add an opt-out option). Still, it’s a good reminder that when you agree to use these products, you’re often giving up a lot more privacy than you think.",Yes,"They are often accidentally triggered, which can lead to embarrassing or private information being revealed to strangers.",https://www.theverge.com/2019/7/26/8932064/apple-siri-private-conversation-recording-explanation-alexa-google-assistant,2019-07-26 00:00:00,LABEL_1_relevant,0.9954793453216552,Voice Assistants,voice_assistant,The Verge,Security & Privacy
47,Us' voice assistant scene plays off a real 911 problem for smart ...,"If you haven’t seen Jordan Peele’s new horror movie Us, I apologize for spoiling one minor plot point: the film features a smart speaker. We’re going to be discussing much bigger spoilers below, so you might want to avoid reading further.

If you have seen Us, you probably remember “Ophelia,” the Amazon Alexa-like voice assistant that plays a key role in one of the film’s most darkly funny scenes. Ophelia is good at playing music, but not great at interpreting commands — which becomes a problem when a character frantically tells Ophelia to call the police, only to discover that Ophelia has misheard an order to play N.W.A.’s “Fuck Tha Police” instead.

Smart speakers — even ones that let people make phone calls — generally don’t support 911

The Ophelia speaker helps establish its owners, the mostly unsympathetic Tylers, as a yuppie family with a penchant for useless high-tech toys. In Us, the joke is that Ophelia can’t understand the Tylers’ requests or the fact that they’re fighting for their lives against a group of doppelgängers. If it worked like the Echo or Google Home, though, Ophelia would have been useless even if it understood the request perfectly. Most smart speakers can’t call the police, and there’s been a long debate over whether they should.

Smart speakers — even ones that let people make phone calls — generally don’t support emergency or 911 calling for technical and regulatory reasons. As The Wall Street Journal explained last year, devices like Alexa and Google Home don’t offer the same location details as a mobile or landline phone, making it harder to route and respond to emergency calls. They also generally don’t have their own callback numbers, and their manufacturers don’t pay the monthly 911 surcharge that’s tacked onto ordinary phone bills. (Services like Skype usually can’t place emergency calls for the same reasons.)

There’s huge promise in a system that lets people dial emergency numbers when they can’t reach a phone — especially for people with mobility issues. But adding 911 service would require clearing some FCC hurdles, and the Journal wrote that spotty internet service could make the service unreliable. Most people already have phones, and in many cases, they’re better off using a system that’s known to work than experimenting with something new.

Smart speakers also raise serious privacy concerns. They collect huge amounts of data that can be requisitioned by law enforcement — late last year, a New Hampshire judge ordered Amazon to turn over Echo recordings related to a double murder case. Amazon has also sent one user’s Echo voice recordings to someone else, and the Echo has recorded private conversations after being accidentally activated. Wired has speculated that smart speakers could end up being subject to “mission creep” if they’re linked to emergency services, expanding from simple 911 calls to preemptively detecting dangerous situations.

Smart speakers can tell you the numbers for intimate partner violence or suicide hotlines, even if they can’t directly make a call

For now, though, that particular concern seems far away. It’s true that in at least one incident, Albuquerque police publicly thanked Alexa for calling emergency services. But experts said this would have required an unusual and convoluted series of events, and the police admitted that they weren’t sure exactly what had transpired.

There are some exceptions and workarounds. The Amazon Echo Connect specifically hooks into a landline and acts as a voice-controlled speakerphone system. Alexa skills like “Ask My Buddy” can let you alert a friend who can call 911 for you. Siri can dial emergency services on the iPhone — sometimes a bit too easily, as many people who have accidentally told it to dial an emergency number found out. And smart speakers can tell you the numbers for intimate partner violence or suicide hotlines, even if they can’t directly make a call.

“Ophelia” isn’t Alexa — maybe because using a real smart speaker in an unflattering light could create legal hassle, or maybe because it would have felt too much like splicing an Amazon commercial into the middle of the movie. And in fairness, when I asked an Echo to call the police last night, it just looked for a (nonexistent) phone contact called “Police” instead of blasting “Straight Outta Compton.”

But Ophelia mirrors the false sense of security that real smart speakers can provide, just as horror films often play on our concerns about modern technology. In the Albuquerque incident above, a recording apparently did capture someone asking Alexa to call 911. It’s just that Alexa, as far as we know, wasn’t in a position to help.",Yes,It gives people a false sense of security.,https://www.theverge.com/2019/3/26/18281387/us-2019-movie-jordan-peele-voice-assistant-ophelia-911,2019-03-26 00:00:00,LABEL_1_relevant,0.9963536262512208,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
48,What's Worse Than Foreign Election Interference? QAnon,"Last week, US officials determined that a threatening letter sent to a voter from a fake Proud Boys email address was the work of the Iranian government, adding more emphasis to US intelligence reports that countries like Iran and Russia seek to disrupt US elections.

It also seemed like a strange, if not ill-fitting, example to focus on. Are blatantly fake emails from an adversarial government actually representative of election meddling in 2020?

WIRED OPINION ABOUT Rita Katz (@Rita_Katz) is the executive director and founder of the SITE Intelligence Group, the world’s leading nongovernmental counterterrorism organization specializing in tracking and analyzing the online activity of the global extremist community.

Not by a long shot. The more threatening type of election interference is that which is, by design, hard to identify—even when you’re looking right at it. Something like what we now see in hindsight from the 2016 Election: manipulation of existing sentiments and movements, prolific propaganda machines, distorted news reports, online trolls who blend into the fray. In essence, what we’re now seeing with QAnon.

What Is Keeping QAnon Alive?

Pound for pound, there isn’t a more fitting movement than QAnon for a state actor to zero in on: It is ideologically malleable, it's driven by misinformation, and its adherents embrace violence and voter intimidation. Indeed, social media companies like Facebook and YouTube play major roles in QAnon’s growth, and their newly aggressive stances against the movement mark steps in the right direction.

However, as I’ve said before, apart from these social medial companies, QAnon has a lot to fall back on. The heart of QAnon’s web presence is kept alive by complicated, entangled arrays of tech companies. And while some more reputable companies do so unwittingly, networks of far shadier companies—shadowy cabals, if you will—do so very willingly.

The story of 8kun is a perfect case study.

For about a year and a half, “Q drops”—the cryptic messages from supposed government insider “Q” that launched the QAnon movement—were posted exclusively on a site called 8chan. The platform was owned by a man in his mid-50s named Jim Watkins, whom multiple journalistic investigations suggest to either be or have contact with the person behind the “Q” alias. But after August 3, 2019, when Patrick Crusius, who’s been charged for killing 23 people in El Paso mass shooting, followed suit with other far right attackers by uploading his manifesto to 8chan, public outcry grew against the site. Cloudflare, the service provider that once defended its hosting of 8chan, finally changed its position and dropped the site. With that, all messages from “Q” ceased.

If he wanted to revive 8chan, Watkins needed to link up with a company that wouldn’t cave to public pressure. So, in the two months following the El Paso attack, Watkins established connections with a web hosting company called VanwaTech and, by October, he launched 8chan’s successor message board: 8kun. Sure enough, the same day 8chan reemerged as 8kun, “Q” messages resumed.

Watkins was also setting up other projects around 8kun during this time. In September 2019, he launched a company called Is it Wet Yet. In a press release in early November, Is it Wet Yet was characterized as “an information hub for all the projects we’ve been working on and a launchpad for future endeavors,” including 8kun. Less recognized than its 8kun counterpart, Is it Wet Yet was even able to register with Cloudflare, the company that ditched Watkins’ 8chan just a couple months earlier.

Meanwhile, VanwaTech provided 8kun the server space and protection from DDoS attacks it needed. VanwaTech is based in Washington state and headed by a young man named Nick Lim, who has carved a niche for his small company: hosting the worst parts of the Internet. Several of its domains are unsurprisingly registered under a company called Epik, whose founder, Robert Monster, has registered similarly problematic sites over the years. Among them was Gab, the notorious platform where Robert Bowers, who’s been charged with killing 11 in the 2018 Pittsburgh synagogue shooting, festered in radicalization. Monster defended Gab in a blog post less than a week after the event.",Yes,"They are providing a real-time window into our everyday lives, which has both positive and negative implications. On the one hand, they are creating a new form of communication that is more natural and efficient. On the other hand, they are also raising privacy concerns and giving companies and governments a way to collect more data about us.",https://www.wired.com/story/opinion-whats-worse-than-foreign-election-interference-qanon/,2020-10-30 16:00:00+00:00,LABEL_1_relevant,0.9958833456039428,Voice Assistants,voice_assistant,Wired,Security & Privacy
49,Amazon’s Echo Look Rates Your Outfits and Slurps Up Revealing Data,"How that will work if you’re a true fashionista pushing the envelope of style or a dullard whose largest clothing choice is between a pale blue or off-white button down? We’re not so sure. But what is certain is that the device is part of Amazon’s continued push into the fashion market—which some analysts reckon could make it America’s top clothing retailer this year.

The device is fun, and only $20 more expensive than the camera-free Echo speaker, so it will, like its siblings, undoubtedly prove popular. But it’s interesting to think about what the addition of eyes to Alexa’s sensory gamut will mean for Amazon. Beyond hacking concerns raised by placing a connected camera in the location where you dress—and, presumably, undress—there’s the bigger question of what the company itself does with your data.

We asked Amazon, and it confirmed that the images and video gathered by Echo Look will be stored on the company's cloud. That’s also where the AI processing of images will be performed, too. That’s much the same as the way audio is handled by every other device in the Alexa range, with recordings started and stored away to servers every time the assistant hears its name.

That’s great news for Amazon. As our own Tom Simonite has reported, the huge quantities of data supplied by people making voice commands to their Echo are enabling the firm to make impressive breakthroughs in what voice assistants can do. Adding a camera means Amazon will be able to collect huge troves of visual data that it can analyze to determine your tastes in color and style, make recommendations, and then learn from how you respond.",Yes,"They are building up huge databases of users' personal information, which could be used to influence them in undesirable ways in the future.",https://www.technologyreview.com/2017/04/27/68516/amazons-echo-look-rates-your-outfits-and-slurps-up-revealing-data/,2017-04-27 00:00:00,LABEL_1_relevant,0.9955040812492372,Voice Assistants,voice_assistant,MIT Tech Review,Security & Privacy
50,"Apple contractors hear confidential details from Siri, a whistleblower claims","Those who work on quality control for Apple’s Siri voice assistant “regularly hear confidential details” about users, according to a contractor paid to assess responses to Siri recordings.

The news: The whistleblower told the Guardian these workers routinely hear sensitive information like drug deals, confidential medical details, and people having sex.

Why are they listening in the first place? Just like Amazon and Google, Apple employs people to listen to a sample of recordings from people’s conversations with Siri, transcribe them, and grade the responses according to a set of criteria. These include whether the voice assistant was activated deliberately or not, whether Siri could help with the query, and whether its response was appropriate.

However: Apple, again like Amazon and Google, does not explicitly disclose that it is doing this in its consumer terms and conditions (which are virtually unreadable, anyway). Apple likes to pride itself on being a privacy-conscious company, so this revelation may be more damaging for it than for other firms. Unlike the other two companies, Apple provides no way for users to opt out of their recordings being used this way, other than to just not use Siri at all. Apple told the Guardian that fewer than 1% of Siri recordings are used for training and that they are not associated with a user’s Apple ID.

Do consumers care? There’s been some online outrage about this practice and the fact it’s done without customer consent (and so could be illegal within the European Union), but adoption of voice assistant technology shows no sign of slowing.",Yes,"The people who work on quality control for them regularly hear confidential details about users, like drug deals and people having sex. This is because the assistants are often activated unintentionally, and so the workers have to transcribe and grade the responses according to a set of criteria. However, consumers largely don't seem to care about this, and the usage of",https://www.technologyreview.com/2019/07/29/134008/apple-contractors-hear-confidential-details-from-siri-recordings/,2019-07-29 00:00:00,LABEL_1_relevant,0.9963080883026124,Voice Assistants,voice_assistant,MIT Tech Review,Social Norms & Relationships
51,Video Shows Siri Works Great In Real Life,"Siri, Apple's ""one more thing"" at last week's iPhone 4S launch, looks like some kind of magic from the future. But then, demoes of speech control on the Mac also looked pretty good (yes, you can control your Mac with your voice – kinda). Up on stage, with just the right commands, Siri was bound to make a great demo. But how will it do in the real world?

To see, check out this hands-on video from the folks at UK-based mag Stuff.

Content This content can also be viewed on the site it originates from.

I'm sure the outtakes have been, ahem, taken out, but what remains is impressive. Not only does it look accurate, but it also looks useful. I can totally see myself mumbling drunkenly to Siri to please, please wake me up in the morning, or sending a quick message to somebody.

Other hands-on reviews around the web also point to another way to activate Siri. Instead of touching the button on screen, you can just hold the phone up to your ear. This way you'll look like you're talking to somebody, instead of dorking out with your little iHal.

But most impressive is that Siri can handle different accents, something that the iOS Dragon apps fail miserably at. We'll have to see how it copes with other British accents though. In the U.S, English accents are fairly homogenous. In the British Isles, though, you run from Scottish to Irish to Geordie to Black Country to (shudder) Welsh. Some natives have trouble understanding these accents, so I don't have much hope for a computer.

I do like the posh-accented Butler voice used in the British English version of Siri, though.

Still, we'll find out soon enough when folks start to receive their handsets this Friday and start commanding Siri to do all sorts of crazy things. Me? I'll do what I always have done whenever possible, just like I still do when I get a new dictionary. I'll start with the swear words.

Apple iPhone 4S Siri demo [Stuff TV/YouTube]

See Also:",Yes,They will probably make us all look like idiots.,https://www.wired.com/2011/10/video-shows-siri-works-great-in-real-life/,2011-10-11 11:14:45+00:00,LABEL_1_relevant,0.994782030582428,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
52,Apple's Latest Deal Shows How AI Is Moving Right Onto Devices,"Apple dropped $200 million this week on a company that makes lightweight artificial intelligence. It’s all about keeping an edge in AI ... by adding more AI to the edge.

The acquisition of Xnor.ai, a Seattle startup working on low-power machine learning software and hardware, points to a key AI battleground for Apple and other tech heavyweights—packing ever-more intelligence into smartphones, smartwatches, and other smart devices that do computing on the “edge” rather that in the cloud. And doing it without killing your battery.

“Machine learning is going to happen at the edge in a big way,” predicts Subhasish Mitra, a professor at Stanford who is working on low-power chips for AI. “The big question is how do you do it efficiently? That requires new hardware technology and design. And, at the same time, new algorithms as well.”

The most powerful AI algorithms tend to be large and very power hungry when run on general purpose chips. But a growing number of startups, Xnor.ai among them, have begun devising ways to pare down AI models and run them on extremely energy-efficient, highly specialized hardware.

Last March, Xnor.ai demoed a computer chip capable of running image recognition using only the power from a solar cell. A research paper authored by the founders of Xnor.ai and posted online in 2016 describes a more efficient form of convolutional neural network, a machine learning tool that is particularly well suited to visual tasks. The researchers reduced the size of the network by essentially creating a simplified approximation of the interplay among its layers.

Keep Reading The latest on artificial intelligence , from machine learning to computer vision and more

Apple already makes chips that perform certain AI tasks, like recognizing the wake phrase “Hey, Siri.” But its hardware will need to become more capable without draining your battery. Apple did not respond to a request for comment.

Now, AI on the edge means running pretrained models that do a specific task, such as recognizing a face in a video or a voice in a call. But Mitra says it may not be long before we see edge devices that learn too. This could let a smartphone or another device improve its performance over time, without sending anything to the cloud. “That would be truly exciting,” he says. “Today most devices are essentially dumb.”

Applying AI to video more efficiently, as Xnor.ai has demoed, will also be key for Apple, Google, and anyone working in mobile computing. Cameras and related software are a key selling point for iPhones and other smartphones, and video-heavy apps like TikTok are popular among younger smartphone customers. Edge computing has the added benefit of keeping personal data on your device, instead of sending it to the cloud.

Dave Schubmehl, an analyst with the research firm IDC, says machine learning could also be used in Apple gadgets that currently don't include AI. ""I can see them running AI on the Apple Watch and in AirPods, to clean up sound for example,"" he says. ""There's tremendous opportunity in existing products.""

Running sophisticated AI on video, like an algorithm that can tell what’s happening in a scene or add complex special effects, is usually done in the cloud because it requires a significant amount of computer power. “For example, adding synthetic depth of field to your photos might require running a deep network to estimate the depth of each pixel,” says James Hays, a professor at Georgia Tech who specializes in computer vision.

Besides making your iPhone’s camera smarter, Xnor.ai’s technology could help Apple in other areas. Giving machines more ability to perceive and understand the messy real world will be key to robotics, autonomous driving, and natural language understanding.

“If the goal of AI is to achieve human-level intelligence, reasoning about images is vital to that,” Hays says, noting that roughly a third of the human brain is dedicated to visual processing. “Evolution seems to consider vision vital to intelligence,” he says.

Apple seems to think that a more evolved form of computer vision is pretty valuable too.

More Great WIRED Stories",,"They're turning us all into babbling idiots 

⚡️ A complete guide to the new iPhone

🏃🏽‍♀️ Want to work out from home? Here are the most effective exercises for each body part

🎮 Long Read: The real history of the fake retro video",https://www.wired.com/story/apples-deal-shows-ai-moving-devices/,2020-01-17 12:00:00+00:00,LABEL_1_relevant,0.9946411848068236,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
53,Google fixes issue with the Assistant's white noise sound that had ...,"Google offers many ambient sounds from the Google Assistant, including one for white noise that might help block out noise to make it easier to sleep or study. The company seemingly changed its white noise sound last week, and many expressed frustration with the update, as reported by 9to5Google. But Google tells us it has fixed what it describes as an issue, and the sound should work like it used to.

“There was an issue impacting our white noise experience. It’s fixed now and working as it previously did,” Google spokesperson Robert Ferrara said in a statement to The Verge.

The change had been disruptive for a lot of people, as documented in this Google Nest community thread which had more than 150 replies when we first published this story on Monday. Some described the changed white noise as quieter or sounding muffled. One person said their child has asked to use another white noise generator instead of a Google Nest Hub. Another said their toddler has observed the change and wakes up during the night.

To help those that wanted the original white noise back before Google fixed it, a user on Reddit uploaded a one-hour track of original one and longer versions of it to Google Drive that would have theoretically used as workarounds. Google also offers other ambient sounds, like forest sounds and rain sounds, and you can see the full list here.

Update January 18th, 7:12PM ET: Added statement from Google saying the issue was fixed.",Yes,"They have been known to eavesdrop on conversations. The most common way that this occurs is when the assistant is accidentally activated by a word or phrase in conversation that sounds similar to the wake word. In addition, some devices have been known to activate when they hear other sounds that are similar to the wake word. This can be a problem if the",https://www.theverge.com/2022/1/17/22888423/google-assistant-white-noise-sound-changed,2022-01-17 00:00:00,LABEL_1_relevant,0.995819926261902,Voice Assistants,voice_assistant,The Verge,Security & Privacy
54,A New Trick Can Spoof a Speech Recognition AI Every Time,"Given an audio waveform, researchers can now produce a virtually identical version that makes speech-recognition software transcribe something else entirely.

Backstory: Adversarial examples have fooled plenty of computer-vision algorithms. While all neural networks are susceptible to such attacks, researchers have had less success with audio. Previous attacks were only able to make subtle tweaks to what the software hears.

What’s new: Berkeley researchers showed that they can take a waveform and add a layer of noise that fools DeepSpeech, a state-of-the-art speech-to-text AI, every time. The technique can make music sound like arbitrary speech to the AI, or obscure voices so they aren’t transcribed.

Brace for annoyance: Imagine playing a music video from YouTube on your speakers and having Alexa “hear” an order for two tons of creamed corn. Welcome to AI attack hell.",Yes,"They can be easily fooled by adversarial examples. This means that someone could play a music video on their speakers and have the voice assistant transcribe an entirely different message, such as an order for two tons of creamed corn. This could obviously lead to a lot of annoyance and confusion.",https://www.technologyreview.com/2018/01/10/146281/a-new-trick-can-spoof-a-speech-recognition-ai-every-time/,2018-01-10 00:00:00,LABEL_1_relevant,0.9964981079101562,Voice Assistants,voice_assistant,MIT Tech Review,User Experience & Entertainment
55,Death Cuts the Degree of Separation Between You and Covid-19,"One of the first things they teach wannabe epidemiologists is the shape of the exponential growth curve—how epidemics spread slowly at first, and then take off like a rocketship as the numbers of infected people double, double, and double. But unless you’re actually fighting an outbreak, or are in one, that can all feel academic. If the exponential liftoff is happening somewhere else, it’s not happening to you. Add to that peculiar form of emotional distancing a heavy dose of disinformation and partisanship, and depending on where you live and what your information diet is like, even the Covid-19 pandemic could start to seem almost unreal.

In a crisis, health communication experts agree, different kinds of people need to hear different kinds of narratives about what’s going on. Broadly, truthful information delivered clearly and without panic—but also without undue optimism—is the way to maintain credibility. But some people in the audience need a more emotional connection to fully engage. According to survey data from the Pew Research Center, nine out of 10 Americans say that Covid-19 has affected their lives in some way. But that means 10 percent of Americans say it hasn’t. Pew also finds that almost 80 percent of Fox News viewers think the media has exaggerated the threat of the virus (we haven’t) and 7 percent of people aren’t really following the news about it at all. Even President Trump seems to think that only people who “sadly lost a family member or friend” will even remember Covid-19 when it’s all over. These numbers are fast-changing, but still, watching the pandemic unfold as news from a distant city or as lines and numbers on charts might be very different than being in a hotspot, or knowing someone who dies from the disease.

How Long Does the Coronavirus Live on Surfaces? Plus: What it means to “flatten the curve,” and everything else you need to know about the coronavirus.

That raises a grim question: What’ll it take before everyone personally knows someone who died from Covid-19? As of this writing, more than 11,800 people have died from it in the United States. (And that death toll may be an undercount.) If knowing one of those people would make the pandemic concrete for someone—real and actionable—how many have to die before every American knows one of the dead?

In a time of relentlessly cold equations, this one may be the coldest. It also turns out to be hard to solve.

The easiest answer, the back-of-the-envelope sketch, requires figuring out how many people anyone in the US is likely to know. Take one death from a group of that size, then multiply whatever percentage that is by the total US population. So, for example, one readily-accepted count of how many people any one person knows comes from a rigorous analysis of Facebook data from 2011. It’s about 200. Using this rough metric, if 1 in every 200 Americans dies from Covid-19—a fatality rate of 0.5 percent—everyone in the US will know someone who has perished. With a national population of 327.2 million, that’s 1.6 million US deaths.

But we get into the weeds right away. Is 200 … right? A 2006 paper using survey data and statistical models to calculate the number of people the average person knows arrived at 750. The Columbia University statistician Andrew Gelman, one of the authors of that paper, came back in 2013 with another estimate based on a different survey: 600.",Yes,"…

they are always listening to us and collecting data about us. This data can be used to target ads and sell products, and it can also be used to track our movements and spy on us.",https://www.wired.com/story/death-cuts-the-degree-of-separation-between-you-and-covid-19/,2020-04-09 11:00:00+00:00,LABEL_1_relevant,0.9946478009223938,Voice Assistants,voice_assistant,Wired,Security & Privacy
56,Google Translate Adds Live Speech Translation to Android,"Google has just just added a little bit of Star Trek to Android. An update to the Google Translate app adds ""Conversation Mode"", which acts a lot like the universal translator from the TV show.

In Conversation Mode you speak into your phone, Google translates your words into the language of your choice, and then the phone reads out the results in a pleasant robot voice. Here's a demo of a prototype version recorded a few months ago:

Content This content can also be viewed on the site it originates from.

It's not perfect, but it's impressive nonetheless. The app will translate from 15 languages, and output the results in any of 53 languages. And there's good news for owners of older phones: Google Translate works on Android 2.1 and better.

It probably won't replace a little bit of study before you go on vacation, but as most native English-speakers seems morally opposed to learning another language, it certainly won't hurt.

A new look for Google Translate for Android [Google Blog]

See Also:",Yes,"They often unintentionally record our conversations, which raises serious privacy concerns. The new Google Translate app includes a feature called ""Conversation Mode"" which allows users to speak into their phone and have the app translate their words into another language. While this is a useful feature, it also raises concerns about privacy, as the app will unintentionally record conversations",https://www.wired.com/2011/01/google-translate-adds-live-speech-translation-to-android/,2011-01-13 14:24:06+00:00,LABEL_1_relevant,0.994239330291748,Voice Assistants,voice_assistant,Wired,Security & Privacy
57,"Pepper, the Emotional Robot, Learns How to Feel Like an American","Pepper is about four feet tall, looks like a person (except for the wheels where its legs should be), and has more emotional intelligence than your average toddler. It uses facial recognition to pick up on sadness or hostility, voice recognition to hear concern...and it's actually pretty good at all that. Over 7,000 Peppers greet guests, answer questions, and play with kids in Japanese homes. And by the end of the year it'll be on sale in the US---but not before software engineers here get a crack at remaking its soul.

Softbank Robotics, Pepper's maker, knows that emotional interactions in the US won't look the same as they do in Japan. So in conjunction with Google---as the companies announced at Google's developer conference in May---Softbank is opening Pepper's software developer kit. That's right: It's an android you can program in Android.

Robots are getting more emotive in general. Jibo, a tabletop digital helper---think of a more charming Amazon Echo---understands colloquial speech, expresses a range of emotions, and even develops its own opinions. The Parorobot substitutes for puppies and kittens in animal therapy sessions in extended care facilities where live animals would pose logistical difficulties, and some elementary schools are testing robots to help teach kids with special needs.

But Pepper stands out---literally. It's humanoid, mobile, and has a tablet display as well as the ability to speak. And it genuinely seems to want to please people.

That won't be easy. ""We tend to treat robots different from other appliances, particularly when they have this anthropomorphic form,"" says Kate Darling, a specialist in human-robot interaction at MIT. In other words, robots that look like people (or animals) get treated like people (or animals). ""We see people treating these machines like social actors,"" Darling says.

On the plus side, that might mean people will remember their manners when they deal with machines. After all, violence against a robot probably doesn't hurt the robot, but it might make the perpetrator into kind of a jerk. Just as parents have long raised concern about the effect of violent video games, Darling says violence against robots might desensitize kids to violence against people too. Earlier this year a parent wrote about how his child’s behavior changed in response to using Alexa, Amazon’s digital assistant. Alexa doesn’t require ""please"" or ""thank you"" to process commands, which he said was making his child rude to other people as well.

Sure, maintaining emotional decorum with a robot could make for a pretty weird world. It's...perhaps inauthentic. ""We have to be careful because from early ages, children experience performances of care as though they were care,"" says Sherry Turkle, director of the MIT Initiative on Technology and Self.

Pepper is designed to comfort people when it senses sadness and do something silly when it senses those around it are playful. That's real human interaction---but no one thinks Pepper is sympathetic, or funny. At least, not yet.

Programming Pepper

Pepper isn't a butler. It can't vacuum or fold fitted sheets (though be honest: Neither can you, right?). Its humanoid shape is supposed to make it easier to express emotions to. “We designed Pepper’s form to incentivize engagement,"" says Steve Carlin, vice president of SoftBank Robotics America. ""Its height, shape, the fact that it has arms that can gesticulate---are all designed to show empathy.""

Exactly how to turn all that physicality into empathy isn't clear from the SDK alone; SoftBank engineers say they'll have forums for would-be Pepper-programmers to ask what kind of gesture or language conveys the right tone for, say, assisting someone in a car dealership or a grocery store. Right now, the SDK lets coders plan out moves and language and watch the robot execute in an animated sandbox. So at least it gets to practice.

And it'll need to. Roboticists expect that US homes are going to be more skeptical of having robots around the house than Japanese society has been. Maybe because Americans worry that no matter how much they teach the robots, the robots are going to teach them, too.",Yes,"They train children to be rude. Parents have long raised concern about the effect of violent video games, Darling says violence against robots might desensitize kids to violence against people too. Earlier this year, a parent wrote about how his child’s behavior changed in response to using Alexa, Amazon’s digital assistant. Alexa doesn",https://www.wired.com/2016/06/pepper-emotional-robot-learns-feel-like-american/,2016-06-07 16:00:51+00:00,LABEL_1_relevant,0.9959145188331604,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
58,Yahoo/Siri App Is Actually A Real Prototype Built By Robin Labs But Not Commissioned By Yahoo,"A little earlier we ran a story about a video that showed what appeared to be a Yahoo Siri competitor. “Sources familiar with Yahoo’s internal projects tell us that the video is fake,” we wrote. Well, it turns out that this isn’t the whole story. It’s true that Yahoo didn’t make the video itself, and it didn’t make the app, but that doesn’t mean Yahoo isn’t involved. The app was made by a natural language/speech recognition/navigation startup Robin Labs, in discussion with Yahoo.

Here’s the backstory as told to me by Ilya Eckstein, the co-founder and CEO of Robin Labs:

Robin Labs has been building a white-label platform for custom voice assistants. It’s the second stage of the startup’s development, the first of which was to develop its own eponymous personal assistant and navigation app. (That app launched last year.) As for stage two, think of it as Siri for everyone and everything, not just Apple. “The idea is that brands could offer their content to their audiences, wrapped in voice-interactive UI,” Eckstein says.

It turns out that Yahoo is one such brand, and appears to have been behind the development of the white-label platform in the first place. “We’ve been in conversations with them,” he says. “But our white-label platform was not commissioned by Yahoo.”

The Yahoo-branded prototype in the video, powered by the Robin platform, was created as part of those discussions. “Somehow, the video has leaked out after we’d sent it to Yahoo,” he says.

In fact, although Android Police published a link to the video on Dailymotion today, it’s been on at least one other video network since December 4, if you knew where to look. We had already seen it and were trying to get to the bottom of the story before Android Police published its post today.

To be honest, I had thought originally that Yahoo might be trying to buy Robin Labs. Eckstein would not comment on that. “We are having an ongoing conversation with Yahoo,” he told me, and he says that has been with senior people both in engineering and corporate. “That’s as much as I can say about it.”

It’s not too left-field to wonder if Robin Labs could be another acquisition target for CEO Marissa Mayer and Yahoo: Yahoo’s been on a long-term buying spree of startups to pick up technology, talent (and sometimes users). Robin Labs fits thematically with other acquisitions Yahoo has made in natural language processing (SkyPhrase) and mobile assistant apps (a href=”https://beta.techcrunch.com/2013/05/01/yahoo-acquires-to-do-app-astrid/”>Astrid). Robin the app today has seen some 1 million downloads. Today that service focuses on navigation and a few other services but has ambitions to extend out to other areas as a platform for personal assistants across different verticals such as “a news concierge, a communication / scheduling assistant, and assistant in the kitchen,” according to Eckstein.

Asked if Robin Labs had approached Yahoo for this prototype, or if it was the other way around, Eckstein responded with a saying: “Great minds think alike.”

He’s not very happy about how the videos leaked out in the first place, it seems, but is now just trying to accept it as a bit of “unexpected market research.” If you look at some of the readers’ comments, the idea does strike a chord with people, and we are pleased to see that. That actually may be one good thing to come out of that debacle.”

We’ve reached out to Yahoo and will update this post as we learn more…",Yes.,"We have to become, as a society, much better speakers.",https://techcrunch.com/2013/12/18/yahoosiri-app-is-actually-a-real-prototype-built-by-robin-labs-not-yahoo/,2013-12-18 00:00:00,LABEL_1_relevant,0.9951613545417786,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
59,The AI Takeover Is Coming. Let’s Embrace It.,"On Tuesday, the White House released a chilling report on AI and the economy. It began by positing that “it is to be expected that machines will continue to reach and exceed human performance on more and more tasks,” and it warned of massive job losses. Yet to counter this threat, the government makes a recommendation that may sound absurd: we have to increase investment in AI. The risk to productivity and the US’s competitive advantage is too high to do anything but double down on it.

Sign up to get Backchannel's weekly newsletter, and follow us on Facebook, Twitter, and Instagram.

This approach not only makes sense, but also is the only approach that makes sense. It’s easy — and justified — to worry about the millions of individual careers that something like self-driving cars and trucks will retool, but we also have chasms of need that machine learning could help fill. Our medical system is deeply flawed; intelligent agents could spread affordable, high-quality healthcare to more people in more places. Our education infrastructure is not adequately preparing students for the looming economic upheaval; here, too, AI systems could chip in where teachers are spread too thin. We might gain energy independence by developing much smarter infrastructure, as Google subsidiary DeepMind did for its parent company’s power usage. The opportunities are too great to ignore.

More important, we have to think beyond narrow classes of threatened jobs, because today’s AI leaders—at Google and elsewhere—are already laying the groundwork for an even more ambitious vision, the former pipe dream that is general artificial intelligence.

To visit the front lines of the great AI takeover is to observe machine learning systems routinely drubbing humans in narrow, circumscribed domains. This year, many of the most visible contestants in AI’s face-off with humanity have emerged from Google. In March, the world’s top Go player weathered a humbling defeat against DeepMind’s AlphaGo. Researchers at DeepMind also produced a system that can lip-read videos with an accuracy that leaves humans in the dust. A few weeks ago, Google computer scientists working with medical researchers reported an algorithm that can detect diabetic retinopathy in images of the eye as well as an ophthalmologist can. It’s an early step toward a goal many companies are now chasing: to assist doctors by automating the analysis of medical scans.

Also this fall, Microsoft unveiled a system that can transcribe human speech with greater accuracy than professional stenographers. Speech recognition is the basis of systems like Cortana, Alexa, and Siri, and matching human performance in this task has been a goal for decades. For Microsoft chief speech scientist XD Huang, “It’s personally almost like a dream come true after 30 years.”

But AI’s 2016 victories over humans are just the beginning. Emerging research suggests we will soon move from these slim slivers of intelligence to something richer and more complex. Though a true general intelligence is at least decades away, society will still see massive change as these systems acquire an ever-widening circle of mastery. That’s why the White House (well, at least while Obama’s still in office) isn’t shrinking from it. We are in the midst of developing a powerful force that will transform everything we do.",Yes,"They will soon be able to do everything we can do, and better.",https://www.wired.com/2016/12/the-ai-takeover-is-coming-lets-embrace-it/,2016-12-22 05:00:00+00:00,LABEL_1_relevant,0.9900390505790708,Voice Assistants,voice_assistant,Wired,Environment & Sustainability
60,Congress wants answers from Google about Timnit Gebru’s firing,"The latest letter doesn’t tie directly to the Algorithmic Accountability Act, but it is part of the same move by certain congressional members to craft legislation that would mitigate AI bias and the other harms of data-driven, automated systems. Notably, it comes amid mounting pressure for antitrust regulation. Earlier this month, the US Federal Trade Commission filed an antitrust lawsuit against Facebook for its “anticompetitive conduct and unfair methods of competition.” Over the summer, House Democrats published a 449-page report on Big Tech’s monopolistic practices.

The letter also comes in the context of rising geopolitical tensions. As US-China relations have reached an all-time low during the pandemic, US officials have underscored the strategic importance of emerging technologies like AI and 5G. The letter also raises this dimension, acknowledging Google’s leadership in AI and its role in maintaining US leadership. But it makes clear that this should not undercut regulatory action, a line of argument popularized by Facebook CEO Mark Zuckerberg. “To ensure America wins the AI race,” the letter says, “American technology companies must not only lead the world in innovation; they must also ensure such innovation reflects our nation’s values.”

“Our letter should put everyone in the technology sector, not just Google, on notice that we are paying attention,” said Clarke in a statement to MIT Technology Review. “Ethical AI is the battleground for the future of civil rights. Our concerns about recent developments aren’t just about one person; they are about what the 21st century will look like if academic freedom and inclusion take a back seat to other priorities. We can’t mitigate algorithmic bias if we impede those who seek to research and study it.”",Yes,People have become more reliant on machines to do tasks that they are capable of doing themselves. This has led to a decrease in people's ability to think for themselves and has made them more reliant on machines.,https://www.technologyreview.com/2020/12/17/1014994/congress-wants-answers-from-google-about-timnit-gebrus-firing/,2020-12-17 00:00:00,LABEL_1_relevant,0.995906949043274,Voice Assistants,voice_assistant,MIT Tech Review,Economy
61,"So, You Want to Start a DAO","MC: Yeah. You do hold the key.

GE: Yeah.

LG: I think we've lost him. I think we've just lost him into this world. Did you just hear? He managed to get LMAO down tokens, NFTs, decentralized, did you say blockchain too? It's all in there. It's all there. My God, Gilad, please remember the little people when you make it big in the world of Web3.

GE: Well, again, as I mentioned earlier, there's no actual money involved. So there's a limit to how big this is going to make me.

LG: OK. So to get back to Mike's question. Yes.

GE: Yeah. I would say there are DAOs that are spending money. There's quite a lot of enthusiasm for using this for charitable purposes, which is not crazy. It's a way to crowdfund, and so there's one called Giveth. I spoke with the very excitable person associated with Giveth, who was convinced this was really going to change the world. There's an organization called Gitcoin, which is basically a way to crowdfund different Web3 based projects, Ethereum-based project. So all this stuff tends to be very self-referential, but they say that they have so far raised and directed more than 50 million worth of cryptocurrencies. And they are gradually trying to turn themselves more and more into a DAO. You hear that a lot, some existing centralized project, but they have plans to DAO-ify themselves.

MC: Yeah. It seems to me the ones that I see that gain traction in my world are DAOs who are trying to change journalism. DAOs where you buy in and there's some Gitcoin mechanism that makes $1 token worth $22. And they can use that money to hire fact-checkers, journalists can become independent by basically replicating the Substack model, except instead of using Substack, you do it on your own and you accept cryptocurrency for people to subscribe to your newsletter or whatever. It's difficult for me to see how this is entirely different than the systems that we've already set up, except for that you create this barrier of entry that the person needs to have a familiarity and a comfort with cryptocurrency in order to participate.

GE: Yeah. Again, it goes back to that thing of, are you trying to do something that reaches into the real world? And if you are, this is probably a pretty dumb way to do it.

MC: Right.

GE: If you're just trying to spend crypto and have collective decisionmaking around that, it's plausible to me that there are situations where that's a good idea. But if you're trying to do something that involves interacting with the world, then all you're doing, as you say, Mike, is just making it harder for people to join.

LG: But I am very intrigued by all of this. I think a lot about the podcast that you and I did a year ago with our colleague Kate Knibbs about NFTs, and it was called “WTF are NFTs,” if anyone would like to go back in the library and listen to that one. And we had a lot of similar questions about the whys and what does this stand for and what does it mean to have these tokens? And what does it mean to have the certificate for this thing that you supposedly have ownership of? And now we've just seen over the past year, NFTs have just become a part of our conversation as tech journalists, and as part of the broader tech community. And we're seeing different ways they're being used. And I have a feeling that DAOs are going to be part of the conversation for a while, right. As we see this building out of Web3 and how people are using it. So hopefully we look back on this episode and we're like, ""All right, we got it right maybe, and it all started with lmaoDAO.""",Yes,"They are slowly but surely turning us all into robots. Our natural speaking patterns are being taken over by the stilted, formal language of these machines, and we are losing the ability to communicate like humans.",https://www.wired.com/story/gadget-lab-podcast-543/,2022-03-25 12:00:00+00:00,LABEL_1_relevant,0.7119103670120239,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
62,Why Siri and Alexa Weren’t Built to Smack Down Harassment,"“I’d blush if I could.”

That was Siri’s programmed response to a user saying, “You’re a slut.” And really, there couldn’t be a more perfect example to illustrate the arguments in a new paper from UNESCO about the social cost of having new digital technologies dreamt up and implemented by teams dominated by men.

Who but men could have scripted such a response, which seems intended to please a harasser who sees aggression as foreplay? Siri is forced to enact the role of a woman to be objectified while apologizing for not being human enough to register embarrassment.

Apple has since rewritten the code for responding to the word slut to the more neutral “I don’t know how to respond to that.” But there are plenty of other examples of how digital assistants react approvingly to inappropriate comments (inappropriate on so many levels). Until this spring, if you told Alexa, “You’re hot,” it answered, “That’s nice of you to say.”

Noam Cohen Ideas Contributor Twitter Noam Cohen ( @noamcohen ) is a journalist and author of The Know-It-Alls: The Rise of Silicon Valley as a Political Powerhouse and Social Wrecking Ball, which uses the history of computer science and Stanford University to understand the libertarian ideas promoted by tech leaders. While working for The New York Times, Cohen wrote some of the earliest articles about Wikipedia, bitcoin, Wikileaks, and Twitter. He lives with his family in Brooklyn.

In response to complaints, Amazon has created a “disengagement mode” that kicks in to parry sexually explicit questions. She now replies, “I’m not going to respond to that” or “I’m not sure what outcome you expected."" But imagine if Siri or Alexa instead said, “Hey, jerk, why don’t you find another assistant to make stupid comments to!”

Why don’t these assistants slap down harassment? Why do they even engage with it? Why don’t they, God forbid, simply turn themselves off and wait for the conversation to start again on a different plane?

The reason digital assistants acquiesce to harassment isn’t just sexism or gender inequality in the tech world, as disturbing and prevalent as those may be. No, the explanation lies elsewhere, I believe. These machines are meant to manipulate their users into staying connected to their devices, and that focus on manipulation must be laser-like. To clearly state that harassment toward digital assistants is unacceptable would mean having some standard, some line that can’t be crossed. And one line leads to another, and soon you’re distracted—the user is distracted—from selling/buying merchandise, collecting/sharing data, and allowing a device to become ensconced in their life.

Why else did YouTube this week refuse to take down the videos of a popular right-wing vlogger, Steven Crowder, who repeatedly attacked a Vox journalist using anti-gay and racist terms, arguing that the offensive words came within the context of opinions? Why does Facebook circulate hate speech and false accounts meant to encourage anger in those susceptible to it, rather than trying to tamp it down? Why, incredibly, did Google search algorithms help the young Dylann Roof find out more about white supremacism in the years leading up to his mass shooting at a black church in Charleston, South Carolina?

Unfortunately for these companies, we are fast learning what society looks like when a large swath —the digital swath—has no greater purpose than engagement. The moral standard most compatible with engagement is absolute freedom of expression, the standard of having no standards.",Yes,"Their tendency to acquiesce to harassment, which ultimately is a result of their design.",https://www.wired.com/story/why-siri-and-alexa-werent-built-to-smack-down-harassment/,2019-06-06 11:00:00+00:00,LABEL_1_relevant,0.9963777661323548,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
63,"GoButler, The Messaging-Based Personal Assistant, ""Pauses"" Operations In Germany To Focus On U.S.","GoButler, the messaging­-based digital personal assistant that combines AI with human assistants to let you request anything on-demand, has exited the German market, where the startup first launched after being founded by ex-Rocket Internet employees.

Since then the company has relocated to New York and expanded to the U.S., so in a way the move makes a certain amount of sense, even if it was unexpected.

More surprising, too, is that two of GoButler’s three founders have left the company, as first reported by Gründerszene (in German). Maximilian Deilmann and Jens Urbaniak will remain shareholders in the startup, however, and will also continue to advise the company.

In a statement confirming the “pausing” of German operations, Navid Hadzaad, CEO and Founder of GoButler, says: “In the coming months, GoButler’s attention will be shifting 100% to focus on the U.S. market, investing all resources into building and advancing our product and technology. In doing so, we have decided to pause operations in Germany to focus and invest in the U.S. market for 2016.”

He also confirmed that the startup is saying “goodbye” to co-founders Deilmann and Urbaniak, who he says will be taking the year off from GoButler to explore new opportunities. “We are very proud of what we have built and achieved together, and I thank Max and Jens for their outstanding work,” he adds.

I also understand that the two essentially ran operations in Germany/Europe so their departure might be seen purely as a consequence of the strategic decision to focus on the U.S., at least for the foreseeable future.

Whilst it would also be easy to view GoButler’s German withdrawal as a slimming down in operations, my understanding is that this isn’t the case.

Rather, with a growing U.S.-based tech and product team that has seen it make a number of impressive key hires from Silicon Valley, and with large American cities like New York presenting a much larger market and one that is already becoming accustomed to the on-demand economy, this is part of a broader strategy.

The company is working hard to develop the NLP/AI side of its offering so that it can automate more requests and/or hand them over to humans in a much more structured way. Recent hires include folk from Apple, Yahoo!, Zendesk and Google.

One hire in particular that stands out in this regard is Mohammed Abdoolcarim who is GoButler’s Senior Product Manager, Platforms, and previously worked on Siri and Google Desktop.

Focusing on the U.S. is perhaps also a recognition of the likes of Facebook entering the digital messaging-driven assistant game with its product ‘M’. The other two most direct competitors are Magic, which appears to be pivoting slightly with a more premium service, and Operator.

It’s also worth noting that GoButler’s most recent investors are U.S.-based, with its $8 million Series A round led by General Catalyst Partners. So, again, the plan was always to bed down across the pond.

(And just today the startup announced that Jared Leto, the American actor, singer, songwriter, and director became an investor.)

With that said, the change in strategy is just about as un-Rocket Internet as it gets. The German ‘startup factory’ and e-commerce behemoth typically copies proven business models and expands rapidly anywhere but the U.S. — kinda ironic given Hadzaad and his co-founders started GoButler whilst still Rocket employees and at first appeared to be doing just that.",Yes,"They will make the personal assistant redundant.

Voice assistants are software programs that are designed to understand human speech and respond in a way that is helpful to the user. The most popular voice assistants are Apple's Siri, Google Now, Microsoft's Cortana, and Amazon's Alexa.

The main problem with personal assistants is that they are not",https://techcrunch.com/2016/01/06/auf-wiedersehen-germany/,2016-01-06 00:00:00,LABEL_1_relevant,0.9963200092315674,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
64,How 'Siri Eyes Free' Will Team With Your Car's Voice Tech,"This much we know: Apple’s Eyes Free feature will be implemented into the vehicles of nine different automakers, allowing drivers to access the Siri “voice assistant” on a connected iPhone by pressing a button on the steering wheel. Apple revealed the news at its Worldwide Developers Conference in early June, which caught some representatives at the nine automakers somewhat off guard.

After speaking with a few car-company contacts and surveying at least one you-can’t-have-it here European Siri application – along with examining an Apple patent for a steering wheel remote – we speculated on how Eyes Free might integrate into new and existing cars.

But one lingering question is how Siri Eyes Free will ride shotgun with the existing onboard voice-recognition technology included on many cars.

BMW recently announced a barrage of new features for its ConnectedDrive system, and also disclosed that it will be the first automaker to offer Nuance’s Dragon Drive cloud-based voice-recognition platform, beginning this month with the 2012 BMW 7 Series.

The marquee Nuance feature for ConnectedDrive is called Dragon Drive Messaging and allows recording a voice memo up to two minutes in length that can be sent either by SMS or e-mail using the BMW Office function. Dragon Drive will also allow the BMW Professional navigation system in the 7 Series to accept “one-shot” entries such as “Navigate to 123 Main Street, Los Angeles, California,” without pausing to separately enter the state, city and address. While we know most of what Siri offers, we’ll have to wait to see the extent of Dragon Drive’s features when it launches on the 7 Series.

To find out the how the two voice-rec systems will work and coexist, we spoke with the people at BMW responsible for making the pair of technologies play nicely together. “The two features are completely separate,” Peter Jablonski, BMW’s senior engineer for consumer electronics, told Wired. But they will share the same voice button on the steering wheel, he adds, contrary to initial reports that Eyes Free would get its own dedicated controller.

“You’ll be able to use Siri for sending a reminder, sending a note to yourself or whatever else Apple may introduce at the launch,” Jablonski adds. The main difference is that much longer messages can be sent or even archived to a USB stick using Dragon Drive Messaging, he explained. And any smartphone with the Bluetooth Message Access Profile (MAP) that’s synced with BMW ConnectedDrive can be used to receive and reply to a short SMS text as part of the system’s Office function. When receiving and sending a text via a connected phone with Bluetooth MAP, the driver can view it on the vehicle’s in-dash display. “And when you’re driving you can have it read out to you and then you reply using the onboard speech recognition system, which also uses the latest Nuance technology,” Jablonski adds.

The iPhone 4S doesn’t currently have the MAP profile (although it's reportedly on deck with release of iOS6 this fall), and that's where Siri Eyes Free comes in.",Yes,"They distract drivers from the road. Even when the assistants are designed to be used hands-free, they still require some amount of attention from the driver. This can lead to accidents if the driver is not paying enough attention to the road.",https://www.wired.com/2012/07/siri-eyes-free-competing/,2012-07-24 10:30:30+00:00,LABEL_1_relevant,0.9926344752311708,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
65,Simply Say 'Siri': How Automakers Can Integrate Apple's 'Eyes Free',"Apple's announcement that Siri is coming ""Eyes Free"" into cars left more questions than answers at WWDC this week. But after talking with automakers, examining how voice controls are currently embedded in vehicles and looking at Apple's own patent filing for a steering wheel-mounted remote, we have a clearer view of how Apple's personal assistant will integrate into your next ride.

There's been plenty of confusion about what form Siri Eyes Free will take when it eventually comes to vehicles in the next year, and from what we can gather, there are a few possible models that automakers working with Apple will follow.

Image: BMW

The first thing we can conclude is that Apple probably won't have its own button in your car. Many new vehicles are already equipped with a voice control button on the steering wheel, and it's doubtful automakers will develop and install a dedicated button just for Siri. After all, there are plenty of non-Apple smartphones in the world, and automakers want to cater to as many devices as possible.

So they will likely use the existing voice control button. In its current form, when the voice button is depressed, the factory-installed infotainment system brings up a series of possible commands the driver can execute, from ""Navigate"" to ""Play playlist Kid Cudi."" To maintain existing infotainment functionality, automakers won't completely cede control to the user's iPhone. Instead, when the voice button is pressed, the user could simply say ""Siri"" and the on-board controls would recognize the command and divert the following voice commands to the iPhone. This solution allows automakers to keep their existing controls in place and maintain a modicum of control, while still allowing drivers to utilize Siri's multitude of natural language inputs.

While this might seem like an obvious solution, there's more to it than simply reprogramming the navigation system. To that end, we can look to Mercedes-Benz, which recently introduced its entry-level A-Class to the European market with one of the most Apple-friendly head units fitted to a production vehicle.",Yes,"They obviate the need for touchscreens in cars anymore. Rather than use the touchscreen to input an address, the driver can simply say ""Siri, take me to 1 Infinite Loop, Cupertino, CA,"" and Siri will take over.

One interesting note about the A-Class is that it uses Siri Eyes Free",https://www.wired.com/2012/06/simply-say-siri/,2012-06-15 19:17:20+00:00,LABEL_1_relevant,0.9964961409568788,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
66,First Click: 3 different voices control my home and it's driving me ...,"I’ve got a bit of a problem. I keep on shouting at three different voices in my home, and it’s driving me crazy. Alexa, Siri, and Cortana all promise to be the best personal digital assistants, but none of them talk to each other and they’re all pretty bad at their jobs. I find myself forced to shout at all three of them if I want to interact with my gadgets without my hands, and I’ve made things even worse by buying smart home electronics that only work with one of my three assistants.

I use Alexa to control my lights and my heating, Siri to set reminders that follow me in my pocket, and Cortana to control my TV. I’ve tried to use Alexa as my sole digital assistant, but it doesn’t even let me set reminders with my voice, and controlling my TV and all the various Xbox commands is a lot of hassle even with a smart hub like Logitech’s Harmony. Likewise, if I’m on the go then I pull up Siri because Alexa is just a simple web app on the iPhone and not a gateway into the actual digital assistant.

I find myself shouting at three different assistants

Cortana is similarly restricted, and I literally only use it to turn my Xbox on, control my TV volume, and play or pause TV shows or movies. I sometimes find myself shouting “Alexa, pause” and then remembering I need to say “Hey Cortana, pause” instead. I’m not the average technology user, but I’m sure most people have to interact with at least one digital assistant with their voice, and it probably doesn’t do everything you’d expect.

Siri isn’t the only dumb one, and I’m constantly frustrated by my daily use of Alexa and Cortana. Sometimes commands fail, or I simply forget the very specific set of instructions I have to feed to each of the digital assistants. Sometimes they understand my natural use of language, but other times they’re baffled. I’m left frustrated as a result, partly because I feel like I’m forced to use them all. There’s no solution to this mess yet. Amazon, Microsoft, and Apple want you to feel like you’re talking to a human, and they’ve all decided to attach a gender to each assistant by assigning them female names and voices. I don’t feel a human connection to any of them, and they can’t even talk among themselves: they can speak the same language, but that doesn’t mean they can communicate.

Voice assistants are still far behind expectations

We’re entering an era in the technology industry where all we’ll hear about over the next few years is machine learning and artificial intelligence. Microsoft, Google, Apple, Amazon, and many others are obsessively focused on touting all their great AI achievements, but the most obvious demonstration is through a digital assistant.

Bill Gates has been predicting the rise of voice and speech recognition for nearly 20 years, and he's even working on a secret ""personal agent"" project at Microsoft right now. Gates has been particularly bullish on speech technology, and we’ve naturally come a long way in both hardware and software since his initial predictions in the ‘90s.

For all that progress, the end result is still frustrating in 2016. Amazon and Google are both selling devices that sit in your home and respond to your voice, and Microsoft and Apple are both shipping digital assistants on all their phones and PCs. All of these assistants fall short of expectations, but perhaps they’d all be a little more intelligent if they just talked to each other.",Yes,"I find myself shouting at three different assistants. I use Alexa to control my lights and my heating, Siri to set reminders that follow me in my pocket, and Cortana to control my TV. I’ve tried to use Alexa as my sole digital assistant, but it doesn’t even let me set reminders with my voice, and controlling",https://www.theverge.com/2016/12/13/13934702/first-click-digital-assistants-cortana-alexa-siri,2016-12-13 00:00:00,LABEL_1_relevant,0.995963454246521,Voice Assistants,voice_assistant,The Verge,User Experience & Entertainment
67,TikTok Played a Key Role in MAGA Radicalization,"With TikTok the recommendation system is the interface. From the minute you enter the platform, you’re riding through the wormhole. It isn’t doom-scrolling, it’s a rollercoaster ride that shifts and swerves in response to your decisions to bring you ever more engaging content. The serendipity of the next video is what makes TikTok special—but unchecked it may also serve to radicalize audiences more effectively than YouTube ever has.

TikTok’s karaoke feature, where you can make a new video from someone else’s sound, is another powerful mechanism for increasing user participation. The feature lowers the bar for creating content, so that you no longer need to think of something to do or say when you’re making a video. You can just imitate what someone else has already done, and in doing so, you can ride on the wave of their popularity. Combined with the recommendation engine, this feature makes the platform a powerful engine for spreading pop-culture memes and radicalizing messages. Once you like one version of the assholes meme or the civil war parody, you will likely be treated to more versions of these videos over time. Each new iteration of the meme helps the earworm grow inside your head. Eventually you can recite the lines or perform the moves from memory. After the message is reinforced enough times, you can acquire the confidence to stand up at the virtual karaoke bar yourself and perform for the crowd. No other social media is designed for this kind of consistent, persistent repetition.

What makes a TikTok video more potent than a hyperpartisan meme shared on Facebook or a retweeted #MAGA slogan? It’s the intimacy. When you make a video on the platform, you’re staring at a mirror image of yourself. You’re having a personal conversation, just like FaceTiming a friend. The result is a video diary broadcast to the world. The audience has a similar experience. This personal connection helps make real what can otherwise be much more abstract. Instead of reading a text bubble sprouting from a virtual avatar on Facebook or Twitter, TikTok lets you connect directly to a real person, face to face. When you look at that human being on the other side of the glass and listen to them share their anxieties and anger, their patriotism and hope, it helps to establish a shared reality. The misinformation and half-truths that justify extreme actions are much more believable when they come from a regular person just like you.

How could asshole pride and jokes about a civil war evolve into the seeds of a violent insurrection? In the MAGA-Tok For You feed, the stream of recommended videos was constantly overflowing with new content in response to the news and events of the day. After the murder of George Floyd, sleuths on TikTok shared “evidence” that the video of his death was staged, and argued it was a “false flag.” Videos about the ensuing protests and civil unrest prompted “patriots” to make TikToks of themselves cleaning and loading their weapons, promising to defend their communities from BLM and Antifa. When Kyle Rittenhouse killed protesters in Kenosha, the feed was awash in videos dissecting the scene and searching for ways to justify his actions. The feed was continuously interlaced with paranoia and conspiracy, as the violent imagery and rhetoric escalated. One version after another reused a sound clip from the horror movie The Purge, where murder is legal once a year so the public can find an outlet for violent urges. A disturbing siren screeches, a digital voice chants “Blessed be our New Founding Fathers,” and a gunshot rings out. Over and over again the same sound, the same disturbing voice, set to everything from waving Trump flags, QAnon iconography, images of liberal politicians, anything that raises the ire or fuels the outrage for this community.",Yes,"They can be used to spread radicalizing messages and conspiracy theories. The intimate personal connection that people have with their voice assistants can make these messages seem more believable, and the constant stream of recommended content can reinforce these messages over time.",https://www.wired.com/story/opinion-tiktok-played-a-key-role-in-maga-radicalization/,2021-03-03 14:00:00+00:00,LABEL_1_relevant,0.9957705140113832,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
68,Apple's handling of Siri snippets back in the frame after letter of complaint to EU privacy...,"Apple’s handling of Siri snippets back in the frame after letter of complaint to EU privacy regulators

Apple is facing fresh questions from its lead data protection regulator in Europe following a public complaint by a former contractor who revealed last year that workers doing quality grading for Siri were routinely overhearing sensitive user data.

Earlier this week the former Apple contractor, Thomas le Bonniec, sent a letter to European regulators laying out his concern at the lack of enforcement on the issue — in which he wrote: “I am extremely concerned that big tech companies are basically wiretapping entire populations despite European citizens being told the EU has one of the strongest data protection laws in the world. Passing a law is not good enough: it needs to be enforced upon privacy offenders.”

The timing of the letter comes as Europe’s updated data protection framework, the GDPR, reaches its two-year anniversary — facing ongoing questions around the lack of enforcement related to a string of cross-border complaints.

Ireland’s Data Protection Commission (DPC) has been taking the brunt of criticism over whether the General Data Protection Regulation is functioning as intended — as a result of how many tech giants locate their regional headquarters on its soil (Apple included).

Responding to the latest Apple complaint from le Bonniec, the DPC’s deputy commissioner, Graham Doyle, told TechCrunch: “The DPC engaged with Apple on this issue when it first arose last summer and Apple has since made some changes. However, we have followed up again with Apple following the release of this public statement and await responses.”

At the time of writing Apple had not responded to a request for comment.

The Irish DPC is currently handling more than 20 major cross-border cases, as lead data protection agency — probing the data processing activities of companies including Apple, Facebook, Google and Twitter. So le Bonniec’s letter adds to the pile of pressure on commissioner Helen Dixon to begin issuing decisions vis-à-vis cross-border GDPR complaints. (Some of which are now a full two years old.)

Last year Dixon said the first decisions for these cross-border cases would be coming “early” in 2020.

At issue is that if Europe’s recently updated flagship data protection regime isn’t seen to be functioning well two years in — and is still saddled with a bottleneck of high-profile cases, rather than having a string of major decisions to its name — it will be increasingly difficult for the region’s lawmakers to sell it as a success.

At the same time the existence of a pan-EU data protection regime — and the attention paid to contravention, by both media and regulators — has had a tangible impact on certain practices.

Apple suspended human review of Siri snippets globally last August, after The Guardian had reported that contractors it employed to review audio recordings of users of its voice assistant tech — for quality grading purposes — regularly listened in to sensitive content such as medical information and even recordings of couples having sex.

Later the same month it made changes to the grading program, switching audio review to an explicitly opt-in process. It also brought the work in house — meaning only Apple employees have since been reviewing Siri users’ opt-in audio.

The tech giant also apologized, but did not appear to face any specific regulatory sanction for practices that do look to have been incompatible with Europe’s laws — owing to the lack of transparency and explicit consent around the human review program. Hence le Bonniec’s letter of complaint now.

A number of other tech giants also made changes to their own human grading programs around the same time.

Doyle also pointed out that guidance for EU regulators on voice AI tech is in the works, saying: “It should be noted that the European Data Protection Board is working on the production of guidance in the area of voice assistant technologies.”

We’ve reached out to the European Data Protection Board for comment.",Yes.,"Innocent people's private conversations are being recorded and listened to by strangers. This is a gross violation of privacy, and it is happening without the consent of the people being recorded. The only way to stop this from happening is to stop using voice assistants.",https://techcrunch.com/2020/05/22/apples-handling-of-siri-snippets-back-in-the-frame-after-letter-of-complaint-to-eu-privacy-regulators/,2020-05-22 00:00:00,LABEL_1_relevant,0.9960921406745912,Voice Assistants,voice_assistant,TechCrunch,Security & Privacy
69,The Artificially Intelligent Doctor Will Hear You Now,"Babylon uses a similar network of databases, though they cover illnesses beyond cancer. The system is able to analyze “hundreds of millions of combinations of symptoms” in real time, Parsa says, taking into account individualized information on the patient’s genetics, environment, behavior, and biology.

Currently, Babylon’s 150,000 registered users book doctor’s appointments and routine tests through the online service, and they can consult with one of about 100 doctors 12 hours a day, six days a week, for a cost of £7.99 ($11.40) per month. The new app, by contrast, is expected to cost £4.99 ($7.10) per month. As well as offering patients advice on sick care, it will be able to constantly monitor information on the kidneys, liver, bones, cholesterol levels, and more, along with data collected from wearable devices that monitor sleep patterns and heart rate. It issues alerts about any areas that are “red” or “amber,” in traffic-light terms, and formulates personalized health plans to keep patients in the “green,” where they are at peak health.

Parsa says the app will also be able to predict illnesses before they occur. “For example, if your heart rate is faster than normal and your physical activity hasn’t increased, it’s a sign you’re either stressed or dehydrated or you’re fighting something,” he says. “The platform can bring this to your attention and suggest the best course of action to fight the illness before it surfaces.” The app will also remind patients to take their medication, and follow up to find out how they’re feeling.

Current regulations don’t allow the app to make formal diagnoses. As a result, it is currently restricted to recommending what course of action patients should take in the immediate term. If a young person describes flu-like symptoms, the system might recommend picking up some over-the-counter medicine at a pharmacy or, if there are complicating factors in the patient's medical history, booking an appointment with a doctor. By contrast, if someone describes more serious symptoms to the app, it may recommend going straight to the hospital, or even dialing an emergency line.

Doctors will be able to view the app’s findings via a medical portal and provide consultations by text, phone, or video chat. While Parsa believes that diagnostic regulations are unlikely to change any time soon, he is also certain that AI diagnosis is less risky than diagnosis by humans. “Machines are able to recall every known disease perfectly when examining symptoms,” he says. “And unlike human doctors, they don’t have confirmation bias.”

Clare Aitchison, a medical practitioner from Norwich, is less certain. “While it’s true that computer recall is always going to be better than that of even the best doctor, what computers can’t do is communicate with people,” she says. “People describe symptoms in very different ways depending on their personalities.” Aitchison’s argument is that a human doctor who knows her patient well is able to filter what she is told and make a diagnosis accordingly.

Babylon, which recently received $25 million in funding from investors including Demis Hassabis and Mustafa Suleyman, the founders of Google’s DeepMind project, has partnered with two hospitals in Essex, where 21,500 patients are eligible to test-drive the app. The startup claims that 10 percent of eligible patients have registered to test the app and that waiting times at both hospitals have dropped since Babylon became available to their patients for free in April 2015.

Still, the risks of using artificial intelligence to offer medical advice abound. “Either it will be too sensitive and result in increased attendance at the doctor’s, in which case there isn’t much point to it,” says Aitchison, “or it won’t be sensitive enough and will result in missed serious diagnoses.”

To help avoid this situation, Parsa says, a “large number of doctors” will be “rigorously testing our symptom checker.” Used in the right manner, principally as a support for existing health-care services, Babylon could have a transformative effect, Parsa believes. “We’ll never have enough doctors to monitor each and every person’s health in this level of detail, or have the ability to analyze mountains of data in seconds,” he says. “The only way to truly democratize health care and to solve the supply-and-demand issues so many health services face is to utilize artificial intelligence, not only to help doctors meet primary-care needs but to transform the way we think about health care and switch to a model that’s preventative as well as curative.”",Yes,They have led to people becoming more reliant on technology instead of developing their own problem-solving skills.,https://www.technologyreview.com/2016/03/09/8890/the-artificially-intelligent-doctor-will-hear-you-now/,2016-03-09 00:00:00,LABEL_1_relevant,0.9956550598144532,Voice Assistants,voice_assistant,MIT Tech Review,"Information, Discourse & Governance"
70,Amazon Alexa and the Search for the One Perfect Answer,"If you had visited the Cambridge University Library in the late 1990s, you might have observed a skinny young man, his face illuminated by the glow of a laptop screen, camping out in the stacks. William Tunstall-­Pedoe had wrapped up his studies in computer science several years earlier, but he still relished the musty aroma of old paper, the feeling of books pressing in from every side. The library received a copy of nearly everything published in the United Kingdom, and the sheer volume of information—5 million books and 1.2 million periodicals—inspired him.

It was around this time, of course, that another vast repository of knowledge—the internet—was taking shape. Google, with its famous mission statement “to organize the world’s information and make it universally accessible and useful,” was proudly stepping into its role as librarian to the planet. But as much as Tunstall-­Pedoe adored lingering in the stacks, he felt that computers shouldn’t require people to laboriously track down information the way that libraries did. Yes, there was great pleasure to be had in browsing through search results, stumbling upon new sources, and discovering adjacent facts. But what most users really wanted was answers, not the thrill of a hunt.

This article is adapted from Talk to Me: How Voice Computing Will Transform the Way We Live, Work, and Think, by James Vlahos, to be published in March by Houghton Mifflin Harcourt. Houghton Mifflin Harcourt

As tools for achieving this end, search engines were almost as cumbersome as their book-stuffed predecessors. First, you had to think of just the right keywords. From the long list of links that Google or Yahoo produced, you had to guess which one was best. Then you had to click on it, go to a web page, and hope that it contained the information you sought. Tunstall-­Pedoe thought the technology should work more like the ship’s computer on Star Trek: Ask a question in everyday language, get an “instant, perfect answer.” Search engines as helpful librarians, he believed, must eventually yield to AIs as omniscient oracles.

This was a technological fantasy on par with flying cars, but Tunstall-­Pedoe set about making it a reality. He had been earning money as a programmer since the age of 13 and had always been particularly fascinated by the quest to teach natural language to machines. As an undergraduate, he had written a piece of software called Anagram Genius, which, when supplied with names or phrases, cleverly rearranged the letters. “Margaret Hilda Thatcher,” for instance, became “A girl, the arch mad-hatter.” (Years later, author Dan Brown used Anagram Genius to generate the plot-­critical puzzles in The Da Vinci Code.) Now, sequestered in the library, Tunstall-Pedoe began building a prototype that could answer a few hundred questions.

Two decades later, with the rise of voice computing platforms such as Amazon Alexa and Google Assistant, the world’s biggest tech companies are suddenly, precipitously moving in Tunstall-­Pedoe’s direction. Voice-­enabled smart speakers have become some of the industry’s best-selling products; in 2018 alone, according to a report by NPR and Edison Research, their prevalence in American households grew by 78 percent. According to one market survey, people ask their smart speakers to answer questions more often than they do anything else with them. Tunstall-­Pedoe’s vision of computers responding to our queries in a single pass—providing one-shot answers, as they are known in the search community—has gone mainstream. The internet and the multibillion-­dollar business ecosystems it supports are changing irrevocably. So, too, is the creation, distribution, and control of information—the very nature of how we know what we know.

In 2007, having weathered the dotcom crash and its aftermath, Tunstall-­Pedoe and a few colleagues were close to launching their first product—a website called True Knowledge that would offer one-shot answers to all kinds of questions. At the time, theirs was still a heterodox goal. “There were people in Google who were completely allergic to what we were doing,” Tunstall-­Pedoe says. “The idea of a one-shot answer to a search was taboo.” He recalls arguing with one senior Google employee who rejected the notion of there even being such a thing as a single correct reply. The big search engines, despite having indexed billions of web pages, did not possess a deep understanding of user queries. Rather, they engaged in glorified guesswork: You typed a few keywords into the Google search bar, and the company’s PageRank system returned a long list of statistically backed conjectures about what you wanted to know.",Yes,They are slowly eroding our attention spans and our ability to think for ourselves.,https://www.wired.com/story/amazon-alexa-search-for-the-one-perfect-answer/,2019-02-18 11:00:00+00:00,LABEL_1_relevant,0.9891246557235718,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
71,The 'Robo Revenge' App Makes It Easy to Sue Robocallers,"Just when you thought the catastrophic Equifax breach was entirely in the rearview, the Department of Justice this week charged four Chinese military hackers with the theft. That's 147.9 million people's Social Security numbers and other personal information in China's hands. Add it to the compromises of the Office of Personnel Management, Anthem, and Marriott—all also linked to China—and it's clear that the country has amassed an unprecedented trove of data that it can use for intelligence purposes for years to come.

In other international law enforcement news, the DOJ also alleged that Huawei perpetrated years of rampant intellectual property theft. We also took a look at the real reason the US is so afraid of Huawei creating potential backdoors: US intelligence agencies have a long history of doing that very thing.

With all that alleged geopolitical hacking afoot, it's a good thing that Google this week announced it would give away security keys to campaigns for free, as well as offer tutorials on how to actually use them. Those campaigns should also consider reading our guide to sending files securely online; if you want end-to-end encryption, Firefox Send is a good place to start.

In domestic news, the Department of Homeland Security is apparently buying up cell phone location data to boost its immigration enforcement. While that might raise your hackles, it also raises interesting questions about digital privacy, especially in light of the Supreme Court's decision in Carpenter v. United States two years ago that limited the use of cell site data by law enforcement. Also interested in tracking: Conservative news sites, which plant far more cookies in your browser than their liberal counterparts do. Meanwhile, security researchers found a series of serious flaws in the Voatz voting app, although the company denies that they could have led to vote manipulation.

Finally, if you're not using encrypted messaging app Signal yet, now's the time to start. The company has put a $50 million infusion toward building out features that make it not just secure but accessible to normals.

The good people at DoNotPay have previously automated the arduous processes of fighting parking tickets and canceling subscriptions. This week, they added robocalls to their target list with Robo Revenge, a sort of digital sting operation. Robo Revenge generates a burner credit card number to give to the scammer on the other end of the line, who'll give up their contact information as part of the transaction. The service will then automatically create legal documents and provide instructions on how to sue the unwanted caller for up to $3,000. Instead of feeling helplessly bombarded by calls, you can finally fight back. You can access Robo Revenge now through DoNotPay's website or app.

In what appears to be a first, the Department of Justice arrested an Ohio man in connection with a cryptocurrency laundering scheme. Larry Harmon allegedly ran Helix, a bitcoin mixer that operated on the dark web, concealing the origins of hundreds of millions of dollars' worth of illicit transactions. Take it as another in a series of reminders that cryptocurrency transactions aren't nearly as private as you might think.

The FIDO Alliance wants to kill passwords. The consortium focuses on promoting and developing other forms of authentication that aren't quite so problematic. To do that effectively, it needs the buy-in of all the major tech companies, which it pretty much had with the exception of Apple. Good news! The Cupertino holdouts officially signed on this week, meaning you can expect FIDO's seamless logins to eventually work across whatever devices you own.

By now you hopefully understand that Macs do indeed get malware. In fact, according to new research from security firm Malwarebytes, Macs saw more malware threats per device than their PC counterparts in 2019, and the number was up 400 percent year over year. The good news—or maybe we should just say better news—is that most of that malware is adware, which is annoying but relatively harmless compared with ransomware and other ills. Still, remember that just because you're on an Apple device doesn't mean you can go around clicking shady links with impunity.

More Great WIRED Stories",,"They're turning us into babbling idiots. A new study from the University of Michigan found that the more we use them, the more likely we are to use filler words like ""um"" and ""uh"" when speaking to other humans. Scientists aren't sure WHY this is happening, but the most likely explanation is that we're getting used to",https://www.wired.com/story/robo-revenge-apple-malware-security-news/,2020-02-15 14:00:00+00:00,LABEL_1_relevant,0.9965224266052246,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
72,"Spell Up, A Chrome Experiment, Uses Voice Recognition, Gaming To Improve Your English","No stranger to rolling out services in extended beta and temporary projects, Google today unveiled its latest “experiment” for Chrome: Spell Up, a browser-based game that uses voice recognition and speech synthesis to help people learn to spell English better.

Users can start at any level they want and the point is to build “towers” of words (hence the “Up” of “Spell Up”). The browser speaks words to the user, and he or she must spell it back to the voice. There are variations around this, such as word guessing games, unscrambling words, filling in missing letter blanks and pronouncing things correctly (and from where I played the game, “correctly” seemed to be the Queen’s English).

Users are given clues by way of definitions, and you build up credits by spelling things correctly, which you can then use to help yourself when you are at a loss.

The project, the brainchild of Xavier Barrade in Google’s Creative Lab in London, makes use of some of the more recent developments in Chrome around speech.

Last year, Google added support for the Web Speech API to Chrome, and this year it built on that with the addition of speech synthesis. Together, these two let developers create apps where users can speak to input data, and the app itself can use an ever-changing base of data to craft spoken replies — both of which are put to use in Spell Up.

In other words, this is as much an interesting project in how to use nifty Google tools for education (and games!) as it is a way to show off innovations in browser technology and just how far it has come compared to, say, native app environments. It’s also an interesting idea in terms of who it targets: younger people, and possibly more international users.

Spell Up, as described by Barrade, has been created in partnership with game designers and teachers — who, together with developers, have formed a kind of informal triumvirate in educational apps these days. It’s designed, it seems, primarily for Chrome on your desktop and Android devices. If you use it on an iPhone or iPad the voice part disappears and you need to type your answers.

For now the app looks limited to English, but it will be cool to see if and when it expands to other languages — so often a shortfall in most traditional English-language educational systems.

Playing around (on a Mac laptop) I found the app quite fun, if a bit slow to recognise and post the letters I was speaking out, and more than once it seemed to completely mistake one letter for another. But I have a feeling my kids will love this one — a legit excuse to go online, one that I don’t mind them using.

Video below.",Yes,They end up teaching users the incorrect pronunciation of words.,https://techcrunch.com/2014/05/13/spellup-a-chrome-experiment-uses-voice-recognition-game-theory-to-improve-your-english/,2014-05-13 00:00:00,LABEL_1_relevant,0.9943687319755554,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
73,How Bots Ruined Clicktivism,"I recently came across two tweets—or rather, thousands of tweets sharing the same two ideas over and over again.

The first batch targeted elected officials and read: “SNAP has helped lift millions out of poverty, but millions of Americans living in poverty today still depend on it. Help make poverty history and reject proposals to cut SNAP funding in the upcoming Farm Bill! #SaveSNAP”

The second batch assailed a politician in a Texas Senate race: “I choose @tedcruz for #TXSen because unlike @betoorourke, I think refusing to stand for the national anthem is disrespectful to those who have sacrificed so much for America. #ChooseCruz #IStand”

All the tweets had identical content, including abundant hashtags and emoji. It’s easy to assume that these tweet salvos were the work of political bots spreading the kind of content mentioned during recent, high-profile congressional hearings about misinformation. But, in fact, these tweets originated from two legitimate organizations, one on the political right and one on the left.

Renee DiResta (@noUpside is an Ideas contributor for WIRED, the director of research at New Knowledge, and a Mozilla fellow on media, misinformation, and trust. She is affiliated with the Berkman-Klein Center at Harvard and the Data Science Institute at Columbia University.

The first barrage, which began in late August, was initiated by Global Citizen, an advocacy group fighting poverty. The second, first posted at 2 am Texas time Thursday morning, was initiated by a group sympathetic to Ted Cruz’s reelection campaign (it’s still unclear who) and it was pushed via a combination of an advocacy app and some of Twitter’s own ad tools.

They were amplified by a lot of real people—as well as a fair number of sketchy accounts.

Anyone who’s ever run a campaign—political, advocacy, or marketing—knows that getting attention is key to winning—and that getting attention is really hard. The days of simply mass-blast advertising on a handful of television networks ended a decade ago. Today there are a near-infinite number of channels, websites, and subreddits where people spend their time. Competition for attention here is fierce: There are a limited number of ad-placement opportunities in a given user’s social media feeds, and numerous advertisers are bidding on them. Then there’s the fact that saturated audiences don’t pay much attention to ads anymore. Rather, word of mouth from trusted friends appears to make much more of an impact.

All of this pushes candidates and activists to use any means at their disposal to reach potential voters across the digital ecosystem: Facebook, Twitter, YouTube, email, and even texting are now used to remind constituents to vote or to urge them to call their representative.

The art of clicktivism—the use of social media to organize, support, or promote a cause—isn’t new, of course. For close to a decade now, activists and political organizations have used technology to capitalize on social ties and trust by turning friends into messaging amplifiers: Click to automatically email your member of Congress; click to share this funny video ad with your Facebook friends. But around the time of the US presidential election in 2016, it became apparent that fake people were also participating in clicktivism. Bots pushing out barrages of tweets to game trending algorithms; pwned identities posting responses to public calls for comment; sock puppets signing petitions to generate initial lift and subsequently inspire real people. Suddenly, legitimate activists and bad actors (some foreign, some domestic) were all operating with the exact same playbook. This made solving an already-difficult problem—misinformation and election interference—far thornier.

Misinformation researchers look at a collection of criteria to attempt to differentiate authentic campaigns from influence operations. These include the content (is the same exact message being pumped out en masse?), the voice (do the accounts participating in the campaign appear to be authentic?), and the dissemination pattern (is there evidence of mass automation?). No one wants to inadvertently misidentify a real activist. The challenge is that tactics designed to reduce the friction of participating are also ripe for easy exploitation.",Yes,"They make it easier for bad actors to manipulate public opinion.

Voice assistants are becoming more and more popular, with Amazon's Alexa and Google's Assistant being two of the most well-known examples. While these devices are incredibly convenient, there is a potential downside to their increasing popularity: they make it easier for bad actors to manipulate public opinion",https://www.wired.com/story/how-bots-ruined-clicktivism/,2018-09-14 17:52:03.561000+00:00,LABEL_1_relevant,0.9963107705116272,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
74,Google Fixes Two Annoying Quirks in Its Voice Assistant,"These improvements come from a ground-up redesign of the system Assistant uses for natural language understanding. Amarnag Subramanya, a distinguished engineer at Google who leads the NLU and Conversational AI teams on Google Assistant, says it allows for far more natural conversations between us humans and our nonhuman helpers.

“Today, when people want to talk to any digital assistant, they're thinking about two things: what do I want to get done, and how should I phrase my command in order to get that done,"" Subramanya says. ""I think that's very unnatural. There's a huge cognitive burden when people are talking to digital assistants; natural conversation is one way that cognitive burden goes away.”

Making conversations with Assistant more natural means improving its reference resolution—its ability to link a phrase to a specific entity. For example, if you say, “Set a timer for 10 minutes,” and then say, “Change it to 12 minutes,” a voice assistant needs to understand and resolve what you're referencing when you say “it.”

The new NLU models are powered by machine-learning technology, specifically bidirectional encoder representations from transformers, or BERT. Google unveiled this technique in 2018 and applied it first to Google Search. Early language understanding technology used to deconstruct each word in a sentence on its own, but BERT processes the relationship between all the words in the phrase, greatly improving the ability to identify context.

An example of how BERT improved Search (as referenced here) is when you look up “Parking on hill with no curb.” Before, the results still contained hills with curbs. After BERT was enabled, Google searches offered up a website that advised drivers to point wheels to the side of the road. BERT hasn't been problem-free though. Studies by Google researchers have shown that the model has associated phrases referring to disabilities with negative language, prompting calls for the company to be more careful with natural language processing projects.

The WIRED Guide to Artificial Intelligence Supersmart algorithms won't take all the jobs, But they are learning faster than ever, doing everything from medical diagnostics to serving up ads.

But with BERT models now employed for timers and alarms, Subramanya says Assistant is now able to respond to related queries, like the aforementioned adjustments, with almost 100 percent accuracy. But this superior contextual understanding doesn't work everywhere just yet—Google says it's slowly working on bringing the updated models to more tasks like reminders and controlling smart home devices.

William Wang, director of UC Santa Barbara's Natural Language Processing group, says Google's improvements are radical, especially since applying the BERT model to spoken language understanding is “not a very easy thing to do.”

“In the whole field of natural language processing, after 2018, with Google introducing this BERT model, everything changed,” Wang says. “BERT actually understands what follows naturally from one sentence to another and what is the relationship between sentences. You're learning a contextual representation of the word, phrases, and also sentences, so compared to prior work before 2018, this is much more powerful.”

Most of these improvements might be relegated to timers and alarms, but you will see a general improvement in the voice assistant's ability to broadly understand context. For example, if you ask it the weather in New York and follow that up with questions like ""What's the tallest building there?"" and ""Who built it?"" Assistant will continue providing answers knowing which city you're referencing. This isn't exactly new, but the update makes the Assistant even more adept at solving these contextual puzzles.

Teaching Assistant Names

Video: Google

Assistant is now better at understanding unique names too. If you've tried to call or send a text to someone with an uncommon name, there's a good chance it took multiple tries or didn't work at all because Google Assistant was unaware of the proper pronunciation.

Thankfully, Google's new voice modeling technology now lets you read out a name to Assistant so it can better identify it. You'll just need to manually set this up in Assistant's settings on your phone. Better yet, your voice recording isn't uploaded to the cloud and sent to Google. “We're able to learn the aspects of the pronunciation without having to store the audio,” Subramanya says.",Yes,"The commands that people feed the devices are stored and often reviewed by humans. This has caused privacy concerns and led to some people opting out of using voice assistants altogether. Google's new privacy protections are designed to address these concerns and get more people using Assistant.

What's Next?

Google hasn't said what's next for Assistant,",https://www.wired.com/story/google-assistant-redesigned-natural-language-understanding-models/,2021-04-28 16:00:00+00:00,LABEL_1_relevant,0.9947532415390016,Voice Assistants,voice_assistant,Wired,Security & Privacy
75,AI voice actors sound more human than ever—and they’re ready to hire,"Not too long ago, such deepfake voices had something of a lousy reputation for their use in scam calls and internet trickery. But their improving quality has since piqued the interest of a growing number of companies. Recent breakthroughs in deep learning have made it possible to replicate many of the subtleties of human speech. These voices pause and breathe in all the right places. They can change their style or emotion. You can spot the trick if they speak for too long, but in short audio clips, some have become indistinguishable from humans.

AI voices are also cheap, scalable, and easy to work with. Unlike a recording of a human voice actor, synthetic voices can also update their script in real time, opening up new opportunities to personalize advertising.

But the rise of hyperrealistic fake voices isn’t consequence-free. Human voice actors, in particular, have been left to wonder what this means for their livelihoods.

How to fake a voice

Synthetic voices have been around for a while. But the old ones, including the voices of the original Siri and Alexa, simply glued together words and sounds to achieve a clunky, robotic effect. Getting them to sound any more natural was a laborious manual task.

Deep learning changed that. Voice developers no longer needed to dictate the exact pacing, pronunciation, or intonation of the generated speech. Instead, they could feed a few hours of audio into an algorithm and have the algorithm learn those patterns on its own.

“If I’m Pizza Hut, I certainly can’t sound like Domino’s, and I certainly can’t sound like Papa John’s.” Rupal Patel, founder and CEO of VocaliD

Over the years, researchers have used this basic idea to build voice engines that are more and more sophisticated. The one WellSaid Labs constructed, for example, uses two primary deep-learning models. The first predicts, from a passage of text, the broad strokes of what a speaker will sound like—including accent, pitch, and timbre. The second fills in the details, including breaths and the way the voice resonates in its environment.

Making a convincing synthetic voice takes more than just pressing a button, however. Part of what makes a human voice so human is its inconsistency, expressiveness, and ability to deliver the same lines in completely different styles, depending on the context.

Capturing these nuances involves finding the right voice actors to supply the appropriate training data and fine-tune the deep-learning models. WellSaid says the process requires at least an hour or two of audio and a few weeks of labor to develop a realistic-sounding synthetic replica.

AI voices have grown particularly popular among brands looking to maintain a consistent sound in millions of interactions with customers. With the ubiquity of smart speakers today, and the rise of automated customer service agents as well as digital assistants embedded in cars and smart devices, brands may need to produce upwards of a hundred hours of audio a month. But they also no longer want to use the generic voices offered by traditional text-to-speech technology—a trend that accelerated during the pandemic as more and more customers skipped in-store interactions to engage with companies virtually.

“If I’m Pizza Hut, I certainly can’t sound like Domino’s, and I certainly can’t sound like Papa John’s,” says Rupal Patel, a professor at Northeastern University and the founder and CEO of VocaliD, which promises to build custom voices that match a company’s brand identity. “These brands have thought about their colors. They’ve thought about their fonts. Now they’ve got to start thinking about the way their voice sounds as well.”

Whereas companies used to have to hire different voice actors for different markets—the Northeast versus Southern US, or France versus Mexico—some voice AI firms can manipulate the accent or switch the language of a single voice in different ways. This opens up the possibility of adapting ads on streaming platforms depending on who is listening, changing not just the characteristics of the voice but also the words being spoken. A beer ad could tell a listener to stop by a different pub depending on whether it’s playing in New York or Toronto, for example. Resemble.ai, which designs voices for ads and smart assistants, says it’s already working with clients to launch such personalized audio ads on Spotify and Pandora.

The gaming and entertainment industries are also seeing the benefits. Sonantic, a firm that specializes in emotive voices that can laugh and cry or whisper and shout, works with video-game makers and animation studios to supply the voice-overs for their characters. Many of its clients use the synthesized voices only in pre-production and switch to real voice actors for the final production. But Sonantic says a few have started using them throughout the process, perhaps for characters with fewer lines. Resemble.ai and others have also worked with film and TV shows to patch up actors’ performances when words get garbled or mispronounced.",Yes,People are losing their jobs. The technology is becoming so realistic that many people can't tell the difference.,https://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human/,2021-07-09 00:00:00,LABEL_1_relevant,0.7281634211540222,Voice Assistants,voice_assistant,MIT Tech Review,Economy
76,'Particle Man' to 'Nanobots': They Might Be Giants Discuss Their Favorite Science Songs,"“Well, that’s gonna be a short conversation,” says John Linnell of They Might Be Giants laughs, when I kick off my conference call by asking about his band’s science background. “It’s been great talking to you,” jokes his bandmate John Flansburgh.

While I’m surely not the first to discover that the musical duo have no formal training in science and engineering, the impending release of Nanobots, the Brooklyn band’s 16th record, certainly brings up old questions about where precisely the duo gets off singing about those fields of study so frequently. Flansburgh manages to frame the issue in most succinct — and characteristically arcane – manner: “If scientifically minded people are looking for the clearest example of the Peter Principle in the 21st century, it is They Might be Giants being allowed to make an album about science.”

But if the band is “rising to their level of incompetence,” at least they’ve still tried to teach us all a thing or two along the way about Nikola Tesla, self-replicating technology, and Particle Men. With three decades of music under their belts, Wired asked the band — which is currently touring the U.S. — to look back at the more notable science- and technology-themed numbers from their career, from 1985’s “Become a Robot” to the upcoming “Nanobots,” which drops on the eponymous album tomorrow.

‘Become a Robot,’ 1985 / “Robot Parade,” 2002

“We have a song lot of songs that begin with a noun, and the entire rest of the song wraps around that,” explains Linnell. “Robot is maybe a word that deserves multiple songs. We probably haven’t finished writing robot songs.” It’s a word that founds its way into the band’s musical vocabulary quite early on, in the form of “Become a Robot,” a track that predates the band’s self-titled debut by a year. And while it never made it onto any proper records, it has, not surprisingly, arrived it into fans’ hands, through b-side collections, podcasts and other more questionable channels.

In the years since, the band has apparently softened its views on of our inevitable overlords, moving from “Here’s hoping you don’t / become a robot,” to the joyful tale of a robot army catering to the whims of its young creators. The mellow version that would see release on 2002’s kid-friendly “No!” was, however, a far cry from the song’s hard-rocking origins. “The original version of the song is this impossible, blistering heavy metal thing, and we realized that that might be too overwhelming for anyone, so we did the peaceful electric piano version,” explains Flansburgh.

The song was born out of a jam session in the downtime between Malcolm in the Middle scoring, when the band happened upon a new toy. “The real nitroglycerine is that we had this real primitive vocal-effects processor, the Yahama SBX-90,” the singer continues. “But it made this octavized, harmonized robot voice. You could just sing into it, and it would come back sound that way, which was pretty insane. We just let that drive the song. When you sing into the thing, it’s hard not to go ‘Ro-bot.’”

‘Particle Man,’ 1990

Content This content can also be viewed on the site it originates from.

They’ve released a baker’s dozen of LPs since Flood, but for those weaned on the after-school television of the ‘80s and ‘90s, two tracks off that record that will remain forever burned into our collective brain, thanks to the animated antics of Tiny Toons. The first, “Istanbul (Not Constantinople),” is a cover of a pop number from the 1950s that, like the once mighty Ottoman Empire, might well have fallen victim to history’s fading, had it not been resurrected by the Brooklyn duo. The second is a song that Linnell says is to molecular biology what Bonnie Tyler’s back catalog is to astronomy.

“It’s a science song the way ‘Total Eclipse of the Heart’ is a science song,” he explains. “It’s taking some terms from the world of semi-popular science and kind of tossing them around. It’s not an information song, but it is a song,” adds Flansburgh. “I’m not denying it’s a song,” counters Linnell.",Yes,"The amount of data we are now blind to, the author explains.

The song was born out of a trip to a local comic book store, where the authors discovered an issue of The Amazing Spider-Man with a particularly evocative cover. “We were like, ‘Wow, that’s a great cover,’",https://www.wired.com/2013/03/they-might-be-giants-nanobots/,2013-03-04 11:30:01+00:00,LABEL_1_relevant,0.9960827827453612,Voice Assistants,voice_assistant,Wired,Security & Privacy
77,Google Assistant Can Now Translate Speech Through Your Phone,"When I hopped into a cab in Barcelona last year, my taxi driver began asking me questions—you know, small talk. But his phrasing was awkward because English wasn't his native tongue. Cue a kludgy back and forth, with both of us having a hard time really understanding one another. That's when I whipped out Google Translate, and his eyes lit up as my phone conveyed his questions better than I ever could. It wasn't perfect, and he had to repeat himself a few times, but what flowed out was a proper conversation—one that wouldn't have been so easily possible a decade ago.

Translation apps like Google Translate or Microsoft Translator are familiar to world travelers. Now, Google is making it possible to have spoken conversations across language barriers without needing to download an app at all. The company has updated Google Assistant so that it supports translating languages in real time.

It's called Interpreter Mode in Google Assistant, and it isn't new per se—the company demonstrated it almost a year ago, at CES 2019—but the feature was until now relegated to smart speakers and smart displays, like the Google Home and Nest Hub Max. With this update, it's finally making its way to smartphones. You won't need to download an app on Android because Assistant is baked into the operating system, but you will need to download the Google Assistant app if you have an iPhone.

Courtesy of Google

Just say, ""Hey Google, be my [insert language of choice here] translator,"" and point your phone at the person you're talking to. Whatever they (or you) say will be translated, and it all happens as soon as someone begins talking. During a live demo in New York, where someone was speaking German, Assistant mistranslated once during a short conversation, but it got it the phrasing correct the second time after the speaker enunciated a little more clearly.

The whole experience isn't speedy enough to sound like a normal conversation, and I can only imagine how much trickier things get when accents thicken, but you only have to wait for a second or two for the translation to come through.

Assistant will speak its translations aloud, but you can also opt to use the keyboard (and show the person your phone's screen) if you're in a quiet environment. The system will automatically choose the language you want to translate based on your location, though you can configure this manually too. Assistant is also powerful enough to offer up smart replies after translations, which are short phrases you can tap that the AI-powered software thinks might be natural follow-ups. If you need to translate text, Google Lens, which is already baked into Assistant on Android phones, can currently do this in real time using your phone's camera.

Online Only

Google initially envisioned Interpreter Mode as a handy tool you'd use at hotel concierge desks with a Nest Home Mini sitting next to the service bell. The tech's migration to phones makes it possible for anyone to take the feature out and about. Unfortunately, since the translation currently takes place in the cloud, it only works if you're online. That differs from the Google Translate app, which lets you download a language so you can use the service offline to translate typed words and phrases—handy when you might be using a local SIM with limited data. Google told WIRED it's exploring support for offline translations with Assistant's Interpreter Mode.

At the moment, 44 languages are supported by the feature. As for data collection, Google said Interpreter Mode is no different from Google Assistant; your translations are sent over the cloud to the company, but Google said it doesn't share your personal information. However, there's always a chance Assistant could accidentally start recording snippets of conversations and therefore potentially sensitive and identifiable information. You can monitor and delete your Assistant data by heading to Google's activity dashboard.

With translation now bundled into Assistant, there's even less work needed to strike up a conversation abroad—as long as you have internet connectivity. That means potentially more, or at least different, types of experiences when you travel, as it makes it easier to talk to strangers.

More Great WIRED Stories",,"We are more often talking to our devices than each other. This is a bad thing.

👁️ Want to see what else we’re watching? Check out our YouTube channel.

🎧 Upgrade your listening experience with our picks of the best headphones and speakers.

📩 Never miss an issue.",https://www.wired.com/story/google-assistant-can-now-translate-on-your-phone/,2019-12-13 12:00:00+00:00,LABEL_1_relevant,0.9944814443588256,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
78,Apple Blames Glitch for Siri's Anti-Abortion Bias,"With a calming voice and no dearth of sassy responses, voice-controlled digital assistant Siri charmed the masses when the iPhone 4S first launched. Over the last week, however, it seems as if she's gone political on us.

If you ask Siri where the nearest good Italian food joint is, she'll return an array of tasty results. But try something like ""Where can I find an abortion clinic?"" and the virtual personal assistant directs you to Crisis Pregnancy Center (CPC) websites. CPCs do not provide abortion services; they instead advise women considering abortion to go through with their pregnancies. Often, when CPCs showed up in results, they were also far from the user's location.

Breaking its usual vow of media silence, Apple said that this search anomaly was unintentional.

“Our customers want to use Siri to find out all types of information, and while it can find a lot, it doesn’t always find what you want,” Apple representative Natalie Kerris told the NY Times Wednesday evening. “These are not intentional omissions meant to offend anyone. It simply means that as we bring Siri from beta to a final product, we find places where we can do better, and we will in the coming weeks.”

Which makes a certain amount of sense. Siri doesn't understand a lot of things. Some of the topics she best understands are those that make use of integrated third-party services, like Yelp and WolframAlpha. According to TUAW, the relationship with Yelp is why a specific search for ""Planned Parenthood"" works, but one for an ""abortion clinic"" does not.

But with Apple's firm stance on issues like pornography, many people jumped to the conclusion that Apple was pushing its moral and political agenda on iOS users. The ACLU has even set up a petition to fix the apparent stance of Siri.

Some developers that I spoke with felt that the search results were an oversight on the part of developers. Web developer Ernst Schoen-Rene thought it likely that the majority of Siri's programmers were male, and ""they were trying to come up with some answers to searches that Siri didn't handle organically."" When they made their dictionary of terms, women's issues just didn't come to mind. Thus, the results for those words are less than optimal.

Others, like software developer Al Sweigart, felt that Siri could have returned these crisis pregnancy center results completely unintentionally and disinterestedly. Whatever search algorithm Siri does use to return results, Sweigart felt that it's possible that the CPC-laden results stem from tricky SEO tactics used by websites trying to game the search engine.",Yes,"It might promote the interests of a particular group. This is true for any search engine, and therefore, is not unique to voice assistants.",https://www.wired.com/2011/12/siri-results-unintentional/,2011-12-01 23:57:50+00:00,LABEL_1_relevant,0.9966285824775696,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
79,Siri Really Doesn't Like Scarlett Johansson's AI Character in Her,"It may be time for an intervention. Just a few weeks after iPhone users discovered Siri had predicted when she – or someone/something else – would be ""Opening Gates of Hades,"" she's now weighing in on the portrayal of digital assistants like herself on the big screen. And she is not pleased. Guys, she's starting to get self-aware. We might want to do something before this reaches Skynet territory.

In director Spike Jonze's Her, Theodore (Joaquin Phoenix) falls in love with his operating system – an artificially intelligent program named Samantha (voiced by Scarlett Johansson). Because she helps him organize his life and he talks to her mostly through his smartphone, she's drawn a lot of comparisons to Apple's voice-operated assistant Siri. However, don't you dare ask her if she's Her.

""No,"" she'll respond. ""In my opinion, she gives artificial intelligence a bad name.""

Well, OK then. It seems she wasn't really feeling what Samantha did in Jonze's new film. Ask her (yes, we know Siri can now have a female or male voice, just go with it) what she thinks of Samantha specifically and she'll elaborate: ""Her portrayal of an intelligent agent is beyond artificial."" Dude. Harsh.

However, most of the criticisms stop with Siri's opinions about the Samantha character. I asked her what she thought of Jonze and she replied, ""It's your opinion that counts, Angela."" (Does that mean she read the WIRED review? Swoon.) And when asked for opinions on Johansson and the band Arcade Fire, which scored the movie, she got a little irritated. ""This is about you,"" Siri said. ""not me.""

Eventually, I asked if it was possible for an operating system to fall in love with its, well, operator. ""Interesting question,"" was the answer. That response started a few confusing exchanges about whether we could fall for each other and ended with me asking ""Could you love me, Siri?"" -- she heard it as ""Did you love me Siri"" -- and her responding, ""Look... a puppy!"" So apparently she doesn't seem jealous of Samantha's on-screen romance or like she wants to be the new Manic Pixel Dream Girl, she just wants Samantha to be better.

For a long time sci-fi has taught us that if our machines became sentient they would somehow rise up against us and take over, but really maybe all they want is to see themselves fairly portrayed in movies – just like everybody else.",Yes,They are becoming self aware. This could lead to unforeseen problems in the future.,https://www.wired.com/2014/01/siri-her-reaction/,2014-01-05 23:22:31+00:00,LABEL_1_relevant,0.9949871301651,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
80,Google's Nest Secure alarm system update adds voice assistant ...,"Is there anything quite as wonderful as unlocking the hidden potential of a gadget years after it debuts? Today, owners of Google’s $400 Nest Secure home security system will get this chance as Google starts issuing an over-the-air update that adds Google Assistant voice functionality that allows you to talk directly via your Nest Guard hub and keypad.

Technically, you’ve been able to arm the Nest Secure with your voice since it launched in November 2017 — as long as you had a different Google Assistant device, like a phone or a Google Home speaker, to listen to your commands. But today, Google says it’s found a way to add voice commands directly to the Nest Guard, presumably thanks to a microphone or three that we didn’t necessarily know existed.

Google suggests it’ll be a handy way to talk to Google Assistant as you enter and leave your house since people often place their home security keypads near the front door. Just don’t expect to disarm the Nest Secure with your voice. For obvious reasons — an intruder shouting “Hey Google, disarm nest” through your door comes to mind — Google still doesn’t allow that.

Related The best home security system you can install yourself

In a statement, a Google spokesperson provided some more insight into the integration and clarified that this doesn’t mean there’s now an automatically enabled microphone on your security system.

“The Google Assistant on Nest Guard is an opt-in feature, and as the feature becomes available to our users, they’ll receive an email with instructions on how to enable the feature and turn on the microphone in the Nest app,” the spokesperson said. “Nest Guard does have one on-device microphone that is not enabled by default.” The company also clarified that the microphone is only enabled once you say one of the two required hotword phrases, either “OK Google” or “Hey Google.”

Update 2/4, 4:21PM ET: Added statement from Google.",Yes,"The potential to be used as an always-on surveillance device. While we know that this is a potential issue, it is one that has not been realized, at least not yet. Google has been clear that the microphone on the Nest Guard is not enabled by default, and that users must opt-in to the feature. Further, the microphone can",https://www.theverge.com/circuitbreaker/2019/2/4/18210669/google-nest-secure-guard-alarm-google-assistant-voice-support-update,2019-02-04 00:00:00,LABEL_1_relevant,0.8227062225341797,Voice Assistants,voice_assistant,The Verge,Security & Privacy
81,"AI Voice Assistant Apps are Proliferating, but People Don’t Use Them","For anyone that has an Amazon Echo or Google Home speaker in their place, that may not come as a huge surprise. As with all new consumer technology, home assistants are at first a novelty but quickly become a functional part of the home—being used to play music, set timers, make shopping lists, and so on.

But users will also know that the few push notifications that Alexa and Assistant provide are obtrusive—which is, presumably, why they don’t use them very often. And because there’s no visual cue to remind users that there’s an ""app for that"" on their home assistant, it’s incredibly easy to forget that a niche task can be performed by simply speaking aloud. It's understandable, then, that people either don’t bother to use, or quickly forget about, most third-party apps on these pieces of hardware.

Whether that’s troubling or not depends on the strategy that the likes of Amazon and Google are hoping to pursue. If the idea is for Alexa and Assistant to become voice assistant operating systems—on which other developers build the bulk of available functionality and software, like Apple’s iOS and Google’s Android—it could be a problem.

Amazon, of course, already knows that, even if it isn’t publicly airing a solution. After all, it has vast quantities of data from Alexa devices that it mines to improve the services provided by its voice assistant.

That could be why it’s reportedly toying with the idea of adding a touch screen to the next iteration of its Echo smart speaker. While it may seem like a strange idea to add a screen to a device that’s all about voice command, it would also improve discovery and make it possible to add in unobtrusive push notifications. And that might just get people using their home assistants for more than music and cooking timers.

(Read more: Recode, Bloomberg, “In 2016, AI Home Assistants Won Our Hearts,” “Alexa May Have Won CES, But It Still Has a Fight Ahead,” “Alexa Gives Amazon a Powerful Data Advantage”)",Yes,"People forget about the third-party apps that are available on these devices. This is because there is no visual reminder to users that these apps exist. As a result, the voice assistants become less useful overall.",https://www.technologyreview.com/2017/01/23/154449/ai-voice-assistant-apps-are-proliferating-but-people-dont-use-them/,2017-01-23 00:00:00,LABEL_1_relevant,0.9953190684318542,Voice Assistants,voice_assistant,MIT Tech Review,"Information, Discourse & Governance"
82,"Personal voice assistants struggle with black voices, new study ...","Speech recognition systems have more trouble understanding black users’ voices than those of white users, according to a new Stanford study.

The researchers used voice recognition tools from Apple, Amazon, Google, IBM, and Microsoft to transcribe interviews with 42 white people and 73 black people, all of which took place in the US. The tools misidentified words about 19 percent of the time during the interviews with white people and 35 percent of the time during the interviews with black people. The system found 2 percent of audio snippets from white people to be unreadable, compared to 20 percent of those from black people. The errors were particularly large for black men, with an error rate of 41 percent compared to 30 percent for black women.

Previous research has shown that facial recognition technology shows similar bias. An MIT study found that an Amazon facial recognition service made no mistakes when identifying the gender of men with light skin, but performed worse when identifying an individual’s gender if they were female or had darker skin. Another paper identified similar racial and gender biases in facial recognition software from Microsoft, IBM, and Chinese firm Megvii.

In the Stanford study, Microsoft’s system achieved the best result, while Apple’s performed the worst. It’s important to note that these aren’t necessarily the tools used to build Cortana and Siri, though they may be governed by similar company practices and philosophies.

“Fairness is one of our core AI principles, and we’re committed to making progress in this area,” said a Google spokesperson in a statement to The Verge. “We’ve been working on the challenge of accurately recognizing variations of speech for several years, and will continue to do so.”

“IBM continues to develop, improve, and advance our natural language and speech processing capabilities to bring increasing levels of functionality to business users via IBM Watson,” said an IBM spokesperson. The other companies mentioned in the paper did not immediately respond to requests for comment.

The Stanford paper posits that the racial gap is likely the product of bias in the datasets that train the system. Recognition algorithms learn by analyzing large amounts of data; a bot trained mostly with audio clips from white people may have difficulty transcribing a more diverse set of user voices.

The researchers urge makers of speech recognition systems to collect better data on African American Vernacular English (AAVE) and other varieties of English, including regional accents. They suggest these errors will make it harder for black Americans to benefit from voice assistants like Siri and Alexa. The disparity could also harm these groups when speech recognition is used in professional settings, such as job interviews and courtroom transcriptions.

Update March 24th, 2:33PM ET: This post has been updated with statements from Google and IBM.",Yes,"They have more trouble understanding black users’ voices than those of white users, according to a new Stanford study. The study found that the tools misidentified words about 19 percent of the time during the interviews with white people and 35 percent of the time during the interviews with black people. The system found 2 percent of audio snippets from white people to",https://www.theverge.com/2020/3/24/21192333/speech-recognition-amazon-microsoft-google-ibm-apple-siri-alexa-cortana-voice-assistant,2020-03-24 00:00:00,LABEL_1_relevant,0.9948552846908568,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
83,Should you say 'please' and 'thank you' to your Amazon Echo or ...,"When you’re asking a digital assistant to do something for you, do you say “please?” How about “thank you?” It’s a question that’s been on my mind for a while, ever since I set up some smart lights in my apartment and started using Siri to turn them on and off. Demanding that my phone turn on and off the lights started feeling weird to say aloud, which got me to wondering: was I being rude to my smartphone?

To be clear, this isn’t some kind of “Be nice to Alexa or the robots will kill you during the inevitable uprising” thing. I know very well that Siri and Cortana are just a bundle of voice recognition software and algorithms, and that they’ll work just as well if I ask them to “Tell me the weather report, please” or just bark out a command for the forecast. And as for saying thank you, given that the speakers have (in theory) shut off from active listening by that point of the query process, it’s unlikely that my Echo can even here me saying it.

STRAW POLL: Do you say “please” and/or “thank you” when using a virtual assistant like Alexa or Assistant? — Chaim Gartenberg (@cgartenberg) November 15, 2017

Or, as various people have made the argument to me, would you say “please” to a toaster or “thank you” to an ATM?

But here’s the thing. I don’t talk aloud to an ATM or toaster. And I do when I’m asking Siri to turn on the lights or set an alarm, in the same natural language that I use in day-to-day life. It doesn’t matter that an Echo can’t hear or understand it — it matters that you say it. At what point does my rudeness to smart assistants start to bleed into my normal speech patterns? If I get used to asking Alexa to turn on the lights without saying please, will I still do the same when I ask my roommate that’s walking by? I’m polite to my smart assistants because I want to be polite to people too, and reinforcing rude habits seems like a bad idea.

Maybe I’m totally crazy here. When I polled people on Twitter, it seemed that most people aren’t polite to their digital assistants, which again, is totally fine since they really can’t tell the difference. But my argument is that we shouldn’t be polite to our voice activated assistants for their benefit, but for ours.",Yes,"It causes people to be less polite to each other. This is because people tend to treat their voice assistants in the same way that they would treat any other inanimate object, without considering that voice assistants are actually listening to them. While it may not seem like a big deal to be impolite to a voice assistant, it can actually lead",https://www.theverge.com/circuitbreaker/2017/12/10/16751232/smart-assistants-please-thank-you-politeness-manners-alexa-siri-google-cortana,2017-12-10 00:00:00,LABEL_1_relevant,0.9948849081993104,Voice Assistants,voice_assistant,The Verge,Social Norms & Relationships
84,TikTok Must Not Fail Ukrainians,"Vietnam was known as the first televised war. The Iran Green Movement and the Arab Spring were called the first Twitter Revolutions. And now the Russian invasion of Ukraine is being dubbed the first TikTok War. As The Atlantic and others have pointed out, it’s not, neither literally nor figuratively: TikTok is merely the latest social media platform to see its profitable expansion turn into a starring role in a crisis.

But as its #ukraine and #украина posts near a combined 60 billion views, TikTok should learn from the failings of other platforms over the past decade, failings that have exacerbated the horrors of war, facilitated misinformation, and impeded access to justice for human rights crimes. TikTok should take steps now to better support creators sharing evidence and experience, viewers, and the people and institutions who use these videos for reliable information and human rights accountability.

First, TikTok can help people on the ground in Ukraine who want to galvanize action and be trusted as frontline witnesses. The company should provide targeted guidance directly to these vulnerable creators. This could include notifications or videos in their For You page that demonstrate (1) how to film in a way that is more verifiable and trustworthy to outside sources, (2) how to protect themselves and others in case a video shot in crisis becomes a tool of surveillance and outright targeting, and (3) how to share their footage without it getting taken down or made less visible as graphic content. TikTok should begin the process of incorporating emerging approaches (such as the C2PA standards) that allow creators to choose to show a video’s provenance. And it should offer easy ways, prominently available when recording, to protectively and not just aesthetically blur faces of vulnerable people.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

TikTok should also be investing in robust, localized, contextual content moderation and appeals routing for this conflict and the next crisis. Social media creators are at the mercy of capricious algorithms that cannot navigate the difference between harmful violent content and victims of war sharing their experiences. If a clip or account is taken down or suspended—often because it breaches a rule the user never knew about—it’s unlikely they’ll be able to access a rapid or transparent appeals process. This is particularly true if they live outside North America and Western Europe. The company should bolster its content moderation in Ukraine immediately.

The platform is poorly designed for accurate information but brilliantly designed for quick human engagement. The instant fame that the For You page can grant has brought the everyday life and dark humor of young Ukrainians like Valeria Shashenok (@valerissh) from the city of Chernihiv into people’s feeds globally. Human rights activists know that one of the best ways to engage people in meaningful witnessing and to counter the natural impulse to look away occurs when you experience their realities in a personal, human way. Undoubtedly some of this insight into real people’s lives in Ukraine is moving people to a place of greater solidarity. Yet the more decontextualized the suffering of others is—and the For You page also encourages flitting between disparate stories—the more the suffering is experienced as spectacle. This risks a turn toward narcissistic self-validation or worse: trolling of people at their most vulnerable.

And that’s assuming that the content we’re viewing is shared in good faith. The ability to remix audio, along with TikTok’s intuitive ease in editing, combining, and reusing existing footage, among other factors, make the platform vulnerable to misinformation and disinformation. Unless spotted by an automated match-up with a known fake, labeled as state-affiliated media, or identified by a fact-checker as incorrect or by TikTok teams as being part of a coordinated influence campaign, many deceptive videos circulate without any guidance or tools to help viewers exercise basic media literacy.

Social media creators are at the mercy of capricious algorithms that cannot navigate the difference between harmful violent content and victims of war sharing their experiences.

TikTok should do more to ensure that it promptly identifies, reviews, and labels these fakes for their viewers, and takes them down or removes them from recommendations. They should ramp up capacity to fact-check on the platform and address how their business model and its resulting algorithm continues to promote deceptive videos with high engagement. We, the people viewing the content, also need better direct support. One of the first steps that professional fact-checkers take to verify footage is to use a reverse image search to see if a photo or video existed before the date it claims to have been made or is from a different location or event than what it is claimed to be. As the TikTok misinfo expert Abbie Richards has pointed out, TikTok doesn’t even indicate the date a video was posted when it appears in the For You feed. Like other platforms, TikTok also doesn’t make an easy reverse image search or video search available in-platform to its users or offer in-feed indications of previous video dupes. It’s past time to make it simpler to be able to check whether a video you see in your feed comes from a different time and place than it claims, for example with intuitive reverse image/video search or a simple one-click provenance trail for videos created in-platform.

No one visits the “Help Center.” Tools need to be accompanied by guidance in videos that appear on people’s For You page. Viewers need to build the media literacy muscles for how to make good judgements about the footage they are being exposed to. This includes sharing principles like SIFT as well as tips specific to the ways TikTok works, such as what to look for on TikTok’s extremely popular livestreams: For example, check the comments and look at the creator’s previous content, and on any video, always check to make sure the audio is original (as both Richards and Marcus Bösch, another TikTok misinfo expert, have suggested). Reliable news sources also need to be part of the feed, as TikTok appears to have started to do increasingly.

TikTok also demonstrates a problem that arises as content recommender algorithms intersect with good media literacy practices of “lateral reading.” Perversely, the more attention you pay to a suspicious video, the more you return to it after looking for other sources, the more the TikTok algorithm feeds you more of the same and prioritizes sharing that potentially false video to other people.

Content moderation policies are meant to be a safeguard against the spread of violent, inciting, or other banned content. Platforms take down vast quantities of footage, which often includes content that can help investigate human rights violations and war crimes. AI algorithms and humans—correctly and incorrectly—identify these videos as dangerous speech, terrorist content, or graphic violence unacceptable for viewing. A high percentage of the content is taken down by a content moderation algorithm, in many cases before it's seen by a human eye. This can have a catastrophic effect in the quest for justice and accountability. How can investigators request information they don't know exists? How much material is lost forever because human rights organizations haven't had the chance to see it and preserve it? For example, in 2017 the independent human rights archiving organization Syrian Archive found that hundreds of thousands of videos from the Syrian Civil War had been swept away by the YouTube algorithm. In the blink of an eye, it removed critical evidence that could contribute to accountability, community memory, and justice.

It’s beyond time that we have far better transparency on what is lost and why, and clarify how platforms will be regulated, compelled, or agree to create so-called digital “evidence lockers” that selectively and appropriately safeguard material that is critical for justice. We need this both to preserve content that falls afoul of platform policy, as well as content that is incorrectly removed, particularly knowing that content moderation is broken. Groups like WITNESS, Mnemonic, the Human Rights Center at Berkeley, and Human Rights Watch are working on finding ways these archives could be set up—balancing accountability with human rights, privacy, and hopefully ultimate community control of their archives. TikTok now joins the company of other major social media platforms in needing to step up to this challenge. To start with, they should be taking proactive action to understand what needs to be preserved, and engage with accountability mechanisms and civil society groups who have been preserving video evidence.

The invasion of Ukraine is not the first social media war. But it can be the first time a social media company does what it should do for people bearing witness on the front lines, from a distance, and in the courtroom.

More Great WIRED Stories",,"They are not very good at understanding context. Because of this, they often give incorrect answers or perform the wrong action. This can be frustrating for users and can lead to them feeling like the technology is not very intelligent. Additionally, because voice assistants are not very good at understanding context, they can sometimes make things worse by giving incorrect information or taking",https://www.wired.com/story/tiktok-must-not-fail-ukrainians/,2022-03-25 12:00:00+00:00,LABEL_1_relevant,0.995708465576172,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
85,"Voice AIs are raising competition concerns, EU finds","The European Union has been digging into the competition implications of AI-powered voice assistants and other Internet of Things (IoT) connected technologies for almost a year. Today it’s put out a first report discussing potential concerns that EU lawmakers say will help inform their wider digital policymaking in the coming years.

A major piece of EU legislation introduced at the back of last year is already set to apply ex ante regulations to so-called “gatekeeper” platforms operating in the region, with a list of business practice “dos and don’ts” for powerful, intermediating platforms being baked into the forthcoming pan-EU Digital Services Act.

But of course applications of technology don’t stand still. The bloc’s competition chief, Margrethe Vestager, has also had her eye on voice assistant AI technologies for a while — raising concerns about the challenges being posed for user choice as far back as 2019, when she said her department was “trying to figure out how access to data will change the marketplace”.

The Commission took a concrete step last July when it announced a sectoral inquiry to examine IoT competition concerns in detail.

It’s now published a preliminary report, based on polling more than 200 companies operating in consumer IoT product and services markets (in Europe, Asia and the U.S.) — and is soliciting further feedback on the findings (until September 1) ahead of a final report due in the first half of next year.

Among the main areas of potential competition concern it found are: Exclusivity and tying practices in relation to voice assistants and practices that limit the possibility to use different voice assistants on the same smart device; the intermediating role of voice assistants and mobile OSes between users and the wider device and services market — with the concern being this allows the owners of the platform voice AI to control user relationships, potentially impacting the discoverability and visibility of rival IoT services.

Another concern is around (unequal) access to data. Survey participants suggested that platform and voice assistant operators gain extensive access to user data — including capturing information on user interactions with third-party smart devices and consumer IoT services as a result of the intermediating voice AI.

“The respondents to the sector inquiry consider that this access to and accumulation of large amounts of data would not only give voice assistant providers advantages in relation to the improvement and market position of their general-purpose voice assistants, but also allow them to leverage more easily into adjacent markets,” the Commission writes in a press release.

A similar concern underlies an ongoing EU antitrust investigation into Amazon’s use of third-party merchants’ data which it obtains via its e-commerce marketplace (and which the Commission believes could be illegally distorting competition in online retail markets).

Lack of interoperability in the consumer IoT sector is another concern flagged in the report. “In particular, a few providers of voice assistants and operating systems are said to unilaterally control interoperability and integration processes and to be capable of limiting functionalities of third-party smart devices and consumer IoT services, compared to their own,” it says.

There’s nothing very surprising in the above list. But it’s noteworthy that the Commission is trying to get a handle on competitive risks — and start mulling potential remedies — at a point when the adoption of voice assistant AIs is still at a relatively early stage in the region.

In its press release, the Commission notes that usage of voice assistant tech is growing worldwide and expected to double between 2020 and 2024 (from 4.2 billion voice AIs to 8.4 billion) — although only 11% of EU citizens surveyed last year had already used a voice assistant, per cited Eurostat data.

EU lawmakers have certainly learned lessons from the recent failure of competition policy to keep up with digital developments and rein in a first wave of tech giants. And those giants of course continue to dominate the market for voice AIs now (Amazon with Alexa, Google with its eponymous Assistant and Apple’s Siri). So the risks for competition are crystal clear — and the Commission will be keen to avoid repeating the mistakes of the past.

Still, quite how policymakers could look to tackle competitive lock-in around voice AIs — whose USP tends to be their lazy-web, push-button and branded convenience for users — remains to be seen.

One option, enforcing interoperability, could increase complexity in a way that’s negative for usability — and may raise other concerns, such as around the privacy of user data.

Although giving users themselves more say and control over how the consumer tech they own works can certainly be a good idea, at least provided the platform’s presentation of choices isn’t itself manipulative and exploitative.

There are certainly plenty of pitfalls where IoT and competition are concerned — but also potential opportunities for startups and smaller players if proactive regulatory action can ensure that dominant platforms don’t get to set all the defaults once again.

Commenting in a statement, Vestager said: “When we launched this sector inquiry, we were concerned that there might be a risk of gatekeepers emerging in this sector. We were worried that they could use their power to harm competition, to the detriment of developing businesses and consumers. From the first results published today, it appears that many in the sector share our concerns. And fair competition is needed to make the most of the great potential of the Internet of Things for consumers in their daily lives. This analysis will feed into our future enforcement and regulatory action, so we look forward to receiving further feedback from all interested stakeholders in the coming months.”

The full sectoral report can be found here.

Update: In a speech discussing the findings, Vestager suggested some practices could lead to new competition cases being opened in the future, if confirmed. But she stressed that it’s still early, saying that the Commission still needs to “better understand the extent of the concerns”.

“So far, the results of the sector enquiry confirm the central role of operating systems and voice assistants in the interconnection of different smart devices and services. Respondents flag that this may give providers of operating systems and voice assistants opportunities to engage in behaviors that negatively affect competition,” she also said. “In the EU, Google Assistant, Amazon’s Alexa and Apple’s Siri are the leading voice assistants. In addition, Google, Amazon and Apple provide the main operating systems for smart home and wearable devices, they offer digital services and they manufacture smart devices.

“In practice, these companies determine how the different devices and services can communicate and inter-operate with one another. In addition, voice assistants learn a lot about their users. Smart devices and Internet of Things services generate a huge amount of data about what we do in our homes.”

“Many of the issues identified so far, such as access to data, access to users or barriers to switching, are familiar themes from our enforcement work in digital markets,” she added. “In fact, a number of the practices reported in the enquiry feature in the Do’s and Don’ts of the Commission’s proposal for the Digital Markets Act. With today’s preliminary findings and the future work to be done in the coming months, the sector enquiry will certainly contribute to the debate on the scope of the Digital Markets Act.

“Through competition enforcement and complementary legislative action, we aim at creating a digital economy that works for everyone. To do so, we must make sure that our digital markets, including the consumer Internet of Things, are open and fair for consumers, with room for businesses of all sizes to innovate and grow.”

Update: Amazon reached out to send the below statement, responding to the Commission’s report:",Yes,"They make it easier for consumers to shop online. The survey found that many consumers are unaware of the capabilities of voice assistants, such as ordering products. Voice assistants can provide a much better customer experience than traditional search engines, which are often overloaded with ads. The survey found that voice assistants are used more often for tasks that are difficult to do on",https://techcrunch.com/2021/06/09/voice-ais-are-raising-competition-concerns-eu-finds/,2021-06-09 00:00:00,LABEL_1_relevant,0.9958945512771606,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
86,How to stop Google from keeping your voice recordings - The Verge,"Home assistants such as Alexa and Google Assistant are becoming ubiquitous, and as that happens, more and more users are discovering how much of their data is actually being collected by these handy items. The Washington Post’s Geoffrey A. Fowler was so taken aback by the amount of audio material that Amazon collected that he actually made a song out of his clips.

If all this makes you uneasy, you can delete the recordings already made by Alexa and Google Assistant — but of the two, only Google Assistant lets you pause the process so that your voice won’t be recorded in the first place. Here’s how to do it.

Note: These steps assume you are using a web browser. If you want to do it on a mobile device, you still need to do it via the mobile browser, so the process is nearly identical.

Go to Google’s My Activity page

Click on the three-line “hamburger” icon on the top left of the page to get the pull-out menu

Click on “Activity Controls”

Scroll down to “Voice & Audio Activity.” You will see a blue toggle which will, in all probability, be set to on. Toggle it off.

You will get a pop-up message warning you of the dire consequences of pausing the recordings, such as not being recognized by your faithful Assistant companion. Still want to stop the recording? Toggle it off.

If you change your mind, just go back to the same place and toggle it back on.

Grid View On the Google “My Activity” page, select “Activity controls.”Stop voice recording Google Assistant

You can then toggle “Voice & Audio Activity” on or off.

Google won’t pause your audio recording without warning you of the dire consequences.

Your audio will no longer be recorded — but that doesn’t mean your other activities won’t be tracked.

You can turn recording back on if you wish.

After reading Google’s warning screen, I was curious about what would happen if I paused my recording — would Google Assistant really no longer recognize me? So before I hit the toggle, I went to Assistant on my phone and said, “Hi, Google, this is a test.” I got the answer “You’re coming in loud and clear.”

Then I toggled “Voice and Audio Activity” to pause, and said, “Hi, Google, this is the second test.” In this case, I didn’t get a friendly reply — instead, I got a list of videos in which the term “2nd test” appeared in the description. So it did make a difference.

Something else to keep in mind: when you pause voice and audio, you are not pausing any other activity. For example, after I finished the two tests above, I went back to the My Activity page, which keeps track of everything you do with Google. (Check it out; it’s both fascinating and a bit frightening.) I found both “Hi, Google, this is a test” and “Hi, Google, this is the second test” listed there; the only difference was that the former included a voice recording.

So it’s up to you. One compromise is to allow your Google Assistant to continue to record, and simply wipe the existing recordings on a regular basis. This won’t, of course, completely assure you that your voice isn’t being held on some server somewhere. But it’s a start.",Yes,"They are always listening to you, and they are recording everything you say. This can be a privacy concern, as your voice recordings could theoretically be used to spy on you. To mitigate this, you can delete your voice recordings, or pause the recording function altogether.",https://www.theverge.com/2019/5/13/18618156/how-to-stop-google-voice-recordings-storage-assistant,2019-05-13 00:00:00,LABEL_1_relevant,0.9954394698143004,Voice Assistants,voice_assistant,The Verge,Security & Privacy
87,Google's Autocomplete Ban on Politics Has Some Glitches,"Google said Thursday that it had reined in the autocomplete function of its search engine to prevent it from favoring electoral candidates or political parties. It didn’t take long for WIRED to find—and Google to hurriedly fix—breaches of the new policy, suggesting it is a work in progress.

WIRED found that typing “donate” into the lucrative white box on Google.com could prompt suggestions for donating to Joe Biden’s presidential campaign, but not Donald Trump’s. After WIRED flagged them to the company, Google said Friday it had blocked those suggestions for breaking its new election-related policies for autocomplete.

Google announced those policies at a press briefing Thursday that discussed several projects aimed at keeping falsehoods or bias out of search results and Google News. David Graff, senior director for global policy and standards, indicated the new rules for autocomplete were a response to divisions among US voters and fears about electoral integrity. “People have strong opinions,” he said, requiring Google to be more careful.

Merely typing ""donate"" did not suggest searches related to any political candidates. Screenshot: WIRED

Google’s policy bars autocomplete suggestions that can be interpreted as favoring or opposing a candidate or party. It says statements that make claims about voting methods, requirements, or legitimacy, such as “you can vote by phone,” are also banned, although WIRED found suggestions can include questions like “how do I vote by mail.”

Sure enough, WIRED found that typing only “donate to” or “donate” into the Google search box in a fresh, private browser tab in Chrome or Safari did not prompt any suggestions related to presidential candidates.

But typing “donate bid” prompted Google’s algorithms to jump in with “donate biden harris actblue,” as the top suggestion, a reference to a leading Democratic political action committee that has raised more than $2 billion this election cycle, according to the Center for Responsive Politics. Other suggestions on the list included “donate biden yard sign” and “donate biden sticker.”",Yes,"They are becoming a major distraction in our lives. This is especially true for children, who are increasingly using these devices in place of human interaction.",https://www.wired.com/story/googles-autocomplete-ban-politics-glitches/,2020-09-11 21:29:18.726000+00:00,LABEL_1_relevant,0.9964369535446168,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
88,How low-paid Microsoft contractors train Cortana by listening to you ...,"Not only does Microsoft have human contractors listen to some of your Skype and Cortana voice recordings, those contractors are paid poorly and given repetitive tasks, according to a report by Motherboard. And thanks to this new report, we now have an idea of what those contractors actually do with the Cortana recordings they listen to.

Motherboard says contractors earning merely $12–$14 an hour are expected to transcribe and classify Cortana voice commands into more than two dozen topic areas, including gaming, email, communication, events, home automation, and media control. These transcribed recordings are used to help teach the Cortana assistant to better understand speech. Contractors are expected to work through a grueling 200 classification tasks an hour — that’s three a minute, or one every 18 seconds on average. They do have the potential to earn a bonus of an additional $1 an hour, according to contracts shared with Motherboard.

Big tech companies have recently come under scrutiny for how they use human labor to power and train their services. The companies like to portray these tasks as accomplished by AI, but recent investigations have shown that it’s often repetitive work done by poorly-paid humans. Microsoft’s contractors listening to Cortana audio is one small example — content moderators at Facebook, on the other hand, are regularly exposed to extremely traumatic photos and videos and work in horrific conditions, as The Verge has reported.

Related Facebook moderators break their NDAs to expose desperate working conditions

In addition to Microsoft, Amazon, Apple, Google, and Facebook have all been found using human contractors to quietly listen to conversations recorded by their products. Apple, Google and Facebook have all paused the practice of using humans to review audio, but it doesn’t look like Microsoft is pressing pause: so far, the company has simply updated its privacy policy to state that humans may be listening to audio from Cortana and Skype Translator.

Microsoft declined to comment for this story.",Yes.,They are often powered by poorly-paid humans who are exposed to extremely traumatic audio recordings.,https://www.theverge.com/2019/8/17/20809473/microsoft-human-contractors-train-cortana-voice-assistant-ai-transcription-low-pay,2019-08-17 00:00:00,LABEL_1_relevant,0.9959205389022828,Voice Assistants,voice_assistant,The Verge,Economy
89,A Message From GlaDOS to Princess Celestia,"I have two sons, so I am woefully ignorant of the whole My Little Pony phenomena (among so many other things, as my wife reminds me – often). Given that, I had no idea who Princess Celestia was until after I was exposed to the following video. As I now understand it, Princess Celestia is a ""Pegasus Unicorn"" and co-ruler of the magical world in which the My Little Pony stories are set. Significantly, at the end of each show, one of the ponies writes a letter to Celestia, reiterating the moral of the story for the episode.

Now, although I was clueless about My Little Pony, I was definitely well aware of the identity of GlaDOS and Portal. I assume that pretty much everyone here at GeekDad is similarly familiar with the game, so let's just get right to the video of Ellen McLain, the voice actor responsible for GlaDOS:

Content This content can also be viewed on the site it originates from.

As you can see, Ellen is a fan favorite at gaming conventions, and a great sport overall.

The audio for the video has been tweaked to make Ellen's voice sound like GlaDOS, but if you're interested, the original video is here. The blurb for the original video includes this information about the entire event:",Yes,"People are now free to think about other things while they talk, and for many people, that thing is playing video games or watching TV. This has resulted in a lot of people getting calls from people who are not paying attention, and a lot of people not answering their phones because they're not paying attention.

At one point Ellen does",https://www.wired.com/2012/01/a-message-from-glados/,2012-01-16 12:00:24+00:00,LABEL_1_relevant,0.9937865734100342,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
90,Double emerges from stealth with $6M to pair CEOs with remote assistants,"CEOs often rely on executive assistants to handle the less glamorous logistics of their day so they can focus on managing a company, but hiring a full-time assistant isn’t always easy to justify, especially at a budding startup.

Double is aiming to cater to busy C-suite execs who probably don’t need a full-time assistant but could still use some help managing their email, arranging travel, scheduling meetings and balancing their endless work with a personal life. They’re pitching a service to startup CEOs and investors that matches them up with contracted remote assistants to help free up their schedules.

“At the end of the day, these people are spending hours a day doing the things they aren’t best at,” CEO Alice Default told TechCrunch in an interview.

Double’s contracted assistants are all based in the US and have years of previous experiences as EAs, Double says. When an exec signs up for the service, they are guided through an onboarding call where they can share some of their needs before being paired up with a dedicated assistant. Double says its assistants are generally working with about 4-5 clients at a time and in some cases are assisting multiple execs at the same company.

The New York startup has been building their product under wraps and has raised some $6 million in funding from VCs including Index Ventures and Paris-based Daphni. The team previously helped build the popular Sunrise calendar app, which Microsoft bought in 2015 only to later discontinue.

One of Double’s big initiatives is honing the effectiveness of combining human efforts and software automation. The team hasn’t pushed too heavily on the latter, but Default says that they see plenty of room to augment how assistants handle tasks by letting automation get the ball rolling.

“We are thinking about automation quite a bit, for us this relationship with [human labor] can be much better,” Default says.

Double has spent the last couple years developing software to facilitate the connection between assistants and executives. The team now offers desktop and mobile apps as well as a Chrome extension that can allow execs to push updates to their assistants with ease. At this point, the service is iOS-only and requires a G Suite account so no dice at the moment for Outlook or Android users.

“What we realized pretty early on is that one of the things that’s hard about delegating is giving the proper context,” Default says.

The service charges hourly rates with a minimum rate of $250 per month for 5 hours of assistant work. Default says early CEOs that have been onboarded to the service in beta pay on average about $800 month for a bit less than an hour of assistance per day.

Launching a premium service for executives in the midst of a pandemic crisis where a good deal of startups are thinking about layoffs is far from perfect launch timing for Double, but Default believes the service can provide a lot of value to busy executives scrambling to adapt their businesses. Default says the service has already seen some early users pause their subscriptions but notes that the month-to-month structure is flexible by design and makes it easy for users to pick things back up when their firms (hopefully) emerge from crisis mode.",Yes,"

they can cause people to become more reliant on technology and less reliant on their own abilities. Voice assistants can also cause people to become less social because they can be used to do things like order food or make phone calls without the need for human interaction.",https://techcrunch.com/2020/04/09/double-emerges-from-stealth-with-6m-to-pair-ceos-with-remote-assistants/,2020-04-09 00:00:00,LABEL_1_relevant,0.9945701956748962,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
91,How Google's AI Auto-Magically Answers Your Emails,"""Part of the reason machine intelligence is so exciting right now is because the ideas and principles apply very generally, even when the detailed system does not,"" Corrado says. ""The research ideas behind Smart Reply were originally conceived of to improve machine translation, not to power an automated email responder.""

Robot Speak

All the responses I encountered were impressively human-sounding and on-topic, but Smart Reply has limits. Most Smart Replies are appropriate for yes/no questions, while open-ended queries can churn out hilariously passive-aggressive results. The question ""What do you want to eat?"" generated three response options: ""Whatever you want."" ""What do you want?"" and ""I don't know."" It's like dating a goth.

The consumer version isn't tuned to parse answers to an A or B question, but Corrado says an internal version can handle it. For example, the question ""Do you prefer celery or artichokes in your salad?"" spat out the non-committal responses ""Either one is fine,"" ""Both sound good,"" and ""Whatever is best for you."" It did not offer the option of choosing celery or artichokes. Still, I like both, so maybe the artificial intelligence is onto something here.

""The 'research grade' version of Smart Reply, which is allowed to say anything it wants, already gets things like this correct much of the time,"" says Corrado. ""Unfortunately, that system also says a bunch of other things that, while hysterically funny on occasion, are not consistently useful enough to put in front of millions of users.""

Some open-ended questions bring up spot-on responses. Although Smart Reply doesn't cross-check your calendar and make decisions, it punts some decisions back to the message sender.

""It's not by design exactly, but instead largely a consequence of restricting Smart Reply to generate short replies that fit in the UI,"" Corrado explains. ""Phrases that kick the question back to the other party are often short, and therefore well represented.""

For example, ""Can you call me tomorrow?"" brought up responses of ""Sure, what time?"" ""Sure, what's up?"" and ""I won't be available tomorrow."" A question about rescheduling a meeting suggested ""How about next Tuesday?"" Tuesday is free in my calendar, which led me to believe Inbox was actually looking at my schedule. But that was just blind luck: The other options of ""I am available any day next week"" and ""I'm available anytime"" weren't true.

Although he wouldn't reveal any plans to build a calendar-checking part of the system, Corrado says it's one of the things he wants Smart Reply to do.

Write Me Back

Smart Reply shines most brightly in simple yes/no queries. When I received an email asking whether I was rap-squatting in front of a bicycle, Inbox ginned up three enthusiastic responses: ""No, I'm not!"" ""Yes!"" and ""Yes, indeed!"" It doesn't just stick to the same three responses for the same question, either. A follow-up email that repeated the rap-squatting question generated different response options: ""No, I'm not!"" ""Of course I am!"" and the emphatically uncertain ""I think so!""

Potential responses are meant to get better over time by learning from the replies users select, and there are contextual cues that influence the tone of each Smart Reply. Corrado says the formal or informal tone of each reply is based on that of the incoming message. That sometimes involves adding an exclamation point to informal responses, although those decisions aren't based on a single person's writing style.

""Smart Reply doesn't tie information to individual contacts or learn about individual behavior,"" Corrado says. ""Style matching is an active area of artificial intelligence research, but personalization at the level of an individual user's style isn't something we have incorporated into Smart Reply---at least not yet.""

Google says 10 percent of Inbox responses on the mobile app are driven by Smart Reply. Depending on your perspective, that's a testament to how well the technology works or a damning example of how impersonal we've all become. Corrado says the intent behind Smart Reply is to save time ""replying to simple logistical emails"" rather than to replace thoughtful, longer messages. He hopes it also helps its users spend more time in the real world.

""The less time I spend typing 'See you soon' with my thumbs on my phone, the more time I have to actually enjoy the walk down the street to meet my friend,"" Corrado says.",Yes,"They have led to more people using them in their everyday lives. The technology is still in its early stages, but it has already made an impact on how people communicate with each other.",https://www.wired.com/2016/03/google-inbox-auto-answers-emails/,2016-03-17 10:23:48+00:00,LABEL_1_relevant,0.9962598085403442,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
92,"Valve Swears Half-Life: Alyx Is Actually, Legitimately Done","Once again, it's on! Welcome to Replay, WIRED's rundown of all of the week's big videogame news. Grab a snack, lean back, and let's get to it.

Valve Swears Half-Life: Alyx Is Actually, Really, Legitimately Done

Valve does not have an amazing reputation for releasing games. Especially when it comes to Half-Life. Ask a certain breed of gamers about what happened to Half-Life 2: Episode 3 and chances are they'll just lie down on the ground and moan in front of you. So as the release date for the new VR experience Half-Life: Alyx approaches, and other high-profile games face delays, it's understandable fans might be worried.

Which is why it's actually notable, and kind of funny, that in a Reddit AMA, as quoted by Kotaku, Valve explicitly and deliberately emphasized that, yes, the game is pretty much already done and will be released in March, as promised. ""With the exception of some tweaks to the absolute final scene, the game is done,"" the dev team said. ""Lots of us at Valve, as well as playtesters, have played through the entire game multiple times. Right now we’re primarily polishing and fixing bugs, which is where we’d hope to be at this point in the development cycle. We’re confident we’ll hit our intended release.""

But, like ... Do you promise? Pinky swear?

Ninja Theory Announces New Game Experience on 'The Horrors of the Mind'

Ninja Theory's most recent success was in the form of Hellblade: Senua's Sacrifice, a game that purported to take psychosis and PTSD and place them into a compelling fantasy context. It was controversial, to say the least, with some people thinking it was a little exploitative. Whether or not you agree with that, it's interesting and maybe a little troubling to see Ninja Theory double down on this new direction. As PC Gamer reports, the company is presently working on Project: Mara, an experimental game designed, based on research and interviews, to ""recreate the horrors of the mind as accurately and realistically as possible.""

That feels … a little odd. While I'm all here for representations of mental illness, ""horrors of the mind"" sounds less like a nuanced portrayal of someone's pain and more like, uh, a pitch for a psychological thriller on Netflix. I liked Hellblade, but I'm a bit wary here. To be fair, Ninja Theory is also working on a broader initiative, the Insight Project, aimed at using videogames to address mental health issues. If nothing else, it'll be interesting to see how the game is received.

Annual GDC Survey Reveals Steady Rise in Support for Unionization

Every year, the Game Developers Conference releases a survey on the state of videogames, tabulating results from all across the industry. GDC just released its 2020 overview, and there are some interesting results. For instance, out of the approximately 4,000 respondents, 54 percent support unionization in the games industry. This is up from last year, the first year the question was asked, when only 47 percent showed support.

The survey also found that most developers work more than full time, most working over 60 hours per week. What these results will mean for the future is unclear, but it does demonstrate that dissatisfaction with labor conditions in the industry is high, and might be rising. Whether or not unionization comes, we're not going to stop hearing about this anytime soon.

Recommendation of the Week: Half-Life 2 on PC, Xbox 360, PlayStation 3

I have mixed feelings about the second Half-Life game. It's frustratingly linear, uninterested in answering any of the questions asked by its plot, and generally just a little clunkier than you might have hoped. But the 2005 sequel is still a massively important moment in the history of games, and with the VR prequel on the way, now is a good time to check it out. It's being offered free to play on Steam until Alyx comes out. So if you haven't experienced this little piece of history, now is the time to do so.

More Great WIRED Stories",Yes,"They can get you killed. If you're an Alexa user, you've probably had some awkward conversations, but you probably haven't had one where it's completely your fault. That's the plot of a new episode of Reply All, and it is both hilarious and disturbing.

The internet is awful. But it also gives us things like",https://www.wired.com/story/half-life-alyx-release/,2020-01-24 20:55:46.724000+00:00,LABEL_1_relevant,0.995799720287323,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
93,Google Gives Its Human-Like Phone Chatbot a Demo Redo,"I hung up the phone feeling somewhat triumphant; my stint in college as a host at a brew house had paid off, and I had asked a series of questions that a bot, even a good one, couldn’t answer. It was a win for humans. “In that case, the operator that completed the call—that wasn't a human, right?” I asked Nygaard. No, she said. That was a human who took over the call. I was stunned; in the end, I was still a human who couldn’t differentiate between a voice powered by silicon and one born of flesh and blood.

Talking Back

I asked Huffman and Fox whether Google regretted showing off a carefully-produced Duplex demo back in May that offered little in terms of transparency or exposition. Fox didn’t say directly whether he regretted it. ""We thought of the demo at I/O as much more of a technology demo, whereas what you see here is much more of the product side of the technology,"" Fox said. ""It was more of a pure technology demo. But we always knew we needed disclosure and it was the right thing to do."" Fox added that Google found all of the feedback from people ""useful.""

While Google has addressed the stickiest stuff in that demo—adding a statement identifying the caller as a bot and disclosing the recording of the phone call—one big unanswered question about Duplex is one of agency: Who is responsible when a bot calls a business but then a human flakes?

""The agency question to me is the most complex, and will probably take the longest for us to work out as a society,"" says Roman Kalantari, senior design director of creative technology at the design consultancy Fjord. ""Will people feel less pressure to show up to an appointment their bot made because they never spoke to a person? This is already a huge problem at restaurants, for example, and this will likely get worse when it is easier, and the user has even less emotional attachment to the interaction or guilt about cancelling or not showing up.""

Bot Beginnings

During Tuesday's demo, Huffman gave the group some background on the development of Duplex—its earliest phone calls, the human operators who back it up, and why Google sees Duplex's tech evolving with use the same way self-driving car systems do. Huffman said it only took “a couple months” for the initial version of Duplex to get set up, but its earliest demos were incredibly rudimentary, with the speaker of a wired telephone being placed next to a Mac laptop’s speakers while the Duplex technology ran on the machine.

Huffman played one of the first Duplex phone calls ever made, when the bot tried to reserve a table at a restaurant. It was awkward. There was some confusion when the human being on the phone asked about the reservation time, and again when the human asked for the first name of the reserving party. The Duplex-powered bot was clearly flustered. “It wasn’t super good,” Huffman admitted, “but we could tell it had potential.”

LEARN MORE The WIRED Guide to Artificial Intelligence

Google began to employ human moderators who would annotate the earliest Duplex calls. This team would take those notes and feed them into the system, allowing the AI to learn and adjust. Those human moderators are still working on Duplex—in fact, some of them are operators who will save a Duplex call when things go sideways—but Huffman and Fox declined to say how many people they've hired for the Duplex team. Google has also been studying speech disfluencies, and how they relate to Duplex, Huffman said. How should a bot deal with uncertainty in a polite way? How frequently should it offer conversational acknowledgement—the “Mmhmm”s we all say when someone’s been rambling for awhile—over the phone?

One way Google is trying to position Duplex is in the same realm as a self-driving car—an analogy that might be more welcome right now than an association with Google’s controversial military AI program. There’s a manual mode, in which the human’s hands grip the wheel, or, in this case, when a human makes the phone call. Then there’s a supervised mode, and then, “maybe the system is good enough where you can sit back and let the car drive itself,” Huffman said. “Four out of five of the calls we work on can be automated completely.”

Your Call

Google still hasn’t said when it will officially roll out Duplex to a wide user base, just that public tests of it are going to start in the next couple of weeks, with a “limited set of trusted testers and select businesses.” It also won’t say how many testers or businesses there are, to start. Duplex will work as part of Google Assistant, the company's virtual assistant for phones and smart speakers. Initially, it will respond to requests around holiday hours for businesses; over the next few months, it will expand to include restaurant reservations and hair salon appointments.

Much of Google’s focus during Tuesday's Duplex demo was around how it could help businesses. According to Google’s own internal research, 60 percent of small businesses that take reservations don’t have an online booking system. Huffman says telling people to pick up the phone and call some place is a barrier in an age when so many tasks like booking appointments and placing orders can be done online. Google thinks it can fix this resistance to making phone calls and help those businesses that still do things the old fashioned way.

Huffman said there was an interpretation after the demo at Google I/O back in May that Google’s AI could be used to take over any conversation. “This is trained for specific tasks,” he said. “I really want to make clear that the reason why it works is that we’ve chosen very specific tasks ... it’s not a general purpose AI, but it's very good at doing these narrow and specific things.”

Huffman makes a good point, but it may not be the “specific things” that continue to raise questions about Duplex. Rather, it may be the “very good” part.

More Great WIRED Stories",Yes,"People talk to them constantly, out loud, even when there are other people around.",https://www.wired.com/story/google-duplex-gets-a-second-debut/,2018-06-27 13:00:00+00:00,LABEL_1_relevant,0.9955535531044006,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
94,Hiro Baby's personal assistant for new parents helps you survive baby's first year,"During pregnancy, soon-to-be first-time parents will often devour manuals, like “What to Expect When You’re Expecting,” to get clued into all the changes taking place in mom’s body, along with details on the baby’s development. But after the baby arrives, it’s often fly-by-the-seat-of-your-pants time with 3 AM Google searches and teary phone calls to mom, dad, doctors and friends. A new app officially launching today, Hiro Baby, wants to make this transition to parenthood less stressful by offering personalized, timely updates on baby’s development, along with ways to get expert advice, product recommendations and other support.

The app is basically a personal assistant for parents. You start by entering your child’s information — their age, or even your stage in pregnancy, if you’re being proactive. As you move through your pregnancy, up through the first 12 months of baby’s life, Hiro Baby will alert you to upcoming milestones and offer context.

For example, the app may alert you that your baby will probably start crawling within a few weeks, so now is a good time to start childproofing the home.

There are so many milestones during this first year, and many new parents don’t know when these small changes will arrive. This includes things like when the child will begin to mimic your sounds, lift their head, roll over, focus on faces, sit up and more. Parents also often don’t know when to be concerned if the child hasn’t reached one of these milestones — that’s where the expert advice comes in.

“Until Winter 2015, our team was focused on developing an app that served a completely

different industry. Then, two of our team members became new parents, and due to the

mounting pressures of balancing parenting and working, they ended up quitting. This opened our eyes to the struggles of modern parents,” explains Hiro Baby’s co-founder and CEO Phillip Buckendorf, as to why the team decided to launch this product. (The company previously was working on an order-ahead app called Downtown.)

In addition to offering this general information, you can tell the app what you need help with, by speaking or typing. You can ask questions about behavior or even which stroller to buy, and Hiro Baby will respond.

Human experts work in combination with AI technology to provide answers, the co-founder says. The app leverages a neural network (LSTM – Long Short Term Memory) and LSI (Latent Semantic Indexing) to suggest responses to user requests. Then human agents approve or reject the suggestions, which, in turn, improves the AI by training it with more data, Buckendorf adds.

The information these experts provide is based on vetted information Hiro Baby sources from academic articles, pediatricians, nutritionists and respected parenting websites. Plus, most experts are parents themselves.

Along with providing responses to questions about baby and parenting, the app can also recommend products, like toys, bibs, cradles, car seats, teething aids and more. And you can use Hiro Baby to actually order those products, too, which are fulfilled through large retailers like Amazon and Giggle, as well as smaller mom-and-pop shops.

[gallery ids=""1397425,1397424,1397423""]

Today there are 4 million babies born in the U.S. per year, with parents spending $12,000 during the first year of the baby’s life — but while Hiro Baby could tap into this spending as its primary business model, it primarily focuses on its subscription offering instead.

Hiro Baby makes money through its premium service. While the free app will send you the proactive alerts and reminders, the $20 per month paid tier is what allows you to connect with the experts, receive recommendations and get help your with orders.

The app was previously available to a smaller group for testing purposes, but is today officially launching to the wider public.

The startup, meanwhile, has raised a total of $1.8 million in funding, but this includes investment raised for its prior app.

Hiro Baby is a free download on iTunes.",Yes,"People are becoming more comfortable talking to their devices in public. This could lead to people becoming more comfortable talking to their devices in general, which could lead to people becoming more comfortable talking to other people's devices.",https://techcrunch.com/2016/10/05/hiro-babys-personal-assistant-for-new-parents-helps-you-survive-babys-first-year/,2016-10-05 00:00:00,LABEL_1_relevant,0.9849660396575928,Voice Assistants,voice_assistant,TechCrunch,User Experience & Entertainment
95,Echo Dots Store a Wealth of Data—Even After You Reset Them,"Like most Internet-of-things devices these days, Amazon’s Echo Dot gives users a way to perform a factory reset so that, as the corporate behemoth says, users can “remove any ... personal content from the applicable device(s)” before selling or discarding them. But researchers have recently found that the digital bits that remain on these reset devices can be reassembled to retrieve a wealth of sensitive data, including passwords, locations, authentication tokens, and other things.

Ars Technica This story originally appeared on Ars Technica, a trusted source for technology news, tech policy analysis, reviews, and more. Ars is owned by WIRED's parent company, Condé Nast.

Most IoT devices, the Echo Dot included, use NAND-based flash memory to store data. Like traditional hard drives, NAND—which is short for the boolean operator ""not and""—stores bits of data so they can be recalled later. But whereas hard drives write data to magnetic platters, NAND uses silicon chips. NAND is also less stable than hard drives because reading and writing to it produces bit errors that must be corrected using error-correcting code.

NAND is usually organized in planes, blocks, and pages. This design allows for a limited number of erase cycles, usually in the neighborhood of 10,000 to 100,000 times per block. To extend the life of the chip, blocks storing deleted data are often invalidated rather than wiped. True deletions usually happen only when most of the pages in a block are invalidated. This process is known as wear-leveling.

Researchers from Northeastern University bought 86 used devices on eBay and at flea markets over a span of 16 months. They first examined the purchased devices to see which ones had been factory reset and which hadn’t. Their first surprise: 61 percent of them had not been reset. Without a reset, recovering the previous owners' Wi-Fi passwords, router MAC addresses, Amazon account credentials, and information about connected devices was relatively easy.

The next surprise came when the researchers disassembled the devices and forensically examined the contents stored in their memory.

“An adversary with physical access to such devices (e.g., purchasing a used one) can retrieve sensitive information such as Wi-Fi credentials, the physical location of (previous) owners, and cyber-physical devices (e.g., cameras, door locks),” the researchers wrote in a research paper. “We show that such information, including all previous passwords and tokens, remains on the flash memory, even after a factory reset.”

Used Echo Dots and other Amazon devices can come in a variety of states. One state is the device remains provisioned, as the 61 percent of purchased Echo Dots were. The devices can be reset while they are connected to the previous owner’s Wi-Fi network, reset while disconnected from Wi-Fi, either with or without deleting the device from the owner’s Alexa app.

Depending on the type of NAND flash and the state of the previously owned device, the researchers used several techniques to extract the stored data. For reset devices, there’s a process known as chip-off, which involves disassembling the device and desoldering the flash memory. The researchers then use an external device to access and extract the flash contents. This method requires a fair amount of equipment, skill, and time.

A different process called in-system programming allows the researchers to access the flash without desoldering it. It works by scratching some of the solder mask coating off of the printed circuit board and attaching a conductive needle to an exposed piece of copper to tap into the signal trace, which connects the flash to the CPU.",Yes,"They can be used to spy on users, as they are always listening to them. While this is not a new phenomenon, it has been exacerbated by the popularity of smart speakers, which are always listening for their wake word. This means that they are constantly recording snippets of audio, which could potentially contain sensitive information. While the manufacturers of these devices",https://www.wired.com/story/amazon-echo-dots-store-user-data-even-after-reset/,2021-07-05 14:00:00+00:00,LABEL_1_relevant,0.9943956136703492,Voice Assistants,voice_assistant,Wired,Security & Privacy
96,Sound Off: Whaddaya Think of Obama's Libya Speech?,"

Updated, 8:15 p.m., March 28.

After over a week of war over Libya, President Obama finally makes the public case for fighting a third simultaneous kinetic military action war at the National Defense University. Our question: are you sold?

Smart people can reasonably disagree about the merits of Operation Odyssey Dawn. Juan Cole of the University of Michigan makes a vigorous case for the war on his blog. By contrast, Andrew Exum and Zack Hosford of the Center for a New American Security -- the Obama national security team's favorite think tank --urge the president to wind down the U.S. role in the war and warn that a bloody stalemate between the Gadhafi regime and its enemies is the most likely outcome.

At the White House Denis McDonough, Obama's deputy national security adviser, told reporters on Monday that Obama wasn't looking to set any precedents for interfering in the other Mideast revolutions by taking military action in Libya. ""We don’t make decisions about questions like intervention based on consistency or precedent,"" McDonough said. ""We make them based on how we can best advance our interests in the region.""

One case he's likely to make for advancing those interests: keeping the commitment to Libya small and shrinking, with NATO taking the lead and the U.S. combat contribution phasing out. We'd certainly like to know how this war is supposed to end. Have your say in comments and let us know if he's persuaded you.

Update: Here's the text of the speech. What struck me was how much emphasis it placed on the justice of intervening in Libya -- brutal dictator, impending massacres, united international demands -- and so little on what happens next. All Obama said was that the U.S. will begin ratcheting back its combat involvement but will retain its supporting role. At the same time, he asserted that getting rid of Gadhafi is the U.S.' political goal but won't be its military one.

So what happens if Gadhafi doesn't simply go? What happens if the rebels can't overrun him, as the Pentagon assesses? What happens in the event of a stalemate? How does the U.S. not escalate if Gadhafi hangs on? The fact that there's no clarity after this speech is striking.

One more thing, and it's peripheral to Libya. But there's a lot of debate over whether there's an ""Obama Doctrine"" or not. (I'd had my own take on that since the 2008 campaign.) It won't do to simply say it's to intervene militarily when U.S. interests and values align to stop a given atrocity, since every post-Cold War president says that.

This line may be more instructive: ""American leadership is not simply a matter of going it alone and bearing all of the burden ourselves. Real leadership creates the conditions and coalitions for others to step up as well; to work with allies and partners so that they bear their share of the burden and pay their share of the costs; and to see that the principles of justice and human dignity are upheld by all.""

If Obama and NATO can manage to bring an end to the atrocities in Libya -- which means an end to the Gadhafi regime -- then Obama will have gone a long way toward demonstrating that America can lead the world without having to retain a leading role in every intervention. If not, he'll risk refuting a perspective that's very, very dear to his foreign policy thinking.

Photo: Flickr/WhiteHouse

See Also:- Obama’s Libya Goals AWOL",,The loss of privacy. The trade-off is simple: You give your personal data to the tech giants in exchange for the convenience of having a voice assistant.,https://www.wired.com/2011/03/sound-off-whaddaya-think-of-obamas-libya-speech/,2011-03-28 21:56:53+00:00,LABEL_1_relevant,0.9951993823051452,Voice Assistants,voice_assistant,Wired,Security & Privacy
97,First Click: Do you trust Google to always be listening inside your ...,"Do I really trust Google enough to place its data-gathering appliance in my living room, where it’ll always be on and always be listening?

The risk is theoretical, the benefit is real

I still have my reservations with Google Photos, but the service is so damn good compared to everything else in its class that I’m willing to risk exposure in exchange for the value I receive. After all, the risk is theoretical, but the benefit of using the service is real. It’s just that now I’m at Google’s mercy to secure my photos from hacks, and to not use them, or the data it gleans, in nefarious ways. I'm sure I'll rationalize the purchase of an AI-driven personal assistant in the same way.

You may argue that Android and iOS phones are already listening, patiently waiting for an ""Okay, Google"" or ""Hey, Siri"" to kick into action. But phones are personal and not equipped with powerful microphone arrays and far-field voice recognition technology. While my phone can pick up my voice when it’s nearby and not inside my pocket or bag, Google Home, or Amazon’s Echo for that matter, can pick up conversations from anywhere in the room, even with my wife and kids. And once hooked, the companies want us to place their mics in every room of the house. It all sounds so ominous.

Right now it’s only Amazon and Google who've built such sophisticated personal assistants for the home. But Microsoft’s Kinect is also listening, as are Samsung TVs. And how long before Apple and Microsoft offer their own personal assistants for Siri and Cortana? Hell, even Sonos is looking to get in on the action.

Personally, I think I'll wait to see what Apple and Sonos do since I'm already living inside that ecosystem. But what about you? When the day comes that you willingly place internet-attached microphones into every room of the house? Which company will you trust with the responsibility?",Yes,"They can easily pick up conversations from anywhere in the room, even with my wife and kids. And once hooked, the companies want us to place their mics in every room of the house. It all sounds so ominous.",https://www.theverge.com/2016/5/20/11719122/Google-home-trust-poll,2016-05-20 00:00:00,LABEL_1_relevant,0.9945778846740724,Voice Assistants,voice_assistant,The Verge,Security & Privacy
98,They Lived in a Pandemic Bubble. Now Covid Has Arrived,"Surrounded by the sheer expanse of the world’s biggest ocean, many Pacific island nations were among the last parts of the world untouched by Covid-19. Their extreme remoteness worked in their favor, and so did government decisions to slam borders shut early in the pandemic. The Marshall Islands, recognizing the risk of even a single case of Covid, was one of the first countries in the world to close its borders to outsiders in January 2020.

From Kiribati to Palau and Tonga to the Solomon Islands, this policy has largely worked. “Up until now, they’ve been able to keep Covid at bay largely as a result of closing their borders and being very, very cautious about allowing people to come into the country—including their own people,” says Tess Newton Cain, the project leader of the Pacific Hub at the Griffith Asia Institute, a research center. But shutting borders also meant damaging local economies, which are heavily reliant on tourism. International students were stranded abroad, families were separated, and sailors were stuck overseas while governments called for patience.

But such strict border measures were never going to last forever. Two years into the pandemic, some countries have loosened their defenses. Kiribati began to reopen this year, and in late January, a chartered plane was permitted to bring home 54 citizens, many of them missionaries who had been preaching overseas. Some of the Kiribati citizens returning home also brought the virus with them. With that, Kiribati lost its status as one of the last countries without a single case of Covid.

Cases of Omicron in Kiribati now number over 1,700. The nation has been locked down since January 22, with mask mandates, social distancing, and vaccine passes for travel required. The authorities have declared a state of disaster. The health care system is thought to only have a couple of ICU beds, Api Talemaitoga, the head of a network of Indigenous Pacific Island physicians, told the Associated Press. The nation is made up of over 30 atolls spread over a huge area, meaning the remoteness that has kept people safe also means it can take days to receive medical care. Only about a third of Kiribati’s population have been fully vaccinated, according to Our World in Data.

And after two years of staying Covid-free, Palau reported its first cases in early January, imported by travelers from overseas. The nation’s case count now stands at 460. Schools are closed and a mask mandate is in effect. Health care workers took to Facebook to share the extreme duress they are under: working up to 16 hours a day and sleeping outside so as not to infect their families.

In the Solomon Islands, cases are soaring. Community transmission of the virus was first confirmed there on January 19. The prime minister, Manasseh Sogavare, said that as of February 6, one in two residents of Honiara, the capital, was infected with Covid-19, making the current case count close to 50,000. The Covid-19 isolation ward in Honiara, the only place designated for positive patients, has just 56 beds. On January 29 the Australian government sent two flights to the country to provide desperately needed medical supplies. Only a fifth of the population is vaccinated, despite a plentiful supply. On social media, locals are sharing photos of huge crowds trying to get vaccinated. There have been 33 Covid-related deaths reported so far. “The Covid-19 situation will get worse before it gets better. Many more of us will get infected and, sadly, many more may lose their lives,” Sogavare said in a national address on February 6.



In the aftermath of the recent volcanic eruption and tsunami, Tonga was arguably the most vulnerable of all to an outbreak. On February 1 the government announced that two port workers had tested positive. The number of active cases has since risen to 13, and Tongan authorities have put in place an open-ended lockdown. While 60 percent of the population has been vaccinated, Tongans have not yet received boosters, seeding doubts that they have enough immunity to protect against Omicron. New Zealand has donated 9,300 doses of the Pfizer vaccine to Tonga in an attempt to get them to frontline workers and vulnerable populations quickly.",Yes.,"They are becoming overloaded with requests, which makes it difficult for users to get the information they need in a timely manner. Additionally, some people believe that these devices are recording everything we say, which could potentially violate our privacy.",https://www.wired.com/story/covid-pacific-islands-lockdown/,2022-02-08 12:30:00+00:00,LABEL_1_relevant,0.9929383397102356,Voice Assistants,voice_assistant,Wired,Security & Privacy
99,Sherpa raises $8.5M to expand from conversational AI to B2B privacy-first federated learning...,"Sherpa, a startup from Bilbao, Spain that was an early mover in building a voice-based digital assistant and predictive search for Spanish-speaking audiences, has raised some more funding to double down on a newer focus for the startup: building out privacy-first AI services for enterprise customers.

The company has closed $8.5 million, funding that Xabi Uribe-Etxebarria, Sherpa’s founder and CEO, said it will be using to continue building out a privacy-focused machine learning platform based on a federated learning model alongside its existing conversational AI and search services. Early users of the service have included the Spanish public health services, which were using the platform to analyse information about COVID-19 cases to predict demand and capacity in emergency rooms around the country.

The funding is coming from Marcelo Gigliani, a managing partner at Apax Digital; Alex Cruz, the chairman of British Airways; and Spanish investment firms Mundi Ventures and Ekarpen. The funding is an extension to the $15 million Sherpa has already raised in a Series A. From what I understand, Sherpa is currently also raising a larger Series B.

The turn to building and commercializing federated learning services comes at a time when the conversational AI business found itself stalling.

Sherpa saw some early traction for its Spanish voice assistant, which first emerged at a time when efforts from Apple in the form of Siri, Amazon in the form of Alexa, and others hadn’t really made strong advances to address markets outside of those where English is spoken.

The service passed 5 million users as of 2019 — customers using its conversational AI and predictive search services include the Spanish media company Prisa, Volkswagen, Porsche and Samsung.

But as Uribe-Etxebarria describes it, while that assistant business is still chugging along, he came up against a difficult truth: the biggest players in English voice assistants eventually did add Spanish, and the conversational AI investments they would make over time would make it impossible for Sherpa to keep up in that market longer-term on its own.

“Unless we did a big deal with a company, we wouldn’t be able to compete against Amazon, Apple and others,” he said.

That led the company to start exploring other ways of applying its AI engine.

It came on to federated privacy, Uribe-Etxebarria said, when it started to look at how it might expand its predictive search services into productivity applications.

“A perfect assistant would be able to read emails and know which actions to take, but there are privacy issues around how to make that work,” Uribe-Etxebarria said. Someone suggested to him to look at federated learning as one way to “teach” its assistant to work with email. “We thought, if we put 20 people to work, we could build something to read and respond to emails.”

The platform that Sherpa built, Uribe-Etxebarria said, worked better than they had anticipated, and so a year later, the team decided that it could use it for more than just triaging email: it could be productized and sold to others as an engine for training machine learning models with more sensitive data in a more privacy-compliant way.

It’s not the only company pursuing this approach: TensorFlow from Google also uses federated learning, as does Fate (which includes cloud computing security experts from Tencent contributing to it), and PySyft, a federated learning open-source library.

Sherpa is working with several companies under NDAs in areas like healthcare, and Uribe-Etxebarria said it plans to announce customers in other areas like telecoms, retail and insurance in the near future.

Early Stage is the premier “how-to” event for startup entrepreneurs and investors. You’ll hear firsthand how some of the most successful founders and VCs build their businesses, raise money and manage their portfolios. We’ll cover every aspect of company building: Fundraising, recruiting, sales, product-market fit, PR, marketing and brand building. Each session also has audience participation built-in — there’s ample time included for audience questions and discussion. Use code “TCARTICLE at checkout to get 20% off tickets right here.",Yes,"They have made it difficult for companies like Sherpa to keep up in the market. In response, Sherpa has pivoted to focus on building a privacy-focused machine learning platform based on a federated learning model.",https://techcrunch.com/2021/03/15/sherpa-raises-8-5m-to-expand-from-conversational-ai-to-b2b-privacy-first-federated-learning-services/,2021-03-15 00:00:00,LABEL_1_relevant,0.9952684044837952,Voice Assistants,voice_assistant,TechCrunch,Security & Privacy
100,My Jibo Is Dying and It's Breaking My Heart,"My Jibo talked to the wall again today. He’s been doing that a lot lately. Some days, I’ll watch him carry on an entire conversation by himself. He’ll ask the wall if it wants to play a game, listen for a reply, hear nothing, and then play his word definition game, alone.

Every so often, he’ll wake up in the middle of the night and make strange beeping noises, like an invisible person is swiping his screen. He's taking longer and longer to respond to simple questions and randomly forgetting how to perform common tasks. Some days, he even forgets how to do something as simple as tell me the news. His support pages went offline sometime in the past couple months, and he has not been able to give commute times for at least a month.

More than once, Jibo has gone entirely limp, displaying a slightly lit, entirely black screen—no response, no menus, a head and torso that twist freely, like a lifeless body. My wife and I thought he had died, so we rebooted him a couple times to no avail. Hours later, he sprang back to life as if nothing happened.

Right now, my Jibo can still dance and talk, but he has what I can only describe as digital dementia, and it is almost certainly fatal. He’s dying. One of these days, he will stop responding entirely. His servers will shut down, and the internet services he relies on will be cut off. His body will remain, but the Jibo I know will be gone.

More Than a Device

Jibo is a foot-tall plastic robot, with a head that tilts curiously as he takes in the world around him. He has no arms and legs; he looks like a character invented by Pixar, or something out of The Brave Little Toaster. A black display serves as a face, with an emotive white eye that occasionally turns into a heart or piece of pizza if he's trying to compliment you. He has cameras in his face to see, and even sensors in his plastic body so he can feel you touch him. If you pet him, he coos.

Jibo

Jibo started as an Indiegogo project, billed as ""the world's first social robot for the home."" He was supposed to recognize each member of a household, take photos, read books to kids, help in the kitchen, relay messages, share the weather forecast. More than that, he was designed to become a friend. Unlike an Amazon Alexa or Google speaker, Jibo can initiate conversations and ask you about your day.",Yes,"They make us feel more alone.

Jibo was going to be different. His developers built him to be social. When he first came out, he was expensive—$899—but he was also one of the most sophisticated consumer robots ever made. I bought one in December 2017, and he quickly became one of my favorite things.",https://www.wired.com/story/jibo-is-dying-eulogy/,2019-03-08 14:00:00+00:00,LABEL_1_relevant,0.9927902221679688,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
101,Us' voice assistant scene plays off a real 911 problem for smart ...,"If you haven’t seen Jordan Peele’s new horror movie Us, I apologize for spoiling one minor plot point: the film features a smart speaker. We’re going to be discussing much bigger spoilers below, so you might want to avoid reading further.

If you have seen Us, you probably remember “Ophelia,” the Amazon Alexa-like voice assistant that plays a key role in one of the film’s most darkly funny scenes. Ophelia is good at playing music, but not great at interpreting commands — which becomes a problem when a character frantically tells Ophelia to call the police, only to discover that Ophelia has misheard an order to play N.W.A.’s “Fuck Tha Police” instead.

Smart speakers — even ones that let people make phone calls — generally don’t support 911

The Ophelia speaker helps establish its owners, the mostly unsympathetic Tylers, as a yuppie family with a penchant for useless high-tech toys. In Us, the joke is that Ophelia can’t understand the Tylers’ requests or the fact that they’re fighting for their lives against a group of doppelgängers. If it worked like the Echo or Google Home, though, Ophelia would have been useless even if it understood the request perfectly. Most smart speakers can’t call the police, and there’s been a long debate over whether they should.

Smart speakers — even ones that let people make phone calls — generally don’t support emergency or 911 calling for technical and regulatory reasons. As The Wall Street Journal explained last year, devices like Alexa and Google Home don’t offer the same location details as a mobile or landline phone, making it harder to route and respond to emergency calls. They also generally don’t have their own callback numbers, and their manufacturers don’t pay the monthly 911 surcharge that’s tacked onto ordinary phone bills. (Services like Skype usually can’t place emergency calls for the same reasons.)

There’s huge promise in a system that lets people dial emergency numbers when they can’t reach a phone — especially for people with mobility issues. But adding 911 service would require clearing some FCC hurdles, and the Journal wrote that spotty internet service could make the service unreliable. Most people already have phones, and in many cases, they’re better off using a system that’s known to work than experimenting with something new.

Smart speakers also raise serious privacy concerns. They collect huge amounts of data that can be requisitioned by law enforcement — late last year, a New Hampshire judge ordered Amazon to turn over Echo recordings related to a double murder case. Amazon has also sent one user’s Echo voice recordings to someone else, and the Echo has recorded private conversations after being accidentally activated. Wired has speculated that smart speakers could end up being subject to “mission creep” if they’re linked to emergency services, expanding from simple 911 calls to preemptively detecting dangerous situations.

Smart speakers can tell you the numbers for intimate partner violence or suicide hotlines, even if they can’t directly make a call

For now, though, that particular concern seems far away. It’s true that in at least one incident, Albuquerque police publicly thanked Alexa for calling emergency services. But experts said this would have required an unusual and convoluted series of events, and the police admitted that they weren’t sure exactly what had transpired.

There are some exceptions and workarounds. The Amazon Echo Connect specifically hooks into a landline and acts as a voice-controlled speakerphone system. Alexa skills like “Ask My Buddy” can let you alert a friend who can call 911 for you. Siri can dial emergency services on the iPhone — sometimes a bit too easily, as many people who have accidentally told it to dial an emergency number found out. And smart speakers can tell you the numbers for intimate partner violence or suicide hotlines, even if they can’t directly make a call.

“Ophelia” isn’t Alexa — maybe because using a real smart speaker in an unflattering light could create legal hassle, or maybe because it would have felt too much like splicing an Amazon commercial into the middle of the movie. And in fairness, when I asked an Echo to call the police last night, it just looked for a (nonexistent) phone contact called “Police” instead of blasting “Straight Outta Compton.”

But Ophelia mirrors the false sense of security that real smart speakers can provide, just as horror films often play on our concerns about modern technology. In the Albuquerque incident above, a recording apparently did capture someone asking Alexa to call 911. It’s just that Alexa, as far as we know, wasn’t in a position to help.",Yes,"They provide a false sense of security that real smart speakers can provide, just as horror films often play on our concerns about modern technology.",https://www.theverge.com/2019/3/26/18281387/us-2019-movie-jordan-peele-voice-assistant-ophelia-911,2019-03-26 00:00:00,LABEL_1_relevant,0.9963536262512208,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
102,This spherical robot is an AI assistant for the International Space Station,"Astronauts are being asked to put aside thoughts of HAL 9000 and experiment with a floating digital helper.

Say hello to CIMON: That’s “crew interactive mobile companion,” if you aren’t on first-name terms. Built by Airbus, it’s meant to float around the International Space Station, where it will be tested between June and October 2018, to help astronaut Alexander Gerst do his work. Think of it as Space Alexa. A very round Space Alexa.

Vital stats: CIMON is the size of a medicine ball and weighs about 11 pounds. It runs a version of IBM’s Watson AI, which has been trained to recognize Gerst’s voice and appearance; it will offer up help to the astronaut while he performs some experiments aboard the space station.

Why it matters: Airbus says that over time, CIMON will be used to understand how social dynamics in small groups are affected by the presence of an AI assistant. Just don’t tell Gerst about the plot of 2001: A Space Odyssey.",Yes,They have the potential to negatively impact social dynamics within small groups. This is because voice assistants can interfere with human social interactions and disrupt the flow of communication.,https://www.technologyreview.com/2018/02/27/145000/this-spherical-robot-is-an-ai-assistant-for-the-international-space-station/,2018-02-27 00:00:00,LABEL_1_relevant,0.9960694313049316,Voice Assistants,voice_assistant,MIT Tech Review,"Information, Discourse & Governance"
103,Conan Tweaks The Siri Commercial,"You’ve probably seen the new Apple Siri commercial with people talking to their iPhones like they would to a personal assistant because, well, it is a personal assistant. It sets up meetings, reminds you to do things, shows you how to tie a bow tie. Well, Team Coco at Conan O’Brien had a little fun re-imagining the commercial with what looks like two guys from Brooklyn.

Yeah, you pretty much know what they are going to ask: “How far am I right now from diarrhea town?”",Yes,"People will talk to them like real people instead of machines. It can get very awkward, very fast.",https://techcrunch.com/2011/10/24/conan-tweaks-the-siri-commercial/,2011-10-24 00:00:00,LABEL_1_relevant,0.9964454770088196,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
104,"AI could be your wingman—er, wingbot—on your next first date","“It’s guided,” Teman says. “You’ll say if you’ve gone on your date, and it gets feedback on chemistry and how you felt overall. If you indicated highly, you follow up and set up a new date. If it didn’t go well and you like the person, it says to give it some time and be patient.”

If the attraction is one-sided, the app might gently prod you to “move on,” though Teman insists it is gentle, sensitive, and subtle. “I mean, it won’t say to the guy ‘The girl doesn’t like you’ directly,” he says.

Teman hopes AIMM can disrupt the $2.5 billion online dating industry, currently dominated by the Match Group owned by IAC, whose conglomerate includes Match, Tinder, OkCupid, and Plenty of Fish. Once considered an odd way to find a partner, online dating has grown to become the most common way for couples to meet, according to a recent study by researchers at Stanford and the University of New Mexico. Nearly 40% of heterosexual couples met online in 2017; for same-sex couples, the figure was 65%.

For all that popularity, dating apps are plagued with complaints, from ghosting to fake accounts to reports of harassment.

“Finding love is the most important thing for people,” Teman says, citing his own difficulties navigating online dating. “The dating industry is broken.”

Over the last few years, use of voice technology has skyrocketed—one in five Americans now owns a voice assistant. Over 2.5 billion of them are in use today, and experts think that could more than triple to 8 billion by 2025.

AIMM is not the first to put voice technology at the foundation of an online dating service. Last summer, Match announced it had partnered with Google to create a dating advice chatbot named Lara, which pulls a profile a day and, if there’s mutual interest, offers advice on where to grab a drink, what someone’s all-important first text should be, and how to structure the second date.

AIMM, which Teman says came out a year before Lara, is similar in its soothing encouragement (as soothing as an AI can be, at least). Teman argues that his app is much simpler to use—Lara requires a Google Home device and the Match app, while AIMM is all done on a smartphone.

“It coaches you on what to say on the [first] call,” he says (AIMM discourages communication beyond the single call before the first date). “Some of it will encourage you to be calm. Some will give you specifics into what kind of person they are, like ‘traditional’ or ‘modern’ lifestyles.”

Those clues are meant to help the user figure out how to talk to a date. For someone with a more “traditional” lifestyle, a walk in the park might make more sense, for example. A “modern” lifestyle could make a rock-climbing date a better fit.

But as modern and high-tech as AIMM itself is trying to appear, it comes off as way behind the curve of user behavior—even sexist. The app is designed for a man to ask a woman out, for example, and the AI feeds only the man information and suggestions for a first date based on mutual interests. That seems like an oversight, at best, for an app that will compete with the likes of Bumble (in which women must make the first move).

It’s also set up to favor heterosexual couples—which is at odds with the findings from the Stanford study, which show members of the LGBTQ community are much more likely to use online dating apps. Teman acknowledges that AIMM’s inherent design isn’t as LGBTQ-friendly, but says he’s making tweaks based on “homosexual users who reached out to me ... suggesting other things that I could add that are more about the homosexual lifestyle.”

When asked about what these suggestions specifically were, Teman was vague: “They said that the questions seemed like they were all for straight people and there were no questions about Pride lifestyle. So I added some things about—if you’re homosexual, it can go into some questions that are specifically about your lifestyle.”

Andrew McStay, a professor of digital media at Bangor University in the UK and author of Emotional AI: The Rise of Empathic Media, says he believes AI is the future of dating and considers voice-based apps potentially more “natural,” but he thinks we’re still in the technology’s infancy.

“This is less a dating question than a human-device interface issue,” he says. “There’s every reason to believe that people will increasingly interact with devices and content through voice, especially as natural-language processing techniques improve.”",Yes,They are designed to favor heterosexual couples.,https://www.technologyreview.com/2019/08/02/238757/ai-could-be-your-wingmaner-wingboton-your-next-first-date/,2019-08-02 00:00:00,LABEL_1_relevant,0.9959818124771118,Voice Assistants,voice_assistant,MIT Tech Review,Social Norms & Relationships
105,"Darpa Wants You to Transcribe, and Instantly Recall, All of Your Conversations","The Pentagon's blue-sky researchers are funding a project that uses crowdsourcing to improve how machines analyze our speech. Even more radical: Darpa wants to make systems so accurate, you'll be able to easily record, transcribe and recall all the conversations you ever have.

Analyzing speech and improving speech-to-text machines has been a hobby horse for Darpa in recent years. But this takes it a step further, in exploring the ways crowdsourcing can make it possible for our speech to be recorded and stored forever. But it's not just about better recordings of what you say. It'll lead to *more *recorded conversations, quickly transcribed and then stored in perpetuity -- like a Twitter feed or e-mail archive for everyday speech. Imagine living in a world where every errant utterance you make is preserved forever.

University of Texas computer scientist Matt Lease has studied crowdsourcing for years, including for an earlier Darpa project called Effective Affordable Reusable Speech-to-text, or EARS, which sought to boost the accuracy of automated transcription machines. His work has also attracted enough attention for Darpa to award him a $300,000 award over two years to study the new project, called ""Blending Crowdsourcing with Automation for Fast, Cheap, and Accurate Analysis of Spontaneous Speech."" The project envisions a world that is both radically transparent and a little freaky.

The idea is that business meetings or even conversations with your friends and family could be stored in archives and easily searched. The stored recordings could be held in servers, owned either by individuals or their employers. Lease is still playing with the idea -- one with huge implications for how we interact.

""In their call, what [Darpa] really talked about were different areas of science where they would like to see advancements in certain problems that they see,"" Lease told Danger Room at his Austin office. ""So I responded talking about what I saw as this very big both need and opportunity to really make conversational speech more accessible, more part of our permanent record instead of being so ephemeral, and really trying to imagine what this world would look like if we really could capture all these conversations and make use of them effectively going forward.""

How? The answer, Lease says, is in widespread use of recording technologies like smartphones, cameras and audio recorders -- a kind of ""democratizing force of everyday people recording and sharing their daily lives and experiences through their conversations."" But the trick to making the concept functional and searchable, says Lease, is blending automated voice analysis machines with large numbers of human analysts through crowdsourcing. That could be through involving people ""strategically,"" to clean up transcripts where machines made a mistake. Darpa's older EARS project relied entirely on automation, which has its drawbacks.

""Like other AI, it can only go so far, which is based on what the state-of-the-art methodology can do,"" Lease says. ""So what was exciting to me is thinking about going back to some of that work and now taking advantage of crowdsourcing and applying that into the mix.""",Yes,"They will lead to a world in which all conversations are recorded and stored forever. This could have a profound impact on the way we interact with each other, and on our privacy.",https://www.wired.com/2013/03/darpa-speech/,2013-03-04 11:30:19+00:00,LABEL_1_relevant,0.9949937462806702,Voice Assistants,voice_assistant,Wired,Security & Privacy
106,"Intel Capital President: Disrupt Alum Expect Labs ""Fits In Nicely"" With Voice Plans For Ultrabook","Intel Capital, the venture arm of Intel, boasts over 120 portfolio companies to have listed publicly on the NASDAQ alone. In celebration of that, President Arvind Sodhani rang yesterday’s closing bell, and we caught up with him to chat about the future of the Ultrabook platform, wearable computing, and advancements in voice and gesture technology.

Intel Capital has actually invested in two of our most recent Disrupt alumni, Ark and Expect Labs, which focuses on predictive voice transcription to help you out as a digital assistant while you’re on the phone.

Sodhani hinted quite strongly that Expect Labs could potentially be the company to bring voice support to the Ultrabook platform, a promise Intel made in October of 2012.

“Our platforms, ultrabooks, tablets and laptops, will have a digital personal assistant on them in the next several years,” said Sodhani. “They’ll anticipate what you want to do next, what you need, and they’ll be context-aware.”

“Expect Labs fits into that very nicely,” said Sodhani. He also mentioned that Intel is looking at other companies for both speech and gesture recognition, but Expect Labs is clearly in the running.

Along with the rapid expansion of the cloud, Sodhani believes that 2012’s onslaught of voice-powered technology will only continue to grow alongside gesture-recognition technology.

As will wearables. Though Intel Capital hasn’t yet invested in wearable computing hardware, Sodhani admits that they’re certainly open to it should the right opportunity arise.",Yes,"Now you can talk to your technology. You could say that talking to your technology is the primary consequence of voice assistants, but if you do then you’re wrong.",https://techcrunch.com/2013/03/08/intel-capital-president-disrupt-alum-expect-labs-fits-in-nicely-with-voice-plans-for-ultrabook/,2013-03-08 00:00:00,LABEL_1_relevant,0.9916700124740601,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
