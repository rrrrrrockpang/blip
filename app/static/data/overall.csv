index,title,text,answer,gpt_summary,url,date,prediction,score,sector,sector_class,magazine,label
0,"To Survive, Byte Needs to Win Over Creators Where Vine Failed","When Twitter suddenly announced it was shutting down Vine in 2016, fans immediately started campaigning to bring the beloved short-form video app back. Dom Hofmann, one of the original cofounders, has been working on doing just that for years. After a few stops and starts, his new app Byte finally launched last week. It already has 1.3 million downloads, according to Sensor Tower. It also appears to have learned some lessons from Vine’s demise, but that won’t necessarily be enough for another social video app to survive in 2020.

Vine’s six-second format was a novelty when it came out in 2012, and the limitation birthed a unique, off-the-wall style of comedy. The app incubated some of the internet’s biggest celebrities, including singer Shawn Mendes, comedian Brittany Furlan, and Logan and Jake Paul. But as obituaries for the service noted, it didn’t do enough to support its rising stars and was often criticized for overlooking creators of color. Many eventually left for platforms where they could create longer content and make more money, like YouTube and Instagram.

This time around, Hofmann and his team are trying to do things differently. Byte’s tagline is “creativity first,” and the company appears to be prioritizing creators from the start. There's a dedicated forum where they can discuss issues with one another and Byte staff, creating a more personal feel than larger platforms. Hofmann told TechCrunch he plans to give creators a cut of profits, though the exact details on how the program will work haven’t been released. Byte did not respond to a request for comment from WIRED.

Content This content can also be viewed on the site it originates from.

A lot has changed since Vine died. Instagram Stories exploded in popularity, and TikTok has entered the scene. The Chinese-owned app has been downloaded over 1 billion times, propelled by a powerful algorithm that makes it easy to lose hours to the app. Like Vine before it, TikTok’s features have given rise to new meme formats, uber-viral stars, and even Los Angeles mansions filled with creators. People might still miss Vine, but some see the six-second video as a format whose time has mostly passed.

“It was so interesting because it was so new and different. They created this whole new type of content that’s been expanded and has evolved,” says James Nord, the CEO of Fohr, an influencer marketing platform. “I don’t know that nostalgia on the internet is a business plan.”

It’s unclear what a long-term business plan for Byte might look like. Unlike YouTube and Instagram, the platform is mainly a place for comedy sketches and stunts. That’s not the type of content most brands are comfortable working with creators to produce, says Nord. Brands have a hard time being genuinely funny, and it’s not exactly easy for a creator to give an authentic product testimonial in six seconds.

LEARN MORE The WIRED Guide to Influencers

Beyond making money, creators will need to work with Byte’s stripped-down design. Unlike TikTok, Byte doesn’t have filters or elaborate in-app editing tools, and you can’t add music clips to videos. If you’re used to watching TikTok, the lack of pre-programmed sounds on Byte can seem, well, quiet. Bytes also often feel super short. Brevity is what made Vine unique but could limit the scope of content on Byte. A few seconds is probably not long enough to film a cooking or makeup tutorial—two genres that have flourished on TikTok.

Not every creator is dissuaded by those limitations. “We explore every platform to see if it'll be a right fit for our brand,” says Jacob Pace, the CEO of Flighthouse, a media company with more than 22 million followers on TikTok. Byte “might have a space since it's different (though shorter) than other short-form mobile social media apps.”

So far, Byte’s users have begun populating the platform with the same breed of weird humor that made Vine famous. A clip edited to make Kanye West look like he's FaceTiming with a hamster. A girl crashing into a bush while riding in a Target shopping cart. Shrek vibing to rap music. It’s hard to say anything definitive about a new platform, especially when content may be individualized, but a cursory look at Byte’s Popular section suggests many users are older than Generation Z.",Yes,"People stop interacting with the physical world around them. Instead, they become focused on the virtual world of social media and the internet. This can lead to a number of problems, such as:

-People becoming more isolated and lonely

-People becoming addicted to social media and the internet

-People losing touch with reality and becoming",https://www.wired.com/story/byte-needs-to-win-over-creators-vine/,2020-01-30 17:44:02.637000+00:00,LABEL_1_relevant,0.9950388073921204,Social Media,social_media,Wired,Health & Wellbeing
1,How the Internet Gave All of Us Superpowers,"Somewhere around 1997 I was filling my car with gas, and I spotted a web address on a small orange sticker on the pump. It was an ad of some kind; I don’t remember what for, but I do remember having a realization: The mass adoption of the internet was real. Against all odds, a disturbance in the force had unleashed an entirely new culture outside the established channels, and now commerce was flocking to where the action was. But the main event had hardly started.

The main event—the great cascade that was unthinkable before it became obvious—was what you might call the rise of the bottom. In the late ’90s and early 2000s, masses of people who had previously been ignored as inert, dumb, or passive were awakened by a new level of power. Members of the once dormant TV audience now created and shared their own shows online. The lowly readers of magazines and newspapers wrote their own news in the form of blogs, by the millions, on nearly every subject, all free. In 1999, Napster launched the first music-sharing site. Now music fans—not executives or critics or agents, but ordinary fans—could broadcast their favorite tunes to each other, sharing what they loved, song by song, ditching packaged albums. Oh, and the music was free.

October 2018. Subscribe to WIRED. Plunkett + Kuhr Designers

This bottom-up, peer-to-peer, noncommercial distribution system exploded in popularity and shocked the established players. The ancient business model of paying for a copy was dying. And if music was the first fatality in this revolution, other media businesses felt they would be murdered next. The powers that be promptly and fiercely shut Napster down. That, of course, did not stop the rising tide.

In 2001, Wikipedia erupted. It was the dumbest idea in history—that unknown amateurs anywhere in the world could write a reliable and useful encyclopedia with little supervision. Everyone knew that only world-class experts overseen by ruthless editors could do that. But Wikipedia steadily swelled in size and depth and quality.

Retail commerce, too, was rapidly being overrun by the same bottom-­uprising. In 1997 a San Jose-based site called AuctionWeb sold its millionth item and renamed itself eBay. In 1999, Alibaba launched in China, close to the epicenter of manufacturing. Small-time “factories”—some just one room with a machine—sold direct to citizens around the world. In 2000, Craigslist—once just Craig Newmark’s emailed list of events around San Francisco—expanded its free classifieds, and within a few years it was in 32 cities in North America. By 2001, eBay was reportedly hosting 200 million auctions a year. Then it bought PayPal, a secure, ­person-to-person virtual payment system. Within eBay, just about anyone could sell anything, globally. Manufacturers could sell directly to customers, bypassing traditional retailers.

The rise of the small also created entirely new kinds of giants. Google, born in 1998, was not the first search engine, but it was the first major search engine to rank results based on the number and quality of links to pages. Each time someone linked to another site, they were making ­Google better, fueling its rise to dominance. Although we have forgotten it, the first large-scale social network to exploit people’s connections to each other was Friendster, launched in 2002. LinkedIn, in 2003, found a way to create a business out of networking regular workers together. YouTube would launch a couple years later and demonstrate, once and for all, that the most powerful force that has been released by tiny chips is not speedy compute cycles but the creations of ordinary people.

There were setbacks of course—for a time, the tech boom stalled under its own acceleration, and 9/11 threw the nation into crisis—but, curiously, the dotcom bust didn’t affect the steady growth of the digital masses. The astounding number of new people jumping online kept increasing. Moore’s law continued. Computers got more powerful, bandwidth kept expanding. When it came to the internet, there was a long boom.

The grand migration to bits between 1998 and 2003 delivered many other bright, shiny gifts. During these years we got camera phones, early podcasts, the full human genome sequence, tablet computers, reliable in-car GPS, MP3 players, and home Wi-Fi. Yet none of these superpowers, cool as they may be, can match the superpower of humans connected to one another in real time. For the first time on our planet, hundreds of millions of minds were linked together in thousands of novel ways, each arrangement yielding potential benefit.",Yes,"It has given a voice to previously underrepresented groups of people. In the past, mass media was controlled by a small number of people, who decided what stories were told and how they were told. This resulted in a relatively one-sided view of the world. Social media has changed that by giving everyone a platform to share their stories. This",https://www.wired.com/story/wired25-kevin-kelly-internet-superpowers-great-upwelling/,2018-09-18 10:00:00+00:00,LABEL_1_relevant,0.986543834209442,Social Media,social_media,Wired,"Information, Discourse & Governance"
2,What’s Your Data Worth? Senators Want Facebook to Tell You,"In these days of anti-tech ire, it’s a popular cocktail hour topic: How much is Facebook making off my data? Last year, I spent a month trying to find out, hawking my personal data on blockchain-based marketplaces. I came away with $0.003. On Monday, when Senator Mark Warner (D-Virginia) announced a proposal to force tech companies to tell users the value of their data, he was slightly more generous, ballparking the average at $5 a month.

Gregory Barber covers cryptocurrency, blockchain, and artificial intelligence for WIRED.

In truth, it’s probably only Facebook or Google (and their advertisers) who could hazard a good guess. “The cards are really stacked against us,” says David Carroll, a professor of media design at the New School known for his extensive quest to reclaim his Cambridge Analytica data.

Policymakers wrestling with the value of an individual’s data face a fundamental problem: While data often gets compared to oil, there’s no equivalent to benchmark crude for bits and bytes. The value is dependent on context: who has the data, where it’s going, who it came from. Beyond Warner’s bill, California Governor Gavin Newsom has mused about a “data dividend” through which residents would be paid for the data they generate for tech companies. Newsom acknowledged that it’s unclear what that check would look like.

Warner’s proposal, introduced with Senator Josh Hawley (R-Missouri), would require companies with more than 100 million monthly users to disclose the types of data they collect about each user and to assess its value. It also includes privacy rules similar to those passed last year in California, where you will soon be able to request to delete all or some data held by others, and opt out from having it sold. The Senate bill directs the Securities and Exchange Commission to figure out how companies should calculate that value, with flexibility for different business models and ways data is used. Best of luck to them.

“There’s a little bit of a political scramble to get these ideas out there and it’s going ahead of anyone having an approach that makes sense,” says Glen Weyl, a senior researcher at Microsoft.

The tech companies offer hints about our value, buried in financial reports. In the first quarter of 2019, for example, Facebook says it generated about $2 in revenue per month from advertising for each user. That’s the global figure. In the US, it’s just under $10. (Lucky us!)

But the trick is figuring out what your data is worth to a given platform. That could be more or less, depending on things like how useful you are to the company’s ad-targeting algorithms and how valuable you are to the advertisers themselves. “Any attempts are going to be an artificial average or need to be sensitive to how the market is constantly valuing data,” says Carroll. The harsh reality is that you and I probably aren’t worth very much at all to Facebook or Twitter, at least not alone.

Weyl, who has consulted with California lawmakers on the data dividend idea, suggests that banding together might help improve our lot. One method could involve mediators to help us negotiate collectively, he says---perhaps a kind of data union that could revoke access to our data if it can’t strike a good deal, or if our privacy is violated. Weyl says people can’t individually value their data and the labor that goes into producing it. If the company names a price, we don’t have the know-how (or the will, given how small the individual amounts will likely be) to argue otherwise. I learned that the hard way while acting as my own data broker.

""The cards are really stacked against us.” David Carroll, the New School

We’re all in this together, in other words. That’s all the more true given how data is used for targeting ads: It’s aggregated, mixed and matched, joined with data from elsewhere, repurposed in models used for artificial intelligence. In those cases, Warner leaves open the idea of “joint ownership” between the user and company---suggesting that only some of the value derived from your data is considered yours. That gets complicated, fast. “When data is transferred into models, it becomes an abstraction of your data,” says Carroll. Say there’s a medical trial based on health data from your phone. If you’re perfectly healthy or represent a common demographic for a medical trial, your data probably isn’t worth much to researchers. Should sick people get paid more for signing up? There are experimental ways of figuring out the value of one person’s data to a particular model, but they’re complicated to calculate, and have to be calculated independently for each model.

Perhaps, in the end, the answer will be that simpler is better. Any hard figure to put on our individual data would be an important step forward, says Ashkan Soltani, a former chief technologist of the Federal Trade Commission, especially when it comes to holding companies accountable. “We have this constant issue in privacy that when there’s a data breach, companies argue that there’s no harm because information doesn’t necessarily have any value,” he says. “This [bill] would force companies to articulate that value.” One method might involve developing a value based on real-time bids for our attention from advertisers.",Yes.,It has created a new way of monetizing users by selling access to their attention to the highest bidder.,https://www.wired.com/story/senators-want-facebook-price-data-possible/,2019-06-26 11:00:00+00:00,LABEL_1_relevant,0.9954832792282104,Social Media,social_media,Wired,Economy
3,Facebook grilled in Senate hearing over teen mental health,"Last night, Facebook published two annotated slide decks in an attempt to contextualize the documents that The Wall Street Journal published this month, which reported evidence that the company is aware of its negative impact on teen mental health. These documents were released in anticipation of today’s Senate hearing on the mental health harms of Facebook and Instagram.

The Senate Committee on Commerce, Science, & Transportation questioned Facebook Global Head of Security Antigone Davis over two and a half hours, but lawmakers grew frustrated with Davis’ reticence to answer their questions directly, or provide much information that hasn’t been written in Facebook blog posts rebuking the WSJ reports.

Davis insisted that research from Facebook and Instagram has shown eight out of 10 young people say they have a neutral positive experience on the app, and that her team wants 10 out of 10 young users to have a good experience. But Senators pushed back with other findings from Facebook’s own data, like the fact that among teenagers with suicidal thoughts, 13% of British users and 6% of American users said they could trace those thoughts to Instagram. Senator Richard Blumenthal (who serves as Chair of the Subcommittee on Consumer Protection, Product Safety, and Data Security) said that his office did their own research by creating an account pretending to be a 13-year-old girl. Senator Blumenthal said they followed “easily findable accounts associated with extreme dieting and eating disorders.” Within a day, he said, the account’s recommendations were solely composed of accounts promoting self-harm and disordered eating.

“I congratulate you on a perfectly curated background,” Tennessee Senator Marsha Blackburn chided Davis. “It looks beautiful coming across the screen. I wish the messages that you were giving us were equally as attractive.”

“That is the perfect storm that Instagram has fostered and created. Facebook has asked us to trust it. But after these evasions and these revelations, why should we?” Senator Blumenthal asked.

But in the midst of filibustering tactics that fit right in on the Senate floor (“We’re pretty good at filibustering in the Senate, too,” Senator Klobuchar told Davis), the Facebook Global Head of Safety did elaborate on some of the company’s plans to improve young users’ experience, which Head of Instagram Adam Mosseri previously mentioned on Twitter.

“Young people indicated that when they saw uplifting content or inspiring content, that could move them away from some other issues that they’re struggling with,” Davis said at the hearing. “So one of the things that we’re actually looking at is called ‘nudges,’ where we would actually nudge someone who we saw potentially rabbit-holing down content towards more uplifting or inspiring content.”

In addition to a “nudges” feature, Davis said that the company is looking at a “take a break” feature, which would encourage users to stop looking at the app if they’ve been browsing certain content for too long. In 2018, Instagram introduced a “you’re all caught up” notice, which would appear when the user had scrolled through all posts from the last two days. This feature was introduced alongside “do not disturb” toggles, which helped users control when they wanted to receive notifications. These updates were part of “Time Well Spent” initiatives, designed to curb screen time and encourage healthier social media habits. But by 2020, the space beneath the “caught up” notice was turned into a feed of suggested posts and ads.

Big Tech continues to blatantly prioritize raking in revenue over protecting children and teens, and that must stop. We know that these companies won’t change their ways unless Congress forces them to. That’s why I am re-introducing the KIDS Act. pic.twitter.com/8rgZCevj15 — Ed Markey (@SenMarkey) September 30, 2021

At the hearing, Massachusetts Senator Ed Markey (a social media star in his own right) announced that he would reintroduce legislation with Senator Blumenthal called the KIDS (Kids Internet Design and Safety) Act, which seeks to create new protections for online users under 16. The bill would prohibit platforms directed at children from leveraging follower and like counts, push alerts that encourage users to use the app more, auto-play settings, badges that award elevated levels of engagement, or any design feature that unfairly encourages a user (“due to their age or inexperience,” the bill specifies) to make purchases, submit content, or spend more time on a platform.

Previously introduced in March 2020, Facebook has known about the proposed legislation for almost a year and a half.

“I think our company has made its position really well known that we believe it’s time for the update of internet regulations, and we’d be happy to talk to and work with you on that,” Davis told Senator Markey.

But when Markey directly asked if Facebook would support the KIDS Act, Davis said that Facebook would follow up on the question later.

“Well, your company has had this legislation in your possession for months. And you’re testifying here today before the committee that would have to pass this legislation,” said Senator Markey. “I just feel that delay and obfuscation is the legislative strategy of Facebook, especially since Facebook has spent billions of dollars on a marketing campaign calling on Congress to pass internet regulations, and Facebook purports to be committed to children’s well being.”

At the end of the hearing, Davis said that she hopes the Senate will have hearings with companies that have kid-focused apps, like TikTok and YouTube. Currently, Facebook has a Messenger Kids app, but the company but its Instagram for kids product on hold in light of WSJ’s reporting. Though WSJ has published six leaked documents from Facebook, the company itself only annotated and re-published two of them.",Yes,"It’s created a generation of people who are really good at talking about themselves, but not very good at listening to others.

In other words, social media has created a generation of people who are really good at talking about themselves, but not very good at listening to others.",https://techcrunch.com/2021/09/30/facebook-grilled-in-senate-hearing-over-teen-mental-health/,2021-09-30 00:00:00,LABEL_1_relevant,0.9963691234588624,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
4,Jack Dorsey on ProPublica's Experimental Journalism,"WIRED ICON

Jack Dorsey, cofounder of Twitter

NOMINATES

ProPublica, investigative journalism nonprofit

October 2018. Subscribe to WIRED. Plunkett + Kuhr Designers

I discovered ProPublica about two years ago and became interested in it right away because of its mission: To expose abuses of power and betrayals of the public trust by government, business, and other institutions, using the moral force of investigative journalism. It’s independent, it’s nonpartisan, and it’s powered mainly by philanthropy. ProPublica actually measures success by whether it sparks real-world change. This was a new take on a business model that I hadn’t really thought a lot about.

The most recent ProPublica story that stood out to me was the publication of the audiotape of immigrant children being held in a US Customs and Border Protection facility. There have also been a number of stories on wrongfully convicted folks that have prompted cases to be reopened.

Tweet by @jack on 8/8/18:

“We certainly have a lot of work to do to help journalists do their jobs better. That’s our intent. Help with reach, balance, and economic incentives. We’re behind on those last two.”

I’ve watched how they use Twitter’s technology. Twitter’s superpower is conversation; it’s carrying the chatter. So traditionally, journalists write a few characters and tweet a link to their article and that’s it. But ProPublica threads the key parts of an article, so you end up with a thread that’s 10 tweets long. We asked them why they do that and they said, “We’re going to meet people where they are. They’re coming to a service that is focused on brevity, so we need to translate our stories into that format.”

It’s a creative use of a technology that doesn’t intuitively seem to fit with traditional journalism. That’s definitely more of a risk if your business is based on advertising and sending people to your site. But ProPublica has been doing this for over 10 years. And I think there’s a lot for us to learn in terms of what that means for media. A donation-driven news outlet may seem extreme, but it’s important right now to experiment. This is an experiment that’s done quite well. —As told to Lauren Goode

This article appears in the October issue. Subscribe now.

MORE FROM WIRED@25: 2008-2013

Join us for a four-day celebration of our anniversary in San Francisco, October 12–15. From a robot petting zoo to provocative onstage conversations, you won't want to miss it. More information at www.Wired.com/25.",Yes,"It has 

negatively impacted our ability to interact with each other in person.",https://www.wired.com/story/wired25-jack-dorsey-propublica-experimental-journalism/,2018-09-18 10:00:00+00:00,LABEL_1_relevant,0.9775820374488832,Social Media,social_media,Wired,"Information, Discourse & Governance"
5,"Targeting Black Americans, Russia's IRA Exploited Racial Wounds","Two days before the 2016 presidential election, @woke_blacks posted an anti-voting polemic to its Instagram account. “The excuse that a lost Black vote for Hillary is a Trump win is bs. Should you decide to sit-out the election, well done for the boycott,” the caption read. “I remind us all one more time, anyone who wins can literally change less about the state of Black people, we are on our own, esp. after Obama. Wise up my people!” Another user, @afrokingdom_, shared a comparable sentiment: “Black people are smart enough to understand that Hillary doesn’t deserve our votes! DON’T VOTE!”

According to a new report commissioned for the Senate Intelligence Committee by cybersecurity firm New Knowledge, those accounts, along with dozens more, were part of an extensive and complex campaign to suppress the black American vote by the Russian firm Internet Research Agency.

In late 2017, it was first reported by CNN that the pro-Kremlin enterprise masterminded a plot to infiltrate online communities of the Black Lives Matter movement. But New Knowledge’s report, released Monday, shows a much more sustained and purposeful focus on black Americans—as the IRA went about instigating mistrust in law enforcement and political institutions, while cultivating seemingly authentic narratives of black pride.

The report details how black Americans were among the most exploited online communities by the IRA, cataloging how the Russian firm developed an “expansive cross-platform media mirage” that specifically targeted black people by leveraging popular social media sites. The campaign was “designed to exploit societal fractures” and “erode our trust in media entities and the information environment, in government, in each other, and in democracy itself,” the report states. “This campaign pursued all of those objectives with innovative skill, scope, and precision.”

The 100-page report is based on data sets provided to the Senate by Facebook, Twitter, and Alphabet, comprising tens of thousands of posts between 2015 and 2017. In addition to social media sites, the IRA’s vast and varied disinformation campaign played out across issue-specific domains that they bought such as blackmattersus.com, black4black.info, and blacksoul.us—“a complex effort” to manipulate public opinion with the intention of siphoning votes from Hillary Clinton to help elect Donald Trump. The black community, often a reliably strong voting bloc for Democrats, was a substantial target.

The IRA went about instigating mistrust in law enforcement and political institutions, while cultivating seemingly authentic narratives of black pride.

Spread among Facebook pages, Instagram accounts, Twitter accounts, and YouTube channels, the IRA’s tactics ranged from ad targeting, meme warfare, video manipulation, and the creation of sham account profiles. As my WIRED colleagues point out, much of this content was designed to encourage a lack of faith among black Americans in democratic institutions and decelerate black turnout for Clinton. “As the election became imminent,” the New Knowledge report says, “those themes were then tied into several varieties of voter suppression narratives: don’t vote, stay home, this country is not for Black people, these candidates don’t care about Black people.” (A second report, also produced for the Senate, by Oxford University’s Computational Propaganda Project and Graphika comes to similar conclusions about the IRA’s work, according to the Washington Post.)

One of the more fascinating bits from the New Knowledge report was YouTube’s role as an influence platform. Of the 1,107 videos linked to the IRA, the majority of the content—96 percent, according to researchers—centered on Black Lives Matter and police brutality. A video by Williams & Kalvin, titled “The truth about elections,” is cited as one of the more “overt suppression narratives” employed by an IRA-affiliated entity. In another instance, the IRA co-opted a human-interest story about “an inspirational young Black American who made the national news for a device he invented.” The IRA accounts repurposed that story across its web of counterfeit accounts with messages meant to cultivate distrust in American society: “These are stories of Black children the media don’t want you to see” and “White people invent tools for killing, this Black child is inventing a tool for saving lives.”

Of the Russian firm’s race-specific domains, Blackmatters.us was perhaps the most effective in amplifying the IRA’s intended media mirage. It essentially operated like a digital marketing company as it built an online presence across multiple platforms, creating various accounts to reinforce its brand and distribute content wholesale. Their strategy took on real-world resonance in other instances, as well, as they instituted recruitment efforts. “Posts encouraging Americans to perform various types of tasks for IRA handlers appeared in Black, Left, and Right-targeted groups,” the report states, noting that such efforts had the biggest presence in black communities. “Many posts solicited protestors, writers, activists, lawyers, and photographers to attend the property’s numerous events. They posted job ads for real American writers to create content … a clear example supporting the hypothesis that the IRA engaged in narrative laundering.”",Yes,"It creates a filter bubble. People tend to seek out information that conforms to their beliefs and to avoid information that challenges their beliefs. This leads to the formation of echo chambers, where people only hear their own perspectives and become increasingly polarized.",https://www.wired.com/story/russia-ira-target-black-americans/,2018-12-17 18:25:18.261000+00:00,LABEL_1_relevant,0.9966660141944884,Social Media,social_media,Wired,"Information, Discourse & Governance"
6,"Talenthouse flotation finally reveals a $40M exit for EyeEm, Europe's almost-Instagram","Talenthouse AG has made a name for itself as an aggregator of content creators, which is then commissioned by brands for their own social media channels. Brands just can’t seem to come up with the same kind of authenticity, so they farm it out in this manner. But business is going well, so Talenthouse is now listing on the SIX Swiss Exchange (ticker THAG) in response to this demand for content creation and ability to tap into the ‘creator economy.’

The move is significant in tech startup terms because in June last year New Value AG (trading as Talenthouse) acquired long-time social photos startup EyeEm. When it launched in 2011, only a year after Instagram, it was often talked of in the same breath, but the EyeEm founders studiously repeated that it was a place for for high-end content creators and photographers to sell their wares.

By the time it had been acquired, EyeEm had raised a total of $24 million in VC backing. No price was released at the time for the ‘mostly shares, some cash’ deal.

But the public markets flotation of Talenthouse reveals some interesting data.

Roman Scharf, Co-Founder of Talenthouse told me: “The initial shares will trade around one Swiss franc, and we have 400 million shares outstanding. So the market cap tomorrow will start around 400 million Swiss francs. We paid 37,348,490 million shares for Eyeem plus some cash. Those shares at current share price are $37.3 million. But their exit was roughly $40 million, since they also got cash.”

Talenthouse, with its operational headquarters in London, has 14 million members across brands including EyeEm, Ello, Zooppa and Jovoto. Members of these brands produce content that can be acquired or commissioned by companies such as PayPal, Netflix and Nike.

The Creator Economy generates $2,250 billion annually, employing 30 million people worldwide, according to UNESCO. For instance, in a commission for the UN, Talenthouse took in 16,700 submissions from 142 countries, for a campaign around the Covid-19 pandemic.

In a statement, Clare McKeeve, CEO of Talenthouse said: “We’ve acquired and developed brilliant companies within our portfolio so that our creatives have the tools to be part of an active community whilst successfully monetizing their skills.”

Speaking to me over a call Scharf added: “The SIX market floatation is like a sandbox environment where we learn how to behave as a public company. And then we’re going to go for a NASDAQ listing, because you know Tanenhaus really is about the creative economy. And in the German-speaking world, investors don’t really understand what the creative economy is. They have no clue about Tik Tok.”

Eyeem was started by Florian Meissner, Ramzi Rizk, Gen Sadakane and Lorenz Aschoff in 2011. Photographers could offer their pictures for sale via the portal and Eyeem retained part of the income. But the end, the four founders each held 1.7 percent of their company.

Meissner and Rizk have since moved on to work on the app Aware, an analysis service for health data, and also become Angel investors, taking early stakes in the Gorillas delivery service.",Yes,"Brands are now outsourcing their content creation to the ‘creator economy’, which is being facilitated by Talenthouse. And Talenthouse is now public, via a listing on the SIX Swiss Exchange.",https://techcrunch.com/2022/03/28/talenthouse-flotation-finally-reveals-a-40m-exit-for-eyeem-europes-almost-instagram/,2022-03-28 00:00:00,LABEL_1_relevant,0.9923729300498962,Social Media,social_media,TechCrunch,Economy
7,Millions of people rely on Facebook to get online. The outage left them stranded.,"But in 2016, the program (by now renamed Free Basics) was banned by India’s Telecom Regulatory Authority, which claimed that it violated net neutrality. Despite that setback, it has continued to roll out, with less fanfare, to other countries in the developing world. In 2018, Facebook said Internet.org had put 100 million people online. In 2019, FreeBasics was available in 65 countries, around 30 of them in Africa. Last year, the firm began rolling out Facebook Discover, which allows internet users to access low-bandwidth traffic to all websites (not just Facebook properties) even if they’ve run out of data.

Versions of these programs also exist in Afghanistan, where many new internet users equate Facebook, Facebook Messenger, and WhatsApp with the whole internet. Even among those who have broader access to the full web, Facebook’s suite of products play a vital role. WhatsApp calls, for example, have long since replaced more expensive—and less secure—phone calls globally. Around the world, many small businesses rely on Facebook’s tools to sell and advertise their products.

All of this means that even temporary outages have a big effect, both for advocacy organizations, like the ad hoc groups helping Afghans escape the country, and the vulnerable individuals that are already isolated, like the Afghans in hiding, afraid of Taliban retribution, and waiting for news—often via Whatsapp—for updates.

They ""are already incredibly fatigued and anxious. Losing connection with each other and with trusted allies in the outside world is… devastating"" says Ruchi Kumar, an Istanbul-based Indian journalist (and MIT Technology Review contributor) who is also involved in Afghan evacuation efforts. ""A number are on the verge of suicide, given the deaths and violence they've been witnessing this past month."" The unexplained outage of their primary channel of communication with the outside world compounded the despair, uncertainty, and feelings of abandonment. Losing a chance for evacuation, meanwhile ""is literally life or death.""

It was past midnight for Kumar and Bezhan when Facebook began coming back to life, but even then, some of its functionalities, including search and notifications, were not yet available. Bezhan hadn’t heard back yet about whether she could add that additional name for evacuation.

But she was also concerned that her Afghan friends might be jumping to conclusions about what caused the outage. For weeks since the fall of Kabul, there had been rumors that the Taliban had cut access to the internet. “I bet they are creating rumors and coming up with stories about how the new government is blocking the media,” she says.

They wouldn’t be alone. Responding to similar concerns, a spokesperson for the Democratic Republic of Congo’s ministry of communication—a country known for government-induced Internet shutdowns— took to Twitter to set the record straight: “The internet connection has not been cut,” he wrote at 4:05 p.m. ET. “It is a global blackout crippling WhatsApp, Facebook, and Instagram. Other applications like Twitter are functioning normally. The same goes for the rest of the web.”

This story has been updated with details from Kumar about the impact of the outage.",Yes,"

the outage of platforms like Facebook and WhatsApp can have a devastating effect on groups and individuals who rely on them for communication and coordination, especially in times of crisis.",https://www.technologyreview.com/2021/10/05/1036479/facebook-global-outage/,2021-10-05 00:00:00,LABEL_1_relevant,0.9955958724021912,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
8,Social networks are broken. This man wants to fix them.,"At home, Zuckerman can work while walking on a treadmill with his laptop on a small shelf above it. A hot tub is in the room, too, in case he wants to take a dip.

Zuckerman spoke with MIT Technology Review about how social media started controlling us rather than the other way around.

How are people using technology—rather than, say, lobbying for laws to be passed—to force change in new ways?

We used to make change mostly using law as our primary lever. Now we use the legal lever less; we use the levers of norms of markets and technology more often. #MeToo is an example of a norms-based campaign. It’s basically saying, “We’re going to challenge how people talk about sexual assault and sexual harassment.” And once we change that norm, there’s other legal pieces, market pieces, that’ll come into play. But at its heart it’s trying to change how we have certain conversations.

The point in all of these is that if you can’t get social change done through the traditional model of civics, there is a whole new set of tools, and people are starting to learn how to use these things.

But social networks like Facebook and Twitter control, or at least direct, the information we see by using algorithms to filter what we see in our feed. You worked with two colleagues—Chelsea Barabas of MIT’s Center for Civic Media and Neha Narula at the Media Lab—to build a tool called Gobo that lets people aggregate and filter their feeds on their own. Why?

What this is meant to do is to say, “Look, it’s really a mistake to give one or two companies control over our digital public sphere.” Instead, we need competing platforms. We’re trying to make the case that you want those different social networks because you want more control over your filters about what you see and what you don’t see.

If we need competing platforms, we need tools that would let us use those competing platforms. Gobo is one of those tools. Gobo is an aggregator. It aggregates Twitter and the “aggregateable” parts of Facebook—the public pages.

“Because Facebook’s so friggin’ big, they get all sorts of advantages that make it very hard to catch up with them … So when someone shows up as a meaningful competitor, [Facebook is] more likely to buy them and eat them up than they are to actually have to fight them in the marketplace.”

So first we built the aggregator. And then we built the algorithms [that determine which posts you’ll see]. And rather than making them a top-secret black box, we made it an open box where you can reach in and set the sliders and experiment and say, “Oh, I like how this works. Now let me change it this way and see if it works better for me.”

Where we want to get in the longer term is even more of an open box; we built Gobo so that other people can write filters for it.

After a lot of criticism related to the ways its news feed filters content, Facebook has started pushing posts from users’ friends and family more and deemphasizing ones from brands. Do you feel this move shows Facebook is actually starting to shift its focus?

I don’t believe that this is changing yet, and I won’t believe it until I see a credible business model based on something other than targeted advertising.

I think that building an internet where we didn’t have to pay for anything, because our attention was going to be the commodity that was traded, is one of the most destructive and shortsighted decisions that we could have made. And I do mean ��we,” because I was very much part of that. Until I see Facebook saying, “Look, you’re going to use this as a service and you’re going to pay us for the service,” as opposed to “We’re going to capture your attention and repackage and sell it,” I won’t believe it.

A growing chorus of former Facebook executives and investors have been speaking out against Facebook—saying, for example, that social media is “ripping apart the fabric of how society works.”

I think what’s happening is that some of these people who are stepping out of the really intense “I’ve been in the process of building it” are starting to look at it from the outside and say, “Oh, wow, okay; now I can see the politics from the outside, and I’m not thrilled about what I’ve been associated with.”

We need to figure out how to have those conversations a lot earlier. We should be having those conversations with people who are working at these companies and who are making these design decisions. I want to be having those conversations with my students, because my students are often going to these companies and often find themselves with the opportunity to make those design ­decisions.

Why is it so hard for anyone who’s not Facebook, Instagram (which is owned by Facebook), Twitter, or Snapchat to compete in this social sphere?

Network effects basically say, “I gotta be on Facebook ’cause everybody I know is on Facebook.” Because Facebook’s so friggin’ big, they get all sorts of advantages that make it very hard to catch up with them. They get more bandwidth, they get cheaper servers.

So when someone shows up as a meaningful competitor, [Facebook is] more likely to buy them and eat them up than they are to actually have to fight them in the marketplace.

You wrote a piece in the Atlantic that suggested a publicly supported social network as a potential solution to social media’s echo-chamber effect. Could this actually happen?

I think it’s wholly unrealistic in the United States. It’s something that could be realistic in Europe, [where] you have a public media culture that accepts the idea that you might want to invest money in people having some basic knowledge about politics, the world, the people around them. I could imagine an innovative European public broadcaster saying, “Maybe we build a social network that’s compatible with other social networks, has algorithms designed to help you tune whether you’re getting news about the world, news about your community, and makes those levers visible and controllable.”",Yes,"It started controlling us rather than the other way around. We used to make change mostly using law as our primary lever. Now we use the legal lever less; we use the levers of norms of markets and technology more often. Gobo is an example of a tool that is meant to say, “Look, it’s really a",https://www.technologyreview.com/2018/02/09/3406/social-networks-are-broken-this-man-wants-to-fix-them/,2018-02-09 00:00:00,LABEL_1_relevant,0.9959391355514526,Social Media,social_media,MIT Tech Review,Security & Privacy
9,Facebook's New Privacy Controls Are Long Overdue,"Throughout its 16 years of existence, Facebook has struggled to provide the privacy controls users really want and need to safeguard the data they post to the platform. It's been a challenging project, with plenty of major detours along the way. But today Facebook is announcing a new tool for managing your posts. It may be the most intuitive version of the controls yet, because it's basically what Gmail has offered for email since the year Facebook launched.

The new feature, known as Manage Activity will create the concept of an ""archive"" on Facebook, allowing you to move any or all of your past posts to a secret new home that only you can see. Manage Activity will also introduce a Facebook Trash folder so you can delete posts more easily. The new feature is rolling out in Facebook's mobile apps first.

""Whether you're entering the job market after college or moving on from an old relationship, we know things change in people’s lives, and we want to make it easy for you to curate your presence on Facebook,"" the company says in a blog post.

Screenshot: Facebook

Archive and Trash are two concepts most people are familiar with from email. As with Gmail, posts you move to the trash will stay there for 30 days and then be deleted unless you manually eliminate them sooner or reinstate them. And Manage Activity is introducing batch actions for multiple posts at once so you can view and organize in bulk, rather than going post by post. You can filter by date ranges, types of posts (Photos and Videos, Posts From Other Apps, etc.), specific people, and other categories, and then select individual posts, batch-archive, or batch-delete as needed.

If you archive or delete a post that people are tagged in they’ll lose access to the post. Facebook says they won't be notified of the change, though, so you can archive stealthily. You can't archive other people's posts that you are tagged in—this is only for content you've shared.

Facebook already offers a Select Privacy menu for each of your posts where you can choose who can view the content; your options include Public, Friends, Specific friends, and so on. In some ways, archiving a post will be similar to choosing Only Me, in the sense that only you will be able to view the post and no one else. But if you've been using Only Me as a makeshift archiving feature, Manage Activity isn't going to help put everything in one place. All archived posts will show up in one place, but Facebook says there isn’t a way to view the collection of posts you've marked Only Me.

After dedicating 2018 to figuring out how to ""fix"" Facebook, founder and CEO Mark Zuckerberg published ""A Privacy-Focused Vision for Social Networking"" in March 2019. In it he discussed a problem that every Facebook user has likely been aware of for more than a decade.",Yes,"It blurs the line between public and private, and instead encourages users to share and connect in a way that is more public than private. The result is a platform that is built on top of a very public database of information, which can lead to some very personal data being shared without the user's consent or knowledge.

To address this",https://www.wired.com/story/facebook-manage-activity-privacy-settings/,2020-06-02 14:00:00+00:00,LABEL_1_relevant,0.9456381797790528,Social Media,social_media,Wired,Security & Privacy
10,How DARPA Took On the Twitter Bot Menace with One Hand Behind Its Back,"For instance, the group calling itself Islamic State uses online social media to persuade young people to embrace their cause. Some observers believe Russia embarked on a major social media disinformation campaign the annexation of Crimea. Others say bots played a significant role in influencing the outcome of elections in India in 2014.

So a way of reliably spotting influence bots on Twitter would be hugely useful. Last year, the Defense Advanced Research Projects Agency (DARPA) set out to find such a method by running a four-week competition in which teams were asked to spot bots in a stream of posts on the topic of vaccinations. One team emerged as a clear winner, and the results demonstrated some significant new strategies for identifying bots in the real world.

Today we get a unique insight into this competition and the strategies the teams employed thanks to a paper by V.S. Subrahmanian at the University of Maryland in College Park and Sentimetrix and a few pals.

The competition was about as realistic as DARPA could make it. The tweets were messages harvested from the Twitter stream during a 2014 debate on vaccinations. In this debate, a number of bots had been created as part of a competition to see how they could influence the discussions. So DARPA had ground truth knowledge of which accounts were artificial and which were real.

In total, the dataset contained over four million messages from more than 7,000 accounts of which 39 were bots in either the pro- or anti-vaccination lobbies. Each message contained a unique ID, a user profile including an image, a url, and a picture, where these were included. The data also included a time and date stamp as well as information about followers and when one account unfollowed another. All this was played to the competitors in a synthetic Twitter environment over four weeks in February and March.

The teams then had to analyze this Twitter stream and guess which users were bots. Each correct guess got them a single point but a team lost 0.25 points for each incorrect guess. A team that guessed all the bots d days before the end of the challenge also got d points, since DARPA is particularly interested in the early detection of influence bots.

The winning team was from the social media analytics company Sentimetrix, which guessed all the bots 12 days ahead of the deadline while making only one incorrect guess. That gave them a score of 50.75 points. (The second-place team, from the University of Southern California, scored 45 points, finding all the bots six days ahead of the deadline with no incorrect guesses.)

The winning strategies are revealing. The teams began by attempting to identify an initial set of bots in the data. Interestingly, none of the teams were able to automate this step and most used significant human input.

Sentimetrix used a pretrained algorithm to search for bot-like behavior. The team had trained this algorithm on Twitter data from the 2014 Indian election which featured many bots. It looked for unusual grammar, the similarity of the linguistics to natural language chatbots such as Eliza, and unusual behaviors such as extended periods of tweeting without a break that a human could not easily perform.

This revealed four accounts that were clearly bots, and Sentimetrix then used these to find others. One assumption was that bot-makers tend to produce many similar bots and link them to each other to inflate their popularity. So the team was able to use network and cluster analysis to find other likely bots, which they then compared to known bots.

The team also used features such as the temporal activity of the accounts on the assumption that an automated account would show unusual regularities. Sentimetrix also looked for users who changed allegiance during the debate from pro- to anti-vaccination (or vice versa). This they assumed could be a bot strategy for infiltrating one side of the argument and then posting opposing arguments.

A key feature in Sentimetrix’s success was the way it visualized the results of its work on an online dashboard so that a human user could easily see the status of analysis for each user.

In this second stage, Sentimetrix identified another 25 bots. That gave them enough data to train a machine learning algorithm to hunt through the data for other bots. And this approach led them to the remaining 10 bots.

The teams did not know how many bots were at work so a major problem was to know when to stop searching. Sentimetrix, for example, stopped when it could no longer find accounts that looked like bots.

That’s impressive work that could have an important influence on efforts to find bots that are attempting influence online discussions in inappropriate ways. Publishing the strategies like this should help other players develop anti-bot tactics, too.

But it could also have a negative impact. The battle between bots and bot-hunters is one that is constantly evolving. With papers like this, the bot-hunters are revealing their hand in a way that allows bot-makers to design strategies to specifically defeat these algorithms. In a way, it is like fighting with one hand tied behind your back.

Nevertheless, the temptation to keep bot-hunting strategies secret would be a dangerous one to promote. This kind of openness is part of our free society and surely one of the key reasons it is worth fighting to preserve.

Either way, this cat-and-mouse battle is set to continue.

Ref: The DARPA Twitter Bot Challenge : arxiv.org/abs/1601.05140",Yes.,"It can be easily used to spread disinformation by bots, which are automated accounts that can influence online discussions. A recent study found that a significant number of bots were used to influence the outcome of elections in India. Another study found that bots played a significant role in the annexation of Crimea.",https://www.technologyreview.com/2016/01/28/163553/how-darpa-took-on-the-twitter-bot-menace-with-one-hand-behind-its-back/,2016-01-28 00:00:00,LABEL_1_relevant,0.996418595314026,Social Media,social_media,MIT Tech Review,Politics
11,Facebook Is Reportedly Trying to Sell Off Its Botched Cryptocurrency,"It's an extremely unpopular idea.

Cashing Out

Social giant Facebook-now-known-as-Meta is attempting to sell off its ill-fated cryptocurrency project Diem, Bloomberg reports — the end of the road, seemingly, for the social media giant’s efforts to cash in on blockchain.

According to Bloomberg‘s sources, the Diem Association, formerly known as Libra, is trying to figure out ways to return money to its investors and perhaps find a new home.

Per Diem

Facebook made a huge splash in 2019 when it announced Libra, a “stable digital cryptocurrency” meant to revolutionize global financial technologies. Even the likes of Uber, Spotify, PayPal, Visa, and Mastercard were on board, signing on as corporate partners with a $10 million stake each.

But the ambitious venture imploded after facing fierce backlash from US regulators and even governments around the world.

Advertisement

Advertisement

First, high-profile backers started to jump ship. Then in October 2019, Facebook CEO Mark Zuckerberg got grilled in front of Congress for what they called a blatant attempt to circumvent regulatory approvals. By April 2020, Libra was scaled back significantly to a shadow of what it once was.

Last year, the US Federal Reserve buried any chances of Diem — Libra’s newly adopted name — taking off after striking down a potential deal with holdings company Silvergate Capital, according to Bloomberg.

The initiative kept looking for issuers of what had since turned into a US dollar-backed stablecoin. Government officials, however, were still unhappy with the plans, warning that regulators may never give it the greenlight, Bloomberg reports.

According to the outlet, Meta still owns a third of the Diem Association, but the venture’s future is still looking grim, given its sheer unpopularity.

Advertisement

Advertisement

READ MORE: Mark Zuckerberg’s Stablecoin Ambitions Unravel With Diem Sale Talks [Bloomberg]



More on Libra: Facebook’s Cryptocurrency is Collapsing in Front of Our Eyes

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Yes.,"People form communities with people like them, creating echo chambers and filter bubbles that reinforce their own existing views. As a result, people are less likely to be exposed to new or different ideas, and social media can actually make people more polarized in their views.",https://futurism.com/the-byte/facebook-reportedly-selling-cryptocurrency,,LABEL_1_relevant,0.9964765906333924,Social Media,social_media,Futurism,"Information, Discourse & Governance"
12,How Twitter Could Better Predict Disease Outbreaks,"Over several months, the researchers were able to gather a database of over 50 million geo-located tweets which could then be compared to official data from the U.K.’s national health service on flu incidence by region. By figuring out which keywords in the database of tweets were associated with elevated levels of flu, Lampos et al. were able to create a predictive model that transformed keyword incidence in future tweets into a prediction of the severity of flu for a given area.

This flu-predicting signal from Twitter is an independent stream of information that can “complement or improve the signal coming from search engine queries,” says Cristianini.

Cristianini notes that all approaches that track self-reported symptoms suffer from the same bias, however: the more the media hypes a Flu epidemic, the more likely people are to go to their doctors (distorting the “official” numbers) and talk about suspicious symptoms on Twitter or other services.

Future work might involve information from Facebook and other sources of status updates, allowing researchers to become ever more adept at pinpointing outbreaks in their earliest stages.

Follow Christopher Mims on Twitter, or contact him via email.",Yes,It can distort the “official” numbers of an event.,https://www.technologyreview.com/2010/07/14/202024/how-twitter-could-better-predict-disease-outbreaks/,2010-07-14 00:00:00,LABEL_1_relevant,0.9943252205848694,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
13,WhatsApp's Case Against NSO Group Might Be an Uphill Battle,"WhatsApp just took a hard new line against the malware industry, suing notorious Israeli surveillance contractor NSO Group for attacks on more than a thousand of its users. The case could mark a turning point in Silicon Valley's fight against private-sector espionage mercenaries. But before it can convince a court that NSO engaged in criminal hacking, WhatsApp may have to win a thorny legal argument—one that legal experts say could require some creative contortions.

On Tuesday afternoon, WhatsApp published a statement accusing NSO of targeting 1,400 of its users, including at least 100 members of ""civil society"" such as journalists and human-rights defenders, with malicious voice calls designed to infect targeted phones with malware and steal messages despite WhatsApp's end-to-end encryption. Those numbers would represent a new scale for NSO, whose malware has already been linked to attacks against activists ranging from the now-imprisoned United Arab Emirates dissident Ahmed Mansoor to Mexican activists opposing a soda tax.

WhatsApp paired its statement with a lawsuit in a Ninth Circuit court, accusing NSO of violating the Computer Fraud and Abuse Act, as well as state-level charges including breach of contract and interfering with their property. The case represents a bold attempt to use the CFAA in an unusual way: to punish not just hackers who breach a company's computers, but those who exploit its software to breach the computers of its users.

But some hacking-focused lawyers who have analyzed WhatsApp's complaint warn that—noble as its attempt to slap back NSO and protect its users may be—its central argument may not fly in court.

""Absent more, they won’t be able to hook the CFAA violation on the terms of service."" Riana Pfefferkorn, Stanford University

That's because, fundamentally, the CFAA outlaws so-called ""unauthorized access,"" explains Tor Ekeland, a well-known hacker defense attorney. To make that charge stick, WhatsApp will have to show that NSO obtained illegal access to WhatsApp's own systems. Given that NSO's targets were WhatsApp users rather than, say, WhatsApp's servers, they'll have to find an argument that they, as the plaintiff, were the victim. ""The fundamental question is, what’s the unauthorized access?"" says Ekeland. ""You might be able to argue that NSO hacked WhatsApp and not just their users. Maybe they’re trying to make that argument. But they’re not being clear about it, and that lack of clarity is an attack vector for the defendant.""

WhatsApp's most obvious unauthorized access argument relates to its terms of service, which prohibit reverse-engineering WhatsApp's code, harming its users, or sending malware via WhatsApp. The company might argue that by agreeing to those terms of service and then violating them, NSO's use of WhatsApp was unauthorized all along. The complaint appears to lay the groundwork for that case: It points out that NSO Group staff ""created various WhatsApp accounts and agreed to the WhatsApp Terms.""

But that terms-of-service argument will be an uphill battle, says Ekeland. Terms of service have long been a controversial element of hacking cases, from the 2009 cyberbullying case of Lori Drew to the hacking charges against information freedom activist Aaron Swartz. And the Ninth Circuit in particular has set a clear precedent that terms-of-service violations alone don't constitute unauthorized access. ""A terms of service violation under the CFAA is a very thin reed to hang your case on,"" Ekeland says.

WhatsApp parent company Facebook has sought out CFAA rulings against terms-of-service violators in the past. It sent a warning to a company called Power Ventures, which created its own user interface for Facebook and other social media sites, to stop violating its terms. It then sued under the CFAA only after the company persisted. In that instance, a judge ruled explicitly that Power Ventures had broken the CFAA—but that it wouldn't have if Facebook hadn't first told it to stop accessing its site.",Yes,It has given a platform to anyone with a computer and an internet connection to be heard. Such platforms have been a boon to political dissidents and other marginalized groups by allowing them to bypass traditional media gatekeepers and reach a global audience directly. But they've also been exploited by government-backed trolls and other malicious actors to spread disinformation and sow discord.,https://www.wired.com/story/whatsapp-nso-group-lawsuit/,2019-10-30 02:03:32.943000+00:00,LABEL_1_relevant,0.9966089725494384,Social Media,social_media,Wired,"Information, Discourse & Governance"
14,Users Sue Juul for Addicting Them to Nicotine,"Juul Labs, the San Francisco-based e-cigarette company, is under pressure from parents, schools, public health advocates, lawmakers, and the Food and Drug Administration for its popularity with younger users, who have gravitated to Juul’s discrete rechargeable vaping device and nicotine pods in flavors like mango and fruit medley. Now come the lawsuits.

Since April, consumers have filed at least three complaints against Juul. Two of the lawsuits were filed in California and allege Juul deceptively marketed the product as safe, when it contains more potent doses of nicotine than cigarettes. Both seek monetary damages, as well as an injunction to curb Juul’s marketing practices.

In the first case, filed in US District Court in Northern California in late April, Bradley Colgate of La Jolla and Kaytlin McKnight of Arroyo Grande say they first purchased Juul in 2017. The suit claims that McKnight became addicted to nicotine salts and now vapes several Juul pods each week. Colgate purchased Juul to help him quit smoking, according to the suit, but “the intense dosage of nicotine salts delivered by the Juul products resulted in an increased nicotine addiction, and an increased consumption of nicotine by Colgate.”

In the second case, in San Francisco Superior Court, Carl Cooper also says he purchased Juul in hopes of quitting smoking. But Cooper, who the suit says began smoking at the age of 15 in 2010, alleges that Juul worsened his addiction, turning him from someone who mostly smoked on the weekends to a habitual daily user within a couple of weeks. The suit claims Cooper becomes agitated and moody if he doesn’t get regular doses of nicotine salts from Juul pods. “Whereas Cooper had never felt the need to smoke on a daily basis, he now finds that he feels compelled to vape Juul pods every day,” the complaint claims.

The most recent complaint, filed in US District Court in New York in June, came from the mother of a 15-year-old, identified only as “D.P.”, alleging Juul designed its product to contain more nicotine than necessary to satisfy the cravings of an adult smoker. D.P. became “heavily addicted to nicotine,” the suit claims, making him “anxious, highly irritable and prone to angry outbursts,” and perform poorly in school.

The complaint says that D.P. began using Juul after he started a specialized high school in Rockland County, New York offering programs that fit his interest in technology and carpentry. Juul use was pervasive at the school, including on the school bus, in bathrooms, outside school, and even in class, the complaint says. His parents switched him to another high school, but Juul was prevalent there as well. In order to deter his use, the complaint says the teenager’s parents removed the door from his bedroom, locked parts of their house, instructed school officials not to let him use the bathroom unaccompanied, and subjected D.P. to regular urine tests.

“Yet despite all these measures, D.P. is unable to stop Juuling,” the complaint says. His “urges” become so powerful, the complaint says, that “he is unable to avoid Juuling even though it subjects him to disciplinary measures at home and at school.”",Yes,Addiction.,https://www.wired.com/story/users-sue-juul-for-addicting-them-to-nicotine/,2018-07-23 11:00:00+00:00,LABEL_1_relevant,0.9967063069343568,Social Media,social_media,Wired,"Information, Discourse & Governance"
15,"Facebook whistleblower, Frances Haugen, raises trust and security questions over its e2e encryption","Frances Haugen, one of (now) multiple Facebook whistleblowers who have come forward in recent years with damning testimony related to product safety, gave testimony in front of the UK parliament today — where, in one key moment, she was invited to clarify her views on end-to-end encryption following a report in the British newspaper the Telegraph yesterday.

The report couched Facebook’s plan to extend its use of e2e encryption as “controversial” — aligning the newspaper’s choice of editorial spin with long-running UK government pressure on tech giants not to expand their use of strong encryption in order that platforms can be ordered to decrypt and hand over message content data on request.

In its interview with Haugen, the Telegraph sought to link her very public concerns about Facebook’s overall lack of accountability to this UK government anti-e2ee agenda — claiming she had suggested Facebook’s use of e2e encryption could disrupt efforts to protect Uighur dissidents from Chinese state efforts to inject their devices with malware.

The reported remarks were quickly seized upon by certain corners of the Internet (and at least one other ex-Facebook staffer who actually worked on adding e2e encryption to Messenger and is now self-styling as a ‘whistleblower’) — with concerns flying that her comments could be used to undermine e2e encryption generally and, therefore, the safety of scores of Internet users.

Sounding unimpressed with the Telegraph’s spin, Haugen told UK lawmakers that her views on e2e encryption had been “misrepresented” — saying she fully supports “e2e open source encryption software”; and, indeed, that she uses it herself on a daily basis.

What she said she had actually been querying was whether Facebook’s claim to be implementing e2e encryption can be trusted, given the tech giant does not allow for full external inspection of its code as is the case with fully open source e2ee alternatives.

This is another reason why public oversight of the tech giant is essential, Haugen told the joint committee of the UK parliament which is scrutinizing (controversial) draft online safety legislation.

“I want to be very, very clear. I was mischaracterised in the Telegraph yesterday on my opinions around end-to-end encryption,” she said. “I am a strong supporter of access to open source end to end encryption software.

“I support access to end-to-end encryption and I use open source end-to-end encryption every day. My social support network is currently on an open source end-to-end encryption service.”

“Part of why I am such an advocate for open source software in this case is that if you’re an activist, if you’re someone who has a sensitive need, a journalist, a whistleblower — my primary form of social software is an open source, end-to-end encryption chat platform,” she also said, without naming exactly which platform she uses for her own e2ee messaging (Signal seems likely — a not-for-profit rival to Facebook-owned WhatsApp which has benefited from millions of dollars of investment from WhatsApp founder Brian Action, another former Fb staffer turned critic; so maybe ‘meta’ would in fact be a perfect new brand name for Facebook).

“But part of why that open source part is so important is you can see the code, anyone can go and look at it — and for the top open source end-to-end encryption platform those are some of the only ways you’re allowed to do chat in say the defence department in the US.

“Facebook’s plan for end-to-end encryption — I think — is concerning because we have no idea what they’re doing to do. We don’t know what it means, we don’t if people’s privacy is actually protected. It’s super nuanced and it’s also a different context. On the open source end-to-end encryption product that I like to use there is no directory where you can find 14 year olds, there is no directory where you can go and find the Uighur community in Bangkok. On Facebook it is trivially easy to access vulnerable populations and there are national state actors that are doing this.

“So I want to be clear, I am not against end-to-end encryption in Messenger but I do believe the public has a right to know what does that even mean? Are they really going to produce end-to-end encryption? Because if they say they’re doing end-to-end encryption and they don’t really do that people’s lives are in danger. And I personally don’t trust Facebook currently to tell the truth… I am concerned about them misconstruing the product that they’ve built — and they need regulatory oversight for that.”

In additional remarks to the committee she further summarized her position by saying: “I am concerned on one side that the constellation of factors related to Facebook makes it even more necessary for public oversight of how they do encryption there — that’s things like access to the directory, those amplification settings. But the second one is just about security. If people think they’re using an end-to-end encryption product and Facebook’s interpretation of that is different than what, say, an open source product would do — because an open source product we can all look at it and make sure that what is says on the label is in the can.

“But if Facebook claims they’ve built an end-to-end encryption thing and there’s really vulnerabilities people’s lives are on the line — and that’s what I’m concerned about. We need public oversight of anything Facebook does around end-to-end encryption because they are making people feel safe when they might be in danger.”

Haugen, a former Facebook staffer from the civic integrity team, is the source for a tsunami of recent stories about Facebook’s business after she leaked thousands of pages of internal documents and research reports to the media, initially providing information to the Wall Street Journal, which published a slew of stories last month, including about the toxicity of Instagram for teens (aka the ‘Facebook Files‘), and subsequently releasing the data to a number of media outlets which have followed up with reports today on what they’re calling the Facebook Papers.

The tl;dr of all these stories is Facebook prioritizes growth of its business over product safety — leading to a slew of harms that can affect individuals, other businesses and the public/society more generally whether as a result of inadequate AI systems that cannot properly identify and remove hate speech (leading to situations where its platform can whip up ethnic violence), or which allow engagement based ranking systems to routinely amplify extreme, radicalizing content without proper mind to risks (such as forming conspiracy-theory touting echo chambers forming around vulnerable individuals, isolating them from wider society), or overestimation of its ad reach leading to advertisers being systematically overcharged for its adtech.

During her testimony today, Haugen suggested Facebook’s AIs were unlikely to even be able to properly distinguish dialectal distinctions and nuances of meaning between UK English and US English — let alone the scores of languages in countries where it directs far less resource.

Parliamentarians probed her on myriad harms during around 2.5 hours of testimony — and some of her answers repeated earlier testimony she gave to lawmakers in the US.

Many of the UK committee’s questions asked for her view on what might be effective regulatory measures to close the accountability gap — both on Facebook and social media more generally — as MPs sought to identify profitable avenues for amending draft online safety legislation.

“The danger with Facebook is not individuals saying bad things, it is about the systems of amplification that disproportionately give people saying extreme polarising things the largest megaphone in the room,” argued Haugen.

Her list of suggestions for fixing a system of what she couched as broken incentives under Facebook’s current leadership included mandatory risk assessments — which she warned need to cover both product safety and organisational structure since she said much of the blame for Facebook’s problems lies with its “flat” organizational structure and a leadership team that rewards (and thus incentivizes) growth above all else, leaving no one internally who’s accountable for improving safety metrics.

Such risk assessments would need to be carefully overseen by regulators to avoid Facebook using its customary tactic in the face of critical scrutiny of just marking its own homework — or “dancing with data” as she put it.

Risk assessments should also involve the regulator “gathering from the community and saying are there other things that we should be concerned about”, she said, not just letting tech giants like Facebook define blinkered parameters for uselessly partial oversight — suggesting “a tandem approach like that that requires companies to articulate their solutions”.

“I think that’s a flexible approach; I think that might work for quite a long time. But it has to be mandatory and there have to be certain quality bars because if Facebook can phone in it I guarantee you they’ll phone it in,” she also told the committee.

Another recommendation Haugen had was for mandatory moderation of Facebook Groups when they exceed a certain number of users.

Whereas — left unmoderated — she said groups can be easily misappropriated and/or misused (using techniques like ‘virality-hacking’) to act as an “amplification point” for spreading discord or disseminate disinformation, including by foreign information operations.

“I strongly recommend that above a certain sized group they should be required to provide their own moderators and moderate every post,” she said. “This would naturally — in a content-agnostic way — regulate the impact of those large groups. Because if that group is actually valuable enough they will have no trouble recruiting volunteers.”

Haugen also suggested that Facebook should be forced to make a firehose of information available to external researchers (as Twitter, for example, already does) — in a privacy-safe way — which would allow outside academics and experts to drive accountability from the outside by investigating potential issues and identifying concerns freed from Facebook’s internal growth-focused lens.

Another of her recommendations was for regulators to demand segmented analysis from Facebook — so that oversight bodies get full transparency into populations that disproportionately experience harms on its platform.

“The median experience on Facebook is a pretty good experience — the real danger is that 20% of the population has a horrible experience or an experience that is dangerous,” she suggested.

She went on to argue that many of Facebook’s problems result from the sub-set of users who she said get “hyper exposed” to toxicity or to abuse — as a consequence of an engagement-driven design and growth-focused mindset that rejects even small tweaks to inject friction/reduce virality (and which she suggested would only mean Facebook giving up “small slivers” of growth in the short term and yield a much more pleasant and probably more profitable product over the longer term).

“As we look at the harms of Facebook we need to think about these things as system problems — like the idea that these systems are designed products, these are intentional choices and that it’s often difficult to see the forest for the trees. That Facebook is a system of incentives, it’s full of good, kind, conscientious people who are working with bad incentives. And that there are lack of incentives inside the company to raise issues about flaws in the system and there’s lots of rewards for amplifying and making things grow more,” she told the committee.

“So I think there is a big challenge of Facebook’s management philosophy is that they can just pick good metrics and then let people run free. And so they have found themselves in a trap where in a world like that how do you propose changing the metric? It’s very very hard because 1,000 people might have directed their labor for six months trying to move that metric and changing the metric will disrupt all of that work.

“I don’t think any of it was intentional — I don’t think they set out to go down this path. And that’s why we need regulation — mandatory regulation, mandatory actions — to help pull them away from that spiral that they’re caught in.”

Legislation that seeks to rein in online harms by applying regulations to platform giants like Facebook must also not focus only on individual harms — but needs to respond to societal harms, she also emphasized.

“I think it is a grave danger to democracy and societies around the world to omit societal harm. A core part of why I came forward was I looked at the consequences of choices Facebook was making and I looked at things like the global south and I believe situations like Ethiopia are just part of the opening chapters of a novel that’s going to be horrific to read. We have to care about societal harm — not just for the global south but for our own societies.

“When an oil spill happens it doesn’t make it harder for us to regulate oil companies. But right now Facebook is closing the door on us being able to act — we have a slight window of time to regain people-control over AI; we have to take advantage of this moment.”

Facebook has been contacted for comment.",Yes,"It can be used to spread disinformation and hate speech, which can lead to societal harms.",https://techcrunch.com/2021/10/25/facebook-whistleblower-frances-haugen-raises-trust-and-security-questions-over-its-e2e-encryption/,2021-10-25 00:00:00,LABEL_1_relevant,0.9966400861740112,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
16,Facebook Had an Incredibly Busy Weekend,"While millions of Americans were enjoying a warm spring weekend, Facebook employees were hard at work responding to an avalanche of news about their company. After an already busy week for the social media platform—including a lawsuit from the Department of Housing and Urban Development, as well as a policy change regarding white nationalist and separationist content—five major Facebook stories broke over the last few days, including a Washington Post op-ed in which CEO Mark Zuckerberg calls for the social network to be regulated. Here’s what you need to know to get caught up.

Facebook Explores Restricting Who Can Livestream

The torrent of Facebook news began Friday, when COO Sheryl Sandberg said the company was “exploring restrictions on who can go Live depending on factors such as prior Community Standard violations.” The decision came less than three weeks after a terrorist attack in Christchurch, New Zealand, that killed 50 people was livestreamed on Facebook. The social network, as well as other companies like YouTube, struggled to stop the shooter’s video from being reuploaded and redistributed on their platforms.

In 2016, Zuckerberg said that live video would “create new opportunities for people to come together."" Around the same time, the company invested millions of dollars to encourage publishers like Buzzfeed to experiment with Facebook Live. The feature provided an unedited, real-time window into events like police shootings, but it was also repeatedly used to broadcast disturbing events. After the Christchurch attack, Facebook is now reexamining who should have the ability to share live video, which has proven difficult for the company to moderate effectively.

Sandberg also said Facebook will research building better technology to “quickly identify edited versions of violent videos and images and prevent people from re-sharing these versions.” She added that Facebook had identified over 900 different variations of the Christchurch shooter’s original livestream. Sandberg made her announcement in a blog post published not to the Facebook Newsroom but to Instagram's Info Center, indicating Facebook wants its subsidiaries to appear more unified.

Old Zuckerberg Blog Posts Disappear

Also on Friday, Business Insider reported that years of Zuckerberg’s public writings had mysteriously disappeared, “obscuring details about core moments in Facebook’s history.” The missing trove included everything the CEO wrote in 2007 and 2008, as well as more recent announcements, like the blog post Zuckerberg penned in 2012 when Facebook acquired Instagram.

Facebook said that the posts were mistakenly deleted as the result of technical errors. “The work required to restore them would have been extensive and not guaranteed, so we didn’t do it,” a spokesperson for the company told Business Insider. They added that they didn’t know exactly how many posts were lost in total.",Yes,"It leads to the spread of misinformation (fake news) and the amplification of polarizing views.

This can have a number of negative effects on individuals and society, including:

-Making people more susceptible to fake news and misinformation
-Increasing the echo chamber effect, where people only see information that supports their existing beliefs
-",https://www.wired.com/story/facebook-busy-weekend-news-feed-livestream/,2019-04-01 21:46:51.866000+00:00,LABEL_1_relevant,0.9687605500221252,Social Media,social_media,Wired,"Information, Discourse & Governance"
17,Help WIRED Track How Political Ads Target You on Facebook,"With a user base of more than 2 billion people who can be chopped and sorted by almost any conceivable data point—men ages 21 to 45 living in the United States who are parents to preteens and like Fortnite; women with a bachelor’s degree who are away from family and whose friends are recently engaged—Facebook advertising is an incredibly powerful tool. Much attention has been paid to how foreign influence operations used the platform to target and manipulate US citizens in 2016, and with good reason. But that election also demonstrated that political campaigns and local organizations can take advantage of these features. Regulations around political ads on the web has been uneven at best. And thanks to microtargeting, today’s advertisements can be that much harder to track.

Amid backlash and increased scrutiny, not to mention the threat of regulation, Facebook and other social media companies have taken some steps to increase transparency around ads, especially political ones, but they’re still an enormous target for bad actors. Even today, Facebook announced it was removing hundreds of pages and accounts, run by US citizens, that used “sensational political content” to drive traffic to their websites in ways that “have consistently broken our rules against spam and coordinated inauthentic behavior.” The New York Times reports that at least one of the pages, Right Wing News, used Facebook ads.

With just a few weeks left before the midterm elections, and 2020 looming after that, the many ways in which political advertising on Facebook can shape our society—whether it’s a grassroots campaign rallying supporters in innovative new ways, or clickbait farms adding fuel to the partisan fire for a quick buck—demands further exploration.

That’s why WIRED is partnering with ProPublica, the award-winning news nonprofit, on their multi-newsroom project to collect and analyze political ads targeted to Facebook users. Campaigns use all sorts of online platforms to reach constituents, but few of them come close to Facebook's power. And as ProPublica noted last year, “Facebook gives users more information about why a particular ad is targeted to them than other online platforms provide to their customers.” So they built a tool, the Facebook Political Ad Collector, to scrape that information, as well as the ads themselves, and put it all in a searchable database.

ProPublica’s database is entirely crowdsourced from volunteers who browse Facebook while using the extension. The more people who do that, the more ads it will collect and the more robust its database will be.

The project has collected more than 76,000 ads so far, and journalists have used the tool to discover campaigns that violated FEC rules and scams posing as political ads. (The tool has also exposed nonpolitical ads, like job postings, whose narrow audience targeting may violate antidiscrimination laws.)

The tool is just a simple browser extension, and that’s where you, our readers, can help. ProPublica’s database is entirely crowdsourced from volunteers who browse Facebook while using the extension. The more people who do that, the more ads it will collect and the more robust its database will be.

All you need to do is install ProPublica’s free extension on Chrome or Firefox, sign into Facebook, and scroll scroll scroll away. (Note: If you have an ad blocker enabled, it won’t work.)

If you’re feeling especially ambitious, you can help train the algorithm ProPublica developed to automatically filter political ads. The extension can show you ads from your own News Feed, as well as those others have seen, which you can then mark as “political” or “normal.”",Yes,"It can be used as a tool for political campaigns to microtarget ads to very specific groups of people. This is a very powerful tool that can be used for good or bad. If you want to help ProPublica track political ads on Facebook, you can install their free extension and mark ads as political or normal.",https://www.wired.com/story/facebook-political-ad-collector-wired-propublica/,2018-10-12 11:00:00+00:00,LABEL_1_relevant,0.9961901903152466,Social Media,social_media,Wired,Politics
18,Facebook and the Folly of Self-Regulation,"My late colleague, Neil Postman, used to ask about any new proposal or technology, “What problem does it propose to solve?”

When it comes to Facebook, that problem was maintaining relationships over vast time and space. And the company has solved it, spectacularly. Along the way, as Postman would have predicted, it created many more problems.

Last week, Facebook revealed the leaders and first 20 members of its new review board. They are an august collection of some of the sharpest minds who have considered questions of free expression, human rights, and legal processes.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

They represent a stratum of cosmopolitan intelligentsia quite well, while appearing to generate some semblance of global diversity. These distinguished scholars, lawyers, and activists are charged with generating high-minded deliberation about what is fit and proper for Facebook to host. It’s a good look for Facebook—as long as no one looks too closely.

What problems does the new Facebook review board propose to solve?

In an op-ed in The New York Times, the board’s new leadership declared: “The oversight board will focus on the most challenging content issues for Facebook, including in areas such as hate speech, harassment, and protecting people’s safety and privacy. It will make final and binding decisions on whether specific content should be allowed or removed from Facebook and Instagram (which Facebook owns).”

Only in the narrowest and most trivial of ways does this board have any such power. The new Facebook review board will have no influence over anything that really matters in the world.

It will hear only individual appeals about specific content that the company has removed from the service—and only a fraction of those appeals. The board can’t say anything about the toxic content that Facebook allows and promotes on the site. It will have no authority over advertising or the massive surveillance that makes Facebook ads so valuable. It won’t curb disinformation campaigns or dangerous conspiracies. It has no influence on the sorts of harassment that regularly occur on Facebook or (Facebook-owned) WhatsApp. It won’t dictate policy for Facebook Groups, where much of the most dangerous content thrives. And most importantly, the board will have no say over how the algorithms work and thus what gets amplified or muffled by the real power of Facebook.

This board has been hailed as a grand experiment in creative corporate governance. St. John’s University law professor Kate Klonick, the scholar most familiar with the process that generated this board, said, “This is the first time a private transnational company has voluntarily assigned a part of its policies to an external body like this.”

That’s not exactly the case. Industry groups have long practiced such self-regulation through outside bodies, with infamously mixed results. But there is no industry group to set standards and rules for Facebook. One-third of humanity uses the platform regularly. No other company has ever come close to having that level of power and influence. Facebook is an industry—and thus an industry group—unto itself. This is unprecedented, though, because Facebook ultimately controls the board, not the other way around.

We have seen this movie before. In the 1930s the Motion Picture Association of America, under the leadership of former US postmaster general Will Hays, instituted a strict code that prohibited major Hollywood studios from showing, among other things, “dances which emphasize indecent movements.” The code also ensured that “the use of the [US] flag shall be consistently respected.” By the 1960s, American cultural mores had broadened, and directors demanded more freedom to display sex and violence. So the MPAA abandoned the Hays code and adopted the ratings system familiar to American moviegoers (G, PG, PG-13, R, NC-17).

It’s a good look for Facebook—as long as no one looks too closely.

One reason the MPAA moved from strict prohibitions to consumer warnings was that American courts had expanded First Amendment protection for films, limiting how local governments could censor them. But all along, the MPAA practiced an explicit form of self-regulation, using a cartel that represented the interests of the most powerful studios to police behavior and represent the industry as a whole to regulators and the public.

No one can look at the history of American film and seriously argue that either method of self-regulation really served the public. Standards have been sloppily and inconsistently enforced. Through both the Hays code and the rating system, the MPAA limited artistic expression and the representation of lesbian, gay, and transgender issues and stories. But it sure helped Hollywood by keeping regulators at bay.",Yes,The social media platform itself.,https://www.wired.com/story/facebook-and-the-folly-of-self-regulation/,2020-05-09 18:58:57.543000+00:00,LABEL_1_relevant,0.9951855540275574,Social Media,social_media,Wired,"Information, Discourse & Governance"
19,Retro Gaming's Misogyny Is Brought to Light After Tragedy,"In 2015, record-breaking retro game champion Rudy Ferretti made a homebrew game for the Atari 2600. It was called Pigs in the Castle. “I’m tired of all you feminists and your bullshit,” he said in a video preceding its launch. It’s understood that the pigs in the game are women.

The game’s official Facebook page describes how the purpose of the game is to “kill 100 or more pig bitches to get the boss.” He elaborates: “simple???????? Fuck no it’s my game it’s hard.” In a video of the game, preserved on the YouTube channel of “Rudy Ferretti aka the console player of the century,” Ferretti's character navigates a pixelated castle killing “evil” pigs.

On August 10, police in Dover, New Hampshire, discovered Ferretti deceased in his bed. A firearm lay nearby. In the same apartment’s living room, police identified the body of his ex-girlfriend, Amy Molter. According to a medical examiner, Molter and Ferretti both died of gunshots to the head—Molter from homicide and Ferretti from suicide, the police suspect. The investigation into the circumstances surrounding the deaths remains ongoing.

Longtime members of the retro and arcade gaming scene say they warned community leaders and even police about Ferretti’s threatening behavior for years. For close to a decade, they say, Ferretti had harassed, stalked, and threatened gamers, particularly women, pushing some out of the niche gaming scene entirely. He flashed guns in tirade YouTube videos and bragged on Facebook about bringing one to an event at the Museum of Pinball in 2017.

Arcade game collector and researcher Catherine DeSpira and video game historian and storage auction buyer Patrick Scott Patterson—two of Ferretti’s most public targets—say they collectively contacted police in different states a half-dozen times to report Ferretti’s threats against themselves and others. They say those attempts ultimately had no effect. All the while, clusters of retro gamers across the country egged Ferretti on in private messages and on forums, leveraging his apparent instability and misogynist inclinations against women they didn’t want in the scene.

“You’d think anyone would look at it and go, ‘Hey, this guy’s gone, out there,’” says Patterson. “But people weren’t doing it. They were emboldening it, pushing him, giving him a support system.”

As the community processes last week’s tragedy, some described a culture of complicity in old-school gaming’s strongholds that didn’t do enough to protect women.

Forty years after the release of classic arcade games like Pac-Man and Joust, an active and enthusiastic connoisseur culture around these games still thrives at gaming conventions and online. Collectors, historians, nostalgics, and competitors share a deep love for modern gaming’s roots and its physical manifestations—blocky consoles, rare arcade cabinets. Back in 1983, a sociologist surveying arcades reported that 80 percent of players were men. Sources say that male dominance has carried through into 2020. Even though women now make up about 46 percent of all gamers, retro gaming’s conventions, online forums, and publications boast a self-reinforced culture of masculinity that, members of some communities say, abetted and allowed a vocal minority to target women.",Yes,The way it has allowed some gamers to target women.,https://www.wired.com/story/retro-gaming-violent-tragedy-rudy-ferretti/,2020-08-19 21:25:25.649000+00:00,LABEL_1_relevant,0.995603322982788,Social Media,social_media,Wired,Equality & Justice
20,Instagram's newest test mixes 'Suggested Posts' into the feed to keep you scrolling,"The days of a scrolling to the end of your Instagram feed look to be coming to an end. After adding algorithmic suggestions to the bottom of the app last year, Instagram is running a test that would splice more recommended posts from accounts you don’t follow into the feed with those you do.

In the next few days, the company will begin testing an expansion of “Suggested Posts” which would sprinkle that content through the regular feed. As it stands now, Suggested Posts appear at the bottom of Instagram after you’ve scrolled through all of the content from people you follow and hit the “You’re all caught up” message that the app implemented in 2018. Depending on how many accounts you follow, it’s possible to not run into that message or Instagram’s recommendations very often, if at all.

In addition to boosting the prominence of Suggested Posts, Instagram will test an option that lets users “snooze” the feature, removing it from the feed for 30 days. Anyone in the test will be able to offer feedback when a specific post doesn’t interest them, but it sounds like you won’t be able to disable Suggested Posts in the feed in a permanent way.

The Suggested Posts expansion will be accompanied by a way for users to shape what they see through managing their interests — stuff like cats, makeup or basketball. If you’ve seen enough cats, you can toggle that interest off or tell Instagram that you never wanted to see those damn cats to begin with when it shows you the next one.

A Facebook spokesperson described the expansion of Suggested Posts to TechCrunch as an “extension” of the Instagram feed, noting that the ratio of these algorithmic recommendations to posts from followed accounts will be variable based on how someone uses the app.

The test will roll out to a small number of users in English-speaking countries only, though the company declined to specify how many accounts will be involved.

The experiment might not make it into the final product, but from the way the winds over at Facebook have been blowing lately it looks pretty likely. Like we mentioned, Instagram and parent company Facebook introduced some tools to give people more control over their own behavior on the notoriously addictive-by-design apps back in 2018, including the “You’re all caught up” message and a way to track time spent.

Those tools weren’t a sea change for a company that generally values keeping people glued to its services (and its ads) at all costs, but they showed that Facebook was at least mildly self aware of the conversation about social media addiction sweeping through the tech world at the time.

In 2020, it sounds like Facebook is done humoring those concerns. Instagram is feeling the heat from TikTok’s preternaturally well-tuned endless algorithmic feed and booming success. Like it has so many times in the past, the company is looking to shift its own identity to chase a threatening competitor rather than staying the course or trying something new.

The new way Suggested Posts work is just a test for now, but mixing algorithmic suggestions into the feed with posts from accounts you follow would be a pretty big change to the core way the app works. As it stands, if people want a truly endless Instagram experience they could turn to the Explore tab or scroll past the “caught up” message. Many doubtless did to stave off boredom, to the likely detriment of their mental health.

But under the test, it will be less possible to use Instagram to only keep up with just the accounts that you’ve got a personal interest in, whether they’re friends, local businesses or influencers of your choosing. Instagram wants to inject more of what it wants you to see into that experience, or what the company believes you’d want to see but you just don’t know it yet.

The end result might not be that noticeable for people who follow huge swaths of accounts already and rarely meet the end of their feed, but it strays even further from the original product — a distant memory at this point — while giving Instagram a way to keep people on the app for longer while serving them more ads.",Yes.,Addiction.,https://techcrunch.com/2021/06/23/instagram-suggested-posts-test-topics/,2021-06-23 00:00:00,LABEL_1_relevant,0.9957542419433594,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
21,"In India, Smartphones and Cheap Data Are Giving Women a Voice","R. Mallika taps on the WhatsApp icon on her phone and selects the chat with S. Thanaraj. It's a long conversation, with messages back and forth about conserving her tribe's forest lands. It’s like billions of messages sent each day on WhatsApp, with one important exception—there are no written words.

“I never attended school,” says Mallika, a member of an indigenous nomadic tribe in southern India. “I can sign my name, guess where a bus goes, or what a road sign says, but not much beyond that.” (She believes she is 38 but doesn’t have a proper birth record.) She never owned a feature phone because she couldn’t read or write well enough to create a list of contacts.

Mallika, like 200 million other women in India, is illiterate. In the past few years though, millions of Indian women have gone online thanks to cheaper smartphones and mobile data, and apps that let them communicate using sounds and images. Anecdotal evidence suggests the phones are empowering many women to access information, build networks, and participate in markets.

For Mallika, the ability to use smartphones visually and orally has been a game changer. The internet is no longer sealed off with written words. She uses photographs and audio memos to communicate with friends and family, and voice commands to look for videos.

R. Mallika uses a smartphone to chronicle deforestation in southern India. Photograph: S. Thanaraj

“I can tap on the picture of Annan (what she calls Thanaraj) and send him an audio message,” she says. Before getting the phone, she had to travel by foot over mountains to enter Madurai and interact with Thanaraj—her single point of contact with the outside world. “The phone connects her directly to the people who can help her,” says Thanaraj. “I am no longer the only person she speaks to.”

Mallika is part of a WhatsApp group where she shares videos and photographs of the forest with local journalists. Illegal logging is a persistent problem. “Sometimes teak or sandalwood trees go missing,” she says. “I take pictures and compare them to older photographs.” She shares the photos with rangers and forest officials. In case of a confrontation, “my husband videotapes the skirmish to protect me. We send the video along with a voice message to the journalists’ group.” She also watches videos of activists in other parts of India.

“The phone connects her directly to the people who can help her.” S. Thanaraj, conservation activist

Mallika is benefiting from two seismic changes that happened in 2016. China’s Xiaomi and others began manufacturing smartphones in India and introduced entry-level models that cost less than $100. The same year, telecom company Reliance Jio launched a new service with free nationwide voice and data for one year. About 100 million people subscribed to the network in the first six months. In one year, the average cost for accessing 1 GB of data fell from $3.10 to $1.90. Since then, data costs have plunged further, to an average of 9 cents per GB. The phone and data plan still consume about 10 percent of Mallika’s income. But suddenly, it wasn’t only urban middle- and upper-class folk who were lit by a halo of blue light.

Ravi Agarwal, author of the 2018 book India Connected: How the Smartphone Is Transforming the World's Largest Democracy says that for many Indians, the smartphone is their first private TV screen, personal music player, computer, and camera. Agarwal compares it to the experience of owning a car for the first time—autonomy, privacy, and mobility.

The WIRED Guide to the iPhone Its influence goes far beyond other phones—the infrastructure that made the iPhone also enabled drones, smart-home gadgets, wearables, and self-driving cars.

This is particularly true for women, who are less likely to be literate or employed in the formal workforce. Even among the literate, many read and write in one of India’s more than 30 official languages—another hurdle to accessing the internet on personal computers and laptops with English keyboards. In 2015, only 10 percent of internet users in India’s rural areas were women. As smartphones and data plans have become more accessible, that figure has risen to roughly 30 percent, according to IAMAI, a trade group of internet and telecom companies.

Companies including Google, Intel, and Facebook have worked with local organizations to make it easier for women to access the internet. Google and Tata Trusts, for example, run the Internet Saathi, or Internet Friend, program, which trains rural women to be digital pioneers. They are taught to use smartphones in sessions where they are provided with phones and power banks. By December 2019, the program had trained more than 83,300 women to be Saathis. In turn, they had introduced over 34 million women to the internet.",Yes,"It can lead to internet addiction. A recent study in the journal Psychology of Popular Media Culture found that the more time people spend on social media, the more likely they are to be addicted to the internet.",https://www.wired.com/story/india-smartphones-cheap-data-giving-women-voice/,2021-01-04 12:00:00+00:00,LABEL_1_relevant,0.9913970232009888,Social Media,social_media,Wired,"Information, Discourse & Governance"
22,Instagram Boosts Its Security With a Fave-Worthy Update,"Social media platforms' struggle with safety and security is like a game of Whac-A-Mole. One day, the threat is coordinated bot activity; the next, it's SIM hijackers stealing the identities of regular users. In an effort to protect Instagram users from these and other threats, the company announced a set of features today designed make Instagram feel ""safer,"" including ways to protect your own account and to verify whether the accounts you follow are genuine or not.

Instagram

First, all users will soon be able to use a more robust form of two-factor authentication to log into Instagram. Previously, Instagram offered two-factor authentication with a code sent via SMS—better than nothing, but insufficient to protect all Instagram users from having their accounts compromised. (Users with ""valuable"" handles may be more vulnerable to scams like SIM hijacking, where hackers access a person's phone number and use it to log into their accounts and steal their usernames.) Now, the platform will allow integration with third-party authenticators, like DUO Mobile and Google Authenticator, which supply two-factor codes locally and provide an additional layer of security against account hacking.

Instagram

To help users differentiate between real and fake accounts, Instagram will now make it easy to look up information about individual accounts—including the date the account was created, its country of origin, and a record of username changes over the past year. You’ll also be able to see any ads the account is running and similar accounts with shared followers. To surface this information, tap the three dots on an Instagram profile page and select the new tab, “About This Account.” The feature will roll out to accounts with large followings, like celebrities, public figures, and accounts “sharing information related to current events, political or social causes.”

What's more, accounts with large numbers of followers will now be able to request verification from Instagram. The platform already gives blue checkmarks to some celebrity users and brands—WIRED's Instagram, for example, has one—but the verification process is mysterious, and Instagram hasn’t previously let users request verification. The new verification process involves a request form along with a place to upload a photo of a government-issued photo ID.

Post Mates

Instagram says the new changes are part of an effort to make the platform feel safe and to empower users to follow genuine accounts over fake ones.

“Keeping people with bad intentions off our platform is incredibly important to me,” Instagram’s co-founder and CTO, Mike Krieger, wrote in a blog post today. “That means trying to make sure the people you follow and the accounts you interact with are who they say they are, and stopping bad actors before they cause harm.”

The platform is also hoping to avoid some of the problems befalling its parent company, Facebook, which has struggled to keep fake accounts, misinformation campaigns, and untrustworthy pages off its service. Facebook said it has deactivated millions of fake accounts this year, and that some malicious actors are becoming harder to trace.

Instagram is, of course, a different beast. As it grows, it will have to face decisions about how to create community and trust on a global platform of over 1 billion users. Checkmarks and two-factor authentication aren't the end of that story. But they're a good place to start.

More Great WIRED Stories",,"It can lead to a loss of social connection. This is due to the way social media platforms are designed, which encourages users to share personal information and connect with others. The loss of social connection can lead to feelings of isolation, anxiety, and depression.",https://www.wired.com/story/instagram-security-update/,2018-08-28 17:00:00+00:00,LABEL_1_relevant,0.951593816280365,Social Media,social_media,Wired,Social Norms & Relationships
23,"Watch lawmakers grill Snap, TikTok and YouTube on kids' online safety","After dragging in the same companies and their reticent, overtrained executives time and time again, Congress is turning its attention to two of the tech industry’s fresh but important faces: TikTok and Snap.

On Tuesday, lawmakers on the Senate Subcommittee on Consumer Protection, Product Safety, and Data Security will question policy leads from those two companies and YouTube on how their platforms affect vulnerable young users. Facebook whistleblower Frances Haugen testified before the same committee on parallel issues in early October, shortly after revealing her identity.

The hearing will air on Tuesday at 7AM PT, featuring testimony from Snap VP of Global Public Policy Jennifer Stout, TikTok’s VP and Head of Public Policy Michael Beckerman and Leslie Miller, who leads government affairs and public policy at YouTube. A livestream is embedded below.

Subcommittee chair Senator Richard Blumenthal (D-CT) will lead the hearing, which will focus on social media’s detrimental effects on children and teens. “The bombshell reports about Facebook and Instagram—their toxic impacts on young users and lack of truth or transparency—raise serious concerns about Big Tech’s approach toward kids across the board,” Blumenthal said, connecting reports about Instagram’s dangers for teens to social media more broadly. The subcommittee’s ranking Republican Marsha Blackburn (R-TN) has signaled that she’s particularly interested in privacy concerns around TikTok.

We’d expect topics like eating disorders, harassment, bullying, online safety and data privacy to come up as members of the subcommittee take turns pressing the three policy leads for answers. The group of lawmakers also plans to discuss legislation that could help protect kids and teens online, though how solutions-oriented the hearing will be remains to be seen. Some of those potential solutions include the KIDS Act (Kids Internet Design and Safety), which would create new online protections for people under the age of 16. Blumenthal and fellow Democratic Senator Ed Markey reintroduced the bill last month.

The mental health of kids and teens isn’t the only pressing societal crisis that social platforms are implicated in at the moment, but it’s one Republicans and Democrats are both rallying around. For one, it’s a rare arena of criticism with plenty of political overlap for both sides. Both parties do seem to agree that tech’s biggest companies need to be controlled in some way, though they generally play up different parts of the why: for conservatives it’s that these companies have too much decision making power when it comes to what content gets wiped from their platforms. On the opposite side of the aisle, Democrats are generally much more worried about the kind of content that gets left up, like extremism and misinformation.

Tuesday’s hearing will also likely dive into how algorithms amplify harmful content. Because social media companies play their cards close to the chest when it comes to how their algorithms work, hearings are a rare opportunity for the public to learn more about how these companies serve their users personalized content. Ideally we’d be learning a lot about that kind of thing in the often lengthy, repetitive tech hearings Congress has held in the last couple of years, but between lawmakers pushing uninformed or irrelevant lines of questioning and evasive tech executives with hours of media training under their belts, the best we can usually hope for is a few new tidbits of information.

While Facebook won’t appear at this particular hearing, expect recent revelations around that company and Instagram to inform what happens on Tuesday. All three social media companies set to testify have had an eye on the public response to leaked Facebook documents and more reporting on that data just landed on Monday.

Just after the initial reports that Instagram is aware of the risks it poses to teen users, TikTok introduced a new set of safety measures including a well-being guide, better search interventions and opt-in popups for sensitive search terms. Last week, Snap announced a new set of family-focused safety tools to give parents more visibility into what their kids are up to using the platform. Both social networks skew heavily toward younger users compared to platforms like Facebook, Instagram and Twitter, making robust safety tools even more of a necessity. Leading into the hearing, YouTube announced some changes of its own around what kind of kids content will be eligible for monetization while also highlighting its other kid-centric safety measures.",Yes,"It harms the mental health of children and teens. The hearing will focus on how social media's detrimental effects on children and teens, and how to protect them.",https://techcrunch.com/2021/10/25/snap-tiktok-youtube-senate-hearing-congress/,2021-10-25 00:00:00,LABEL_1_relevant,0.9964458346366882,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
24,YouTube’s Push to Counter Toxic Videos With ‘Good’ Creators,"For his next video, he tells the group, Nahdi wants to go back to Indonesia to find out more about his grandmother’s life. Through her story, he wants to illustrate the sacrifice that so many millions of immigrants have made. But more than that, he wants to tell his own story. As the son of immigrants, he never really felt at home in London. Would Indonesia be any different?

“It’s not just about the journey, but also the sacrifice that generations before made to make me who I am today,” Nahdi says. “I’m just wondering: Does it hit that mark hard enough?” He pauses to scan the circle for reassurance. He doesn’t just want to hit that mark. He needs to hit that mark. The moral of the story is the reason he’s here.

YouTube launched the experimental program in late 2016, just as the world was waking up to the idea that Silicon Valley’s most celebrated tech companies might be introducing more harm than good into the world. That’s true of Facebook, Twitter, and most certainly, YouTube. Terrorists use these platforms to encourage violence. Russian trolls use them to undermine democracies. Conspiracy theorists use them to slander the victims of mass shootings, and kids use them to bully their classmates. With 400 hours of video being uploaded to YouTube every minute, the company has struggled to stay on top of every video that violates its policies against hate speech, harassment, and other harmful content. It has struggled even more with the question of whether and where to draw those lines in the first place.

YouTube has, like other tech companies, developed technology to spot the obvious garbage---the ISIS propaganda, the child pornography---and it’s hiring 10,000 human moderators to vet videos that get flagged. But it’s also taking another approach to the problem, investing in so-called “counterspeech” initiatives. It has become a buzzword in the tech industry lately, based on the idea that, as Supreme Court Justice Louis Brandeis once put it, the proper remedy to bad speech isn’t forced silence. It’s more speech.

Counterspeech tactics, which have been used to try to defuse tensions in the real world, are now being applied online. Rather than just shutting down content that’s bad for its users, tech companies are looking for ways to nudge them toward posts that, in their judgment anyway, are good for them. Facebook is doing this by adding fact-based related articles to news stories that are marked by its fact-checkers as false, essentially offering an antidote to fake news. YouTube uses a tool called the Redirect Method to direct users searching for certain terrorism-related keywords to carefully curated playlists of videos that debunk the extremist narratives.

Creators for Change is equal parts public relations and public policy. It’s a bet, though not an altogether sure one, that by empowering its more virtuous creators with funding, training, and exposure, YouTube can help them expand their reach and maybe even steal some of the audience from the network’s nastier voices. “There are a lot of these really positive voices, and we wanted to elevate those voices, whether it was dealing with LGBTQ issues, xenophobia, Islamophobia, or bullying,” says YouTube’s CEO Susan Wojcicki.

Creators for Change is YouTube's bet, that by empowering its more virtuous creators with funding, training, and exposure, the company can help them expand their reach and maybe even steal some of the audience from the network’s nastier voices. Alexander Coggin

But one person’s counter-speech is another one’s propaganda. For a company that likes to frame itself as a neutral platform, it’s an unusual approach. YouTube, and particularly its sister company Google, have recently faced charges of liberal bias from the right. They've defended against those charges by insisting their platforms welcome all political viewpoints. The same goes for Creators for Change. ""When [creators] apply to the program, we don't consider political affiliation or viewpoints,"" a YouTube spokesperson told me. ""Instead we look at their proposals for promoting tolerance, understanding, and bridging divides with constructive dialogue."" And yet, while the company doesn't explicitly advertise that conservatives need not apply, with Creators for Change, YouTube is at least implicitly aligning itself with a certain set of progressive ideals, accusations of bias be damned.

In January, YouTube announced it was expanding the initiative, investing $5 million in this year’s cohort, up from $1 million the year before. To YouTube’s parent company, Alphabet, which is worth a whopping $725 billion, that’s a couple of rusty quarters in the front seat cup holder. But small as it still is, Creators for Change is a valuable test case for a big idea. At a time when open platforms like YouTube face a choice between protecting their users and protecting their users’ ability to speak freely, counterspeech represents a tantalizing third possibility. Whether it's truly an effective foil or simply glorified marketing remains to be seen.",Yes,"It can lead to the spread of fake news, which can have harmful effects on people's lives.",https://www.wired.com/story/youtubes-creators-for-change-counterspeech/,2018-10-31 12:30:00+00:00,LABEL_1_relevant,0.9966237545013428,Social Media,social_media,Wired,"Information, Discourse & Governance"
25,How beer-related tweeting climaxed during the World Cup,"The question that Roller and co selflessly investigate is how interest in beer changes before, during, and after a game in each of the competing countries.

To find out, they collected Tweets published during the tournament that contained the beer emoji or others related to alcohol (such as sake), along with a country of origin. To find out the background rate of tweets with these emoji, they worked out how often they appear when a game isn’t being played.

The results show that Brazil and England lead the world by this measure. “On average, more than half of the BEER Tweets per day come from Brazil and England together,” point out the researchers.

That’s something of a puzzle given that Brazilians consume significantly less beer per capita than countries such as Germany, France, and England. But it’s no surprise that Saudi Arabia brings up the rear.

This ranking provides a background level against which to judge the rate of messaging during games. And the results make for interesting reading.

Roller and co say that the rate of tweeting about beer increases dramatically in most countries during the games they play. The largest increase was in Morocco, where beer-related tweets increased by over 400% during its matches. Morocco also showed the biggest decline in beer-related tweets after it was knocked out of the tournament. “Generally, the results show that from almost all countries more alcohol related Tweets can be found during the tournament,” say the team.

There are some exceptions. Some countries showed no increase—for example, Japan and Saudi Arabia. And beer-related tweets in Peru actually decreased during games, suggesting an admirable focus on the sport itself.

Roller and co say the results have implications for health, since alcohol consumption is linked to a wide range of medical conditions. “We know that increased alcohol consumption can lead to serious health issues, such as cancer, cardiovascular diseases and diseases of the digestive system, to mention a few,” they say.

So the national health of countries in which the teams were knocked out earlier must have benefited. “As many people tweet less after their national team left the tournament, we draw the conclusion that leaving the tournament early, as Germany did, is the healthiest solution—unless you are Peru,” say Roller and co, with their tongues firmly in their cheeks.

Whether or not that’s true, Roller and co end by pointing to their most significant conclusion: “Finally, we showed that Brazil tweets by far the most about beer. Cheers!” Quite!

Ref: arxiv.org/abs/1811.03809 : Football and Beer—A Social Media Analysis on Twitter in Context of the FIFA Football World Cup 2018",Yes,"It provides a platform for people to share information and connect with others, which can lead to unexpected outcomes such as the spread of misinformation or the reinforcement of negative stereotypes.",https://www.technologyreview.com/2018/11/22/138987/how-beer-related-tweeting-climaxed-during-the-world-cup/,2018-11-22 00:00:00,LABEL_1_relevant,0.995072901248932,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
26,"By Defying Apple’s Rules, Facebook Shows It Never Learns","If an app on Facebook behaved the way Facebook has been behaving, Facebook would probably have shut it down by now.

Tuesday’s scathing TechCrunch investigation all but guarantees it. The report found that Facebook has been paying people as young as 13 years old to download an app that grants Facebook access to users’ entire phone and web history, including encrypted activity and private messages and emails. The app, called Research, allows Facebook to see how people’s friends, who have not consented to having their data collected, interact with those users, too.

Facebook says the app was purely for market research. Explained another way: The app allowed Facebook to spot competitive threats on the horizon to help it retain its unprecedented power. Facebook has used another app, called Onavo, to collect similar information; for example, data from Onavo alerted Facebook to the growing popularity of the messaging app WhatsApp before the company acquired it in 2014.

""I think it speaks to the growth-at-any-cost mentality of the company,"" says Ashkan Soltani, who served as chief technologist to the Federal Trade Commission during its 2011 investigation of Facebook.

Facebook didn't respond to WIRED's request for comment.

At a time when Facebook is under the microscope for violating its users' privacy, such techniques are bold enough. But what makes the operation even more brazen is that Facebook continued running the program, which launched in 2016 and was sometimes called Atlas, even after Apple banned Onavo from the App store less than six months ago. Apple said it would no longer allow developers to collect information from other third-party apps.

Apparently undeterred, Facebook created a workaround for the Research app. It circumvented Apple’s vetting process using a technical loophole that is only intended for apps Facebook distributes to its own employees. That allowed Facebook to ingest everything a user did on their phones, including teens and minors. While kids under the age of 17 had to receive parental consent to participate, the disclosure form analyzed by TechCrunch minimized the extent of what could be done with all that data. “There are no known risks associated with the project,” it read. Facebook told TechCrunch only 5 percent of the app's users were teens.

Still, even the solicitations adults would have received about the app weren't entirely forthcoming. When users referred their friends to the app, for which they could also get paid, the email they received encouraged them to “Install it and forget it,” making the act of giving away unlimited access to their private communications sound as harmless as setting up a Ronco Rotisserie.

""I think it speaks to the growth-at-any-cost mentality of the company."" Ashkan Soltani

The Research app is just the latest example of Facebook’s doublespeak. In public and even under oath, executives like Mark Zuckerberg and Sheryl Sandberg have spent at least a year---if not their entire careers---promising to do better by their users. But in private, evidence abounds that the company continues to flout every rule and attempt at oversight placed before it. They've promised to protect user privacy by cutting off developer access to data while continuing to give it away to corporate giants and major advertisers. They've vowed to investigate foreign interference in elections, all while withholding information about the extent of that interference on Facebook. They've launched efforts to make their ads more transparent, while crippling external efforts by organizations like ProPublica to pull back the curtain even further.

Even as privacy hounds and antitrust watchdogs at the FTC and on Capitol Hill sniff and scratch at Facebook’s door, the social media giant, apparently high on hubris, just keeps tossing them red meat. If Facebook has learned anything from the last two years of public and regulatory scrutiny, it has a funny way of showing it.",Yes,"It has given rise to a new breed of internet user that is more narcissistic and self-centered than ever before. These users are more likely to share personal information, engage in risky online behavior, and be less concerned about the privacy of others.",https://www.wired.com/story/facebook-research-app-lessons/,2019-01-30 19:37:29.439000+00:00,LABEL_1_relevant,0.996469020843506,Social Media,social_media,Wired,Security & Privacy
27,Someone Hacked a US Warship Facebook Account to Stream Games,"Well, it was a very bad, no good, week for Twitch. The Amazon-owned streaming service suffered a devastating hack, with Twitch source code, streamer payouts, internal security tools, and more all dumped on 4chan. It's about as thorough a data dump as you'll see. Among the many repercussions: Twitch streamers experienced sudden, forced pay transparency, which quickly became a meme on social media and Twitch itself.

Twitch wasn't the only unlucky tech titan this week. Facebook, Instagram, and WhatsApp all got knocked out of service for several hours on Monday. No, it wasn't hackers. Instead, Facebook accidentally withdrew its BGP route, effectively making it impossible for the rest of the internet to find it. Not only that, but it knocked Facebook's own networks off the internet's maps, meaning its engineers couldn't fix the problem remotely. A real mess! The company experienced down time Friday afternoon as well, but not nearly as extensive an outage.

Are you using a password manager yet? And while we're at it, is the rest of your family? They should be! If you're having a hard time convincing them, we've put together a few suggestions that might help get them to buy in.

A simple bug leaves AirTag users potentially vulnerable. And a judge found that the internet infrastructure company Cloudflare isn't responsible for copyright-infringing sites that use its services.

And there's more! Each week we round up all the security news WIRED didn’t cover in depth. Click on the headlines to read the full stories, and stay safe out there.

Everything’s fine now, it's resolved; the Navy has its Facebook page back. But sometime Sunday night, someone managed to break into the account of the USS Kidd, a US Navy destroyer, and used that access to, well, stream the real-time strategy game Age of Empires. A Navy spokesperson confirmed to the military-focused news site Task & Purpose on Wednesday that someone gained ""unauthorized access"" to the Facebook page.



The streams were accompanied by brief messages like ""hi guys"" and ""play game"" and ""ffffffffffff."" The likeliest culprit seems to be a family member of someone who has the keys to the USS Kidd’s social media account.

Firefox is a great browser for the privacy-minded, but maybe not in this specific instance? In the latest version, when you type into the address bar you'll get ""new, relevant suggestions from our trusted partners based on what you're searching for."" In other words, a type of ad in an unexpected place. You can turn the feature off by heading to Settings, then Privacy & Security, then uncheck Contextual suggestions down under Address Bar — Firefox Suggest. But c'mon—you shouldn't have to do this in the first place, and you definitely should have a better heads-up about where the URL prompts in your address bar are coming from.

A very big hack that did not get nearly enough attention was quietly disclosed in an SEC filing by Syniverse, a telecom infrastructure company. As Motherboard first reported, hackers managed to stay in Syniverse systems for years, and they would have had access to all manner of phone records, potentially including text messages. It's unclear if there's been any fallout yet from the prolonged incident, but it's a potential treasure trove for international espionage.

Speaking of international espionage! Google this week sounded the alarm that Russia's Fancy Bear hackers had unleashed a broad, sophisticated phishing campaign that targeted 14,000 Gmail users last month. Individual accounts that were targeted received alerts, and the group appears to have included journalists and security researchers along with people in a variety of other industries. Google said all of the attempts were caught by spam filters, but the scale of the effort was still notable.

More Great WIRED Stories",Yes,"We are now living in a world where the most popular and influential people are those who have proven themselves to be the most adept at gaining and maintaining the attention of billions of people. And that is bad for democracy.

🎧 WIRED's favorite stories, every day in your inbox. By signing up, you agree to our Privacy",https://www.wired.com/story/navy-facebook-page-hack-russia-phishing-syniverse-hack-security-news/,2021-10-09 13:00:00+00:00,LABEL_1_relevant,0.9965458512306212,Social Media,social_media,Wired,"Information, Discourse & Governance"
28,Twitter Continues Cleanup and Cracks Down on Malicious Apps,"Several weeks ago, a friend signed up for a Twitter application that promised to delete old tweets. It was advertised as a simple way to clear your online presence, but after he used it, my friend noticed that his account began retweeting spam. Out of an abundance of caution, he deleted his Twitter account entirely.

On Tuesday, Twitter announced steps it was taking to curb this exact sort of abuse. Between May and June of this year, the social network says it deleted more than 143,000 apps that violated its policies prohibiting developers from using its APIs to automate spam, abuse, or violate people’s privacy, among other rules.

The move is part of a greater cleanup happening at Twitter. Earlier this month, the company announced that it was removing some suspicious accounts from people’s follower lists, and the The Washington Post reported it had recently suspended more than 70 million fake accounts entirely.

Automated accounts often play a central, positive role on Twitter. There’s a Twitter bot that creates emoji aquariums, one that spits out “deep” questions about the universe, and another that tweets anonymized information about individual Americans each hour using data from the decennial US census. Developers also use Twitter’s API to create all sorts of tools, like Nuzzel, an app that displays news stories the people you follow on Twitter are talking about.

There are thousands of other useful, delightful, or otherwise harmless apps on Twitter, but the social network has also been plagued by ones that automate abuse, leveraging Twitter’s tools to violate its own policies. Twitter apps have been created that direct-message spam, help carry out misinformation campaigns, and assist governments conducting online surveillance. Now, the social network is cracking down on them, as part of a wider effort to make Twitter “healthier.”

In addition to removing problematic apps, Twitter announced Tuesday that it is rolling out new policies to prevent ill-intentioned developers from getting access to its APIs in the first place. The company will now require all developers interested in creating apps to go through a stricter application process, which was initially launched in November for access to its “premium” APIs. Developers already using Twitter’s APIs will also need to go through the process; Twitter says they will be given 90 days notice before enforcing the requirement.

The application will require developers provide “detailed information about how they use or intend to use Twitter’s APIs so that we can better ensure compliance with our policies,” wrote Yoel Roth, Twitter’s platform policy manager, and Rob Johnson, a senior product management director, in their blog post announcing the change.

Twitter is also imposing new limits on developers whose applications are accepted. For example, a single developer account can now only register 10 apps by default, and must submit a request for the ability to create more. Apps will now also be limited in the amount of automated activity they can facilitate. A Twitter bot, for example, will only be allowed to tweet or retweet 300 times in an hour and follow only 1000 people in a day. In addition, Twitter rolled out a new tool where users can report malicious apps and API abuses.",Yes,"It can be abused by automated accounts that violate Twitter's policies. In response, Twitter is changing its policies to make it more difficult for developers to create these types of apps, and is also imposing new limits on the amount of automated activity that apps can facilitate.",https://www.wired.com/story/twitter-continues-cleanup-cracks-down-on-malicious-apps/,2018-07-24 18:31:37.758000+00:00,LABEL_1_relevant,0.9957740902900696,Social Media,social_media,Wired,"Information, Discourse & Governance"
29,Today's real story: The Facebook monopoly,"Facebook is a monopoly. Right?

Mark Zuckerberg appeared on national TV today to make a “special announcement.” The timing could not be more curious: Today is the day Lina Khan’s FTC refiled its case to dismantle Facebook’s monopoly.

To the average person, Facebook’s monopoly seems obvious. “After all,” as James E. Boasberg of the U.S. District Court for the District of Columbia put it in his recent decision, “No one who hears the title of the 2010 film ‘The Social Network’ wonders which company it is about.” But obviousness is not an antitrust standard. Monopoly has a clear legal meaning, and thus far Lina Khan’s FTC has failed to meet it. Today’s refiling is much more substantive than the FTC’s first foray. But it’s still lacking some critical arguments. Here are some ideas from the front lines.

To the average person, Facebook’s monopoly seems obvious. But obviousness is not an antitrust standard.

First, the FTC must define the market correctly: personal social networking, which includes messaging. Second, the FTC must establish that Facebook controls over 60% of the market — the correct metric to establish this is revenue.

Though consumer harm is a well-known test of monopoly determination, our courts do not require the FTC to prove that Facebook harms consumers to win the case. As an alternative pleading, though, the government can present a compelling case that Facebook harms consumers by suppressing wages in the creator economy. If the creator economy is real, then the value of ads on Facebook’s services is generated through the fruits of creators’ labor; no one would watch the ads before videos or in between posts if the user-generated content was not there. Facebook has harmed consumers by suppressing creator wages.

A note: This is the first of a series on the Facebook monopoly. I am inspired by Cloudflare’s recent post explaining the impact of Amazon’s monopoly in their industry. Perhaps it was a competitive tactic, but I genuinely believe it more a patriotic duty: guideposts for legislators and regulators on a complex issue. My generation has watched with a combination of sadness and trepidation as legislators who barely use email question the leading technologists of our time about products that have long pervaded our lives in ways we don’t yet understand. I, personally, and my company both stand to gain little from this — but as a participant in the latest generation of social media upstarts, and as an American concerned for the future of our democracy, I feel a duty to try.

The problem

According to the court, the FTC must meet a two-part test: First, the FTC must define the market in which Facebook has monopoly power, established by the D.C. Circuit in Neumann v. Reinforced Earth Co. (1986). This is the market for personal social networking services, which includes messaging.

Second, the FTC must establish that Facebook controls a dominant share of that market, which courts have defined as 60% or above, established by the 3rd U.S. Circuit Court of Appeals in FTC v. AbbVie (2020). The right metric for this market share analysis is unequivocally revenue — daily active users (DAU) x average revenue per user (ARPU). And Facebook controls over 90%.

The answer to the FTC’s problem is hiding in plain sight: Snapchat’s investor presentations:

This is a chart of Facebook’s monopoly — 91% of the personal social networking market. The gray blob looks awfully like a vast oil deposit, successfully drilled by Facebook’s Standard Oil operations. Snapchat and Twitter are the small wildcatters, nearly irrelevant compared to Facebook’s scale. It should not be lost on any market observers that Facebook once tried to acquire both companies.

The market Includes messaging

The FTC initially claimed that Facebook has a monopoly of the “personal social networking services” market. The complaint excluded “mobile messaging” from Facebook’s market “because [messaging apps] (i) lack a ‘shared social space’ for interaction and (ii) do not employ a social graph to facilitate users’ finding and ‘friending’ other users they may know.”

This is incorrect because messaging is inextricable from Facebook’s power. Facebook demonstrated this with its WhatsApp acquisition, promotion of Messenger and prior attempts to buy Snapchat and Twitter. Any personal social networking service can expand its features — and Facebook’s moat is contingent on its control of messaging.

The more time in an ecosystem the more valuable it becomes. Value in social networks is calculated, depending on whom you ask, algorithmically (Metcalfe’s law) or logarithmically (Zipf’s law). Either way, in social networks, 1+1 is much more than 2.

Social networks become valuable based on the ever-increasing number of nodes, upon which companies can build more features. Zuckerberg coined the “social graph” to describe this relationship. The monopolies of Line, Kakao and WeChat in Japan, Korea and China prove this clearly. They began with messaging and expanded outward to become dominant personal social networking behemoths.

In today’s refiling, the FTC explains that Facebook, Instagram and Snapchat are all personal social networking services built on three key features:

“First, personal social networking services are built on a social graph that maps the connections between users and their friends, family, and other personal connections.” “Second, personal social networking services include features that many users regularly employ to interact with personal connections and share their personal experiences in a shared social space, including in a one-to-many ‘broadcast’ format.” “Third, personal social networking services include features that allow users to find and connect with other users, to make it easier for each user to build and expand their set of personal connections.”

Unfortunately, this is only partially right. In social media’s treacherous waters, as the FTC has struggled to articulate, feature sets are routinely copied and cross-promoted. How can we forget Instagram’s copying of Snapchat’s stories? Facebook has ruthlessly copied features from the most successful apps on the market from inception. Its launch of a Clubhouse competitor called Live Audio Rooms is only the most recent example. Twitter and Snapchat are absolutely competitors to Facebook.

Messaging must be included to demonstrate Facebook’s breadth and voracious appetite to copy and destroy. WhatsApp and Messenger have over 2 billion and 1.3 billion users respectively. Given the ease of feature copying, a messaging service of WhatsApp’s scale could become a full-scale social network in a matter of months. This is precisely why Facebook acquired the company. Facebook’s breadth in social media services is remarkable. But the FTC needs to understand that messaging is a part of the market. And this acknowledgement would not hurt their case.

The metric: Revenue shows Facebook’s monopoly

Boasberg believes revenue is not an apt metric to calculate personal networking: “The overall revenues earned by PSN services cannot be the right metric for measuring market share here, as those revenues are all earned in a separate market — viz., the market for advertising.” He is confusing business model with market. Not all advertising is cut from the same cloth. In today’s refiling, the FTC correctly identifies “social advertising” as distinct from the “display advertising.”

But it goes off the deep end trying to avoid naming revenue as the distinguishing market share metric. Instead the FTC cites “time spent, daily active users (DAU), and monthly active users (MAU).” In a world where Facebook Blue and Instagram compete only with Snapchat, these metrics might bring Facebook Blue and Instagram combined over the 60% monopoly hurdle. But the FTC does not make a sufficiently convincing market definition argument to justify the choice of these metrics. Facebook should be compared to other personal social networking services such as Discord and Twitter — and their correct inclusion in the market would undermine the FTC’s choice of time spent or DAU/MAU.

Ultimately, cash is king. Revenue is what counts and what the FTC should emphasize. As Snapchat shows above, revenue in the personal social media industry is calculated by ARPU x DAU. The personal social media market is a different market from the entertainment social media market (where Facebook competes with YouTube, TikTok and Pinterest, among others). And this too is a separate market from the display search advertising market (Google). Not all advertising-based consumer technology is built the same. Again, advertising is a business model, not a market.

In the media world, for example, Netflix’s subscription revenue clearly competes in the same market as CBS’ advertising model. News Corp.’s acquisition of Facebook’s early competitor MySpace spoke volumes on the internet’s potential to disrupt and destroy traditional media advertising markets. Snapchat has chosen to pursue advertising, but incipient competitors like Discord are successfully growing using subscriptions. But their market share remains a pittance compared to Facebook.

An alternative pleading: Facebook’s market power suppresses wages in the creator economy

The FTC has correctly argued for the smallest possible market for their monopoly definition. Personal social networking, of which Facebook controls at least 80%, should not (in their strongest argument) include entertainment. This is the narrowest argument to make with the highest chance of success.

But they could choose to make a broader argument in the alternative, one that takes a bigger swing. As Lina Khan famously noted about Amazon in her 2017 note that began the New Brandeis movement, the traditional economic consumer harm test does not adequately address the harms posed by Big Tech. The harms are too abstract. As White House advisor Tim Wu argues in “The Curse of Bigness,” and Judge Boasberg acknowledges in his opinion, antitrust law does not hinge solely upon price effects. Facebook can be broken up without proving the negative impact of price effects.

However, Facebook has hurt consumers. Consumers are the workers whose labor constitutes Facebook’s value, and they’ve been underpaid. If you define personal networking to include entertainment, then YouTube is an instructive example. On both YouTube and Facebook properties, influencers can capture value by charging brands directly. That’s not what we’re talking about here; what matters is the percent of advertising revenue that is paid out to creators.

YouTube’s traditional percentage is 55%. YouTube announced it has paid $30 billion to creators and rights holders over the last three years. Let’s conservatively say that half of the money goes to rights holders; that means creators on average have earned $15 billion, which would mean $5 billion annually, a meaningful slice of YouTube’s $46 billion in revenue over that time. So in other words, YouTube paid creators a third of its revenue (this admittedly ignores YouTube’s non-advertising revenue).

Facebook, by comparison, announced just weeks ago a paltry $1 billion program over a year and change. Sure, creators may make some money from interstitial ads, but Facebook does not announce the percentage of revenue they hand to creators because it would be insulting. Over the equivalent three-year period of YouTube’s declaration, Facebook has generated $210 billion in revenue. one-third of this revenue paid to creators would represent $70 billion, or $23 billion a year.

Why hasn’t Facebook paid creators before? Because it hasn’t needed to do so. Facebook’s social graph is so large that creators must post there anyway — the scale afforded by success on Facebook Blue and Instagram allows creators to monetize through directly selling to brands. Facebooks ads have value because of creators’ labor; if the users did not generate content, the social graph would not exist. Creators deserve more than the scraps they generate on their own. Facebook suppresses creators’ wages because it can. This is what monopolies do.

Facebook’s Standard Oil ethos

Facebook has long been the Standard Oil of social media, using its core monopoly to begin its march upstream and down. Zuckerberg announced in July and renewed his focus today on the metaverse, a market Roblox has pioneered. After achieving a monopoly in personal social media and competing ably in entertainment social media and virtual reality, Facebook’s drilling continues. Yes, Facebook may be free, but its monopoly harms Americans by stifling creator wages. The antitrust laws dictate that consumer harm is not a necessary condition for proving a monopoly under the Sherman Act; monopolies in and of themselves are illegal. By refiling the correct market definition and marketshare, the FTC stands more than a chance. It should win.

A prior version of this article originally appeared on Substack.",Yes,"It has suppressed wages in the creator economy. Facebook, as the dominant player in social media, has been able to do this because it has a monopoly. The FTC's recent refiling of its case against Facebook seeks to address this issue.",https://techcrunch.com/2021/08/19/todays-real-story-the-facebook-monopoly/,2021-08-19 00:00:00,LABEL_1_relevant,0.9951475262641908,Social Media,social_media,TechCrunch,Economy
30,Cloudflare Ditches 8chan. What Happens Now?,"The internet infrastructure firm Cloudflare said it would cut service on Sunday evening to 8chan, the infamous online forum that has housed numerous posts and manifestos linked to horrific mass shootings in the United States and around the world. The move comes nearly two days after a mass shooting at an El Paso, Texas, Walmart left 20 dead and dozens wounded. The alleged gunman appears to have posted his manifesto on 8chan 20 minutes before the shooting. Cloudflare CEO Matthew Prince spoke with WIRED Sunday night about his decision.

Cloudflare provides infrastructural support, like content delivery services and DDoS protection, to 19 million online properties. Revoking that support can effectively shut a site down, at least until it finds a new provider. But the company has long maintained that it should not serve as an arbiter of speech online, with one notable exception: Cloudflare severed ties with white supremacist site the Daily Stormer two years ago.

As pressure mounted in the wake of the El Paso shooting, Cloudflare at first maintained that it would not drop 8chan. But by Sunday evening, the company had reversed course.

""8chan has been on our radar for a long time as a problematic user,"" says Prince. ""But we have a responsibility, which is much beyond ‘We terminate sites we don’t like.’ I’m nervous about whether we’ve made the right decision, and I’m nervous about how this will set precedent in the future.""

Prince argues that, rather than Cloudflare, platforms like Facebook, YouTube, and Twitter should decide what belongs on their own sites. Cloudflare shouldn't make those calls any more than asphalt should set speed limits. The major platforms can, and do, moderate their own content and manage violent, destructive trends themselves—even if it's been an imperfect system in practice. But Prince says that he didn't account for platforms like 8chan that are intentionally created as a forum for unregulated expression.

""When you have platforms that are effectively lawless like this, then maybe that shifts the responsibility further down the stack,"" Prince says. Looking at Daily Stormer and now 8chan, Prince says that Cloudflare is attempting to find the line where ""a site has shown repeatedly that it is causing active, real harm.""

Cloudflare gets requests from individuals, institutions, and governments worldwide to take down sites every day, though, because of their alleged real-world harm. It doesn't act on those requests precisely because that allegation is so subjective. As a result, the actual parameters for what merits a takedown remain extremely murky.

And even when Cloudflare takes the step to unilaterally cut service, nothing stops those shunned sites from buying the services they need elsewhere. The Daily Stormer, for example, was back online within a couple of days, albeit harder to find. And though Google stopped indexing the 8chan homepage back in 2015, taking some wind out of its sails, the site has still managed to become a go-to for hate speech of all sorts and, of late, mass shooters spreading their extremist propaganda.

""Some of you might’ve read the @Cloudflare news already,"" a Twitter account associated with the site posted on Sunday evening. ""There might be some downtime in the next 24-48 hours while we find a solution.""

""It solves the problem for us, but it doesn’t solve the problem for the internet."" Matthew Prince, Cloudflare CEO

Even if 8chan itself somehow never reemerges, which seems unlikely, discussions like those hosted on the forum can easily move to another platform. 8chan itself grew out of the forum 4chan in 2013. Meanwhile, the anything-goes social network Gab went down temporarily last fall, because its providers—including the domain registrar GoDaddy and web host Joyent—dropped it after the alleged shooter in the Pittsburgh synagogue shooting made threats on the social network. But Gab came back as soon as other companies stepped in to provide services. Cloudflare stuck with Gab throughout.

Infrastructure providers have the power to let a site go down, and maybe even do it irreparable damage in the process. But the internet was built to be decentralized precisely to protect speech, so regardless of what you think of the precedent Cloudflare is setting, 8chan will likely be back soon, something Prince readily acknowledges.

""The real thing that kills me is that, at the end of the day, [dropping 8chan] doesn’t actually fix the problem,"" Prince says. ""It solves the problem for us, but it doesn’t solve the problem for the internet. It doesn't address the core causes of why hate festers online.""

UPDATE 8/5/209 10:15 am EST: This story has been updated to reflect that Google removed the 8chan homepage from search results in 2015, but still indexes certain other 8chan pages.

More Great WIRED Stories",,"Platforms like Facebook and Twitter now serve as hosts for a wide variety of extremist content with little to no regulation. This has led to a situation where, for example, the Christchurch shooter was able to livestream his attack on Facebook, and the El Paso shooter was able to post his manifesto on 8chan. While Cloudflare's decision to drop",https://www.wired.com/story/cloudflare-8chan-support-ddos/,2019-08-05 04:28:21.437000+00:00,LABEL_1_relevant,0.9951393604278564,Social Media,social_media,Wired,"Information, Discourse & Governance"
31,Facebook admits that the internet can be bad for democracy,"The company now says social media may hurt civic discourse, but it promises to do its “moral duty” by understanding how to mitigate such problems.

What it says: Of the thousands of words published in a pair of new essays on the topic, the choicest cut comes from Facebook’s product manager for civic engagement, Samidh Chakrabarti:

If there’s one fundamental truth about social media’s impact on democracy it’s that it amplifies human intent— both good and bad.I wish I could guarantee that the positives are destined to outweigh the negatives, but I can’t.

Why it matters: It’s another sign, along with a commitment to build a “more meaningful” social network, that Facebook is responding to criticism about fake news, Russian meddling, and addictive content.

Now what? Chakrabarti says the company has a “moral duty to understand how [its] technologies are being used and what can be done to make … Facebook as representative, civil and trustworthy as possible.”",Yes,"It amplifies human intent— both good and bad. Facebook has a moral duty to understand how its technologies are being used and what can be done to make Facebook as representative, civil and trustworthy as possible.",https://www.technologyreview.com/2018/01/22/146091/facebook-admits-that-the-internet-can-be-bad-for-democracy/,2018-01-22 00:00:00,LABEL_1_relevant,0.9959419369697572,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
32,2018 Was a Rough Year for Truth Online,"Earlier this month, I was on the phone with Ryan Fox, cofounder of New Knowledge, a cybersecurity firm that tracks Russian-related influence operations online. The so-called Yellow Vest protests had spread across France, and we were talking about the role disinformation played in the galvanizing French hashtag for the protests, #giletsjaunes. Conversations like these are a regular part of my job and usually focus on the quantifiable aspects of social media manipulation campaigns—volume of posts, follower count, common keywords, signs of inauthenticity, that sort of thing. But something else creeped into our discussion, an immeasurable notion so distracting and polarizing for most in the disinformation research community that I learned long ago to stop bringing it up: What is the impact of these misinformation campaigns?

While I didn't ask this question of Fox, he addressed it as though I had: “We get this question a lot: Did they cause this? [Meaning, the gilets jaunes protests.] Did they make it worse? They're pouring fuel on the fire, yes. They are successful at exacerbating the narrative. But I don't know what the world would look like had they not done it.”

Oft asked and rarely satisfactorily answered, the question of impact is the disinformation research community’s white whale. You can measure reach, you can measure engagement, but there’s no simple data point to tell you how one coordinated influence campaign affected an event or someone’s outlook on a particular issue.

There has never been a more exciting or high-stakes time to study or report on social media manipulation, yet therein lies the issue. It’s difficult to balance the urge to report complicated and impressive analyses of large swaths of data from propaganda-pushing networks with the responsibility to hedge your findings behind the seemingly nullifying admission that there is no way to truly understand the actual effect of these actions. Especially when much of the discourse on the subject is plagued by inaccuracies and exaggerations, often caused by media efforts to simplify pages of nuanced research into something that fits in a headline. Coordinated influence campaigns are reduced to “bots” and “trolls,” despite the fact that those are rarely, if ever, accurate descriptions of what’s actually going on.

The internet has always been awash with misinformation and hate, but never has it felt so inescapable and overwhelming as it did this year. From Facebook’s role in fanning the flames of ethnic cleansing in Myanmar to the rise of QAnon to the so-called migrant caravan to the influence campaign conducted by the Kremlin’s Internet Research Agency, 2018 was a rough year to be online, regardless of the strength of your media literacy skills.

Want more? Read all of WIRED’s year-end coverage

It has become increasingly difficult to parse the real from the fake, and even harder to determine the effect of it all. On December 17, cybersecurity firm New Knowledge released a report on the IRA's campaign to sow division and influence American voters on Twitter, Facebook, and other platforms. It's one of the most thorough analyses of the IRA’s misdeeds to take place outside of the companies themselves. At the behest of the Senate Intelligence Committee, New Knowledge reviewed more than 61,500 unique Facebook posts, 10.4 million tweets, 1,100 YouTube videos, and 116,000 Instagram posts, all published between 2015 and 2017. But even with that mountain of data, the researchers were unable to reach concrete conclusions about impact.

“It is impossible to gauge the full impact that the IRA’s influence operations had without further information from the platforms,” the authors wrote. New Knowledge said that Facebook, Twitter, and Google could provide an assessment of what users who were targeted by the IRA thought of the content they were exposed to.

This is a significant claim, but the researchers say the platforms could study the activities of the victims of information warfare rather than the perpetrators, and ask: What were users saying in the comments of voter suppression attempts on Instagram? What conversations were happening between IRA members and users in DMs? Where did users go on the platform, and what did they do after being exposed to IRA content? But the platforms failed to turn any of this information over. This is particularly problematic, the researchers said, because “foreign manipulation of American elections on social platforms will continue to be an ongoing, chronic problem,” and by keeping people in the dark about the effectiveness of old tactics—which have almost certainly been improved upon in the years since—platforms leave users vulnerable to any future attempts.",Yes,"We can no longer have a shared reality. Instead, we have competing narratives.",https://www.wired.com/story/2018-was-a-rough-year-for-truth-online/,2018-12-26 12:00:00+00:00,LABEL_1_relevant,0.9928119778633118,Social Media,social_media,Wired,"Information, Discourse & Governance"
33,Trump's TikTok Drama Is a Distraction,"Millions of young Americans were sent into a panic last weekend when President Donald Trump told reporters that he was “banning” TikTok from the United States. White House officials had already been discussing taking action against the social media platform and other Chinese-owned apps over national security concerns. But as is typical for Trump, it’s not exactly clear what, if anything, will happen next. On Sunday, Microsoft released a statement saying it had spoken with the president and planned to continue discussions about buying TikTok from its parent company, ByteDance, by September 15. For now, TikTok is still available in the US.

The episode is just the latest flare-up in the ongoing deterioration of relations between the US and China. Whether TikTok is sold or banned, the underlying security concerns it raises—about privacy, espionage, foreign manipulation and propaganda, human rights and civil liberties—are going to remain. In that respect, the drama unfolding around TikTok risks overshadowing larger questions about the future of the world’s two biggest superpowers. “I just think the notion that TikTok is the big issue in US–China relations is silliness, and I think it distracts from very important issues,” says Graham Webster, the editor in chief of the DigiChina Project, a collaboration between Stanford University and New America.

One of the biggest worries about TikTok is whether the Chinese government could compel ByteDance to hand over user data on Americans. TikTok has repeatedly said it wouldn’t share information with the Communist Party even if asked, but the possibility can’t be dismissed. At the same time, focusing on TikTok alone ignores a perhaps even more troubling reality: The United States doesn’t have robust data protection rules in place for any company. “Countries around the world are working on that type of regime, and the US is frankly not,” says Webster. After Facebook’s Cambridge Analytica scandal, Congress briefly appeared interested in passing federal privacy legislation, but that effort seems to have stalled. It shouldn’t have: As more Americans work and socialize online, they’re creating even more personal data. Already, the US government says Chinese hackers have pilfered poorly protected information from a number of American institutions, including government agencies and a major credit bureau.

Another concern is whether the Chinese government could turn TikTok into a vector for propaganda. Not that they need a locally owned app to do that: The Chinese Communist Party has already been accused of launching disinformation campaigns on Facebook and Twitter targeting pro-democracy activists in Hong Kong. But because the algorithms powering TikTok are still largely opaque and its parent company is in Beijing, the fear is that US officials wouldn’t catch state-sponsored manipulation until it was too late, as was the case when the Russian government interfered in the 2016 presidential election. Social media companies also haven’t historically been eager to grant outside researchers or observers access to their technologies. TikTok has announced it is opening a transparency center in Los Angeles, where experts can see in person how it moderates content. And on Monday, the company said it was making it easier for users to report election misinformation and will expand partnerships with fact-checking organizations.

“The notion that TikTok is the big issue in US–China relations is silliness, and I think it distracts from very important issues.” Graham Webster, DigiChina Project

TikTok has over 100 million users in the US, but there are much larger, and potentially more serious, targets for foreign adversaries in the country. That includes the industrial infrastructure powering electric grids and water systems, for example, and experts say those areas could use the same kind of attention now being paid to a single app. “Industrial control systems is where you will see the most sophisticated, well-financed nation-state attacks first,” says Monta Elkins, “hacker in chief” at the security firm FoxGuard Solutions and an instructor at the SANS Institute. Elkins says there’s a shortage of professionals trained to work in industrial control systems and that some of the equipment is decades old. “Right now, we don’t run antivirus on these machines. They’re computers running software that might be vulnerable,” he says.

Bolstering national security and combating China’s authoritarian regime is also, of course, about more than just cybersecurity. TikTok’s critics have said Chinese-owned apps can’t be trusted because of the country’s terrible track record on human rights, particularly its oppression of Uighur Muslims in Xinjiang and its treatment of Hong Kong. At the same time, the US government has come under fire for failing to adequately address these abuses—and taking action on a social media app would do nothing to change that.

US officials have placed sanctions on companies who supply the surveillance technology used in Xinjiang, and Congress recently passed the historic Uyghur Human Rights Policy Act, which Trump signed. But the president also said he declined to sanction Chinese officials because of trade talks. John Bolton, Trump’s former national security adviser, claims Trump told Chinese leader Xi Xingping last year that building concentration camps for Uighurs was the right thing to do. Experts note that the US doesn’t have an asylum program for China’s Muslim minorities, nor nonpartisan funding for things like cultural education. “If we want to take the cultural genocide claim seriously, the way to do that is to promote Uighur heritage,” says Darren Byler, a postdoctoral fellow at the Center for Asian Studies at the University of Colorado.",Yes,It can be a tool for foreign adversaries to manipulate Americans. The best way to combat this is for the US to have robust data protection rules in place for all companies.,https://www.wired.com/story/trump-tiktok-drama-security-distraction/,2020-08-05 19:05:05.639000+00:00,LABEL_1_relevant,0.9958329796791076,Social Media,social_media,Wired,Security & Privacy
34,Twitter Blue Is for People Who Love Reading the News,"Should you get Twitter Blue? That depends on whether you consider yourself a “power user.” The platform’s new subscription service, which costs $3 per month, comes with a suite of requested features: bookmark folders for organizing saved tweets, a “reading mode” that declutters long threads, and a (sort of) edit button, good for 30 seconds of revisions after a tweet is sent. It also comes with ad-free access to articles from a number of journalism outfits, like The Washington Post, Rolling Stone, and The Atlantic.

Compared to the number of third-party apps that give Twitter more brawn—plug-ins that help you auto-delete your tweets or search within the tweets you’ve liked—Twitter Blue feels a little overwhelming. But considering it’s the company’s first-ever paid product, Twitter has chosen to go narrow, catering to a small group of people by giving them exactly the features they want.

Photograph: Twitter

Twitter Blue doesn’t get rid of ads in a user’s feed or change much about the tweeting experience itself. But it is part of the company’s plan to diversify its revenue beyond advertising and become more than a place to share thoughts in 280 characters. In January, Twitter acquired the newsletter company Revue and Breaker, an audio social service, to build up new product offerings around newsletter subscription and audio chat. In March, it acquired Scroll, an ad-free news reader.

People who liked Scroll will find a lot to love about Twitter Blue. It’s almost exactly the same service, but less costly: Scroll charged $5 per month. Part of the Twitter Blue subscription fees will go directly back to the publishers it partners with. Twitter did not disclose how much but wrote in a blog post that “our goal is to help each publishing partner make 50 percent more per person than they would’ve made from serving ads to that person.”

Photograph: Twitter

Twitter seems to be defining its “power users” as people who consume a lot of news or primarily use the platform for sharing and reading articles. For those people, Twitter Blue certainly is a nicer reading experience. It’s convenient to browse the day’s top articles and then click into them without ads, which also makes them load faster. And Twitter threads look fantastic in “reading mode,” a design that emphasizes the text rather than the replies. On the other hand, “reading mode” makes it harder to see the full scope of a conversation, which used to be the whole point of Twitter.",Yes,The way it isolates users and creates echo chambers.,https://www.wired.com/story/twitter-blue-is-not-for-everyone/,2021-11-13 12:00:00+00:00,LABEL_1_relevant,0.9938982725143432,Social Media,social_media,Wired,"Information, Discourse & Governance"
35,Young politicians hustle to keep followers under House social media ...,"Today, Rep. Alexandria Ocasio-Cortez’s (D-NY) Instagram followers got an unexpected message from the newly elected congresswoman — she created a new Instagram account for official government business.

Ocasio-Cortez’s announcement came on her personal Instagram account, @ocasio2018, on Thursday by way of an Insta-story. “House Rules say I can’t post IG stories from my personal account while I’m in my office,” she wrote on a screenshot from her new official account. “So I’ll post on-the-job IG stories from @repocasiocortez.”

This isn’t the first official account she’s created since she took her oath of office. Ocasio-Cortez also has a new Twitter account (@RepAOC) that was created last week for official government business.

She’s not the only lawmaker juggling multiple accounts. Members of Congress were sworn into office on January 3rd and many freshman have already set up new social media accounts, starting fresh and rebuilding the massive followings they created on the campaign trail. These official accounts must clearly state in their bios that the person it represents is a member of Congress, according to the longstanding guidelines in the Members’ Congressional Handbook. Many of the social media darlings from the past election, like Ocasio-Cortez and Rep. Dan Crenshaw (R-TX), have tweeted out messages citing these ethics rules, asking for fans to follow them to new accounts as they embark on their first year under the Capitol dome.

Well I guess I am back to having no followers. Thanks Ethics Rules. #myfirstTweet #retweetthis #followme — Rep. Dan Crenshaw (@RepDanCrenshaw) January 4, 2019

The Members’ Congressional Handbook doesn’t explicitly say that lawmakers are required to make new accounts, but in most cases it’s easier to separate their government resources and personal ones in order to avoid ethics violations. The rules do prohibit lawmakers from using any of their newfound government resources to maintain their personal accounts, whether that be their new staff or office funds.

You can typically tell the distinction between an official and personal congressional account by the handle. A member’s title like “Rep.” or “Sen.” usually sits before their name on the government-maintained account. For example, Sen. Ben Sasse’s (R-NE) personal account is just his name, @bensasse, and his official account is @SenSasse. Congressional staff members are only allowed to post onto the official account, which is strictly for government business. Anything having to do with re-election or grassroots lobbying must stay on the personal accounts.

So far, Reps. Ocasio-Cortez and Crenshaw haven’t suffered too much under the rules. Crenshaw’s official congressional Twitter account has already acquired about half of his formerly 243,000 followers. Ocasio-Cortez still posts from her personal account to over 2 million followers, @AOC, and seems to let her staff handle the content on her official one (@RepAOC) with just over 40,000 followers. Her new Instagram was launched today and already has over 66,000 followers, a fraction of the 1.7 million on her personal account.

Facebook and Twitter have been standard platforms for politicians looking to build followings, but throughout the past few months, lawmakers have made headlines for their use of Instagram. Most lawmakers have an official Instagram account, but the bulk of the viral content and memes are generated from their personal accounts. Sen. Elizabeth Warren (D-MA) recently cracked open a beer with her husband on her personal account, but sticks to photos with constituents and policy talk on her official one. Beto O’Rourke caused a stir by interviewing his dental hygienist on his personal account, but if he were still in the House, he’d be expected to keep it off of his government page.

But when it comes to the presence congressional committees have on social media, there’s little guidance for members and their staff. The House Administration says that any committee website or social media account “must be recognizably derivative or representative of the name of the Committee sponsoring the Website,” and that the names of these pages cannot be deceptive, slogans, or imply that the body endorses any commercial products.

These rules were confusing for some people on Twitter last week after bodies like the House Science Committee switched up their handles. What was previously @HouseScience, was changed by Republican staff to @HouseScienceGOP.

Followers of the account, who expected it to represent the entire committee, were tweeting out messages suggesting that the Republicans had stolen the account or refused to hand it over to the Democrats, who are now in the majority. “The outgoing committee switched the name of the account instead of handing it over to the Dems, taking all the followers with them,” one user wrote. But that wasn’t the case. The Republicans had always owned the account in question, but only changed the name to properly identify their new status as the minority, according to a Democratic spokesperson for the committee.

If the House Administration rules don’t provide guidance, committees make their own rules, and so long as both parties are in agreement, Twitter helps make any changes necessary as new congressional sessions begin.

Updated 1/10/19 at 4:50 p.m. EST: Corrected official government Twitter handle for Rep. Alexandria Ocasio-Cortez.

Updated 1/11/19 at 4:27 p.m. EST: Updated to change which party the spokesperson represented.",Yes,"It pulls people away from real-world social interaction.

This can have a number of negative effects, including:

-Isolation and loneliness
-Depression
-Anxiety
-Poor sleep
-Weight gain
-Negative impact on relationships",https://www.theverge.com/2019/1/10/18176814/ocasio-cortez-instagram-congress-social-media-rules,2019-01-10 00:00:00,LABEL_1_relevant,0.9955256581306458,Social Media,social_media,The Verge,Social Norms & Relationships
36,Facebook Tamed the Feral Internet—and That’s Too Bad,"I have zero claim to l337ism but I’ve been around this strange old internet for the past 40 years. Grandma, please no stories about the early internet. I get it. But in honor of Facebook’s 15th birthday, I want to reminisce, just for a minute.

The internet of the ’80s seemed like an infinitely weird Cabinet of Wonders, filled with goth and carny stuff: skulls, fetuses in formaldehyde, half-working contraptions that made ascii art. The docents in the joint were backroom clerics, the odd refugee from CB culture (literally “breaker, breaker”), and wallflowers hoping for a new shot at social life. I was that last type. It was deeply embarrassing to be known as a computer geek; you hid it like eczema. Still, at nine, I started encountering the clerics—some of them actual “clerics,” as D&D was everywhere—and getting in the kind of preteen and then teen trouble that involved mostly trickery, masquerade, and multiplayer roleplaying. There were also legitimate wonders. Seeing a whole universe form before one’s very eyes, for one, to say nothing about the birth of a new life form: Avatars.

Virginia Heffernan (@page88) is an Ideas contributor at WIRED. She is the author of Magic and Loss: The Internet as Art, a cohost of Trumpcast, an op-ed columnist at the Los Angeles Times, and a frequent contributor to Politico.

Now I’m really going to go full kids these days. Because: Facebook steamrolled everything. AOL and CompuServe had sat like circus makeup on top of a bunch of tangled and bottomless code. The ISPs tried with all the powers of middlebrow marketing to make something user-friendly that wasn’t friendly at all—a half-orc painted up to look like a Disney character. Still, AOL and CompuServe deserve credit: The first wave of the digital middle class poured in through the Mall of America–style UX, and AOL’s and CompuServe’s illusion of order gentrified things. By my twenties, I was ready for some gentrification. I couldn’t spend my life torrenting lawlessly in some open source drug den; CompuServe now suited me. I was 773.143@compuserve.com—another anonymous handle. But 2004 came. And Facebook wrecked the specialness of those wilder days.

First came the rumbling from Harvard, of all places. Shortly after, as it spread to all colleges and then outside dot-edu, some Ohio University kids set up an account for me. With trepidation I entered in my real name, first and last, the one on my Social Security card and paychecks and permanent record. That was the red pill. I swallowed hard.

It seemed that no sooner than I’d hit the last “n,” an imperial fleet of earth-moving armored bulldozers come to churn up the old digital townships—the websites, eccentric blogs, and zines like WIRED’s own Suck.com we all loved. (You can find remnants of the old haunts on Craigslist, Match.com, and Wikipedia—communitarian places that, all things considered, still operate in good faith.) Razed, our settlements were paved over. Thousands, then millions, then billions of identical structures erected. Blue blue blue blue. Bank blue. Sterile blue that sowed the fields with salt. The Jane Jacobs charm and power of the tunnels, community gardens, chapels, and dens of iniquity around the edges were ... dead. No more would esoteric stunts like “I Can Haz Cheezburger” or Coke & Mentos sweep the nation.

Of course, though it seemed swift, it took Facebook eight or 10 years to really choke the life off the old internet. Suck.com folded in earnest in 2015 (having not had new content since 2011). After all, Facebook addressed itself to something that wasn’t a problem: the occasional incoherence of the Web 1.0 villages. In Japan, according to sociologist Con Isshow, the internet was always seen as rich, thick soil for the young—its generative chaos vitally necessary for the development of jungly imaginations. Teenagers and twentysomethings could flee the stultifying stratification of Japanese society in favor of roleplaying where they could experiment with identities: heroic, gender-bending, scholarly, angry, romantic. Above all they could improvise in a symbolic, cerebral order that would save them the life-destroying consequences of traditional experiments: drugs, cutting, self-starvation, violence, cults, unwanted and unsafe sex. Then Facebook came and demanded we give our names, locations, family photos, political opinions, consumer preferences. This wasn’t exploration. This was crowd control. The better to fumigate us, stratify us, monitor us, and take us for all the data we had.",Yes,"Everyone, but especially the young, are much less free to be creative and imaginative online.",https://www.wired.com/story/facebook-tamed-the-feral-internetand-thats-too-bad/,2019-02-22 13:00:00+00:00,LABEL_1_relevant,0.9937731623649596,Social Media,social_media,Wired,"Information, Discourse & Governance"
37,The Facebook Fallacy,"At the same time, network technology allows advertisers to more precisely locate and assemble audiences outside of branded channels. Instead of having to go to CNN for your audience, a generic CNN-like audience can be assembled outside CNN’s walls and without the CNN-brand markup. This has resulted in the now famous and cruelly accurate formulation that $10 of offline advertising becomes $1 online.

I don’t know anyone in the ad-­supported Web business who isn’t engaged in a relentless, demoralizing, no-exit operation to realign costs with falling per-user revenues, or who isn’t manically inflating traffic to compensate for ever-lower per-user value.

Facebook has convinced large numbers of otherwise intelligent people that the magic of the medium will reinvent advertising in a heretofore unimaginably profitable way, or that the company will create something new that isn’t advertising, which will produce even more wonderful profits. But because its stock has been trading at about 40 times its expected earnings for the next year, these innovations will have to be something like alchemy to make the company worth its sticker price. For comparison, Google has been trading at a forward P/E ratio of around 11. (To gauge how much faith investors have that Google, Facebook, and other Web companies will extract value from their users, see this graphic.)

Facebook currently derives 82 percent of its revenue from advertising. Most of that is the desultory, ticky-tacky display advertising that litters the right side of people’s Facebook profiles. Some is a kind of social marketing: a user chooses to “like” a product, which is supposed to further social relationships with companies. The social network sells its ads by valuing various combinations of the cost of a thousand ad impressions (or CPM) and the cost of a click (CPC). Both forms of ads are more or less coarsely targeted to users on the basis of information they’ve volunteered to provide to Facebook and the sharing or “liking” of media within Facebook’s universe. General Motors recently announced it would no longer buy any kind of Facebook ad.

Facebook’s answer to its critics is: Pay no attention to the carping. Sure, grunt-like advertising produces the overwhelming portion of our $4 billion in revenues, and yes, on a per-user basis, these revenues are in decline. But this stuff is really not what we have in mind. Just wait.

It’s quite a juxtaposition of realities. On the one hand, Facebook is under the same relentless downward pressure as other Web-based media. The company’s revenue amounts to a pitiful $5 per customer per year, which puts it ahead of the Huffington Post but somewhat behind the New York Times’ digital business. (Here’s the heartbreaking truth about the difference between new media and old: even in the New York Times’ declining traditional business, a subscriber is still worth more than $1,000 a year.) Facebook’s business grows only on the unsustainable basis that it can add new customers at a faster rate than the price of advertising declines. It is peddling as fast as it can. And the present scenario gets much worse as people increasingly interact with the social service on mobile devices, because on a small screen it is vastly harder to sell ads and monetize users.

On the other hand, Facebook is, everyone has come to agree, profoundly different from the Web. First of all, it exerts a new level of hegemonic control over users’ experiences. And it has its vast scale: 900 million, soon a billion, eventually two billion people. (One of the problems with the logic of constant growth at this scale and speed is that eventually Facebook will run out of humans with computers or smart phones.) And then it is social. Facebook has, in some yet-to-be-defined way, redefined something. Relationships? Media? Communications? Communities? Something big, anyway.

The sweeping, basic, transformative, and simple way to connect buyer to seller and get out of the way eludes Facebook. It has to sell its audience like every humper on Madison Avenue.

The subtext—an overt subtext—of the popular account of Facebook is that the network has a proprietary claim to and special insight into social behavior. For enterprises and advertising agencies, it is therefore the bridge to new modes of human connection. Expressed so baldly, this account is hardly different from what was claimed for the companies most aggressively boosted during the dot-com boom. But there is, in fact, one company that created and harnessed a transformation in behavior and business: Google. Facebook could be, or in many people’s eyes should be, something similar. Lost in such analysis is the failure to describe the application that will drive revenues.

Google is an incredibly efficient system for placing ads. In a disintermediated advertising market, the company has turned itself into the last and ultimate middleman. On its own site, it controls the space where a buyer searches for a thing and where a seller hawks that thing (AdWords, its keywords advertising network). Google is also the cheapest, most efficient way to place ads anywhere else on the Web (through the AdSense network). It’s not a media company in any traditional sense; it’s a facilitator. It can eliminate the whole laborious, numbing process of selling advertising space: if a marketer wants to place an ad (that is, if it is already convinced it must advertise), the company calls Mr. Google.

And that’s Facebook’s hope, too: it wants to be a facilitator, the inevitable conduit at the center of the world’s commerce.

Facebook has the scale, the platform, and the brand to be the new Google. It lacks only the big idea. Right now, it doesn’t actually know how to embed its usefulness into world commerce (or even, really, what its usefulness is).

But Google didn’t have the big idea at its founding, either. The search engine borrowed the concept of AdWords from Yahoo’s Overture network (a lawsuit for patent infringement and a settlement followed). Now Google has all the money in the world to buy or license the ideas that could make its platform and brand pay off.

What might Facebook’s big idea look like? Well, it does have all this data. The company knows so much about so many people that its executives are sure the knowledge must have value (see our feature “What Facebook Knows”).

If you’re inside the Facebook galaxy—a constellation that includes an ever-­expanding cloud of associated ventures—there is endless chatter about a near-utopian new medium for marketing. Round and round goes the conversation: “If we just … if only … when we will …” If, for instance, frequent-flier programs and travel destinations actually knew when you were thinking about planning a trip … If a marketer could identify the person who has the most influence on you … If an advertiser could introduce you to someone who would relay the advertising message … Get it? No ads, just friends! My God!

But so far the sweeping, basic, transformative, and simple way to connect buyer to seller and get out of the way eludes Facebook.

So the social network is left in the same position as all other media companies. Instead of being inevitable and unavoidable, it has to sell its audience like every humper on Madison Avenue.

But that’s what Facebook is doing: selling individual ads. If you consider only its revenue, it’s an ad-sales business, not a technology company. To meet expectations—the expectations that took it public at $100 billion—it has to sell at near hyperspeed.

The growth of its user base and its ever-swelling page views mean an almost infinite inventory to sell. But the expanding supply, together with equivocal demand, results in ever-lowering prices. The math is sickeningly inevitable. Absent that earthshaking idea, Facebook will look forward to slowing or declining growth in a tapped-out market, and ever-falling ad rates, both on the Web and (especially) in mobile applications. Facebook isn’t Google; it’s Yahoo or AOL.

Oh, yes … in its Herculean efforts to maintain its overall growth, Facebook will force the rest of the ad-driven Web to lower its prices, too. The low-level panic the owners of every mass-traffic website feel about the ever-downward movement of their CPM is turning to dread. Last quarter, some big sites observed as much as a 25 percent decrease, following Facebook’s own attempt to book more revenue.

You see where this is going. As Facebook gluts an already glutted market, the fallacy of the Web as a profitable ad medium will become hard to ignore. The crash will come. And Facebook—that putative transformer of worlds, which is, in reality, only an ad-driven site—will fall with everybody else.

Michael Wolff writes a column on media for the Guardian; is a contributing editor to Vanity Fair; founded Newser; and was, until October of last year, the editor of AdWeek.

This article was revised on June 16, 2012.",Yes,It has become a vehicle for advertisers to more precisely locate and assemble audiences outside of branded channels.,https://www.technologyreview.com/2012/05/22/255726/the-facebook-fallacy/,2012-05-22 00:00:00,LABEL_1_relevant,0.9096501469612122,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
38,UK Group Threatens to Sue Facebook Over Cambridge Analytica,"Lawyers for a group of UK residents whose Facebook data was harvested by Cambridge Analytica are now threatening to sue for damages. In a 27-page letter served to the company Tuesday, they accuse Facebook of violating British data privacy regulations. The letter before claim, as it's called, is the first step in the UK's legal process for filing a class action suit. It warns Facebook that if it does not adequately respond to a list of questions regarding user privacy within 14 days, the claimants may take legal action against the company in the United States, the United Kingdom, and Ireland. Nearly 1.1 million British citizens could be eligible to join such a suit if it goes forward.

The warning comes from the UK-based law firm Irvine Thanvi Natas Solicitors, which is representing dozens of people who argue that Facebook misused their personal data in violation of UK law. It follows an announcement Monday by separate group, called the Fair Vote Project, that is also launching a class action suit against Facebook in the UK.

The UK Information Commissioner’s Office already said earlier this month that it intends to fine Facebook more than $600,000 under the country's Data Protection Act for allowing Cambridge Analytica, the now-defunct political consulting firm, to collect information on tens of millions of users without their knowledge. But Ravi Naik, a lawyer with Irvine Thanvi Natas Solicitors, says the individuals affected also have a right to answers, and may have a right to damages if those answers aren’t satisfactory.

""People should realize that data rights are real rights, and we have a mechanism to enforce them,"" Naik says.

WIRED reviewed a copy of the letter Naik submitted to Facebook Tuesday. It reads like a checklist of Facebook's failures, dating back to its decision in 2009 to no longer allow users to keep their friend lists private. The letter also notes Facebook's decision to let app developers scoop up data not just on their apps' own users but on those users' friends. Facebook didn't put a stop to this practice until 2015. By then, Cambridge Analytica had already gained access to as many as 87 million Facebook users' data through a personality quiz app designed by a University of Cambridge researcher named Aleksandr Kogan.

These original sins, the letter argues, opened Facebook up to legal liability from several angles. For starters, the UK's Data Protection Act requires companies to get user consent to process their data. Naik says that Facebook essentially misled users about what they were consenting to. In Facebook's privacy settings at the time, users could determine whether to share their posts with ""only friends"" or ""friends of friends,"" but this only applied to what other Facebook users could see. App developers had their own permissions, which, in some cases, gave them the ability to tap into their users' friends' data.

This broad allowance for app developers also triggered regulators in the United States to investigate Facebook for deceptive practices in 2011. At the time, the US Federal Trade Commission wrote that while Facebook had given users the impression they could share information only with their friends, those privacy restrictions ""would be ineffective as to certain third parties."" The FTC argued Facebook failed to make that clear to users. It also considered Facebook's 2009 decision to stop letting users make their friend lists private to be an ""unfair and deceptive"" practice. Facebook settled the matter by signing a consent decree with the FTC in 2011, and vowed not to mislead users about their privacy. After reports about Cambridge Analytica surfaced this spring, the FTC announced it was investigating whether Facebook broke that promise, a violation that could come with hefty fines.",Yes,"Private data is being harvested and used without the user’s knowledge or consent. This has led to a loss of privacy for users and has raised concerns about how this data is being used.

The UK’s Information Commissioner’s Office is investigating Facebook for allowing Cambridge Analytica to harvest data from users without their consent",https://www.wired.com/story/uk-group-threatens-to-sue-facebook-over-cambridge-analytica/,2018-07-31 09:30:00+00:00,LABEL_1_relevant,0.9967650175094604,Social Media,social_media,Wired,Security & Privacy
39,“He put QR-coded wristbands on each of the chickens”,"So how do you keep people in the countryside but at the same time give them economic opportunities, especially because farming is not an easy job, and increasingly the younger generation doesn’t want to be stuck doing manual labor in the field? The national government is entranced with some of the same shiny keywords as policymakers everywhere—“e-commerce” “blockchain,” “AI”—so it’s supported a lot of initiatives by small privatized companies that employ those technologies.

Q: One of these inspired your book’s title. Can you explain what makes a “blockchain chicken farm”?

A: It’s a small farm in rural Guizhou where the farmer had been raising free-range chickens for quite a long time, but he couldn’t convince people that they were actually free-range. Then a Shanghai tech company came along and said, “Blockchain is the solution!” They worked with the farmer and he put QR-coded wristbands on each of the chickens, so that they could be surveilled by cameras to prove that they were truly free-range and never tampered with.

Q: You also cover AI machine-­learning models like Alibaba’s “ET Agricultural Brain,” which became a tool for combating African swine fever (ASF) in pigs during a disastrous outbreak that began in 2018.

A: I was frustrated because much media coverage of the ASF outbreak focused on how Alibaba was doing this heroic thing to save all these pigs and guarantee food safety using AI to monitor the herds via video, temperature, and sound sensors. In fact, for decades now, there’s been a push to industrialize hog farming in China, and these technologies were an attempt to produce even more pigs at an unprecedented scale. These industrial farms and increased pressure for output set up the conditions for epidemics like swine fever in the first place. What are the paradoxes it reveals?

Q: What’s an example of a farming technology that is taking a different path?

A: Like many countries, China is a place where the government tried to modernize agriculture by using pesticides and fertilizers as the “scientific” way of farming the land. In one rice-farming village in Guangdong province, farmers did that and over time noticed that their soil was just not as fertile anymore, that they had to keep using more and more fertilizer. This led to the Rice Harmony Collective, which revived traditional techniques like “rice duck fish” agriculture, where fish and ducks in the rice paddy act as a natural pesticide. They also introduced a lottery system for rice paddy location that shifts each season, so that farmers have a greater incentive to follow these organic rules.

Q: The coronavirus pandemic emerged when your book was in production. Do you feel it underscores your themes?

A: I had been looking at a lot of the research of Rob Wallace, an epidemiologist who studies factory farming and zoonotic diseases—which is not to say that all these are coming out of factory farms, but just these profit-driven practices that push humans into previously wild habitats. There’s obviously been a huge acceleration of zoonotic disease.

In the pandemic we’ve all realized that decoupling with China would be hard—we rely on China for so many things. Just the process of setting up a factory; the material supplies, training, machinery; the knowledge of costs and shipping and freight and routes. It’s never been more clear that China is so interwoven with the global supply chain.

Xiaowei Wang at Urban Tilth Farm CHRISTIE HEMM KLOK

Q: What do you wish Americans understood about Chinese wet markets?

A: So, I love wet markets. They are a place where fresh food is readily available to all, and it’s an important livelihood for many people who aren’t these large supermarket chains. You’ve got the garlic lady who sells her homegrown crop at wet markets. They’re a crucial connection for local and regional farmers. They’re so common not just in China, but around the world—in Latin America and so on. I think it’s sad [that people blame covid-19 on wet markets]—and a xenophobic example of people thinking that Chinese food is somehow “dirty.” It infuriates me so much, because all the science says it [the coronavirus] probably came from outside [the market], and it probably came from a bat.

Q: What are the trends you are seeing in the countryside? Has the pandemic contributed to new ones?

A: I would say the general trend is that there’s lots of optimism. Because of the pandemic, a lot of migrant workers in cities had to go back to their rural homes, and maybe they’ll stay there and pursue other kinds of opportunities. Even before the pandemic, I observed a lot of young people thinking and talking about “Oh, maybe I can move back to my hometown” and start some kind of business that would be cheaper than in the city.

""A farmer had been raising free-range chickens but couldn’t convince people that they were actually free-range. Then a Shanghai tech company came along and said, 'Blockchain is the solution!' and put them under surveillance to prove they were truly free-range.""

I think, too, the live-­streaming economy is a weird microcosm of this—people have a notion of farming in the countryside, live-streaming it, and getting patrons. I think for urban youth, that trend is increasing. Back in 2009, when I was living in Beijing and trying to do urban gardening, no one was interested. Everybody was like, “Ugh, this is what my parents had to do. I’m not doing this, it’s gross.” But today, there’s a huge demand for organic farmers’ markets, and influencers getting into farming. I just heard about a popular beekeeping influencer, who has a whole brand and blog talking about the ancient art of beekeeping. It makes sense: the urban 20-somethings who have only known the city their entire lives, they’re under so much pressure, and so of course they’re going to romanticize the countryside.

Q: The term “Sinofuturism” refers to concepts and aesthetics of a “Chinese future.” It’s been explored by artists, designers, and thinkers in critical or celebratory ways. How do you personally interpret it?

A: For me, Sinofuturism now contains a certain imperial logic, given how China has operated more and more as an imperial power over the past few years, domestically and internationally. That said, I think there are a lot of interesting and productive parts of Sinofuturism that make us question these innate Western beliefs about the value of individualism, the role of work, the disconnection from natural cycles, the separation between mind and body, that are worth investigating. Sinofuturism is also a way to consider what exactly is this imperial force that China’s becoming, and to provoke questions around that.

Q: Even if they don’t cook them, what do you want readers to get from the Sinofuturist recipes in the book?

A: I’d love for people to say, “Hmm, I don’t have access to moon-grown cornmeal,” but to have a sense of wonder about the ingredients that are available to them, and to frame that reality as a weird form of fiction. To question “Why do we eat what we eat?” and understand how that relates to technological change. I was really inspired by a cookbook by Mary Sia, who talks about how in China you don’t get a lot of baked goods; you get a lot of boiled things, and that’s due to the fact that China simply didn’t have enough trees to cut down to generate as much heat as is needed in baking. For me that was a reminder of how what we cook is totally shaped by what is available, as a result of the technology that we use.

Q: What were some of the inspirations behind the recipes you include in the book?

A: I was seeing my Chinese herbalist, who loves to rant about Western medicine and how it doesn’t fully understand the body, and she was telling me about how the brain is not one of the 11 vital organs in Chinese medicine. It’s not essential to the system of qi. I thought it was fascinating because when I was interviewing computational neuroscientists, you know, the brain is the center of everything in Western clinical medicine. It controls your heartbeat, your lungs; it’s the center of thinking: you wouldn’t be a person without it.

My herbalist gave me some ideas on what nourishes qi, so I decided to use her sage advice in a recipe for AI porridge.",Yes.,"People becoming more and more isolated and disconnected from each other, which leads to more mental health problems.",https://www.technologyreview.com/2020/12/18/1013224/he-put-qr-coded-wristbands-on-each-of-the-chickens/,2020-12-18 00:00:00,LABEL_1_relevant,0.9910966157913208,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
40,Preventing Misinformation from Spreading through Social Media,"Reputation scoring has worked well for e-commerce sites like eBay and Amazon and could help to clean up social media reports in some situations.

Research efforts have also shown how to effectively mobilize many people on social media for a common task. In a 2009 experiment, the U.S. Defense Advanced Research Projects Agency offered $40,000 to the first team that could identify the locations of 10 large red weather balloons lofted by DARPA at undisclosed locations across the United States. The winning team, from MIT, did it in less than nine hours using an incentive structure, fueled by cash rewards, to drum up viral participation on social media. Anyone who found a single balloon would get $2,000; someone who invited that person to join the hunt would get $1,000. A similar but harder challenge, in 2012, asked teams to find specific individuals within cities within 12 hours with only a single mugshot to work with. There again, a distributed cash reward system worked best.

Verily builds on lessons from both contests. The winning mugshot team included one of Verily’s creators, computer scientist Iyad Rahwan, a graduate of MIT who is now at the Masdar Institute of Technology. “Recruiting people to join is part of the issue, but we also need to figure out how to remove false reports,” Rahwan says. “Where the balloon challenge took nine hours, we hope to facilitate the crowdsourced evaluation of multimedia evidence on individual incidents in less than nine minutes.”

The beta version of Verily will first be tested by its creators on a real-world weather disaster such as a hurricane or flood. Since such disasters come with some warning, Verily’s creators can prepare humanitarian agencies to use the platform. A piece of reported news—such as a photo of a flooded hospital circulating on Twitter—would be posted to Verily with a question: is the hospital really flooded? Users would then examine the photo for signs of authenticity and also leverage their own social networks to investigate its authenticity.

Humanitarian agencies working in the region could promote participation, as could the press and Twitter. Voters’ reputation scores would increase or decrease over time; future votes from reliable people would get increased weight. And voters would be encouraged to bring others to the site; anyone brought in by someone with a good reputation would automatically start with a higher reputation themselves.

In many ways the platform is meant to resolve a design problem inherent in sites like Reddit, adds Patrick Meier, director of innovation at the Qatar institute who is a co-creator of Verily and former director of crisis mapping at Ushahidi, the online incident reporting platform (see “Crisis Mapping Meets Check In”). “They don’t have the design to facilitate these kinds of workflows and collaboration,” he says. Verify could provide a rapid means to vet reports arising on sites like Reddit.

The other approaches are more basic. Storyful verifies videos to make sure news organizations don’t get duped by phony ones. Staffers check veracity based on clues like weather reports, the angle of the sun, and visual landmarks. And beyond the Swift River app is a larger platform aimed at letting humanitarian and other agencies manage and make sense of social media reports and other data.

Meanwhile, old-fashioned methods of finding the truth are holding up pretty well. In Boston, the marathon bombers were actually found through conventional witness reports and reviews of video surveillance camera footage at retail stores.",Yes,"It can lead to the spread of false information. To combat this, some organizations are developing platforms that allow users to verify the accuracy of reports. These platforms use reputation scoring and other methods to encourage users to participate in the verification process.",https://www.technologyreview.com/2013/04/23/16168/preventing-misinformation-from-spreading-through-social-media/,2013-04-23 00:00:00,LABEL_1_relevant,0.9954364895820618,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
41,Ninja Is Being Immortalized on a Red Bull Can,"Oh, hello there. Nice to see you. Yes, it’s once again time for Replay, WIRED’s look at the world of videogames. This week, gaming achieved a new, weird kind of crossover success, Notch became a bit less visible as Minecraft's creator, and the scourge of industry layoffs continued apace. Here we go!

Ninja Is Being Immortalized in the Classic American Art Form of the Red Bull Can

Leonardo da Vinci. Picasso. Red Bull. All the great chroniclers of the human form have had their muses, and now, Twitch streamer Ninja is among them. Well, the last one, anyway. In a sponsorship deal for the ages, Ninja, Fortnite player and friend of Drake, will be the next face to adorn cans of Red Bull energy drinks.

This is kind of hilarious, but that’s not the only reason it's newsworthy. This is the type of sponsorship deal normally reserved for athletes, movie stars, and big media franchises. And now, we've got Ninja, a dude who makes his living playing videogames for an audience. If that isn't crossover success, nothing is.

Minecraft Minimizes Mentions of Creator Notch

With Notch, the creator of Minecraft, spending his time on the internet promoting conspiracy theories and generally being kind of a jerk, it was inevitable that Microsoft, the current owners of Minecraft, would eventually want to distance their product from its creator. With the current update, they've done that—to an extent. Notch's name used to appear regularly on splash screens that played while loading, but according to reports by Kotaku, all those mentions have been removed.

Notch still appears in the credits, naturally, but that's about the only mention of him left in the game. Which is probably for the best. You have to think of the children, after all.

Electronic Arts Lays Off Some 4 Percent of Its Employees

It never ends. More downsizing hit the games industry this week as Electronic Arts announced that it's laying off about 350 of its 9,000 workers worldwide. According to an open letter from CEO Andrew Wilson, the layoffs are part of an effort to produce high-quality content and to ratchet down the company's involvement in Japan and Russia.

Adding this to the layoffs from Activision Blizzard, ArenaNet, and others, the amount of displaced game workers from this year alone adds up to well over a thousand. I don't really have anything pithy to add to this. That's just … a lot of people. Too many.

Recommendation of the Week: Wargroove on Nintendo Switch, Xbox One, and PC

Wargroove is by far the cutest tactics game. Taking a grid-based approach to strategic warfare reminiscent of Fire Emblem, it's all bright colors and quirky units, including a warrior general dog and cute cavalry units that are, uh, also dogs. There are a lot of dogs in this game. And plant monsters, and vampires, and heroes, and villains of all stripes. The tactical combat is crisp and challenging, and the whole experience is just broadly pleasing. If you're into this sort of game and can't wait for a new Fire Emblem to drop later this year, give this a shake.

More Great WIRED Stories",Please provide your answer in the comments below.,"People are more stupid than they used to be. Social media has also led to the rise of a strange kind of anti-intellectualism. And that’s because social media is a reflection of our own behavior. Social media isn’t just a conduit for our own ideas and feelings, it’s also a mirror that",https://www.wired.com/story/ninja-red-bull-can/,2019-03-29 13:00:00+00:00,LABEL_1_relevant,0.9964184761047364,Social Media,social_media,Wired,Social Norms & Relationships
42,Key Takeaways From the UK's Massive Facebook Document Dump,"In an unprecedented move Wednesday, British lawmakers published hundreds of pages of internal Facebook emails and other documents that previously had been ordered sealed as part of an ongoing legal case between Facebook and a now-defunct app developer called Six4Three.

The documents, which date back to 2012, provide a rare window into CEO Mark Zuckerberg's thoughts on how to expand his social media juggernaut as users made the transition from desktop to mobile phones. They also suggest a willingness within Facebook to sacrifice user privacy and undercut its competitors to continue driving growth.

""I believe there is considerable public interest in releasing these documents. They raise important questions about how Facebook treats users' data, their policies for working with app developers, and how they exercise their dominant position in the social media market,"" tweeted Damian Collins MP, who heads up Parliament's Digital, Culture, Media, and Sport Committee. The committee, which is conducting an investigation into Facebook privacy concerns, seized the documents from Six4Three's founder while he was traveling in London last month.

Facebook says the documents are “very misleading without additional context.” “Like any business, we had many internal conversations about the various ways we could build a sustainable business model for our platform,” a spokesperson said in a statement. “But the facts are clear: We've never sold people’s data.”

The documents were collected by Six4Three's legal team as part of the discovery process for a lawsuit that alleges Facebook defrauded app developers by luring them with the promise of data, only to later cut them off from that information. The unredacted exhibits posted by Collins on Wednesday include internal emails, presentations, and memos. In one email, Zuckerberg personally approves a decision to shut down API access to Vine, a video-based social network backed by Twitter, in January 2013. In another, Facebook executives discuss giving Android devices access to users' call logs without requiring their informed consent. Zuckerberg himself toys with the idea of trading app developer access to Facebook’s APIs for advertising revenue from those developers in 2012. That same year, he expresses openness to “locking down” developers’ access to their users’ friends data. Facebook wouldn’t actually announce that change for another two years, even as it built relationships with developers on the back of that data.

In 2012, Zuckerberg voices his skepticism in an email to Facebook’s then-director of product management, Sam Lessin, that sharing friend data with app developers might ever pose privacy risk. “I just can’t think if any instances where that data has leaked from developer to developer and caused a real issue for us,"" he writes. ""Do you have examples of this?”

Six years later, amid ongoing global investigations into how an app developer working with the political firm Cambridge Analytica was able to weaponize data for political purposes, that question looks remarkably naive. The Cambridge Analytica scandal, which made international headlines in March and elevated a global conversation about the need for stricter data privacy laws, also cast the lawsuit between Six4Three and Facebook in a new light. The case began in 2015, after Facebook changed its API to cut developers off from friend data. Six4Three’s app, Pikinis, used friend data to let users find people’s bathing suit photos. Without access to friend data, the Pikinis app shut down, and its founder Ted Kramer sued Facebook, asking the company to either reinstate access or pay damages.",Yes,"It can have a negative impact on the mental health of its users. It can cause feelings of envy, inadequacy, and isolation.",https://www.wired.com/story/uk-facebook-document-dump/,2018-12-05 18:28:37.944000+00:00,LABEL_1_relevant,0.9954873919487,Social Media,social_media,Wired,"Information, Discourse & Governance"
43,This Is How Porn Ended Up on Ninja’s Zombie Twitch Channel,"Anyone who visited the dormant Twitch channel of Fortnite legend Ninja this weekend may have gotten a lot more than they bargained for. The page, it turns out, was directing users to a livestream of hardcore porn on Sunday instead of videogame battles. Ninja, also known as Tyler Blevins, understandably wasn't happy to learn his channel had veered into NSFW territory—he still has 14.7 million followers on the platform. “Disgusted and so sorry,” he wrote on Twitter. Emmett Shear, the CEO of Twitch, apologized to Ninja for the mistake the same night and said that the channel streaming porn had been taken down.

“We have also suspended these recommendations while we investigate how this content came to be promoted,” Shear wrote in a series of tweets Sunday. “It wasn’t our intent, but it should not have happened. No excuses.”

The timing couldn’t have been worse. Earlier this month, Ninja left Twitch and took his Fortnite talents to Mixer, a rival streaming platform owned by Microsoft. Around the same time, Twitch began experimenting with a new recommendation feature, which turned Ninja’s zombie profile into advertising space for promoting other, active livestreams. Typically, when Twitch creators aren’t active, their channels simply feature past videos. In an apparent attempt to keep viewers on the site for longer, Twitch began experimenting with recommending other, active livestreams on dormant channels instead.

Recommendations can be powerful engines for mobilizing audiences. YouTube has attributed more than 70 percent of time spent on its platform to its recommendation algorithm (which has also been criticized for helping to radicalize its users). The feature Twitch was testing simply displayed the top streams in a certain category. Since Ninja primarily played Fortnite on Twitch, the company tried populating his zombie profile with other popular streams featuring the same game. A bad actor appears to have artificially inflated the number of viewers on their porn stream, which was added to the Fortnite category, sending it to the top of the charts—and thus to Ninja’s page. The incident was a high-profile mistake, but it also raises questions about Twitch’s moderation efforts more broadly.

Louise Matsakis covers Amazon, internet law, and online culture for WIRED.

Twitch is by far the largest platform in the world for watching gaming content, and now one of the most popular websites on the entire internet, visited more than Pornhub, according to Alexa rankings. Its stars have turned into bonafide celebrities who can earn seven figures each year in sponsorship deals. Twitch creators predominantly livestream themselves playing videogames and talking, often for up to 18 hours or more at a time. All the while, fans flood an accompanying chat box with jokes, observations, compliments, pleas for attention, and harassment. Roughly a million people, by some estimates, are watching a Twitch stream at any given moment.

To moderate this ecosystem, Twitch relies on a blend of tech tools, professional moderators, and volunteers, just like Reddit or Wikipedia. (It's also recently tried suing bad actors in court.) Streamers appoint volunteer moderators to maintain order on their livestream chats, while the company relies on both artificial intelligence and humans to detect streams that break its rules. Twitch can’t heavily lean on AI designed to analyze written text, like Facebook and Twitter do, because the site isn’t primarily text-based. And it can’t preemptively scan videos for things like nudity before they’re published, because they unfold in real time.

So it might not be surprising that Twitch has hosted some problematic streams over the years. Last year, for example, a livestreamed shooting reportedly stayed up on the site for hours. But moderating live content has been a challenge for every social network, and the problem isn’t unique to Twitch. In March, Facebook was chastised for letting a livestream posted by a mass shooter who killed 51 people in New Zealand go viral. And YouTube, which also has a livestreaming feature, struggles even with videos that aren’t broadcast live. The difference is that significantly more information has been made public about how Facebook and YouTube police offensive content than about how Twitch does, in part because it relies on a more decentralized model.",Yes,It can be used to spread false information and hate speech.,https://www.wired.com/story/porn-ninja-zombie-twitch-channel/,2019-08-13 22:53:40.794000+00:00,LABEL_1_relevant,0.9956939220428468,Social Media,social_media,Wired,"Information, Discourse & Governance"
44,How WeChat Censored the Coronavirus Pandemic,"When the novel coronavirus was first discovered in China last winter, the country responded aggressively, placing tens of millions of people into strict lockdown. As Covid-19 spread from Wuhan to the rest of the world, the Chinese government was just as forceful in controlling how the health crisis was portrayed and discussed among its own people.

Politically sensitive material, like references to the 1989 Tiananmen Square protests, have long been forbidden on China’s highly censored internet, but researchers at the University of Toronto’s Citizen Lab say these efforts reached a new level during the pandemic. “The blunt range of censored content goes beyond what we expected, including general health information such as the fact [that] the virus spreads from human contact,” says Masashi Crete-Nishihata, the associate director of Citizen Lab, a research group that focuses on technology and human rights.

Citizen Lab's latest report, published earlier this week, finds that between January and May this year, more than 2,000 keywords related to the pandemic were suppressed on the Chinese messaging platform WeChat, which has more than 1 billion users in the country. Many of the censored terms referenced events and organizations in the United States.

Unlike in the US, internet platforms in China are responsible for carrying out the government’s censorship orders and can be held liable for what their users post. Tencent, which owns WeChat, did not comment in time for publication. WeChat blocks content via a remote server, meaning it’s not possible for research groups like Citizen Lab to study censorship on the app by looking at its code. “We can send messages through the server and see if they are received or not, but we can't see inside of it, so the exact censorship rules are a bit of a mystery,” Crete-Nishihata says.

For its latest report, Citizen Lab sent text copied from Chinese-language news articles to a group chat it created on WeChat with three dummy accounts, one registered to a mainland Chinese phone number and two registered to Canadian phone numbers. They used articles from a range of outlets, including some based in Hong Kong and Taiwan as well as Chinese state-controlled publications. If a message was blocked, the researchers performed further tests to identify which words triggered the censorship. Some of the blocked messages had originally been published by Chinese state media. In other words, while a person or topic may be freely discussed in the government-controlled press, it’s still banned on WeChat.

Read all of our coronavirus coverage here.

The Citizen Lab report demonstrates the extent to which the Chinese government tried to control the narrative from the beginning. As residents in Wuhan remained in lockdown, WeChat blocked phrases about Li Wenliang, a local doctor who warned colleagues about a new infectious disease before it was disclosed by the government, and who became a popular hero for free speech after he died of Covid-19 in February. WeChat also blocked its users from discussing an announcement by Chinese officials that they had informed the US government about the pandemic for the first time on January 3, almost three weeks before they said anything to their own citizens. And it censored mentions of the US Centers for Disease Control and Prevention when the organization was coupled with the word “coronavirus.”

By March, Covid-19 had become a global pandemic, and WeChat began blocking some mentions of international groups like the World Health Organization and the Red Cross. It also censored references to outbreaks in other countries like Saudi Arabia, Turkey, Russia, and the United Kingdom. Citizen Lab found that the majority of blocked words related to international relations were about the United States, the subject of the third portion of the report.

Relations between the US and China were already strained at the start of the year, and the pandemic has become a major flashpoint between the two countries. In late February, some US officials began elevating a conspiracy theory that the novel coronavirus was a biological weapon manufactured by the Chinese government. The false claim was also circulated by right-wing figures like former Trump adviser Steve Bannon. WeChat promptly blocked mentions of “Bannon and Bio Lab,” and other related terms. In May, as relations between the US and China sank to their lowest point in decades, a group of Republican senators introduced a bill that would rename the street outside the Chinese embassy in Washington “Li Wenliang Plaza,” after the Wuhan doctor. WeChat quickly censored a number of key terms related to the legislation.",Yes,"It can be used to spread false information and rumours quickly and easily. This is especially problematic during a crisis, when accurate information is critical.",https://www.wired.com/story/wechat-chinese-internet-censorship-coronavirus/,2020-08-27 21:30:00+00:00,LABEL_1_relevant,0.996081292629242,Social Media,social_media,Wired,"Information, Discourse & Governance"
45,Grief in the Time of Facebook,"(The Times has a bunch more of these.)

Twitter filled up with #RIPMCA, as did the comments on every Beastie Boys video on YouTube. Those of a certain generation couldn’t open Facebook without seeing another Adam Yauch meme.

It’s moments like these when it feels as if grieving is exactly what social media is for. Like it’s better at this than pretty much anything else, including celebration (too remote, and liable to inspire jealousy), the everyday (too trivial) or anger (too much like bitterness or schadenfreude).

Whether or not you care who MCA was, the expressions on social media feel authentic, maybe because they’re unadorned, straightforward and nakedly imperfect. Everyone who writes about their grief knows in advance that words fail, so the usual look-at-me temptation to be too clever by half disappears.

Sherry Turkle, professor at M.I.T. and author of “Alone Together: Why We Expect More From Technology and Less From Each Other,” recently described in the Times how we use mobile devices and social media to filter and sanitize our interaction with the real world.

We are tempted to think that our little “sips” of online connection add up to a big gulp of real conversation. But they don’t. E-mail, Twitter, Facebook, all of these have their places — in politics, commerce, romance and friendship. But no matter how valuable, they do not substitute for conversation.

So do a thousand microblogged tributes to MCA mean anything to all the Gen Xers who thought they would be young forever, but who discover that, to a degree that makes the realization as mundane as dust, they too will die? What conversations aren’t happening because we’ve dispensed with our grief in the “safe” realm of the Internet?

But that, I think, is too cynical a view. Expressions of grief have always been public – that’s the catharsis part of the process. That’s what social media is good for.

In the case of grief, at least, Turkle’s thesis doesn’t quite apply. Because in any age, after all the public mourning is done, the unchangeable core of the human condition is that we are all very much alone, privately meditating on death and its implications for ourselves.

Seen in this light, the title of Turkle’s book, “Alone together,” becomes the ultimate testament to the therapeutic power of social media. We don’t need technology or a dissertation on its impacts to tell us that we are alone. But surely it’s all the better to be alone, together.",Yes,It makes us feel more alone together.,https://www.technologyreview.com/2012/05/05/85931/grief-in-the-time-of-facebook/,2012-05-05 00:00:00,LABEL_1_relevant,0.9949426054954528,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
46,India says WhatsApp's lawsuit over new regulations a clear act of defiance,"India said on Wednesday that WhatsApp’s lawsuit challenging the new local IT rules is an “unfortunate last moment” attempt to prevent new regulations from going into effect in “a clear act of defiance,” and said the Facebook-owned service hadn’t raised any specific objection about the traceability requirement in writing in more than two and a half years.

Ravi Shankar Prasad, India’s Electronics and IT Minister, said WhatsApp’s refusal to comply with the guidelines, the deadline of which expires Wednesday, is a “clear act of defiance of a measure whose intent can certainly not be doubted.”

WhatsApp sued the Indian government earlier on Wednesday in a Delhi court, saying the world’s second-largest internet market’s new IT rules could allow authorities to make people’s private messages “traceable,” and conduct mass surveillance.

The Ministry of Electronics and IT said the government needs to trace the first originator of a message for the “purposes of prevention, investigation, punishment etc. of inter alia an offence relating to sovereignty, integrity and security of India, public order incitement to an offence relating to rape, sexually explicit material or child sexual abuse material punishable with imprisonment for not less than five years.”

“It is in public interest that who started the mischief leading to such crime must be detected and punished. We cannot deny as to how in cases of mob lynching and riots etc. repeated WhatsApp messages are circulated and recirculated whose content are already in public domain. Hence the role of who originated is very important.”

India first proposed WhatsApp to deploy software changes to make the originator of a message traceable in 2018. The suggestion came at a time when WhatsApp was grappling with containing the spread of false information in India, where circulation of such hoaxes resulted in multiple real-life casualties. The suggestion became a part of the new rules in February this year.

WhatsApp has argued that the end-to-end encryption it offers to users — a move that security and policy experts have long praised — makes it impossible for the firm, used by more than 2 billion users worldwide, to find identifying information about a user.

India is the largest market by users for the Facebook-owned popular instant messaging service. According to a government estimate, WhatsApp has amassed over 530 million users in India.

The Ministry of Electronics and IT said any company’s operations in India is “subject to the law of the land,” and argued that similar or tougher regulations have been enforced or proposed in other markets.

“What India is asking for is significantly much less than what some of the other countries have demanded,” it said, adding that it would be “foolhardy” to doubt the objective of the new rules.

On one hand, “WhatsApp seeks to mandate a privacy policy wherein it will share the data of all its user with its parent company, Facebook, for marketing and advertising purposes,” the ministry added. “On the other hand, WhatsApp makes every effort to refuse the enactment of the Intermediary Guidelines which are necessary to uphold law and order and curb the menace of fake news.”

The ministry wrote to social media firms earlier on Wednesday to ask for an update on whether they had complied with the new rules, TechCrunch first reported. In the letter, the ministry sought information (name and contact address) of officials who the firms had appointed as part of the compliance to the new rules that require officers to be available on the ground to address local concerns.

“WhatsApp’s attempt to portray the Intermediary Guidelines of India as contrary to the right to privacy is misguided. […] The Government of India recognises that ‘Right to Privacy’ is a Fundamental right and is committed to ensure the same to its citizens,” the ministry said in a statement.",Yes,The spread of false information.,https://techcrunch.com/2021/05/26/india-says-whatsapps-lawsuit-over-new-regulations-a-clear-act-of-defiance/,2021-05-26 00:00:00,LABEL_1_relevant,0.9961251616477966,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
47,Does Facebook Need an ‘Understand’ Button?,"When I was a child, I am told on reliable account, I would walk around our housing complex and take my thumb out of my mouth to ask neighbors, “Do you like me?” I had even pudgier cheeks back then, and my question came out as, “Do you dike me?” I’m guessing most everyone gave me a smile and thumbs up.

At some point, however, I stopped asking—not sure if it was because I stopped caring about the answer or because I eventually learned that you shouldn’t force people into saying they like you before they even know you.

On Facebook’s latest earnings call last week, Mark Zuckerberg announced that he’s made a similar self-discovery. Recapping a fourth quarter with revenues of $21 billion and profits of $7 billion, Zuckerberg said he no longer cared what people thought about him. “My goal for this next decade,” he said, “isn’t to be liked, but to be understood.” He elaborated at a tech conference in Utah two days later: “This is the new approach, and I think it’s going to piss off a lot of people, but frankly the old approach was pissing off a lot of people too, so let’s try something different.”

His comments were dripping with irony. No institution has committed more fully to the concept of being “liked” than Facebook. Since Facebook introduced a button for producing them in 2009, likes have become the currency of the internet. Everything can be liked—a comment, a news article, a photo, a post. And Facebook’s various platforms highlight the total number of likes for each expression of ourselves online, which quickly devolves into a way of quantifying and comparing your relative popularity and adjusting your behavior to be most liked.

It took a project team almost two years to win Zuckerberg’s approval for the Like button, which had been conceived with noble intentions. (Stop me if you’ve heard that one before.) Facebook’s self-imposed mission is to connect the world, and a simple process of sending good feelings and support was expected to strengthen ties between people who only communicated online.

The blog post introducing the Like button to the public begins with the liker. “We’ve just introduced an easy way to tell friends that you like what they’re sharing on Facebook with one easy click,” the note says. “Wherever you can add a comment on your friends’ content, you’ll also have the option to click ‘Like’ to tell your friends exactly that: ‘I like this.’ ” By the end of the brief note, the focus had turned to the person being liked.

There was a fear within Facebook that likes would replace written comments entirely, so the note explains the purpose of each. “If you go to the restaurant and have a great time, you may want to rate it 5 stars,” the note says. “But if you had a particularly delicious dish there and want to rave about it, you can write a review detailing what you liked about the restaurant. We think of the new ‘Like’ feature to be the stars, and the comments to be the review.”",Yes,"It encourages people to post, share, and comment on things that will get them more likes, comments, and shares, which then encourages other people to post, share, and comment on things to get more likes, comments, and shares. The end result is that people tend to post, share, and comment on things that will get them more",https://www.wired.com/story/does-facebook-need-an-understand-button/,2020-02-06 14:00:00+00:00,LABEL_1_relevant,0.994721293449402,Social Media,social_media,Wired,User Experience & Entertainment
48,885M Financial Records—Dating Back 16 Years—Exposed Online,"After a solid decade of nonstop corporate data breaches and exposures, you'd think large organizations would have at least fixed the most basic and obviously damaging types of data mishandling. But there's clearly still a long way to go. On Friday, independent security journalist Brian Krebs revealed that the real estate and title insurance giant First American had 885 million sensitive customer financial records, going back to 2003, exposed on its website for anyone to access. And while there isn't currently evidence that anyone actually found and stole the information, it was so easy to grab—and so obviously valuable to scammers—that it's hard to rule out that possibility.

The Hack

Krebs reports that the exposed records included Social Security numbers, driver's license images, bank account numbers and statements, mortgage and tax documents, and wire transaction receipts—an absolute treasure trove for any scammer or identity thief. An attacker who figured out the format of the company's document URLs could have input any ""record number"" they wanted—beginning with ""000000075,"" according to Krebs—and pull up the documents associated with that customer case. First American took down the site that populated the records at 2 pm ET on Friday. Krebs notified the company of the situation earlier this week.

“First American has learned of a design defect in an application that made possible unauthorized access to customer data,"" the company said in a statement. ""The company took immediate action to address the situation and shut down external access to the application. We are currently evaluating what effect, if any, this had on the security of customer information. We will have no further comment until our internal review is completed.”

First American did not answer questions from WIRED about how long the records were exposed online. The company says it has hired a forensic firm to assess whether customer data was ever stolen. First American, which is based in Santa Ana, California, is a Fortune 500 company with more than 18,000 employees.

Who's Affected

Well, lots of people! First American is the top title insurance firm in the United States, which means the company is often party to both the buyer and lender sides of real estate transactions across the country. And the detailed financial and personal information involved in closings potentially involves information about both buyers and sellers.

While the hope is that the data was never actually stolen, millions of people may have been impacted if it was. If you've bought or sold a house in the past several years, there's a decent chance First American had a hand in it.

How Serious Is This?

The First American exposure is a major incident, because it underscores just how little progress many institutions have made on locking down customer data. Perfect security is impossible, but the stakes are incredibly high and many large organizations still overlook basic errors.

The good news is that exposed data does not necessarily mean stolen data. There's a chance that no one stumbled across this trove before the company had the chance to secure it. But unlike other data leaks of similar scale, which largely involve password and username combinations, the data in the First American haul would have devastating long-term consequences for potential victims.

If you’re a First American customer or think you were party to a transaction that also involved the company there isn’t a lot you can do to protect yourself against the possibility that your data was stolen as a result of this exposure. But watch your bank and credit card statements for suspicious activity. Consider purchasing credit monitoring or, better yet, avail yourself of a free credit monitoring offer from another security incident your data was involved in. By this point, you've almost certainly qualified for it. You can also consider a credit freeze.

Security practitioners always hope that major security incidents, like the notorious Equifax breach, will be a wake up call to all companies. But the consequences for such missteps are only first starting to appear. On Wednesday, for example, Moody’s downgraded its ratings outlook for Equifax. A spokesperson said, “It’s the first time that cyber has been a named factor in an outlook change."" Until other dramatic economic motivators emerge, disasters like First American, or worse, will continue.

More Great WIRED Stories",Yes,"It has completely destroyed our ability to have civil discourse.

The article does not mention how most of us have become addicted to social media, addicted to the point of using it even when we have other things to do. It is now not uncommon to see people sitting in restaurants and cafes with their faces buried in their phones instead of talking with",https://www.wired.com/story/first-american-data-exposed/,2019-05-24 22:49:31.545000+00:00,LABEL_1_relevant,0.9886736273765564,Social Media,social_media,Wired,"Information, Discourse & Governance"
49,What Really Caused Facebook's 500M-User Data Leak?,"Since Saturday, a massive trove of Facebook data has circulated publicly, splashing information from roughly 533 million Facebook users across the internet. The data includes things like profile names, Facebook ID numbers, email addresses, and phone numbers. It's all the kind of information that may already have been leaked or scraped from some other source, but it's yet another resource that links all that data together—and ties it to each victim—presenting tidy profiles to scammers, phishers, and spammers on a silver platter.

Facebook's initial response was simply that the data was previously reported on in 2019 and that the company patched the underlying vulnerability in August of that year. Old news. But a closer look at where, exactly, this data comes from produces a much murkier picture. In fact, the data, which first appeared on the criminal dark web in 2019, came from a breach that Facebook did not disclose in any significant detail at the time and only fully acknowledged Tuesday evening in a blog post attributed to product management director Mike Clark.

One source of the confusion was that Facebook has had any number of breaches and exposures from which this data could have originated. Was it the 540 million records—including Facebook IDs, comments, likes, and reaction data—exposed by a third party and disclosed by the security firm UpGuard in April 2019? Or was it the 419 million Facebook user records, including hundreds of millions of phone numbers, names, and Facebook IDs, scraped from the social network by bad actors before a 2018 Facebook policy change, that were exposed publicly and reported by TechCrunch in September 2019? Did it have something to do with the Cambridge Analytica third-party data sharing scandal of 2018? Or was this somehow related to the massive 2018 Facebook data breach that compromised access tokens and virtually all personal data from about 30 million users?

In fact, the answer appears to be none of the above. As Facebook eventually explained in background comments to WIRED and in its Tuesday blog, the recently public trove of 533 million records is an entirely different data set that attackers created by abusing a flaw in a Facebook address book contacts import feature. Facebook says it patched the vulnerability in August 2019, but it's unclear how many times the bug was exploited before then. The information from more than 500 million Facebook users in more than 106 countries contains Facebook IDs, phone numbers, and other information about early Facebook users like Mark Zuckerburg and US secretary of Transportation Pete Buttigieg, as well as the European Union commissioner for data protection, Didier Reynders. Other victims include 61 people who list the ""Federal Trade Commission"" and 651 people who list ""Attorney General"" in their details on Facebook.

You can check whether your phone number or email address were exposed in the leak by checking the breach tracking site HaveIBeenPwned. For the service, founder Troy Hunt reconciled and ingested two different versions of the data set that have been floating around.

“When there’s a vacuum of information from the organization that’s implicated, everyone speculates, and there's confusion,” Hunt says.

“They’re kind of stuck now, because they apparently didn’t do any disclosure or notification.” Ashkan Soltani, Former FTC chief technologist

The closest Facebook came to acknowledging the source of this breach previously was a comment in a fall 2019 news article. That September, Forbes reported on a related vulnerability in Instagram's mechanism to import contacts. The Instagram bug exposed users’ names, phone numbers, Instagram handles, and account ID numbers. At the time, Facebook told the researcher who disclosed the flaw that the Facebook security team was “already aware of the issue due to an internal finding.” A spokesperson told Forbes at the time, “We have changed the contact importer on Instagram to help prevent potential abuse. We are grateful to the researcher who raised this issue."" Forbes noted in the September 2019 story that there was no evidence the vulnerability had been exploited, but also no evidence that it had not been.

In its blog post today, Facebook links to a September 2019 article from CNET as evidence that the company publicly acknowledged the 2019 data exposure. But the CNET story refers to findings from a researcher who also contacted WIRED in May 2019 about a trove of Facebook data, including names and phone numbers. The leak the researcher had learned about was the same one TechCrunch reported on in September 2019. And according to the September 2019 CNET story, it is the same one CNET was describing. Facebook told TechCrunch at the time, “This data set is old and appears to have information obtained before we made changes last year [2018] to remove people’s ability to find others using their phone numbers.” Those changes were aimed at reducing the risk that Facebook's search and account-recovery tools could be exploited for mass scraping.",Yes,"It makes it very easy for people to connect with each other.

The main intended consequence of social media is that it allows people to connect with each other easily.",https://www.wired.com/story/facebook-data-leak-500-million-users-phone-numbers/,2021-04-06 23:57:48.816000+00:00,LABEL_1_relevant,0.9965736865997314,Social Media,social_media,Wired,"Information, Discourse & Governance"
50,Social Media’s Law of Short Messages,"“If you plot the rate of the messages versus the length, then you can find a mathematical relation between these two things during [major] events,” adds Michael Szell, a researcher at the Senseable City Lab.

Ratti, Szell, and Senseable City researcher Sebastian Grauwin are co-authors of a new paper about the study, “Contraction of online response to major events,” published today in the journal PLoS ONE.

Three words for golf fans: You the man!

To conduct the study, the researchers looked at data from several social-media sources at a variety of moments. The Twitter data, for example, comes from April 2012 and involves tweets on the subject of The Masters, one of golf’s four major tournaments. That year, golfer Bubba Watson won the tournament in a dramatic playoff against Louis Oosthuizen, prompting a flurry of shorter tweets.

As the paper drolly notes, a “large part of these tweets contain the name ‘Bubba’ followed by a varying number of exclamation marks.’”

Other data used in the study includes social media activity during election night in the U.S. in November 2012, and during a major snowstorm in February 2013.

“We wanted to see if this was Twitter-specific or if it happens in other media as well,” Szell says. “Basically we found this effect every time there was an event going on. And when there was no event, we did not find this effect.”

Renaud Lambiotte, a mathematician at the University of Namur in Belgium who has read the study, calls it “an interesting piece of research” that may lead to fruitful follow-up work, “in particular for the modeling of the relation between behavioral response and emotional stimuli.” Lambiotte suggests that well-designed online social experiments may yield further insight about behavior on social media.

The Senseable City Lab conducts large-scale data projects, often based on mobile technology or social media, to evaluate patterns of activity in urban environments or among technologically connected networks of people.

“It’s all about detecting specific signatures of how people behave, in any kind of aspect of their life, whether it’s tweeting, commenting, or moving around,” says Grauwin, who is working in the Senseable City Lab via a fellowship with the technology firm Ericsson.

The current study also revealed what Grauwin calls an “index of frustration” among some social-media users: namely, during major events, a small minority of users run up against the 140-limit on Twitter.

Ratti suggests that further studies of message length will continue to uncover more data that could be useful in the design of social-media platforms. He also thinks more research can shed light on the behavioral mechanisms that lead people to send shorter messages: Are people doing this independently, or in response to seeing other short messages?

“You get this kind of herd effect,” Ratti says. “The paper suggests this is an interesting problem to look at.”",Yes,People tend to use shorter messages during major events.,https://www.technologyreview.com/2014/02/27/13912/social-medias-law-of-short-messages/,2014-02-27 00:00:00,LABEL_1_relevant,0.995793342590332,Social Media,social_media,MIT Tech Review,Environment & Sustainability
51,Facebook Wants to Connect You With Your 'Secret Crush',"Facebook is channeling its earliest days as a hot-or-not website for college students with a new feature called Secret Crush. To be announced today at the social network’s annual F8 developer conference, Secret Crush will allow Facebook Dating users to select up to nine friends they want to express interest in. Think of it like matching on Tinder or Bumble, except you get to hand-pick the specific friends you want to date ahead of time, instead of hoping their profiles show up in the queue. What could go wrong?

Here’s how it’s supposed to work. If someone adds you to their Secret Crush list, Facebook will send you a notification saying “A friend added you as a secret crush.” If you then pick the same person for your list, Facebook will match you together and reveal your names. If the feelings are only one-sided, the unrequited lover’s identity remains secret. The object of their affection is then left to wonder who may have a crush on them, with no way to find out. The feature will be available only in countries where Facebook has rolled out its Dating service, including Colombia, Canada, Thailand, Argentina, and Mexico, as well as 14 new countries in Asia and South America introduced today. If you don’t already have a Dating profile, you can’t receive Secret Crush notifications.

Facebook

Secret Crush will help give Facebook an advantage over competing dating apps, which can’t similarly leverage information about your existing friendships. Tinder is great if you want to meet someone outside your social circle, but it doesn’t offer a direct mechanism to connect you with those you already know in real life. Facebook is likely hoping that instead of getting someone’s number at a party, you’ll add them on Facebook and then to your Secret Crush list.

Other modern dating apps haven’t tried it, but that doesn’t exactly mean Secret Crush is innovative. One of Facebook’s oldest social media competitors, Friendster, introduced a similar feature way back in 2012. The dating site GoodCrush, first launched at Princeton University in 2007, also had a nearly identical premise.

It’s not hard to imagine how Secret Crush could go wrong. You could easily prank or even bully someone by adding them to your crush list under false pretenses. One WIRED staffer who used GoodCrush in college reported they and their best friend “spent a full semester fucking with another friend through this service.” But Secret Crush will likely be great for engagement purposes. Using Facebook to scroll through baby photos may be a snooze, but who doesn’t want to find out that someone has a crush on them?

Facebook says it won’t use data from Dating or Secret Crush to make content or advertising decisions. For example, just because you add someone to your Secret Crush list doesn’t mean you will see more of their posts in your News Feed. “All activity that occurs in FB Dating stays in FB Dating and will not be shared externally,” a Facebook spokesperson said in an email.

Facebook will also announce today that Dating is immediately coming to several new locations in South America and Asia, including the Philippines, Vietnam, Singapore, Malaysia, Laos, Brazil, Peru, Chile, Bolivia, Ecuador, Paraguay, Guyana, Uruguay, and Suriname. Secret Crush will start rolling out to each locale Tuesday. For now, singles in the United States remain out of luck.

More Facebook Stories",Yes,"It leads to echo chambers where people only read, watch, or hear content that reinforces what they already believe and leads to polarization.

If you have any other tips, please let me know.

Thank you!",https://www.wired.com/story/facebook-dating-secret-crush-f8/,2019-04-30 17:30:00+00:00,LABEL_1_relevant,0.994912087917328,Social Media,social_media,Wired,"Information, Discourse & Governance"
52,How Robinhood's Psych Experiment Backfired Horribly,"A year ago Robinhood went from corporate hero to punching bag literally overnight.

As GameStop mania swept up millions of mostly novice investors looking to make a bundle while sticking it to Wall Street, the app-first broker restricted purchases of the struggling retailer and other “meme stocks” following a pre-dawn warning that it faced insolvency. In other words, it had been too successful at getting young people excited about trading.

Courtesy of Portfolio Buy This Book At: Amazon

Bookshop.org

Target If you buy something using links in our stories, we may earn a commission. This helps support our journalism. Learn more.

Perhaps this shouldn’t have been so surprising. Despite being named after the mythical hero who stole from the rich to give to the poor and its stated mission “to democratize finance for all,” Robinhood made its founders Vlad Tenev and Baiju Bhatt billionaires. Ironically, the first company they started helped hedge funds—the investment vehicles their clients would gleefully ambush during GameStop mania—trade more efficiently. Funds were able to buy stocks at almost no cost. How hard would it be to create an app that could do the same thing?

“It became clear to us that the smartphone would be your primary tool for accessing the markets and doing financial transactions in general,” said Tenev in a 2017 interview. Unlike Charles Schwab, Fidelity, or even E-Trade, Robinhood is younger than the iPhone. It is more like an app with a brokerage firm attached to it than a broker that has an app. And Robinhood’s is a thing of beauty, having won the Apple design award the year it was launched.

Even after competitors eliminated their own commissions in late 2019, young, new users came to Robinhood’s shiny app in droves—particularly after the pandemic hit. Funded accounts went from about 7 million when quarantines began to 18 million a year later as GameStop mania cooled. The Silicon Valley wunderkinder were running circles around their older competitors. But unleashing the “move fast and break things” philosophy on a generation’s savings was bound to create some problems. It also made long-standing ones harder to ignore.

When it filed to go public last year, Robinhood revealed that customers who opened the app did so about seven times a day. That made it more like Instagram than any boring financial portal. And Robinhood resembled social networks in another important way: Trading was “free.” As with those services, its users became the product.

Robinhood makes much of its revenue from selling customers’ trades to wholesalers like Citadel Securities. The company keeps fractions of a penny that add up to billions of dollars a year. That business model made it possible for Robinhood to cater to customers with tiny accounts, but only if some were hyperactive. They traded stocks an astounding 40 times as much per dollar in their accounts on average compared with those at staid Schwab.

The reason so many people have become self-directed investors, especially during a bull market, isn’t because they hate paying for professional advice. It is because they hear about others getting rich and then overestimate their ability to guess something almost completely random—stock prices. The phenomenon, called the “illusion of control,” was first described by psychologist Ellen Langer. She gave study participants the ability to buy a lottery ticket for a dollar with half given random numbers and half able to choose their own. They were later offered cash for the tickets. The group that picked their own numbers asked for several times as much.",Yes,"It creates a false sense of control. People see their friends getting rich from investing in stocks, so they try to do the same thing. The problem is that most people are not very good at picking stocks, and even the best investors only get it right about half the time.

The other big problem with Robinhood is that it encouraged",https://www.wired.com/story/robinhood-psych-experiment-gamestop/,2022-02-11 13:00:00+00:00,LABEL_1_relevant,0.9962368011474608,Social Media,social_media,Wired,"Information, Discourse & Governance"
53,An Army of Volunteers Takes On Vaccine Disinformation Online,"As researchers, pharma companies, and governments around the world are racing to make a vaccine against the pandemic coronavirus in record time, there’s a growing concern that many Americans won’t want it when it arrives. In a series of recent polls, only about half of US adults say they would get a Covid-19 vaccine, even though more than 1,000 people are still dying from the disease every day in the US. Some of those surveyed are rightly concerned about the perils of rushed science. But according to one poll conducted by Yahoo News/YouGov, more than a quarter of Americans would decline a shot in part because they believe Microsoft cofounder Bill Gates is trying to slip them a microchip.

Everything You Need to Know About the Coronavirus Here's all the WIRED coverage in one place, from how to keep your children entertained to how this outbreak is affecting the economy.

Conspiracy theories thrive in times of great uncertainty, and the coronavirus pandemic has proven to be a petri dish for particularly harmful ones. This one can be traced back to May 4, when a little-known filmmaker named Mikki Willis posted a 26-minute video called Plandemic to Facebook, YouTube, Vimeo, and a designated website. It featured a discredited scientist describing a bizarre, unsubstantiated plot by global elites like Gates to use a vaccine against the virus to seize power. These ideas, hailing from many sources, had already been swirling on many parts of the internet and were congealing into a narrative involving Gates and microchips, but the Plandemic video became their biggest signal boost. According to an analysis by The New York Times, the video spent about three days incubating on Facebook pages dedicated to conspiracy theories and the anti-vaccine movement. Then, like any efficient pathogen, it went viral. Just a week after its release, the now widely debunked video had been viewed more than 8 million times.

But it didn’t have to be that way. So says Joe Smyser, CEO of the Public Good Projects, or PGP, a public health nonprofit that specializes in using social network analysis to implement large-scale behavioral change programs. His group has built online surveillance tools for tracking outbreaks of misinformation, disinformation, and downright conspiracies. He says they saw most of the sharing activity that fueled this particular theory’s eventual virality within the first 24 hours. “It was right there in the data,” he says. “We didn’t have to wait days to respond to it, because the outcome was predictable. What was lacking was coordination.”

Smyser wants to bring coordination to combating a growing anti-vaccine movement that contributed to a record outbreak of measles last year—the worst in four decades. This week, his organization is launching a vaccine advocacy campaign unlike any other before.

Called Stronger, it aims to take the fight to anti-vaccine organizers where they’ve long had the upper hand: on social media. To do so, PGP plans to conscript the vast but largely silent majority of Americans who support vaccines into any army of keyboard warriors trained to block, hide, and report vaccine misinformation. (According to a recent Gallup poll, 84 percent of Americans say vaccinating children is important.) The effort is backed by a number of pro-immunization coalitions and the National Foundation for Infectious Diseases, and has funding from BIO, the world’s largest biotechnology lobbying group. “We have this tradition in the US that vaccines are solely the domain of public health workers who are trained to not get into fights,” says Smyser. “I think that’s a very antiquated perspective, and it’s left those on the public health side completely outgunned in this new era of social media.”

For nearly as long as there have been vaccines, public health agencies have shied away from a combative approach with people who disagree with their recommendations. Vaccines are unequivocally one of the most transformative technologies in human history. Until vaccines eradicated smallpox, the disease killed millions of children each year—between 300 million to 500 million during the 20th century alone. In 1988, at the start of a worldwide vaccine campaign, approximately 350,000 children were paralyzed by polio each year. In 2018, the virus sickened just 33.",Yes,"It gives a platform to those who would spread misinformation and conspiracy theories. In the case of vaccines, this has led to a resurgence of preventable diseases.

Smyser's organization is launching a campaign called Stronger to take the fight to anti-vaccine organizers where they’ve long had the upper hand: on social media",https://www.wired.com/story/can-a-keyboard-crusade-stem-the-vaccine-infodemic/,2020-06-15 11:00:00+00:00,LABEL_1_relevant,0.9964969754219056,Social Media,social_media,Wired,"Information, Discourse & Governance"
54,Papua New Guinea has imposed a month-long Facebook ban,"The country will prevent residents from accessing the social network, in a bid to clean up the platform.

Why? According to the government, the time will be used to filter out fake accounts and remove pornography.“This will allow genuine people with real identities to use the social network responsibly,” Sam Basil, Papua New Guinea’s communications minister, told the Post Courier, a local publication.

But: Only about 12 percent of the country uses Facebook. Seems like a drastic measure to shut down something that less than an eighth of your population uses.

Why it matters: A Facebook ban usually occurs right before an election, or else it’s imposed indefinitely, as in China. As Aim Sinpeng, an expert in digital media and politics at the University of Sydney, told the Guardian, “These issues with Facebook are being spoken about in a number of other countries, so the fact that Papua New Guinea is on the bandwagon shows how widespread concerns have become.”",Yes,"It can lead to addiction. Although social media can be used to connect with friends and family, it can also lead to people spending too much time on their devices and becoming addicted to the instant gratification that they receive from social media. This can lead to people becoming isolated from the real world and their loved ones.",https://www.technologyreview.com/2018/05/30/142659/papua-new-guinea-has-imposed-a-month-long-facebook-ban/,2018-05-30 00:00:00,LABEL_1_relevant,0.9937135577201844,Social Media,social_media,MIT Tech Review,Health & Wellbeing
55,The Mosque Shooter Exploited the Power of the Internet,"After each new horrific mass shooting, an all-too-familiar cycle often plays out: Reporters (myself included) race to attempt to unpack an alleged shooter’s possible motivations by piecing together clues from their social media accounts and online postings before it all gets scrubbed from the internet. We do this in the hopes that it will somehow provide a window into their mindset in the months leading up to the attack, or at least bring us somewhat closer to answering that ultimately unanswerable question: Why?

But this approach carries with it potentially dangerous unintended consequences. At least 49 people were killed on Friday during attacks on two mosques in Christchurch, New Zealand, and like clockwork the cycle began almost immediately. But this time it was a bit different. The alleged shooter himself had provided the world with more answers and possible motivations for his own actions than it seemed the internet knew how to handle.

Hours before the attack, the alleged shooter took to Twitter and 8chan—an online messaging board known for its distinct brand of toxicity—to announce his plans and share links to a Facebook account which later live-streamed 17 minutes of the massacre. He also linked to a 74-page document littered with awkwardly placed ironic memes and references to various toxic ideologies that many news outlets have since deemed his “manifesto.” On 8chan, the links were accompanied by a request: “I have provided links to my [sic] my writings below, please do your part by spreading my message, making memes and shitposting as you usually do.”

The internet largely did just that. The gory first-person Facebook video of the shooting quickly went viral, spreading across social media platforms like wildfire before platforms could take it down. Since then, pundits, analysts, and internet sleuths have been publicly dissecting and interpreting each line of his lengthy manifesto—along with his equally toxic social media presence—turning public discussion into something closer to a string of far-right rabbithole keywords.

Twitter content This content can also be viewed on the site it originates from.

There was no need for internet sleuths to track down his social media accounts and comb them for clues, as he broadcast their existence publicly: They were, predictably, filled with more made-to-provoke explanations for his actions. It was “a very clear instance of media manipulation,” designed with the world’s eye in mind, says Whitney Phillips, a Data and Society researcher who specializes in troll culture and the amplification of extremism online.",Yes,It gives people a false sense of connection to other people.,https://www.wired.com/story/mosque-shooter-exploited-power-of-internet/,2019-03-15 20:58:52.387000+00:00,LABEL_1_relevant,0.9966183304786682,Social Media,social_media,Wired,"Information, Discourse & Governance"
56,Thank you for posting: Smoking’s lessons for regulating social media,"The comparison is more than metaphorical. It’s a framework for thinking about how public opinion needs to shift so that the true costs of misinformation can be measured and policy can be changed.

Personal choices, public dangers

It might seem inevitable today, but regulating the tobacco industry was not an obvious choice to policymakers in the 1980s and 1990s, when they struggled with the notion that it was an individual’s choice to smoke. Instead, a broad public campaign to address the dangers of secondhand smoke is what finally broke the industry's heavy reliance on the myth of smoking as a personal freedom. It wasn’t enough to suggest that smoking causes lung disease and cancer, because those were personal ailments—an individual’s choice. But secondhand smoke? That showed how those individual choices could harm other people.

Epidemiologists have long studied the ways in which smoking endangers public health, and detailed the increased costs from smoking cessation programs, public education, and enforcement of smoke-free spaces. To achieve policy change, researchers and advocates had to demonstrate that the cost of doing nothing was quantifiable in lost productivity, sick time, educational programs, supplementary insurance, and even hard infrastructure expenses such as ventilation and alarm systems. If these externalities hadn’t been acknowledged, perhaps we’d still be coughing in smoke-filled workplaces, planes, and restaurants.

And, like secondhand smoke, misinformation damages the quality of public life. Every conspiracy theory, every propaganda or disinformation campaign, affects people—and the expense of not responding can grow exponentially over time. Since the 2016 US election, newsrooms, technology companies, civil society organizations, politicians, educators, and researchers have been working to quarantine the viral spread of misinformation. The true costs have been passed on to them, and to the everyday folks who rely on social media to get news and information.

Take, for example, the recent falsehood that antifa activists are lighting the wildfires on the West Coast. This began with a small local rumor repeated by a police captain during a public meeting on Zoom. That rumor then began to spread through conspiracy networks on the web and social media. It reached critical mass days later after several right-wing influencers and blogs picked up the story. From there, different forms of media manipulation drove the narrative, including an antifa parody account claiming responsibility for the fires. Law enforcement had to correct the record and ask folks to stop calling in reports about antifa. By then, millions of people had been exposed to the misinformation, and several dozen newsrooms had had to debunk the story.

The costs are very real. In Oregon, fears about “antifa” are emboldening militia groups and others to set up identity checkpoints, and some of these vigilantes are using Facebook and Twitter as infrastructure to track those who they deem suspicious.

Online deception is now a multimillion-dollar global industry, and the emerging economy of misinformation is growing quickly. Silicon Valley corporations are largely profiting from it, while key political and social institutions are struggling to win back the public’s trust. If we aren’t prepared to confront the direct costs to democracy, understanding who pays what price for unchecked misinformation is one way to increase accountability.

Combating smoking required a focus on how it diminished the quality of life for nonsmokers, and a decision to tax the tobacco industry to raise the cost of doing business.

Now, I am not suggesting placing a tax on misinformation, which would have the otherwise unintended effect of sanctioning its proliferation. Taxing tobacco has stopped some from taking up the habit, but it has not prevented the public health risk. Only limiting the places people can smoke in public did that. Instead, technology companies must address the negative externalities of unchecked conspiracy theories and misinformation and redesign their products so that this content reaches fewer people. That is in their power, and choosing not to do so is a personal choice that their leaders make.",Yes,"It has given a platform to conspiracy theorists and those who peddle misinformation. This has had a negative impact on public trust in key institutions, and has led to the rise of vigilantism.",https://www.technologyreview.com/2020/10/05/1009231/social-media-facebook-tobacco-secondhand-smoke/,2020-10-05 00:00:00,LABEL_1_relevant,0.9959892630577089,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
57,Why the History of Black Twitter Needed to Be Told,"But I think again with Twitter, it's like now Twitter has the power to check, right? It's sort of the checks and balances system, right? Especially when you have Black Twitter behind you because as somebody else said in the oral history, once Black Twitter gets a hold of something, it's hard for it to stop and so once Black Twitter was like, Toronto Burke created this, she's the woman behind and the genius behind this, we need to say something, right? And so I think that's also the power to mobilize and the power to just push against a system that doesn't always have our best interests in mind is really important.

MC: And we'll continue seeing that happen even more recently where people have made a joke or posted a meme and it's origins were in Black Twitter and people will jump on them and say, you have to give credit where credit is due there.

JP: Right. So I think one of my favorite memes that's come out of Black Twitter it's, this you? Where people are sort of checking other folks and saying, hey, we started this, this is ours. It's tough though, I mean, in some ways I'm glad that Black Twitter exists in a sort of entity that it does because I think without it, it'd be harder to document all the things that we created online, right? And all the things that we continue to create, right? And so one thing that Andre Brock was saying how Black Twitter in itself is kind of like a living archive, right? It's constantly being updated and it's constantly growing, right? It's sort of a library of Congress but for Black folks online in a way, right?

And so I think that it's important to have it but it's also important that it acts like a checks and balances system too that say, hey, we're very protective of our space and this is ours and it's important to have things like that. Because I think in the same way that I wanted to document this oral history and I thought it was important to have this official record of Black Twitter, it's the same sort of sense behind that in that we need these things and these things are ours and we need people to know that we created these things.

MC: All right. Well, you can read more about the history of Black Twitter in Jason Parham awesome oral history. Part three is up now, it's a three-part series, you can read the whole thing on WIRED.com. It is also on the cover of WIRED magazine in September with an amazing painting illustration, beautiful cover so check it out. We're going to take a break right now and when we come back we will have our recommendations.

[Break]

MC: All right. Welcome back to the show. Jason Parham, you are in hot seat, what's your recommendation for our audience?

JP: So one show I've been binging lately that I just recently finished. It came on Cinemax in 2019 and I think it was kind of a crime because nobody actually watches shows that come on Cinemax but they have a lot of good content. But now, because I guess of the deal with HBO Max all these Cinemax shows are on HBO Max, it's with Carla Gugino and Giancarlo Esposito, it's called Jett, J-E-T-T. It's a crime drama crime thriller about a thief named Daisy played by Carla Gugino and the crime boss, Giancarlo Esposito who's infamous from Breaking Bad as Gus Fring, it's amazing. There was only one season of it, they're nine episodes, each episode is an hour, it's shot beautifully, it also has Jodi Turner-Smith, all these amazing actors in it. Nobody's heard of this, I don't know if people ever talking about this which made me upset but I've been telling everybody to watch this show so watch this show.",Yes,"It has allowed us to live in our own little echo chambers, forgetting that there are other people out there with different perspectives. It has also led to the rise of ""fake news"" and a general mistrust of the media.",https://www.wired.com/story/gadget-lab-podcast-514/,2021-07-30 12:00:00+00:00,LABEL_1_relevant,0.9950425624847412,Social Media,social_media,Wired,"Information, Discourse & Governance"
58,Everything We Know About Facebook's Massive Security Breach,"Facebook’s privacy problems severely escalated Friday when the social network disclosed that an unprecedented security issue, discovered September 25, impacted almost 50 million user accounts. Unlike the Cambridge Analytica scandal, in which a third-party company erroneously accessed data that a then-legitimate quiz app had siphoned up, this vulnerability allowed attackers to directly take over user accounts.

The bugs that enabled the attack have since been patched, according to Facebook. The company says that the attackers could see everything in a victim's profile, although it's still unclear if that includes private messages or if any of that data was misused. As part of that fix, Facebook automatically logged out 90 million Facebook users from their accounts Friday morning, accounting both for the 50 million that Facebook knows were affected, and an additional 40 million that potentially could have been. Later Friday, Facebook also confirmed that third-party sites that those users logged into with their Facebook accounts could also be affected.

""We were able to fix the vulnerability and secure the accounts, but it definitely is an issue that it happened in the first place."" Mark Zuckerberg, Facebook

Facebook says that affected users will see a message at the top of their News Feed about the issue when they log back into the social network. ""Your privacy and security are important to us,"" the update reads. ""We want to let you know about recent action we've taken to secure your account."" The message is followed by a prompt to click and learn more details. If you were not logged out but want to take extra security precautions, you can check this page to see the places where your account is currently logged in, and log them out.

Facebook has yet to identify the hackers, or where they may have originated. “We may never know,” Guy Rosen, Facebook’s vice president of product, said on a call with reporters Friday. The company is now working with the Federal Bureau of Investigation to identify the attackers. A Taiwanese hacker named Chang Chi-yuan had earlier this week promised to live-stream the deletion of Mark Zuckerberg's Facebook account, but Rosen said Facebook was ""not aware that that person was related to this attack.""

“If the attacker exploited custom and isolated vulnerabilities, and the attack was a highly targeted one, there simply might be no suitable trace or intelligence allowing investigators to connect the dots,” says Lukasz Olejnik, a security and privacy researcher and member of the W3C Technical Architecture Group.

On the same call, Facebook CEO Mark Zuckerberg reiterated previous statements he has made about security being an “arms race.”

“This is a really serious security issue, and we’re taking it really seriously,” he said. “I’m glad that we found this, and we were able to fix the vulnerability and secure the accounts, but it definitely is an issue that it happened in the first place.”

The social network says its investigation into the breach began on September 16, when it saw an unusual spike in users accessing Facebook. On September 25, the company’s engineering team discovered that hackers appear to have exploited a series of bugs related to a Facebook feature that lets people see what their own profile looks like to someone else. The ""View As"" feature is designed to allow users to experience how their privacy settings look to another person.

The first bug prompted Facebook's video upload tool to mistakenly show up on the ""View As"" page. The second one caused the uploader to generate an access token—what allows you to remain logged into your Facebook account on a device, without having to sign in every time you visit—that had the same sign-in permissions as the Facebook mobile app. Finally, when the video uploader did appear in ""View As"" mode, it triggered an access code for whoever the hacker was searching for.",Yes,"It can reduce the level of privacy that an individual has. This can happen in many ways, such as through the sharing of personal information and the loss of control over that information. Additionally, social media can lead to decreased face-to-face interaction, which can reduce the level of intimacy and trust in relationships.",https://www.wired.com/story/facebook-security-breach-50-million-accounts/,2018-09-28 19:03:03.404000+00:00,LABEL_1_relevant,0.99595707654953,Social Media,social_media,Wired,Security & Privacy
59,China censored social media posts about Chloé Zhao's Oscar win ...,"The news of Chloé Zhao’s Oscar win for Best Director has been censored on social media and search engines in China at the direction of the country’s propaganda ministry, The Wall Street Journal reported. Zhao, who was born in Beijing, is the first Chinese woman and first woman of color to take home the Academy Award for directing. Nomadland also won Best Picture at Sunday night’s awards ceremony, and the movie’s star, Frances McDormand, won Best Actress.

But according to the Journal, early messages congratulating Zhao had been scrubbed from Chinese social media sites by midday Monday, and searches on Chinese search engines Baidu and Sogou had few links to the Oscar news. There was no news about Zhao’s win on China Central Television, the Xinhua News Agency, or the Chinese Communist Party’s People’s Daily on Monday, the Journal reported; according to a state media reporter, the country’s propaganda ministry told the outlets not to report the win due to “previous public opinion.”

The Global Times, a news site owned by People’s Daily, posted an op-ed late Monday acknowledging Zhao’s win but criticizing Nomadland as “typically American and far from the real life of the Chinese people.” Of Zhao, the Global Times wrote: “We hope she can become more and more mature. In an era when the China-US confrontation is intensifying, she can play a mediating role in the two societies and avoid being a friction point. She cannot escape her special label, and she should actively use it.”

Another of the few news stories that did surface in China about Zhao’s win came from small news site 163.com, which called Zhao the “second Chinese filmmaker to win the best director Oscar, after Ang Lee,” according to The Hollywood Reporter. Lee, who won Best Director twice, for Brokeback Mountain in 2006 and for Life of Pi in 2013, was born in Taiwan. China considers Taiwan to be part of China, a matter of some contention internationally.

It’s not the first time Nomadland and its director have been hidden by Chinese media; last month, The Associated Press reported that social media posts and other publicity about the movie were removed by China’s censors.

The reason for China’s erasure of Zhao’s win likely stems from her comments in Western media outlets. She said in a 2013 interview with Filmmaker magazine that China was a “place where there are lies everywhere,” a comment that was later removed from that article, according to the AP, but not before it drew considerable backlash in China.",Yes,It can lead to the censorship of information.,https://www.theverge.com/2021/4/26/22403627/china-censored-chloe-zhao-oscars-nomadland-social-media-posts,2021-04-26 00:00:00,LABEL_1_relevant,0.9958482980728148,Social Media,social_media,The Verge,"Information, Discourse & Governance"
60,Viral Videos: the Scourge—and Savior—of Tom Cruise's Career,"Ten years ago, Tom Cruise’s public image was dangerously close to self-destructing. In January of 2008, a nearly 10-minute-long video of Cruise solemnly discussing Scientology wound up on the now-deceased Gawker. “We are the authorities on the mind,” Cruise says in the clip, as a riff on the Mission: Impossible theme plays in the background. “We are the authorities on improving conditions.” In the video, Cruise alternates between uproarious laughter and stern lecturing, extolling the power of his religion—whose members, he says, have the power to stop crime and rescue auto-accident victims. Cruise’s affiliation with the group was never a secret, but the video made his devotion all the more clear. “You’re either on board,” he says, “or you’re not on board.”

[#video: https://www.youtube.com/embed/UFBZ_uAbxS0

At the time, plenty of people were decidedly not on board with Cruise, then stuck in what can now charitably be called his “Weird Tom” era—which had been brought about, in no small part, by the internet. It had begun in May 2005, when Cruise showed up on for an appearance on Oprah Winfrey’s talk show, where audience members screamed maniacally for the actor, leading a keyed-up to Cruise to scamper about the set and, briefly, wind up atop Winfrey’s couch. If the incident had occurred a few years earlier, it likely would have been forgotten—but Cruise’s couch-trip took place just a few months after the introduction of YouTube, and at a peak era for ’00s meme culture. It didn’t take long for someone to add some Return of the Jedi-style Emperor-shocks to Cruise’s appearance, just one of many online responses hinging on the idea that the always-steady Cruise was somehow out of control.

That perception only grew, thanks to a Today Show appearance soon after. During the multi-segment talk, Cruise lectured Matt Lauer on the evils of psychiatry—a practice Scientology abhors—and criticized Brooke Shields, who’d recently disclosed a battle with postpartum depression. Videos of the exchange seemingly commandeered the entire internet, where Cruise was vilified as a bully. The off-putting back-to-back appearances didn’t hurt Cruise’s War of the Worlds (which remains Cruise’s highest-grossing film). But a year later, Viacom CEO Sumner Redstone severed the actor’s long-running production deal with Paramount, the studio behind the Mission: Impossible films, citing the actor’s behavior as “not acceptable.”

By the time Gawker released the widely-seen Scientology video in 2008, Cruise was already in a delicate position. It only grew more precarious when millions of people saw the actor straight-facedly claiming to possess heightened powers, and laughing like he’d just landed a Reebok sponsorship for Rod Tidwell. And the video wouldn’t go away, even after the church tried to pull it from the web, ultimately leading to a war of the words between the organization and Anonymous. Oprah, The Today Show, the Scientology tell-all: The three videos only added to the belief that Cruise was either completely out of touch, or completely out of his mind—possibly both.

So Tom Cruise did what he always does when he’s in trouble: He ran.

Considering he’s been acting for more than thirty years, it seems strange to think that anyone would need a primer on Tom Cruise’s career. But for those who only know him for his ankle-annihilating Mission stunts, a quick recap: Throughout the ‘80s and ‘90s, Cruise was the biggest, most consistent movie star in the world. He made some very good hits (Risky Business, Rain Man), and some very bad hits (Cocktail, Days of Thunder). And he used his industry goodwill and star-charm to lure moviegoers into such potential career danger-zones as Interview with the Vampire, Magnolia, and Eyes Wide Shut—the latter being a nearly three-hour-long drama in which members of a Long Island faux-Illuminati wear fright-masks and languidly bonk each other to gregorian chants.",Yes,"Public figures are taken much less seriously. 

Ten years ago, Tom Cruise’s public image was dangerously close to self-destructing. In January of 2008, a nearly 10-minute-long video of Cruise solemnly discussing Scientology wound up on the now-deceased Gawker. “We are the authorities on the",https://www.wired.com/story/tom-cruise-viral-video-fallout/,2018-08-07 11:00:00+00:00,LABEL_1_relevant,0.995832622051239,Social Media,social_media,Wired,"Information, Discourse & Governance"
61,What's Next for Instagram's Kevin Systrom? Flying Lessons,"Kevin Systrom doesn’t know what’s next, but he’s starting by learning to fly.

Three weeks after he and his Instagram cofounder Mike Krieger abruptly left the Facebook-owned company—and three days since his first solo flight—Systrom says he’s taking time to think about what problem he wants to attack next.

At the WIRED25 summit, Systrom spoke about his decision to leave Instagram, but deflected questions about reports that he and Krieger disagreed with Mark Zuckerberg’s focus on growth and various product changes. “You don’t leave a job because everything’s awesome,” Systrom said, adding: “There are no hard feelings at all. I want this thing to succeed.”

It’s been six years since Systrom and Krieger sold Instagram to Facebook for about $1 billion, and the photo-sharing social network has grown at a dizzying speed since. It hit 1 billion users this year and now generates billions in ad revenue. But its creators were unlikely to stick around forever—as Systrom noted, they stayed far longer than most founders do after selling to a larger company. He compared building a company to preparing a rocket for launch: You design it, fuel it, point it in the right direction, and then let it go. By the time he left, Systrom had been at the company for eight years.

“It [Instagram] didn’t feel done by any stretch of the imagination,” he said. “But it felt like it was in orbit.”

So now what? “I don’t have any plans yet, except for hanging out for a little while,” Systrom said. He’s spending time with his 9-month-old daughter between flying lessons, doing some writing, and working with entrepreneurs—any of which, he noted, could point him to his next project.

If Systrom sticks to social media, he’s got his work cut out. He has long talked about making Instagram a pleasant “place” on the internet; Instagram gave users the right to turn off comments on their posts (thus curtailing engagement, a central goal of any social network) and developed anti-bullying policies. Yet The Atlantic reports that Instagram has deprioritized efforts to stop harassment. Facebook, of course, has its own ream of problems.

These companies have unprecedented power, according to Systrom: the ability to reach billions of people, all at once, in an instant. “We’re learning what it means to have that responsibility as an industry,” he said.

By the time his daughter is old enough to use social media (her current iPhone use is largely drool-oriented), he wants to be sure the people running those networks—whatever form they take—are paying attention to how they impact people’s lives. “Instagram solved a problem for people at the time it was released,” Systrom said. “Photos were grainy and blurry and people were unhappy about sharing them, so we added filters.”

Eight years and a billion people later, the problem’s a lot bigger and more significant, but surely there’s a solution out there. “It’s far more simple than we all understand, if we just focus on humans and the that problems they have.”

More Great WIRED Stories",Yes,"It can lead to decreased empathy for others. The way we consume social media can lead us to believe that the world is full of angry people, which can make us more likely to be angry ourselves. Additionally, social media can give us a false sense of self-importance, which can make us less likely to be interested in the lives of",https://www.wired.com/story/whats-next-instagrams-kevin-systrom-flying-lessons/,2018-10-15 19:36:06.598000+00:00,LABEL_1_relevant,0.9905349612236024,Social Media,social_media,Wired,"Information, Discourse & Governance"
62,"The Download, Feb 3, 2017: Snap’s IPO, Next-gen Nuclear Hurdles, and a Robotic Pack Horse","Snap Is Going Public—Offering Ideas, Not Profits

Snap, the parent company of the messaging app Snapchat, has filed the paperwork required for its initial public offering. It’s thought to be seeking a valuation of up to $25 billion. The company, co-founded by Evan Spiegel, who was one of our 35 Innovators Under 35, is based around the now: capture a moment, share it, and watch it disappear. So far, it’s worked, as the Snapchat app has over 150 million active users. But the IPO paperwork reveals that Snap actually made a loss of $514 million in 2016. Instead of profits, then, it’s offering investors a slice of vision. It will bill itself not as a social media firm, but as a camera company. And it’s pushing new ideas hard: while disappearing messages are its bread and butter, it’s also selling smart glasses, building AR into apps, and developing weird and wonderful new ways to make its experience sticky (read: sell more ads). Whether the market will be won over by such ambitions? That remains to be seen.

Do you need The Download? Sign up here to get it for free in your inbox.



Next-gen Nuclear? Not So Fast

A new kind of safer, simpler nuclear reactor is having a hard time becoming a reality. In the wake of the Fukushima disaster, the nuclear industry decided to introduce more straightforward designs. The result: the generation III+ reactor. These large-scale devices—not to be confused with new small modular nuclear reactors, which may be used to provide localized power—are intended to pump out large quantities of megawatts and ease the route to a renewable future. But projects in France, Finland, and the U.S. are running behind schedule and over budget. And newly committed projects, such as the UK’s Hinkley Point reactor, are proving to be eye-wateringly expensive. What gives? According to Lake Barrett, a former official at the U.S. Nuclear Regulatory Commission who spoke to Bloomberg: “a near-perfect storm of societal risk aversion to nuclear causing ultra-restrictive regulatory requirements, construction complexity, and lack of nuclear construction experience by the industry.”



Blame Your Brain For Computer Hacks

A new study reveals how the brain reacts to computer security alerts—and the findings could help make our devices safer. Our own Tom Simonite explains that research carried out by Anthony Vance from Brigham Young University used functional MRI scans of people’s brains to reveal the unconscious mechanisms behind the way they perceive security warnings. It turns out that alerts often appear when we’re in the midst of doing something else, which makes us less likely to respond to them, as our brain struggles to handle two tasks at once. The warnings are also boringly consistent, which means that we pay them less attention over time. Instead, specially designed warnings developed in collaboration with Google wait for people to complete tasks and have messages appear in different colors. The upshot: people are more likely to respond. Vance says that Google plans to add the feature to an upcoming version of its Chrome browser.",Yes,"It allows people to connect with one another without actually interacting in person. This can lead to a lack of communication skills and social skills, as well as a lack of empathy.",https://www.technologyreview.com/2017/02/03/5694/the-download-feb-3-2017-snaps-ipo-next-gen-nuclear-hurdles-and-a-robotic-pack-horse/,2017-02-03 00:00:00,LABEL_1_relevant,0.9962056875228882,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
63,"The Goat-Birthing, Tomato-Fermenting Homesteaders of YouTube","Weed 'em and Reap is part of a subgenre of channels that make up YouTube’s homesteading movement. In this context, “homestead” no longer carries its original definition—a government-granted plot of undeveloped land—but is meant to evoke pioneer lifestyle and aesthetic. “I dub what we do modern homesteading,” says Al Lumnah, front man for Lumnah Acres. “We all grew up romanticizing Little House on the Prairie, but I like running water. I like my KitchenAid mixer.”

The movement’s values are broadly back-to-the-land, but it contains members on- and off-grid, vegans and experts in hunting and butchery, Floridians harvesting 100-pound bunches of bananas and Alaskans chiseling ice off their outhouses, people with roaring orange tractors and others who slowly, near-silently mow down entire fields using only a scythe.

“You have hippies and people who open-carry firearms in public places—revolutionaries from both sides of the aisle,” says Julianne, whose channel, Dirtpatcheaven, has covered everything from living in a tiny house to composting to mounted archery. “We’re united by our mistrust in government. The more we can produce ourselves in our own homes, the less control the government or our communities have.” It’s little wonder that, as our anxieties keep spiking and public trust in government is hovering near historic lows, these channels continue to grow and multiply.

That said, most of these modern homesteaders didn’t set out on this venture to thumb their noses at the nanny state. Nearly all cited the same impetus: their own failing health, or concerns for the health of their family. “Antidepressants didn’t work for me,” Lumnah says. “But what I ate and how I ate had a huge impact, mentally.” So the Lumnahs started growing their own food, and their homestead grew from there. Their garden needed compost, so they got chickens; the chickens weren’t eating all the excess produce the garden produced, so they got pigs to gobble up the rest of their waste stream (and turn it into even more compost). Most others had similar trajectories. Life in the city and 9-to-5 jobs weren’t working, so they drifted further toward agrarian lifestyles as they built confidence.

The skills they gained are rare, and therefore hard-won. “In some ways there is an overwhelming amount of information. If you search ‘rendering lard’ you’ll get tons and tons of hits,” says Caroline Thomas of Homesteading Family. Trouble is, much of that information is bad or incomplete. “Many people teaching these things have never had to live with the consequences. It was a fun side project for them, not a food source for the next 12 months.” Homesteading Family—like most of these channels—began as a way to fill the knowledge gaps the homesteaders themselves had fallen into. (Thomas is a natural teacher. While we chatted, she shared her family’s favorite way to preserve tomatoes: fermenting them for three weeks minimum. Mine are still doing time in the brine, but they look good and accidentally trendy. The line between homesteader and hipster is sometimes a fine one.)",Yes,It has given rise to a new generation of homesteaders. These homesteaders are united by their mistrust in government and their desire to be self-sufficient. They have found success in teaching others how to live off the land and be self-sufficient as well.,https://www.wired.com/story/the-goat-birthing-tomato-fermenting-homesteaders-of-youtube/,2019-03-12 11:00:00+00:00,LABEL_1_relevant,0.9061511158943176,Social Media,social_media,Wired,Economy
64,Sports social media is increasingly dominated by women - The Verge,"The way people enjoy sports has changed. On Twitter, fans are watching games together, interacting directly with their favorite athletes, and engaging with their teams beyond a logo and a collection of players. Each team’s account has its own voice and personality that they exhibit across platforms.

For men’s professional sports teams, it can be easy to assume that the person behind the keyboard shares certain characteristics with the players they’re tasked with representing — namely, their gender. But it’s becoming increasingly more likely that the person behind your favorite team’s social media account is a woman. Often considered a “pink-collar industry,” social media’s female-dominated workforce has naturally extended to the traditionally male-dominated sports industry, too. Women are not only excelling in these roles, but as the online voices of sports teams, they’re finding themselves insulated from the kind of harassment and abuse that plagues many of the more visible women in the sports industry (and outside of it). For women who want to create sports content without being subjected to misogynist vitriol, a role in sports social media — at least for now — can be an ideal solution.

“It helps fight every terrible Internet comment or snide remark women have ever heard about not knowing sports.”

The feminized nature of social media work has to do with its “characteristic invisibility, lower pay, and marginal status” within the tech industry, say Brooke Erin Duffy, an assistant professor of communications at Cornell, and Becca Schwartz, a researcher at the University of Oxford. The inequity they describe in a study published last year in New Media & Society can easily be compounded as it spreads to industries like professional sports, which already often devalues femininity. (A recent study from the software and data company Payscale also shows the sports world suffers from the same gender wage gap issues as most other industries.)

But as social media becomes a more and more powerful tool, so do the people behind it. And in sports, that’s created a subversive dynamic: the women who have been historically excluded from the major leagues, both behind the scenes and on the field, are now in charge of their voices and public-facing personas.

“Having women-run men’s sports team accounts helps fight every terrible internet comment or snide remark women have ever heard about not knowing sports,” says Olivia Witherite, social media director at the Mid-Atlantic Sports Network (MASN), the broadcast home for the Baltimore Orioles and Washington Nationals. “It’s so satisfying to know that we, as women, can use the platform of social media to inform people [and] engage with fans.”

“It’s fascinating … because, in one sense, [these women] have access to the field where they want to work, but they’re essentially working in a role where they get [no] credit,” says Duffy. “The whole virtue of social media, to do it well, [means] the role of the content creator is to be invisible.”

the women who have been historically excluded from the major leagues are now in charge of their public-facing personas

While the invisibility of these roles is often seen as a negative because “invisible work tends to be dismissed,” Duffy says that when it comes to sports social media jobs, invisibility can actually be a positive. Given the vicious harassment, online misogyny, and very public vitriol women often get as they make inroads into traditionally masculine industries like sports, especially in public-facing roles — a challenge documented last year in the Peabody Award-winning video #MoreThanMean — Duffy suggests that “maybe these roles provide a value in terms of a level of protection” when it comes to “inoculating female social media workers against the kind of harassment and hatred that seems to flourish in these spaces.”

Negativity, of course, will always inevitably be lobbed at teams’ accounts, and not internalizing that can be hard, says Amara Baptist, the digital content manager for the Portland Trail Blazers and co-host of Social on the Sidelines, a podcast about the business of running sports social media accounts. Eventually, Baptist realized “that when people are tweeting horrible things at the team account, they’re not tweeting them at me.”

In her case, sexist assumptions that the person running an NBA team’s account is male have even served to protect her from harassment. She says that when she commented on the Memphis Grizzlies Instagram account (where she worked for two seasons) as the team, many followers assumed she was a man. “I would clap back at people [talking trash] and show the trolls that we’re reading the comments, and almost every commenter thought I was a guy,” she says.

In Samantha Wood’s experience as the director of digital and social for the Philadelphia Eagles, fans are more concerned with the quality of the product than the identity of the person producing it. “Fans care far more [that] the person behind the keyboard is knowledgeable, fair, and creative,” she says.

“Almost every commenter thought I was a guy.”

This season, 44 percent of NFL teams’ social accounts are run by women. Four of the big five teams in Boston have women running their social media — a significant stat both because Boston is one of the largest sports markets in the country and because it has some of the country’s most passionate, and critical, fans.

“It’s definitely a surreal thing, being the ‘voice’ of a historic organization,” says Sue Jo, the social media coordinator for the Los Angeles Dodgers. It’s a high-pressure role for anyone to be in, and the fact that women are trusted with representing men’s sports teams in this way on such a large platform is a big deal.

It also speaks to something else that has been ignored by the sports world for a long time: the fact that fan bases of men’s sports aren’t just men, and those fans want to see content that speaks to them and reflects their interests. Women make up 45 percent of the NFL’s audience; an estimated 86 million women watched the NFL in 2017. As of 2013, MLB and the NBA had audiences that were about 30 percent female, and the NHL and Major League Soccer’s audiences were 32 percent female.

Ok ladies! ‍♀️



We’re now running 44% of @NFL team social accounts. pic.twitter.com/v1hymbmUWO — Amber Derrow (@amberderrow) June 28, 2018

“If you have a team full of white men, you’re not going to be in tune with certain things or know when things sound tone-deaf,” says Baptist. “The people bringing new, creative ideas to the table are women [and] minorities. A lot of basketball fans are female, and I don’t think people realize that. Bringing [us] to the table … brings a totally new tone that [resonates with fans].”

In fact, Baptist says that was part of her pitch when she interviewed for the job with the Trail Blazers, which she started three months ago: she aims to not only appeal to male sports fans, but to reach a broader audience as well. That can mean leaning into animal content or simply “not being afraid to use hearts.” It also means “not being afraid to be softer in your tone, and not being always super-aggressive all the time.”

That might seem counterintuitive, given the intensity often associated with sports, but it’s working. In an interview with Strategy + Business, NBA commissioner Adam Silver estimates that 1.4 billion people are engaging with the NBA in some way, and he indicates that the league was at the vanguard when it came to experimenting with new media.

“A team full of white men isn’t going to know when things sound tone-deaf.”

The number of women in sports social media is indicative of the growing gender diversity in the field as a whole. (However, racial diversity is not growing nearly as fast, and people of color are still woefully underrepresented in sports jobs.) The NBA is also the industry leader among men’s sports for racial and gender hiring practices, according to The Institute for Diversity and Ethics in Sport (TIDES) 2018 report cards.

But while social media jobs are often considered a “pink ghetto” — a term coined by writer Alana Hope Levinson in 2015 — many of the women working in sports social media think that the reasons women are doing so well in the field has less to do with the jobs being thought of as “feminine” or “invisible,” and more with the field of social media being so new that it is not beholden to tradition, as many other industries or departments might be.

“Part of it is that, in social media, there isn’t an established long line of hires in any particular role, so you never get someone who had done Instagram Stories for three decades,” says Arielle Castillo, the former senior manager of social content for Major League Soccer. “I think that helps in that it’s open for people who are younger and for women as well.”

But being relatively uncharted territory also has its drawbacks. Some of the biggest issues that have come up in Duffy’s research about gender and social media jobs is that these jobs are traditionally undervalued, first socially, in terms of colleagues recognizing the value these positions bring to the company, then economically, in terms of how that value is then translated to salary and expectations. “I’ve been struck by the amount of time and labor this entails — [people are] expected to be available around the clock,” says Duffy, who also wrote the book (Not) Getting Paid to Do What You Love: Gender, Social Media, and Aspirational Work.

more than just a backdoor into the sports industry, but a way to forge a career in social itself

One woman who runs social media for an MLB team, who spoke on the condition of anonymity, says that, during the 162-game baseball season, she’s working about 20 hours of overtime each week (though, as an hourly employee, she does get compensated for it). On an episode of Baptist’s Social on the Sidelines podcast from earlier this year, Wood says that the shorter football season (16 games) was a huge motivating factor to accept the job with the Eagles over the Flyers (82 games). “I don’t even know how MLB people do it,” she said.

It helps that the women in the sports social media (#smsports) community have created networks to lift each other up and help each other succeed in a male-dominated industry. And for women who might want to get into the industry, it helps to have examples of people who have come before to let you know it’s possible. Baptist credits other women who already do this job, like Megan Julian with the San Antonio Spurs and Julie Phayer with the Golden State Warriors, as her inspiration to go for it. “They were women in this field already, and they were making a name for themselves, so I already had my eye on the prize,” Baptist says.

Witherite thinks that has something to do with the nature of social media, which, at its best, is about bringing people together and finding support. “While many careers are so much about individualistic accomplishments,” she says, “social media is such a great place to learn from others, get encouragement and grow as a unit.” It’s allowed some of the people in the field to use it as more than just a backdoor into the sports industry, but as a way to forge a career in social itself.

Wood began her career as an intern at the New England Sports Network, which led to another internship with the Boston Bruins, and then, after college, a job with the Philadelphia Flyers. She landed with the Eagles a little over two years ago. Witherite turned her college internship at MASN in 2011 into a career, having worked her way up the ranks. Baptist leveraged her experience with the Grizzlies to get a job with the Trail Blazers, which she says has a larger digital team and more resources on the digital side of things.

“I would love one day to stop being reached out to about articles like this,” Wood says, “and I look forward to the day that it’s business as usual, and my gender has nothing to do with how I do my job.”",Yes,"It has created a safe space for women to have a voice in the world of sports. This is due to the fact that the people running the accounts are often invisible, and therefore not subjected to the same level of harassment that other women in the industry face.",https://www.theverge.com/2018/9/19/17852628/sports-social-media-women-twitter-nfl-nba-mlb,2018-09-19 00:00:00,LABEL_1_relevant,0.9849243760108948,Social Media,social_media,The Verge,Equality & Justice
65,Why WhatsApp Survived Russia’s Social Media Purge,"It has become easier to be labeled an extremist in Russia. On Monday the label—once reserved for the likes of the Taliban and the Islamic State—was given to Facebook’s parent company, Meta.

A Moscow court ruled that Meta was carrying out extremist activities in a decision that effectively banned social media platforms Facebook and Instagram from operating in Russia. But the court ruling included an interesting carve-out: WhatsApp. Both of the other platforms had been blocked earlier in March after clashing with the Kremlin over content referencing the war in Ukraine. But the ruling purposefully allowed the company’s messenger platform WhatsApp to continue operating in the country. “The decision does not apply to the activities of Meta's messenger WhatsApp, due to its lack of functionality for the public dissemination of information,” the court said, according to Reuters.

To some, sparing WhatsApp is a sign of increasingly erratic policymaking by Moscow. “I don't understand how they can do that,” says Kevin Rothrock, managing editor of the English-language edition of Russian news outlet Meduza. “Why only some of Meta’s products are extremist isn’t totally rational to me.”

But for others it is a sign that the Kremlin is worried that ordinary Russians, who are already wrestling with sanctions and shortages, will only tolerate so much disruption to their daily lives. WhatsApp is one of Russia’s few remaining Western services. Although the app is not used to disseminate news in the same way as Facebook or Instagram, both experts and people inside Russia suspect the Kremlin is hesitant to block the country’s most-used platforms. WhatsApp is hugely popular in Russia, with 84 million monthly users in January 2022, according to Statista.

That popularity means officials would risk political backlash if it was blocked, says Alena Epifanova, a research fellow at the German Council on Foreign Relations. “The state is trying to calculate collateral damage,” she says, adding that WhatsApp in Russia is mostly politically neutral, used by people to chat with classmates or their family. “You can’t compare WhatsApp in Russia with WhatsApp in Brazil, for example.” The state doesn't want to risk “an outcry of people who are not really politicized but use WhatsApp for privacy,” she adds.

Alena Georgobiani, a communications expert based in Moscow, also believes WhatsApp is protected by the number of people who use the app, as a move to block the service would inconvenience a lot of Russians. “Everybody uses it. I don't have many people on my contact list who don't have WhatsApp,” she says.

Popularity as a protection mechanism does not only apply to WhatsApp. A look at Statista’s 2020 list of Russia’s most popular platforms shows that the country’s authorities have been blocking from the bottom. Twitter—the 11th-most popular app in the country—was blocked on March 4. TikTok—the eighth-most popular—suspended its services on March 6. Now Facebook and Instagram—seventh- and fourth-most popular respectively—have also disappeared. Yet the two most popular US platforms in the country—YouTube and WhatsApp—are still operating.",Yes,"It has given rise to echo chambers and filter bubbles.

When people only consume information that reinforces their existing beliefs, they become trapped in what is known as an echo chamber. This can lead to a distorted view of the world and a lack of understanding of different viewpoints.

Filter bubbles are a related phenomenon whereby people are only exposed to",https://www.wired.com/story/whatsapp-russia-meta-ban/,2022-03-21 22:30:48.951000+00:00,LABEL_1_relevant,0.9960691928863524,Social Media,social_media,Wired,"Information, Discourse & Governance"
66,The Next Campaign Text You Get May Be From a Friend,"As innovative political tactics go, “Tell your friends” does not sound quite cutting edge. And yet, as the 2020 election lurches into gear, there may be no hotter trend in the campaign tech world than “relational organizing”: apps that help leverage good old-fashioned word of mouth—and the contact list on your smartphone—to drive turnout.

These platforms, with names like Team and Outvote, first sprung up from the wreckage of the Democrats’ loss in 2016, as tech-minded liberals, horrified by Donald Trump’s win, sought ways to turn his massive unpopularity into votes for Democratic candidates. The approach gained wider attention during the 2018 midterms, when Democrats, many wielding the new technology, managed to flip the House of Representatives. Now, as we approach the first presidential campaign in these startups’ brief histories, along with hundreds of down-ballot races that together will determine which party shapes the next decade of American politics, the question is: Can relational organizing give Democrats the edge they need?

Michael Luciani holds the dubious distinction of having worked as an organizer for the Hillary Clinton presidential campaign in Michigan. In 2016, unsure of exactly what his supervisors expected him to be doing, he experimented with his own approaches. He asked his volunteers to post pro-Hillary messages to Facebook. He also asked them to take the list of voters they were supposed to call and, instead of just starting at the top, look through their phone contacts and start by calling the people they actually knew. The results were intriguing, if intuitive: Voters were much more receptive to campaign calls when they came from someone familiar.

But when Luciani approached higher-ups about implementing his methods more widely, he was shut down. His supervisor was being judged by things the campaign could measure, like total phone calls made and doors knocked on. There was no way to get credit for having volunteers reach out to their personal networks. The existing campaign technology wasn’t designed to take advantage of relationships.

That, at any rate, is the origin story Luciani tells about the Tuesday Company, the project he and two friends developed in the rubble of the 2016 election. Their Team app, Luciani explained to me recently, fixes the problems he encountered in Michigan by allowing campaigns to “organize, coordinate, and measure volunteers’ relational communication and social media activity.” In other words, it turns the kind of informal conversations people are already having about politics into the medium of organized persuasion and turnout efforts. Volunteers upload their contacts—friends, family, coworkers, whomever—to the app. The campaign matches those contacts against their voter lists and tells the volunteers which ones to reach out to, when, and on what subject. Organizers can suggest scripted messaging, but the volunteers are ultimately in control of what they write, as well as what medium they use—texting, Facebook, Twitter DM, and so on. They can also use the app to post to social media in a way that allows the campaign to track engagement.

The basic idea is that a message will be more effective at getting you to vote if it comes from someone you know and trust than if it comes through a cold call or campaign advertisement. (In theory, the apps can be used to try to get people to back a particular candidate over another—but one of the firmest findings in political science is that turnout, while difficult, is much easier than persuasion, which is nearly impossible.) Along with the Tuesday Company, other key startups based on this premise include OutreachCircle (formerly called VoterCircle) and Outvote. All three have gotten funding from Higher Ground Labs, a progressive tech accelerator created by former Obama staffers in 2017.",Yes,"It has created an echo chamber. People tend to surround themselves with likeminded individuals and this creates a filter bubble. As a result, people are only exposed to information that they agree with and this ultimately reinforces their pre-existing beliefs.",https://www.wired.com/story/relational-organizing-apps-2020-campaign/,2020-01-22 13:00:00+00:00,LABEL_1_relevant,0.9924551248550416,Social Media,social_media,Wired,"Information, Discourse & Governance"
67,Nikola founder Trevor Milton indicted on three counts of fraud,"Trevor Milton, the fast-talking showman founder of Nikola and the electric truck startup’s former CEO and executive chairman, has been charged with three counts of fraud.

Milton “engaged in a fraudulent scheme to deceive retail investors” for his own personal benefit, according to the federal indictment unsealed by U.S. Attorney’s Office in Manhattan on Thursday. Milton was charged with two counts of securities fraud and wire fraud by a federal grand jury.

Specifically, prosecutors detailed in the complaint how Milton used social media and frequent appearances on television in a PR blitz that flooded “the market with false and misleading information about Nikola” before the company even produced a product.

The charges reflect a fast and furious run for Nikola and Milton, who founded the company in 2015. Milton received more attention after unveiling the first prototype and boasted that the company would produce “the iPhone of trucking.” Promises around other products, including an electric pickup truck called Badger, would soon follow, as well as plans to build a factory in Arizona.

In March 2020, the company announced it would go public via a merger with special purpose acquisition company VectoIQ Acquisition Corp. Milton frequently posted on Twitter, directing his messages to retail investors after the company went public that summer. Then, in September, just days after GM had announced a $2 billion investment in the company, noted short-seller Hindenburg Research accused Nikola of fraud. The U.S. Securities and Exchange Commission opened an inquiry in the matter and within two weeks Milton had stepped down as executive chairman.

Nikola issued a statement Thursday that distances itself from Milton, who is still its largest shareholder.",Yes,"It has given a platform to Trevor Milton, the founder of Nikola, to engage in a fraudulent scheme to deceive retail investors.",https://techcrunch.com/2021/07/29/nikola-founder-trevor-milton-indicted-on-three-counts-of-fraud/,2021-07-29 00:00:00,LABEL_1_relevant,0.9957571625709534,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
68,Thumbtack Tries Bridging the Benefits Gap for Gig Workers,"Even before the online gig economy existed, a simple truth defined life in the American workforce: full-time employees get a safety net—the benefits, the labor protections, the security—and everyone else goes without. Tech companies have revolutionized how people work in countless ways, but this benefits gap persists, especially among low-income workers. The question now is whether these platforms can also be part of the solution.

Thumbtack is a classic gig-economy success story: Founded in 2008, just in time for the recession, the company eventually reached a billion-dollar valuation by matching professionals like tutors and painters to locals in need of their services, with millions of users across all but two zip codes in the US (both, according to the company, are in Oklahoma). This week, Thumbtack announced a pilot partnership with Alia, a portable-benefits app, to help some of those pros access benefits like paid time off.

Alia launched in December as a stand-alone platform for house cleaners to purchase benefits that have traditionally been difficult for them to obtain. It was developed by the innovation lab of the National Domestic Workers Alliance, a nonprofit advocacy group, with funding from Google.org and direct input from domestic workers about the kinds of benefits they most want. Once registered on Alia, clients can set up a monthly contribution for their cleaners, which can then be redeemed for paid time off and, in some states, life, disability, accident, or critical illness insurance. While the tool is only for domestic workers right now, the NDWA wants to expand to other kinds of independent workers.

“We're early in our development. And we're excited to have a platform as large and well-known as Thumbtack to partner with us,” says Palak Shah, the NDWA’s social innovations director. During Alia’s beta testing period last year, Shah and her team were heartened by how eager customers were to support workers; according to the NDWA, the average contribution worked out to $5 per cleaning. Alia would not release numbers on how much money was raised or how many clients participated while the app was in beta, but Shah says it was significant.

Thumbtack says it found a similar eagerness while interviewing users who have hired professionals through its platform. “Our customers want to help and support,” says Marco Zappacosta, Thumbtack’s CEO. “So I think one of the big benefits of what Alia had done is it just made it dramatically easier for people to sort of express their thanks and participate in this program.” The pilot partnership is limited to cleaners in New York and California and is expected to last four to six months. If successful, it could be the first step toward opening Alia benefits up to all contract workers.

“It will be interesting,” Zappacosta says. “Do words turn into action? That’s the big question.”

That question could just as easily be put to Thumbtack’s brethren in Silicon Valley. Tech companies fueling the gig economy have come under intense scrutiny for the way they treat workers. Recent investigations revealed that delivery startups like Instacart and DoorDash, as well as Amazon Flex, were using tips to cover their base pay to contractors, essentially denying workers their tips and enabling the companies to pay lower wages; Instacart said it would stop doing that. In California, a state supreme court ruling last year has sparked an ongoing policy debate over how companies classify workers. Some startups have abandoned the independent contractor model altogether. A number of policymakers and advocates argue that worker protections have not kept up with the reality of making a living in the 21st century.

“The labor conditions that a lot of people are getting agitated and anxious about have always been the conditions of this workforce.” Palak Shah, National Domestic Workers Alliance

“Technology is changing everything that we know about domestic work,” says Shah, from how domestic workers find jobs, and on how many platforms, to how their reputations are formed (online reviews are the new word of mouth). As platforms become more powerful, they are able to influence wages---a growing problem some economists fear can lead to gig workers getting paid unsustainably low wages. And as pay decreases, so does a worker’s ability to buy their own benefits. (Professionals set their own prices for services on Thumbtack, which earns fees for generating new customer leads.)

“You can kind of think about domestic workers as a canary in the coal mine, so to speak,” Shah says. “The labor conditions that a lot of people are getting agitated and anxious about have always been the conditions of this workforce.”",Yes,"It has made it possible for people to connect with others who share their same views and opinions, which has led to the echo chamber effect.",https://www.wired.com/story/thumbtack-alia-benefits-gap-gig-workers/,2019-02-22 12:00:00+00:00,LABEL_1_relevant,0.7733929753303528,Social Media,social_media,Wired,"Information, Discourse & Governance"
69,Instagram rolls out new safety tools for parents,"Meta announced on Wednesday a new set of tools designed to protect young users, an overdue response to widespread criticism that the company doesn’t do enough to protect its most vulnerable users.

Parents, tech watchdogs and lawmakers alike have long called for the company to do more to keep teens safe on Instagram, which invites anyone older than 13 to sign up for an account.

To that end, Meta is introducing something it calls “Family Center,” a centralized hub of safety tools that parents will be able to tap into to control what kids can see and do across the company’s apps, starting with Instagram.

The new set of supervision features lends parents and guardians some crucial transparency into young users’ Instagram habits. The tools will allow parents to monitor how much time a kid spends on the app, be updated about accounts they’ve followed lately and who has followed them and receive notifications about any accounts they’ve reported.

Those tools will roll out today on Instagram in the U.S. and are on the way to Meta’s VR platform in May and the rest of Meta’s apps (remember Facebook?) some time in the coming months, including to global users. The company characterized the tools as the “first step in a longer-term journey,” though why it took so long to take these initial measures to protect teens from the unsavory side of its software is less clear.

For the time being, teenaged Instagram users will have to enable the safety tools from within their own accounts, though the company says the option for parents to initiate the supervision mode will be implemented by June. Instagram also plans to build out more controls, including a way for parents to restrict app usage to certain hours and a setting that would allow multiple parents to co-supervise an account.

Young and vulnerable

In the last year, Instagram has faced intense scrutiny over its lack of robust safety features designed to protect young users. It technically doesn’t allow anyone under the age of 13 to sign up for the app, though there are few obstacles preventing kids from using social media.

Instagram previously announced that it would apply AI and machine learning to the problem of making sure its users are 13 and older, but the company knows that children and tweens still easily find their way onto the app.

“While many people are honest about their age, we know that young people can lie about their date of birth,” the company wrote in a blog post last year. “We want to do more to stop this from happening, but verifying people’s age online is complex and something many in our industry are grappling with.”

The company argues that the challenge of weeding out young users is the reason it planned to build out a version of Instagram designed for kids, which BuzzFeed News revealed early last year. YouTube and TikTok both offer versions of their own social apps customized for children under 13, though Instagram’s own plans look a bit late to the party.

Last September, The Wall Street Journal published an investigative series reporting on the app’s negative impact on the mental health of teen girls, a scandal that hastened the (temporary?) end of Instagram for Kids. Meta went on to pause plans for Instagram for Kids in light of the WSJ reporting, public backlash and an aggressive, unusually bipartisan pushback from industry regulators.

Meta is also facing pressure from the competition. TikTok introduced its own tools to allow parents to monitor their kids’ app usage two years ago and updated those controls to be more granular since. The company launched its own kid-mode app “TikTok for Younger Users,” which restricts risky features like messaging and commenting, all the way back in 2019.",Yes,"The impact on the mental health of teen girls.

The Wall Street Journal published an investigative series last September that reported on the app’s negative impact on the mental health of teen girls, which hastened the (temporary?) end of Instagram for Kids. Meta went on to pause plans for Instagram for Kids in light of the WSJ",https://techcrunch.com/2022/03/16/instagram-rolls-out-new-safety-tools-for-parents/,2022-03-16 00:00:00,LABEL_1_relevant,0.99596905708313,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
70,The uneasy coexistence of Yandex and the Kremlin,"On a snowy afternoon in late February, just before the pandemic gripped Russia, I turned off of a busy Moscow street into a quiet courtyard. I was meeting Rostislav Meshchersky, the 28-year-old manager of one of Lavka’s so-called “dark stores,” the places where the products ordered online are discreetly warehoused for distribution. Meshchersky led me to an open garage door at the back of the courtyard, which led down into a basement lined with shelves filled with everything from pasta to fruit juice to toilet paper. “I joke with my friends that I know immediately where to go in Moscow in the event of the apocalypse,” he said.

Just weeks later, it wasn’t such a joke. In April, Lavka received some 900,000 orders from Russians stuck at home under quarantine, while customers of Yandex’s overall food services—restaurant delivery included—more than doubled. Although the company took a hit in businesses like ride-sharing when its entire fleet was taken off the streets during Russia’s lockdown, the people stuck at home boosted traffic on the company’s search and streaming video platforms.

But Yandex’s success has come at a price. The Kremlin has long viewed the internet as a battlefield in its escalating tensions with the West and has become increasingly concerned that a company like Yandex, with the heaps of data it has on Russian citizens, could one day fall into foreign hands.

This means running a tech giant in Russia is a delicate dance. On the one hand is the Kremlin; on the other is New York, with investors’ demands that the company maintain its independence. But in a pandemic-­stricken world increasingly concerned with protecting borders and regulating the tech industry, Yandex’s dilemma may not be just a Russian story.

A golden arrangement

Yandex—short for “yet another indexer”—didn’t always have its fingers in everything. After getting its start in 1997, the company for years vied for local search-engine supremacy with Rambler, another Russian company.

In the end, Rambler became the Yahoo to Yandex’s Google. But Google itself soon entered the market, and while Yandex had an edge by rooting its search algorithm in the particulars of the Russian language, its California rival began to catch up. “About half a year before Google went public, it made an offer to buy Yandex, and I have to say that we were looking at that offer very seriously,” Leonid Boguslavsky, one of the company’s first investors, told me.

The offer was made in 2003. But one of Yandex’s cofounders, Ilya Segalovich, said, “Let’s fight,” Boguslavsky recalled. Though Segalovich died in 2013 after a bout with stomach cancer, the fight continues to this day: while Google has periodically overtaken Yandex, the Russian firm currently has about 59% of Russian search traffic to Google’s 39%.

The same year Segalovich died, Yandex hired Greg Abovsky, a Ukraine-born, Harvard Business School–educated hedge fund analyst who got his start with Morgan Stanley in New York. “We had a realization right around the time I got here that search is going to slow down at some point,” says Abovsky, who now serves as both CFO and COO. When he joined, advertising from search accounted for around 99% of the company’s revenue. Today it’s about 64%, and total revenue grew from $1.2 billion in 2013 to $2.8 billion in 2019.

But as Yandex developed into the dominant player in the Russian tech market, it also inevitably came under the watchful eye of the authorities.

One of the first moments was in August 2008, when Russia fought a five-day war with neighboring Georgia. As the conflict played out, Yandex News featured Russian-language articles covering both sides of the divide. The next month, according to journalists Andrei Soldatov and Irina Borogan in their book The Red Web, two Kremlin officials visited Yandex’s headquarters. One was Vladislav Surkov, the deputy chief of Russia’s presidential administration—the man who coined the Orwellian term “sovereign democracy” to describe a Russian system of governance that brooks no foreign meddling in its affairs.

In 2008, when Russia fought a five-day war with Georgia, Yandex News featured Russian-language articles covering both sides of the divide. The next month, two Kremlin officials visited Yandex’s headquarters.

Lev Gershenzon, the director of Yandex News at the time, was given the task of explaining to the official visitors how the service worked. According to the book, he recalled showing screenshots of articles that the aggregator’s algorithm had selected as top stories. Surkov interrupted. “This is our enemy,” he said, pointing to a liberal outlet. “That’s what we do not need!”

The company promised from then on to maintain an open line to the Kremlin, though Gershenzon said he would always reiterate that an algorithm, not a person, chose the top news. Still, he didn’t always agree with how the line of communication was maintained.

“Volozh and I went to the presidential administration building several times and I said to him, ‘Listen, you have such a powerful business—why do you go to them? If it’s really needed, let them come to you,’” Gershenzon recalled in Holy War, a documentary miniseries about the Russian-language internet. “Even a geek like me knew that if you bend over for them they’ll never let you bend back upright again.”

That same year, Yandex fought off a potential takeover by Kremlin-linked oligarch Alisher Usmanov, who lobbied for President Dmitry Medvedev’s support on national security grounds. In 2009, to satisfy government interests, Yandex handed Russia’s largest lender, the state-owned Sberbank, a so-called golden share, which allowed the bank to veto transactions involving more than a quarter of Yandex’s stock. For a decade that arrangement appeased the Russian authorities—until it no longer did.

The tightrope walk

Last May, Russia passed a law to create a so-called “sovereign internet,” a state-owned communications infrastructure that would allow the country to cut itself off from the global internet while remaining online in a bubble of Russian-owned services. The law requires internet service providers to install equipment provided by the government for counteracting broadly defined “threats” to the internet’s stability and integrity, and gives the authorities sweeping powers to take control of the network if such threats appear. Over tea at his offices one afternoon last winter, Igor Ashmanov, who was the director of Yandex’s rival Rambler for a time in the early aughts and now is a proponent of the sovereign internet on state television and in government hearings, laid out its purpose.

“Imagine you live in a small village near a city that provides your electricity, and the mayor of the city has said that you are his enemies and that if he can harm you, he will,” Ashmanov told me. “You might decide to buy a generator to make sure your electricity keeps running in case this crazy mayor turns off the switch. This is what the sovereign internet is about.”

Perhaps more important to the Kremlin, the sovereign internet would also give Russia more control over what its own citizens can see online. In 2011 the Arab Spring, buoyed by social media, swept across the Middle East. That December, after Vladimir Putin announced he would run for president once again following an interim stint as prime minister, mass protests—planned on Facebook—rocked Russia. In the wake of the demonstrations, the Kremlin began to see foreign tech companies as tools used by other governments to meddle in its affairs. Putin himself vocalized those concerns at a press conference in 2014, when he described the internet as a “CIA project” and implied that Yandex itself had been “pressured” to include foreigners in its management and was registered overseas “not only for tax purposes but for other reasons.” (The parent company is incorporated in the Netherlands, and six of the 12 current board members are non-Russians, including John Boynton, the chairman, who is based in Massachusetts.)

That fear of foreign interference has only intensified over the years. During a government hearing on national security in 2018, Ashmanov described Facebook, Instagram, and Twitter as American weapons trained against Russia. “What the Americans could do with a company like Yandex in their hands is something I don’t even want to think about,” Ashmanov told me.

As the ground shifted under its feet, Yandex struggled to keep its balance, according to Boynton, the board chairman. “We’ve done everything we can to steer clear of politics,” he said in a phone interview. And yet, he added, the company found that it was increasingly getting “dragged into areas where we don’t necessarily want to be.”

Things came to a head on a Thursday morning in October 2018, when rumors leaked that Sberbank was in talks to buy up to a 30% stake in Yandex to protect the company from “potential trouble.” When trading opened up in New York, its shares plummeted 9.4%, losing over $1 billion in market value, over fears that the state lender could take control of the company. “That was the moment when we realized that there was something bigger afoot,” Boynton recalled.

The next day the company lost another $1 billion. At an emergency meeting that went into the early hours of Saturday, the Financial Times reported, Volozh decided not to pursue the Sberbank deal.

Yandex began talks with Putin’s administration over a new governance structure, but the pressure on it continued to intensify. In June 2019, a little-known lawmaker, Anton Gorelkin, introduced a bill to limit foreign ownership in companies that the Russian government deemed “significant information resources.” Outside investors would be allowed to own only 20% of such companies—a severe blow to Yandex, which had 85% of its shares trading on US markets. When the Kremlin came out in support of Gorelkin’s law a few months later, fears in New York wiped another $1.5 billion off Yandex’s valuation in a single day.

In November last year, after 13 months of grueling negotiations, Yandex announced a solution. It would hand over Sberbank’s golden share—that veto power over major transactions—to a newly formed “public interest foundation” with close government ties. The veto would also be beefed up to include deals and transactions relating to intellectual property or the transfer of Russian users’ data. Although the new foundation would have 11 seats on its board, only three would belong to Yandex; the rest would be divided up among influential business groups and state-affiliated universities. Perhaps most important from the Kremlin’s perspective, the new foundation would be able to block Yandex from entering into agreements with any foreign government.

That seemed to take the heat off. Gorelkin said he would take his law back to the drawing board. Days later, the Russian parliament passed a law requiring Russian tech to be automatically preloaded onto devices sold in Russia, a move that analysts calculated would boost Yandex’s valuation by $1.4 billion. A few weeks after that, Putin, who had criticized Yandex’s foreign ties a few years earlier, praised its projects with foreign partners and spoke positively of a closed-door meeting with its senior management.

Yet even if the Kremlin seems to have been appeased, not everyone is. Power in Russia’s government is split between rival groups, with Putin mediating between them. For the constituency known as the siloviki—officials with ties to law enforcement—the Yandex foundation was seen as a half-victory, says Tatiana Stanovaya, the founder of a political analysis site, R.Politik. “On the one hand, they see that Yandex is indirectly beholden to the government,” she says. “On the other hand, it’s purely technical. Yandex won’t just fulfill any and all demands. And if the confrontation with the West keeps escalating, [the authorities] may rethink this arrangement.”

When I spoke with Boynton last winter after the dust had settled, he was in a buoyant mood. But he also noted that things could quickly change again. “In Russia,” he said, “nothing is guaranteed.”

A template for Big Tech?

If the siloviki see Yandex as an unreliable collaborator, liberal critics see increasing signs that it is in the pocket of the authorities. In late February, for example, a policeman accused of planting drugs on an investigative reporter said he had found the journalist’s address by asking Yandex Taxi to provide it. Yandex responded that it always yields to requests by security services to “help save lives,” though Roskomsvoboda, an anti-­censorship group, pointed out that it is not always legally required to do so.

As the pandemic grew, questions about the company’s independence became only more pointed. In early April, news surfaced that Moscow authorities were considering surveilling foreign tourists via their cell-phone data once borders opened back up again—and that Yandex might develop the tool. The company denied the claim.

Yandex CEO Arkady Volozh (left) with President Vladimir Putin, who was visiting Yandex’s offices in 2017 to mark its 20th anniversary. ITARTASS NEWS AGENCY/ALAMY STOCK PHOTO

Then, when critical comments from opposition activists began popping up next to government buildings in Yandex Navigator, as a sort of digital alternative to street protests, Yandex deleted the messages, saying they were off-topic. Finally, one evening in late April, some internet users noticed that searches on Yandex for opposition leader Alexei Navalny were returning mostly negative content. Yandex apologized, saying that it was an “experiment” shown only to a small number of users. One Russian commentator, Alexander Plushev, noted that such testing is common on all tech platforms, but he added: “Any incident with Yandex is now interpreted through the prism of its control by the authorities.”

If Yandex capitulates too much to state control, it risks losing its most prized asset: its talent. “I always say that my main competitors are [Moscow airports] Sheremetyevo and Domodedovo,” says Misha Bilenko, who heads Yandex’s Machine Intelligence and Research division.

In February, a policeman accused of planting drugs on an investigative reporter said he had found the journalist’s address by asking Yandex Taxi to provide it.

Bilenko himself spent 23 years in the United States, including a decade at Microsoft, before returning to Russia several years ago. What drew him back, he says, was the access to so many different resources within Yandex and the opportunity to help improve the lives of Russians en masse. But as one employee who asked to speak anonymously told me, Yandex would lose that type of draw and power if the government tried too hard to tame it. “We have a lot of progressive people here,” the person said. “If we don’t like what we see, we’ll leave.”

Today Yandex, at least publicly, is claiming that all is well. Its concessions to the Kremlin could have been much bigger. They’re also ones that others may soon consider. “What Yandex has done isn’t only relevant within the context of Putin’s Russia,” Bloomberg columnist Leonid Bershidsky argued last year. “It could be seen as a template for Big Tech.”

Like Yandex, Bershidsky continued, companies such as Google or Facebook could set up quasi-­autonomous governance structures with the right to veto certain decisions. “If such a structure can win approval even from an authoritarian regime such as the Russian one … it could probably satisfy most Big Tech critics in democracies, too,” he wrote.

Indeed, in May of this year Facebook named the first members of its “oversight board” as a response to anger over its opaque content moderation process. The body is stacked with legal and human rights luminaries who can review and overturn some of the platform’s decisions. Though the board has nothing like the power of Yandex’s public interest foundation, it was a big concession from a company that has always fiercely defended its control over what goes on its platform.

With politicians on both ends of the US political spectrum calling for increasing regulation of Big Tech, such moves are likely to keep happening. The kind of flexibility Yandex has had to learn may prove essential for companies that want to not only survive but flourish.",Yes,"It has allowed for the rise of ""echo chambers,"" or areas where people with similar beliefs can share and amplify their views without hearing opposing ones. This has led to increased political polarization, as people are only exposed to information that reinforces their existing beliefs.",https://www.technologyreview.com/2020/08/19/1006438/yandex-putin-arkady-volozh-kremlin/,2020-08-19 00:00:00,LABEL_1_relevant,0.9607166647911072,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
71,Trump’s Executive Orders Hurt More Than TikTok and WeChat,"In another move destined to inflame tensions with the Chinese government, the Trump administration took aim Thursday at TikTok and WeChat. Citing national security concerns, the president signed a pair of executive orders that bar Americans or anyone subject to US jurisdiction from doing business with the social media apps. The orders take effect in 45 days, seemingly giving TikTok leeway to be acquired by Microsoft, which has said it is in talks to buy parts of the video platform from its parent company ByteDance.

While vaguely worded, the orders could have far-reaching consequences, including for the future of America’s relatively free and open internet. WeChat, owned by China’s Tencent, is used by millions of people in the US to maintain personal and business relationships with people in China. Many American technology firms also have customers in the country, and could be hurt by any response from Beijing. “The US business community is really concerned. I mean, who would buy an Apple phone in China if you can't use WeChat on it?"" says Paul Triolo, an expert in global technology policy at Eurasia Group, a think tank.

In a statement, TikTok said it was “shocked” by the orders. The company has repeatedly insisted that it’s not beholden to the Chinese government, and it argues the Trump administration has failed to provide evidence that the app poses a risk to US citizens. “For nearly a year, we have sought to engage with the US government in good faith to provide a constructive solution to the concerns that have been expressed,” TikTok said. “What we encountered instead was that the administration paid no attention to facts, dictated terms of an agreement without going through standard legal processes, and tried to insert itself into negotiations between private businesses.” TikTok, which has tens of millions of users in the US, began attracting the attention of lawmakers last fall, who expressed worries about its ties to Beijing.

A spokesperson for Tencent said the company was “reviewing the order to get more understanding.” WeChat has over 1 billion users, most of them in China, who rely on the app for everything from messaging to paying for coffee and booking doctor’s appointments. In the US, it’s an important way for immigrants and students to reach relatives and friends back home. “If this happens, and it really becomes impossible to use WeChat here, that will be a major factor for Chinese people considering studying here, visiting here, doing anything here,” says Jeremy Goldkorn, editor-in-chief of SupChina and cofounder of the China affairs podcast Sinica.

“The US business community is really concerned. I mean, who would buy an Apple phone in China if you can't use WeChat on it?"" Paul Triolo, Eurasia Group

At the same time, WeChat is strictly censored and surveilled, including abroad, and has become an avenue for the Chinese Communist party to oppress minority groups like Uighur Muslims. Yaqiu Wang, a researcher at Human Rights Watch, says she and other critics of the Chinese government were already moving off of WeChat to private networks and encrypted apps such as Signal before Trump’s order. “There are ways to communicate freely and securely with people in China, for now,” she says. “They are cumbersome, but then you are not subjected to the Chinese government’s censorship and surveillance.”

Tencent also owns Riot Games, maker of the popular League of Legends, and owns a large stake in Epic Games, maker of Fortnite. But a White House official told the Los Angeles Times that the order does not affect the games.

China has long blocked overseas competitors to WeChat like Facebook and Twitter. “The Chinese government is the one that is actively in the banning business, and they’ve got a big head start on the Trump administration,” Donald Clarke, a Chinese law specialist at George Washington University wrote in a blog post.",Yes,"It can lead to addiction. People can get addicted to social media because it provides a dopamine release, which can be addictive. Additionally, social media can be addictive because it is a way to escape from reality and into a fantasy world.",https://www.wired.com/story/trumps-executive-orders-hurt-more-tiktok-wechat/,2020-08-07 20:00:19.122000+00:00,LABEL_1_relevant,0.8975914716720581,Social Media,social_media,Wired,Health & Wellbeing
72,A new social-media platform wants to enforce “kindness.” Can that ever work?,"The policy has already invited some criticism from onlookers who feel it might endanger women and marginalized communities.

“There’s a widely held belief that if people use their legal names they will behave better in social environments because other people can identify them and there can be social consequences for their actions,” says J. Nathan Matias, a professor and founder of the Citizens and Technology Lab at Cornell University. “While there was some early evidence for this in the 1980s, that evidence wasn’t with a diverse group of people, and it didn’t account for what we now see the internet has become.”

In fact, many people who use pseudonyms are from more marginalized or vulnerable groups and do so to keep themselves safe from online harassment and doxxing.

Doxxing—using identifying information online to harass and threaten—is a life-threatening issue for many marginalized people and has soured them on social media like Facebook and Twitter, which has come under fire for not protecting at-risk individuals (though both Twitter and Facebook have tried to make amends of late by removing tweets promoting misinformation, for example). Estévez says that while she’s “very empathetic about this” and that trans people can identify themselves by their chosen name and pronouns, phone-name verification and the app’s invite-only structure were necessary to prevent abusive behavior.

Courtesy Telepath

Estévez, who spent years as a volunteer moderator at Quora before leading the moderation team, imported Quora’s community guideline of “Be Nice, Be Respectful” to Telepath. “People respond well to being treated well,” she says. “People are happier. People are really attracted to kind communities and places where they can have their say and not feel ridiculed.”

Telepath’s team has been very deliberate with its language about this concept. “We made the decision to use the word ‘kindness’ instead of ‘civil,’” Bodnick says. “Civil implies a rule you can get to the edge of and not break, like you were ‘just being curious’ or ‘just asking questions.’ We think kindness is a good way of describing good intent, giving each other the benefit of the doubt, not engaging in personal attacks. We hope it’s the strength to make these assessments that attracts people.”

Historically, platforms have been reluctant to enforce basic user safety, let alone kindness, says Danielle Citron, a law professor at Boston University Law School who has written about content moderation and advised social-media platforms. “Niceness is not a bad idea,” Citron says.",Yes,"It can spread misinformation quickly and easily, which can lead to panic and chaos.

In addition, social media can be addictive and can cause people to spend too much time on their phones or computers, which can lead to social isolation.",https://www.technologyreview.com/2020/10/07/1009499/telepath-wants-to-enforce-kindness-can-that-ever-work/,2020-10-07 00:00:00,LABEL_1_relevant,0.9523326754570008,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
73,Online “auctions” of women are just the latest attacks on Muslims in India,"The slow action is part of a larger pattern, says Meenakshi Ganguly, South Asia director at Human Rights Watch. Authorities are quick to accuse government critics, she says, but “hate speech and violent actions by government supporters are seldom prosecuted.”

Social media companies, which have the ability to take down offensive posts and stem misinformation, are not filling the void. “Tech companies take down content based on their community guidelines and local laws. In this case both were violated,” says Krishnesh Bapat, a Centre for Communication Governance Digital Fellow at the Internet Freedom Foundation in Delhi. “GitHub, to the best of my knowledge, does not proactively take down content. It does so only after it receives a complaint and took longer in this case.” GitHub did not respond to a request for comment about its policies.

In India almost all forms of online harassment fall under the general category of cyberbullying. India’s Information Technology Act, 2000, commonly known as the Cyber Law, governs online abuse. The act was intended to address e-commerce but was adjusted in 2008 to cover cybercrimes as well. Harassment can also fall under the country’s overall penal code, says Nappinai, which can help protect victims in serious cases.

Nevertheless, some say the country’s online laws need revision. Anushka Jain, a lawyer with the Internet Freedom Foundation, believes the digital world has changed too much for the law to be effective. “Some of the provisions of the [Cyber] Act have become redundant and incapable of addressing the currently persisting issues and rapidly evolving changes and threats,” she says. The government, she adds, needs a holistic approach to cyber policy, including stricter laws.

In addition to harassment, Muslims in India are also struggling with misinformation online. For example, last September, ID Fresh, a halal-certified food products company owned by a Muslim family, faced a large-scale misinformation campaign on social media claiming that the company mixes cow bones and calf rennet to increase the volume of ready-to-cook batter and urging “every single Hindu” to avoid the products. The company faced a boycott and saw its sales drop; it had to launch its own campaign in response to set the record straight.

So far, there seems to be little movement to change the situation from either tech companies or the Indian government. That has left little remedy for victims like commercial pilot Hana Mohsin Khan, who took to Twitter to express her anger when she saw her picture in the January auction. “Muslim women were yet again targeted. Yet again there will be no action,” she wrote. “We are caught in a never ending cycle of anger and anguish. Every. Single. Day.”

Safina Nabi is an independent multimedia journalist from South Asia based in Kashmir.",Yes,It can cause people to be harassed.,https://www.technologyreview.com/2022/02/21/1046052/online-auctions-muslim-women-india/,2022-02-21 00:00:00,LABEL_1_relevant,0.995868682861328,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
74,How Facebook Groups Became a Bizarre Bazaar for Elephant Tusks,"Mark Zuckerberg had already been testifying for four hours in the stuffy, wood-paneled room where the House Energy and Commerce Committee held its hearing on Facebook in April, when he got a question he seemed wholly unprepared to answer. Over the course of those four hours, his company had been blamed for enabling the opioid crisis, silencing conservative voices, violating users’ rights to privacy, and setting up a modern day surveillance state akin to J. Edgar Hoover’s COINTELPRO program. It seemed there was no societal ill too grave or niche that Congress couldn’t lay at Facebook’s feet.

Still, Zuckerberg appeared surprised when Georgia representative Buddy Carter asked him about the elephants. “Did you know that there are some conservation groups that assert that there's so much ivory being sold on Facebook that it's literally contributing to the extinction of the elephant species?” Carter asked.

“Congressman, I have not heard that,” Zuckerberg replied.

The timing wasn’t entirely random. Peters and Kohn didn’t initially plan on going public with their complaint. But when Zuckerberg reluctantly agreed to testify before two congressional hearings, they couldn’t pass up the chance to get a question in with all the world watching. So they spoke to the Associated Press, which published a news story about the SEC complaint the same week Zuckerberg was set to testify. Carter seemed to be citing Peters’ own words.

Zuckerberg’s answer or lack thereof revealed just how far down Facebook's to-do list wildlife trafficking ranks. Zuckerberg has repeatedly used a rose-colored glasses defense about disturbing content on Facebook, copping to the fact that he focused too much, for too long, on all the good Facebook has done—while all but ignoring the bad. But that argument is tough to reconcile with the fact that Facebook’s own internal guidelines ban so very many bad things, from cannibalism to infant abuse. Indeed, Facebook is so accustomed to grotesque content appearing on the site, it offers content moderators counseling to help them deal with the horrors they’ve seen.

Facebook’s leaders aren’t unaware of the ugliness that lurks just beyond every pretty selfie. It’s just that, armed with the immunities enshrined in the United States legal code, the company’s never had to do anything about it. “Mark Zuckerberg goes around saying they’re an idealistic company, well I’m idealistic too. We’re trying to save elephants from extinction,” Peters says. “I want to see this firm put some weight behind the idealism and optimism they talk about.”

That’s starting to happen. When news broke last fall that the company had sold thousands of political ads to Russian propagandists in the runup to the 2016 election, Facebook announced it would double its content moderation team to 20,000 people, impose strict verification processes for political advertisers, and create massive repositories of political ads, complete with information on how much they cost, who paid for them, and what demographics they reached.

This spring, when the world found out that a data firm called Cambridge Analytica had amassed data on as many as 87 million Americans without their permission through a silly third-party quiz app, Facebook radically limited app developers’ data access and announced it would be auditing how all apps use Facebook user data, even if it means hiring thousands more people. As Facebook executives recently explained to WIRED, the company has also overhauled its News Feed algorithms to reduce people’s exposure to fake news and outlets it deems untrustworthy. It has also answered accusations of liberal political bias by inviting the conservative Heritage Foundation and others to study the company from the inside out.",Yes,It can be used to spread false information or to interfere in the political process of other countries.,https://www.wired.com/story/tusks-horns-and-claws-inside-the-fight-to-destroy-the-animal-parts-bazaar-on-facebook/,2018-06-05 11:00:00+00:00,LABEL_1_relevant,0.9956246018409728,Social Media,social_media,Wired,Politics
75,How Social Media Became a Pink-Collar Job,"Companies hiring for technical positions often slip language into their job postings that appeals to men. They say they’re looking for “ninjas,” who seek to “obliterate competition,” and are capable of “dominating.” By now, these wordings are a well understood form of bias that produces more male candidates than female.

But one job in the digital economy falls predominantly to women. It’s an oft-overlooked position, drawing on both marketing and editorial skills, that has become increasingly critical both to business success and online discourse. The pay is poor, and the respect can be limited. Take a look at the job posting for any social media manager. You’ll discover the same bias in its language, in reverse: a bias for sourcing female candidates.

By now, these wordings are a well understood form of bias that produces more male candidates than female.

Social media managers are “the behind-the-screens labor involved in media and technology, central to propelling our digital economy forward,” says Brooke Erin Duffy, who is an Assistant Professor in Communications at Cornell. Between 70 and 80 percent of social media workers self-identify as women on the salary compilation site Payscale. The career has been referred to as the Pink Ghetto.

According to a study, published by Duffy and University of Oxford researcher Becca Schwartz in New Media & Society and slated for a print release early next year, companies create this diversity gap by advertising social media as “women’s work”—at the same time as they routinely undervalue it. Duffy and Schwartz studied 150 job postings to determine how businesses recruit social media specialists. These companies, which included BuzzFeed, Equinox and Thrillist, advertised jobs that called for applicants to be sociable, exhibit deft emotional management and be flexible--all traits that Duffy says are typically associated with women.

The feminized nature of social media employment, Duffy and Schwartz argue, is connected to its “characteristic invisibility, lower pay, and marginal status” within the tech industry. The pair cites statistics from Payscale that place average pay for a social media specialist at $41,000. But that’s for staff jobs. Duffy, who last year published the book (Not) Getting Paid to Do What You Love: Gender, Social Media, and Aspirational Work, has been tracking this field for awhile. This spring, she conducted an additional 25 interviews with social media managers to better understand the dynamics of the job. Most social media jobs, she says, are contract positions; the ghost-tweeters responsible for upholding a brand’s social persona, for example, may be balancing two or three clients at a time.

In the job descriptions Duffy and Schwartz studied, which included both entry-level postings and calls for internships, companies often made the jobs sound like non-work: fun hobbies for which people just happened to get paid. (Or, occasionally paid. Many of the internships were offered without pay, or for school credit.) Postings referred to the job as sociable, blurring the boundaries between work and play. Perks on offer included everything from discounts on classes at Equinox to LaCroix and free massages at the digital media company Ranker. “The assumption was that these jobs were extensions of what people would be doing for fun anyway,” says Duffy.

“The assumption was that these jobs were extensions of what people would be doing for fun anyway""

Duffy notes that social media specialists’ roles are not simply to steward a brand’s presence on social media, but to act as a personal round-the-clock ambassador for the brand. Companies sought out workers who had active social followings already, and could prove they use many different services, from Twitter to Instagram to Pinterest, regularly. For these workers, tech addiction or obsession was not pathologized, but in fact “bound up with notions of the idealized worker,” according to the study. Candidates were encouraged to be always online--and passionate personally about the brands for which they worked. Companies sought workers who expressed social allegiance: Candidates were expected to show a “passion for travel and [The Points Guy] brand,” or a “deep passion for the UrbanDaddy brand and lifestyle.”

At the same time, their true identities go unrecognized. Unlike journalists, social media managers have no byline. They don’t reveal who they are when tweeting under a brand’s handle or posting to Pinterest. In that way, social media workers are a digital version of public relations professionals, an often low-status woman-dominated role within corporate America. Social media managers usually command less respect than PR managers, while taking on responsibility for an increasingly important distribution channel. Strategic use of social media has been credited for influencing elections, harnessed to transform fledgling startups into billion-dollar companies, and used as a form of warfare. But this influence doesn’t translate into a higher paycheck or more internal power.",Yes,It makes people more polarized in their views.,https://www.wired.com/story/how-social-media-became-a-pink-collar-job/,2018-05-26 11:00:00+00:00,LABEL_1_relevant,0.9942277669906616,Social Media,social_media,Wired,"Information, Discourse & Governance"
76,'Gaymers' Are Taking Brazil by Storm,"During a livestream, with an LGBTQ+ flag in the background, blue hair, and pop songs boosting his moves, Alcides Furlin, 29, shoots his enemies away. The Brazilian engineer and streamer describes himself as a “nerd boy, an acid drag” and tries to create content that merges the gay and gaming communities.

This approach is becoming a trend among a subset of Brazilian streamers—call them entertainers, artists, performers, or simply The Gaymers.

Their streams have many things in common: witty captions, vibrant backgrounds, silly rules that steer the live chat, and looks that are sometimes boyish, sometimes extravagant, with colorful wigs and expressive makeup. Watching them for the first time feels strangely familiar, like listening to your best friends ramble on about everyday life, dramas, crushes … and games.

“Roughly, a gaymer is a person who identifies as LGBTQ+ and actively participates in the gaming community,” explains Lucas Goulart, a social psychology doctor at the Federal University of Rio Grande do Sul. Goulart specializes in the intersection between queer culture and video games, and, according to him, gaymers are also caught between what can be seen as contrasting worlds. “It’s hard because sometimes they don’t see themselves in either community,” he tells WIRED.

There are stereotypes surrounding gaymers, says Goulart, mainly, that a gay man who is into fashion and partying cannot be a ""geek gamer,"" someone who only thinks about video games, as he puts it. But Brazil's gaymers are pushing past these stereotypes to create something new. And since many of their livestreams have over 10,000 active viewers, it is safe to say their audiences value personalities above dichotomies.

Furlin knows that people love his drag queen persona, Lola Dvil. “People respond to transparency,” he says. “I need to be smart about how I approach things because I’m not talking to native gamers, but I’m committed to showing the truth, whether that means admitting difficulties getting an eye-shadow right or being honest about having a hard time to pass levels in a game.”

His awareness pays off. With over 8,000 followers on Twitch, Lola is a part of many LGBTQ+ gaming initiatives, such as the international Stream Queens and the Brazilian Project Fierce. Both work as streaming communities and show a variety of content produced by gaymers. And besides participating in collective projects, Lola’s personal calendar is packed. She livestreams five days a week, anywhere from two to 10 hours, doing everything from playing League of Legends to offering beauty tutorials.

Twitch in Brazil

Twitch saw explosive growth during the pandemic—hitting the mark of 18.6 billion hours of viewed content in 2020. In Brazil, as of July last year, Twitch ushered in new rates for subscriptions: It now costs just R$7.90 ($1.50)—a value 65 percent lower than the original rate in the country.

The move is part of a global initiative that aims to provide greater accessibility in regions where the platform is popular. And in this game, popularity comes with a price. For some time now, Brazilian Twitch can be seen as a good indicator of political tensions and shifts.

In May, Jair Renan, son of Brazilian president Jair Bolsonaro, was banned from the platform after spreading Covid-19 misinformation and encouraging gamers to break social isolation. His father is also having a hard time communicating with the gaming crowd—once a solid foundation among his voters.",Yes,"It creates echo chambers. This is where people only see information that reinforces their current beliefs. This can lead to confirmation bias, which is the tendency to search for, interpret, favor, and recall information that confirms one's preexisting beliefs or hypotheses.",https://www.wired.com/story/brazilian-twitch-streamers-drag/,2022-01-08 14:00:00+00:00,LABEL_1_relevant,0.9958154559135436,Social Media,social_media,Wired,"Information, Discourse & Governance"
77,What Spammers Could Do With Your Hacked Facebook Data,"When Facebook announced at the end of September that it had suffered a data breach that ultimately affected 30 million accounts, it seemed, perhaps, like the work of sophisticated nation state hackers. But a new report from The Wall Street Journal suggests spammers as the culprit instead. That shouldn't make you feel that much better, though, given just how much damage criminals can do with the kind of information stolen from Facebook.

It was, after all, a lot. The sophisticated daisy chain attack that the hackers pulled off garnered the names, phone numbers, and email of 15 million Facebook users. Fourteen million more had their username, date of birth, gender, devices they used Facebook on, and language settings compromised at the very least. Hackers could also have gleaned relationship status, religion, hometown, current city, work, and education info, depending on how fully victims had filled out their profile, along with the 10 most recent locations they checked into or were tagged in, and their 15 most recent Facebook searches. (Here's how to find out if you were affected, and how badly.)

All of which becomes particularly dangerous in the hands of spammers.

""Having accurate, detailed data, and a large amount of data, makes spamming campaigns more profitable,"" says Jérôme Segura, lead malware intelligence analyst at the network defense firm Malwarebytes. ""And this Facebook data is very unique. It has a lot of value, because it's from people supplying the information genuinely and saying 'I checked in at this hotel or here are some of my interests.' It's a priceless database trove for marketers.""

For now, Facebook won't weigh in publicly on who was behind the attack. Guy Rosen, the social network's vice president of product management said repeatedly in a call with reporters last week that, “The FBI is actively investigating and have asked us not to discuss who may be behind this attack.” The company reiterated this to WIRED on Friday.

The possibility that scammers were behind the theft, though, highlights the ways in which centralized data repositories like email accounts and social media profiles are potential gold mines for—and frequent targets of—phishers, spammers, and shady marketers.

""It's a priceless database trove for marketers."" Jérôme Segura, Malwarebytes

Granular data helps spammers craft maximally convincing emails, SMS messages, and calls. The data not only helps improve the general verisimilitude of broad spam campaigns, but also makes it easier to specifically tailor scams to individuals. For example, in one popular scam, an email threatens to release compromising photos of you, and uses information like your old passwords and your phone number to make it seem like the attacker really does have dirt. The more credible they seem, the more likely you are to pay them off. If you were compromised in the Facebook hack, they now also potentially know where you live, where you've worked, and where you've been.

Attackers can use that sort of detailed information in all sorts of other ways, as well. Segura points out that a trove like the one stolen from Facebook would be valuable for launching massive malvertising campaigns that try to entice web users to click on malicious ads, since it contains so many indicators of a person's background and preferences. And having such granular data about people would enrich all sorts of phishing attacks and so-called ""business email compromise"" scams, in which attackers try to gain access to email accounts within a business to gain credibility, and then influence malicious activity like payments to the attacker. You're a lot more likely to think an email is really from your boss if she's referencing your upcoming birthday, and the work trip you went on to Cleveland in the fall. Phishers and BEC scammers could also use details from the breach to send convincing messages externally, posing as a company's client, for example, or a disgruntled customer.

And then there's the matter of stolen identities.",Yes,"A lot of people get their feelings hurt.

When people get their feelings hurt, they often seek revenge.

Revenge is a powerful motivator, and people have been known to do some pretty crazy things when they're feeling vengeful.

So, if someone's feelings get hurt on social media, and they decide",https://www.wired.com/story/facebook-hack-data-spammers/,2018-10-19 15:55:59.693000+00:00,LABEL_1_relevant,0.99623304605484,Social Media,social_media,Wired,Social Norms & Relationships
78,Australia’s new law threatens jail if social-media firms don’t remove violent content,"The new legislation means social-media executives could face up to three years in jail, or be fined up to 10% of their company’s annual revenue, if they fail to take down violent content in an “expeditious” manner.

Specifically: The bill defines “abhorrent violent content” as terrorism, murder, attempted murder, torture, rape, and kidnapping. It’s less clear how quickly this sort of content is expected to be removed. The law says companies must take it down “within a reasonable time after becoming aware of the existence.” There are certain exemptions for purposes like law enforcement, court proceedings, artistic work, and journalism.

A reaction: The legislation is being enacted as part of the response to the live-streamed killing of 50 people in two mosques by an Australian gunman in Christchurch, New Zealand, last month. The live video was reportedly up for 20 minutes, but the recording was reposted on YouTube and spread elsewhere online. Facebook said it removed 1.5 million copies over the subsequent 24 hours. Australia’s attorney general, Christian Porter, described the bill as a “world first” and said he hopes other countries will adopt similar laws.

Criticisms: Some lawmakers said the bill had been rushed to get it through ahead of upcoming elections in May. Others said it was ill-conceived. “This bill is clumsy and flawed in many respects,” said Mark Dreyfus, a Labor Party member. He raised concerns that it could force social-media companies to conduct “proactive surveillance” to stay compliant.

Sign up here for our daily newsletter The Download to get your dose of the latest must-read news from the world of emerging tech.",Yes,"It creates an echo chamber where people only see content that reaffirms their existing beliefs. This can lead to people becoming more polarized and less willing to compromise or listen to opposing viewpoints. Additionally, social media can be addictive and lead to people spending too much time on their devices instead of interacting with the people and world around them.",https://www.technologyreview.com/2019/04/04/136196/australias-new-law-threatens-jail-if-social-media-firms-dont-remove-violent/,2019-04-04 00:00:00,LABEL_1_relevant,0.9962263107299804,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
79,Facebook Wants to Take Over TV,"To that end, an announcement last week ushered in a decision to start pushing longer videos into the News Feed. Longer videos, after all, provide more space for ads. But it seems that the news also foreshadowed a larger vision: that the social network doesn’t just like video—it has eyes on taking over TV, too.

On Wednesday, Mark Zuckerberg described how he plans for Facebook to become the place where “when people want to watch videos or want to keep up to date with what’s going on with their favorite show, or what’s going on with a public figure that they want to follow, that they can come to Facebook.”

He then went even further, adding that he would like Facebook to become a place where “you want to watch videos, you want to keep up with the content that you watch episodically week over week.”

In other words: Facebook wants to become a genuine alternative to TV.

That will take a while. Zuckerberg himself also noted that “people will experiment with longer forms of video,” but added that, for now, shorter clips will remain “the primary focus.” TechCrunch suggests you could think of it as more of a YouTube than a Netflix—paying content producers to create shorter-format video, with the occasional longer item, rather than cranking out original feature-length shows. For the time being, anyway.

Not convinced? Well, it’s also rumored that the company is planning to build its own TV apps. The Wall Street Journal (paywall) reports that the company is “developing a video-centric app for television set-top boxes,” like Roku and Apple TV. If that’s true, Facebook will not only be gathering a large reservoir of content that people can share with friends—it’ll also give them an easy-to-reach faucet from which to drink when they sit down in front of the television.

After years of protestation, Zuckerberg last month gestured toward the fact that Facebook may, kind of, perhaps be a media organization. “Facebook is a new kind of platform,” he said in a video. “It’s not a traditional technology company. It’s not a traditional media company.” Not a traditional one, perhaps, Mark. But if you get into the TV game, that will make Facebook a media company all the same.

(Read more: Recode, TechCrunch, Wall Street Journal, “Facebook at a Crossroads,” “Facebook Now Lets You Live-Stream Video From Your iPhone”)",Yes,It can lead to isolation and loneliness.,https://www.technologyreview.com/2017/02/02/5698/facebook-wants-to-take-over-tv/,2017-02-02 00:00:00,LABEL_1_relevant,0.9947996139526368,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
80,When to Reply on Social Media—and When to Not,"I have a tenet that I follow when it comes to social media conflagrations: Don’t add your air to someone else’s fire.

This rule has saved my butt multiple times. For example, during one social media snafu, a writer responded to a post I made of an article I’d written, saying she wanted to discuss our opposing views—in a Facebook forum of thousands of people. The wording and tone of her comment showed she wasn’t interested in a real dialog, so I didn’t respond. Had I agreed to the request or made a snarky comment like “Get your own damn articles published,” I would have been following her playbook to gain attention for herself and undermine me and my work. Should I have done something else? I figured I’d check with the experts.

“You did the right thing by not responding,” says Michele Borba, an educational psychologist and author of Thrivers. “No response is a great response, and often the most powerful response. The person wants the attention, and you are not giving it to them. She clearly wanted to use and undermine you by hijacking your platform. If you shamed her, you would have lost credibility and would be in a position of defending yourself.”

Sameer Hinduja, codirector of the Cyberbullying Research Center agrees, and says, “Whenever we respond to someone trying to insult us, we show we deeply care about their opinion. And then we’ve given them the power to invalidate us.”

Not responding on social media can be the best way of showing strength, rather than lending your voice and energy to the noise. In fact, research published in the journal Psychological Science shows that firing up the keyboard isn’t nearly as effective as speaking to someone one-on-one or sharing visuals. Of course, that works best if you have a real relationship or care about what the person thinks of you. “If it is someone who isn’t really in your life, then what you did was right,” says Ulash Dunlap, a San Francisco–based therapist. “If it’s an important relationship, I suggest you message the person and ask for a phone call to avoid miscommunication.”

Dunlap also recommends taking five minutes and assessing the situation before responding, and avoiding knee-jerk reactions on social media so people can’t see that they’ve pushed your buttons. “If someone is devaluing you or bullying you over your beliefs, or looking to make themselves right and you wrong, or looking for fame through you, then end the conversation, either by not responding or even saying, ‘Thank you for your feedback,’ similar to how corporations respond when criticized.”

So how do we keep ourselves from feeling disempowered when these situations arise? “Remember, if they don’t know you well, the person on the other end doesn’t understand you or your lived experiences. They don’t have the backstory,” says Dunlap. This might also be a person who loves to win. “You can go through the person’s Facebook or Twitter feed, and you will see it. If they are that way, find an exit strategy and end the conversation.”",Yes,"It can give people a platform to bully and attack others. The best way to deal with these situations is to not respond, which can be the best way of showing strength.",https://www.wired.com/story/when-to-reply-on-social-media-and-when-to-not/,2021-05-26 13:00:00+00:00,LABEL_1_relevant,0.9897449612617492,Social Media,social_media,Wired,"Information, Discourse & Governance"
81,"For Trump and Facebook, judgment day is around the corner","For Trump and Facebook, judgment day is around the corner Facebook's experimental Oversight Board is on the cusp of its biggest decision

Facebook unceremoniously confiscated Trump’s biggest social media megaphone months ago, but the former president might be poised to snatch it back.

Facebook’s Oversight Board, an external Supreme Court-like policy decision making group, will either restore Trump’s Facebook privileges or banish him forever on Wednesday. Whatever happens, it’s a huge moment for Facebook’s nascent experiment in outsourcing hard content moderation calls to an elite group of global thinkers, academics and political figures and allowing them to set precedents that could shape the world’s biggest social networks for years to come.

Facebook CEO Mark Zuckerberg announced Trump’s suspension from Facebook in the immediate aftermath of the Capitol attack. It was initially a temporary suspension, but two weeks later Facebook said that the decision would be sent to the Oversight Board. “We believe the risks of allowing the President to continue to use our service during this period are simply too great,” Facebook CEO Mark Zuckerberg wrote in January.

Facebook’s VP of Global Affairs Nick Clegg, a former British politician, expressed hope that the board would back the company’s own conclusions, calling Trump’s suspension an “unprecedented set of events which called for unprecedented action.”

Trump inflamed tensions and incited violence on January 6, but that incident wasn’t without precedent. In the aftermath of the murder of George Floyd, an unarmed Black man killed by Minneapolis police, President Trump ominously declared on social media “when the looting starts, the shooting starts,” a threat of imminent violence with racist roots that Facebook declined to take action against, prompting internal protests at the company.

The former president skirted or crossed the line with Facebook any number of times over his four years in office, but the platform stood steadfastly behind a maxim that all speech was good speech, even as other social networks grew more squeamish.

In a dramatic address in late 2019, Zuckerberg evoked Martin Luther King Jr. as he defended Facebook’s anything goes approach. “In times of social turmoil, our impulse is often to pull back on free expression,” Zuckerberg said. “We want the progress that comes from free expression, but not the tension.” King’s daughter strenuously objected.

A little over a year later, with all of Facebook’s peers doing the same and Trump leaving office, Zuckerberg would shrink back from his grand free speech declarations.

In 2019 and well into 2020, Facebook was still a roiling hotbed of misinformation, conspiracies and extremism. The social network hosted thousands of armed militias organizing for violence and a sea of content amplifying QAnon, which moved from a fringe belief on the margins to a mainstream political phenomenon through Facebook.

Those same forces would converge at the U.S. Capitol on January 6 for a day of violence that Facebook executives characterized as spontaneous, even though it had been festering openly on the platform for months.

How the Oversight Board works

Facebook’s Oversight Board began reviewing its first cases last October. Facebook can refer cases to the board, like it did with Trump, but users can also appeal to the board to overturn policy decisions that affect them after they exhaust the normal Facebook or Instagram appeals process. A five member subset of its 20 total members evaluate whether content should be allowed to remain on the platform and then reach a decision, which the full board must approve by a majority vote. Initially, the Oversight Board was only empowered to reinstate content removed on Facebook and Instagram, but in mid-April began accepting requests to review controversial content that stayed up.

Last month, the Oversight Board replaced departing member Pamela Karlan, a Stanford professor and voting rights scholar critical of Trump, who left to join the Biden administration. Karlan’s replacement, PEN America CEO Suzanne Nossel, wrote an op-ed in the LA Times in late January arguing that extending a permanent ban on Trump “may feel good” but that decision would ultimately set a dangerous precedent. Nossel joined the board too late to participate in the Trump decision.

The Oversight Board’s earliest batch of decisions leaned in the direction of restoring content that’s been taken down — not upholding its removal. While the board’s other decisions are likely to touch on the full spectrum of frustration people have with Facebook’s content moderation preferences, they come with far less baggage than the Trump decision. In one instance, the Oversight Board voted to restore an image of a woman’s nipples used in the context of a breast cancer post. In another, the board decided that a quote from a famous Nazi didn’t merit removal because it wasn’t an endorsement of Nazi ideology. In all cases, the Oversight Board can issue policy recommendations, but Facebook isn’t obligated to implement them — just the decisions.

Befitting its DNA of global activists, political figures and academics, the Oversight Board might have ambitions well beyond one social network. Earlier this year, Oversight Board co-chair and former Prime Minister of Denmark Helle Thorning-Schmidt declared that other social media companies would be “welcome to join” the project, which is branded in a conspicuously Facebook-less way. (The group calls itself the “Oversight Board” though everyone calls it the “Facebook Oversight Board.”)

“For the first time in history, we actually have content moderation being done outside one of the big social media platforms,” Thorning-Schmidt declared, grandly. “That in itself… I don’t hesitate to call it historic.”

Facebook’s decision to outsource some major policy decisions is indeed an experimental one, but that experiment is just getting started. The Trump case will give Facebook’s miniaturized Supreme Court an opportunity to send a message, though whether the takeaway is that it’s powerful enough to keep a world leader muzzled or independent enough to strike out from its parent and reverse the biggest social media policy decision ever made remains to be seen.

If Trump comes back, the company can shrug its shoulders and shirk another PR firestorm, content that its experiment in external content moderation is legitimized. If the board doubles down on banishing Trump, Facebook will rest easy knowing that someone else can take the blowback this round in its most controversial content call to date. For Facebook, for once, it’s a win-win situation.",Yes,It can be used to spread misinformation and incite violence.,https://techcrunch.com/2021/05/04/facebook-trump-oversight-board-decision/,2021-05-04 00:00:00,LABEL_1_relevant,0.991862416267395,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
82,Handmaids Tale Garb Is the Viral Protest Uniform of 2019,"Handmaid garb is the Guy Fawkes mask of 2019. Women dressed in red robes and blinkering white bonnets—the uniform of reproductive slavery in Margaret Atwood's dystopian novel turned Hulu show, The Handmaid's Tale—have become symbols of dissent across the internet and the world. The handmaids are ominous, silent, semi-faceless, and the most powerful protest costume since hacktivist collective Anonymous popularized wearing Fawkes' smirking face over a decade ago. And now, as Hulu's adaptation heads into its third season, they are everywhere.

The trend began two years ago in Texas, with a marketing stunt: Hulu hired scores of women to dress as handmaids and stand creepily motionless at SXSW to promote The Handmaid's Tale's premiere. Local reproductive rights activists at NARAL Pro-Choice Texas knew a meme when they saw one and ran to their local costume shop, dressing 12 women in red robes and sending them into the Texas Senate gallery to protest a bill restricting abortion access. When I spoke to the group's former executive director, Heather Busby, at the time, she admitted to always having social media shareability in mind, but I doubt she had any idea what she had started.

Handmaids have since attended pro-choice protests everywhere from Ireland to Argentina. They protested the Republican health care bill in Washington, DC. They loomed outside Supreme Court Justice Brett Kavanaugh's confirmation hearings, nominally because he opposes abortion, but also because of the sexual assault allegations that surfaced during those hearings. In Croatia and California, handmaids have protested legislators who voted against measures to curb violence against women. In Poland and the UK, they protested the presence of President Donald Trump. When Alabama and Georgia lost reproductive rights, they gained angry, sign-waving handmaids. The Handmaid Coalition, a nonprofit with the tagline ""Fight to keep fiction from becoming reality,"" offers aspiring handmaids organizing and costume construction advice. Everywhere the handmaids go, the media follows: Their image has become a staple of late-night set pieces, campaign emails, and, praise be, Twitter jokes.

Both Atwood and Alan Moore—creator of Anonymous' probable sartorial inspiration, the titular vigilante in V for Vendetta, who wears a Guy Fawkes mask—talk about their most memed characters the same way. People who sprang from their heads have now slipped from their control, taking on many, many lives of their own. Maybe that's why, beyond their eerie presence, internettiness, and tendency to show up en masse, Fawkes mask wearers and handmaids also share a certain ideological vagueness. The face of Guy Fawkes, who in life conspired to blow up Parliament, has always stood against the establishment. But in the hands of Anonymous and others, the establishment has been everyone from Scientology to Wall Street to dictators felled during the Arab Spring.

Emma Grey Ellis covers memes, trolls, and other elements of internet culture for WIRED.

In Atwood's telling (and Hulu's adaptation of it), handmaids are forced to bear their master's children because the state of Gilead is in population crisis, but in the real world, women dressed as handmaids have been protesting situations far more diverse and often less concrete. The costume's flexibility is part of its power, but also keeps handmaids from being real drivers of discourse. Women dressed as handmaids suggest connections between our world and Gilead, but while that's important to consider and the purpose of Atwood's work, it's hardly solution-oriented. Handmaids embody gendered pain and dread so vast it's hard to put into words: sexual violence, physical violence, governments taking control of bodies, bodies valued over beings, being reduced to a womb alone. All they really say is ""No to all that,"" albeit in a highly concise and memorable way. So they win no hearts or minds: Right-wing outlets like Fox News seem to think handmaids are mocking the institutions and people they're protesting, and expend considerable time mocking them in return.

Of course, twas ever thus. Protest fashion is more about communicating rejection and anxiety than creating tangible change on its own, and it's never popular with the olds. In most cases, like the Guy Fawkes masks or women protesters dressing up as witches or even just punks, the protesters arrive as what they hope will be seen as threatening and monstrous, their oppressors' worst nightmare. The handmaids are singular in that they've shown up as their own.

[#video: https://www.youtube.com/embed/RcTvQx1Wot0

More Great WIRED Stories",The article does discuss unintended consequences of social media.,"It's created an environment where assholes can interact with other assholes 24/7, and where everyone is free to be the worst version of themselves.

The future of democracy depends on the internet. Here's how we save it.

A detailed investigation resulted in a 24-year prison sentence for a Florida man.",https://www.wired.com/story/handmaids-tale-protest-garb/,2019-06-05 16:10:28.578000+00:00,LABEL_1_relevant,0.996101975440979,Social Media,social_media,Wired,"Information, Discourse & Governance"
83,How Facebook Killed Your School Reunion,"As my friend Alexis, a 2004 Harvard graduate who also decided to skip his coming five-year reunion, put it: “The infotech out there has reduced the desire to go ‘just to know’ what so-and-so is up to. I know what just about everyone is doing.”

Wortham reported that she and her colleagues were no longer tantalized by the prospect of finding out what had happened to whom.

In 2009 the Facebook-is-killing-reunions trend story became an official part of the annual journalistic calendar, like stories about the war on Christmas or tips for singles on Valentine’s day. Time reported that in some cases Facebook was inspiring reunions that wouldn’t otherwise happen, while in others it was killing them off:

“There was a Facebook page for my 20-year college reunion, which took place this May,” [says Deborah Dietzler, executive director of alumni relations at the University of Georgia]. “I looked at it a couple of times and it didn’t seem like anyone I knew would be there, so I lost interest.”

By 2010 it was on the Huffington Post of big media, CNN, which declared Facebook’s positive and negative effects on reunions a wash. Clearly no one had yet to go beyond studying it anecdotally.

Skip to the present day, and the data have finally arrived. Timothy Davis, co-founder of the reunion-organizing site Classreport.com, tells The Baltimore Sun that he’s noticed fewer reunions nationwide, with the trend especially pronounced among people in their mid to late 20’s.

What’s it all mean? I don’t know. Probably that we’ll have to endure these stories until the only place that even bothers to report this footnote in the great dematerialization brought about by the web and social media is AARP The Magazine.",Yes,"It has killed off the high school reunion, which was once a highly anticipated event.",https://www.technologyreview.com/2011/05/08/118859/how-facebook-killed-your-school-reunion/,2011-05-08 00:00:00,LABEL_1_relevant,0.995622217655182,Social Media,social_media,MIT Tech Review,Economy
84,Jack Dorsey Is Captain of the Twittanic,"On Tuesday, Jack Dorsey, the CEO of Twitter, came to TED 2019 to answer for the sins of his platform. In his signature black hoodie and jeans, unkempt facial hair, and black beanie, he sat with TED head Chris Anderson and Whitney Pennington Rodgers, who curates current affairs for the conference, for a conversation that left all three members, along with the audience, frustrated.

“We’re on this great voyage with you on the Twittanic,” Anderson told Dorsey after roughly 20 minutes of interrupted back and forth. “There are people in steerage who are saying, ‘We are worried about the iceberg ahead!’ And you say, ‘That is a good point’ and ‘Our boat hasn’t been built to handle it,’ and we’re waiting, and you are showing this extraordinary calm and we’re all worried but we’re outside saying, ‘Jack, turn the fucking wheel!’”

Dorsey stoically listened to this comparison, like the meditative yogi he often talks about aspiring to be. “It’s democracy at stake! It’s our culture at stake! It’s our world at stake!” Anderson continued. “You’re doing a brilliant role of listening, Jack, but can you actually dial up the urgency and move on this stuff? Will you do that?”

“Yeah, yeah, yes,” Dorsey replied, but then added, “We could do a bunch of superficial things to address what you’re talking about, but we need to go deep.”

It’s been more than a year since Dorsey publicly committed to “fixing” Twitter, and figuring out what a platform that encourages healthy discussions looks like. He’s been on a mea culpa tour since then, telling the world—and regulators—that he knows Twitter is broken, that it’s toxic and terrible and that he and the team are planning to radically rebuild it. He reiterated all of this on the TED stage, explaining that he wants to rethink what behavior the site incentivizes, for instance, by possibly getting rid of the like button and de-emphasizing follower counts while emphasizing topical interests instead. He repeated that he wants to focus on maximizing the health of conversations, and prioritizing people spending their time learning on the site, rather than getting outraged or harassed. He admitted Twitter was full of problems, problems he didn’t anticipate 13 years ago when the site was founded, and which he’s still trying to figure out how to solve.

Emily Dreyfuss covers the intersection of tech and culture for WIRED.

The urgency of this task couldn’t have been made clearer in the days leading up to Dorsey’s appearance. Over the weekend, Ilhan Omar—a woman of color, an immigrant, and a Muslim representing the state of Minnesota in the US House—reported an increase in death threats after President Trump tweeted out a video that intercut a speech she recently gave with footage of the 9/11 attacks. Many of the threats were made on Twitter. Then on Monday, as Notre Dame burned, people came to the platform to mourn the loss in real time, but also to spread lies and hate as quickly as the flames engulfed the cathedral’s spire. When Omar tweeted her own heartfelt condolences, people replied with more death threats. Twitter was very much itself, showcasing the power of its network as well the danger.

Dorsey didn’t address any of these incidents specifically at TED. In fact, his answers lacked specificity overall. When he was asked pointed questions, he evaded them, as he often does. Rodgers asked him how many people are working on content moderation on Twitter—a number the company has never published, and Tuesday continued the vagueness streak.

“It varies,” Dorsey said. “We want to be flexible on this. There are no amount of people that can actually scale this, which is why we have done so much work on proactively taking down abuse.”",Yes,"It amplifies human behavior. Good behavior and bad behavior.

Twitter is a microcosm of the internet. It’s a platform that allows people to share information and ideas instantly, without much filter. That can be a good thing! It can also be a bad thing. People can use Twitter to share news and have thoughtful",https://www.wired.com/story/ted-2019-jack-dorsey-captain-twittanic/,2019-04-16 20:28:03.193000+00:00,LABEL_1_relevant,0.9949550032615662,Social Media,social_media,Wired,"Information, Discourse & Governance"
85,The exhausting playbook behind Trump’s battle with Twitter,"This moment feels like an inevitable escalation of a conflict that has been playing out across the major social-media companies, but particularly Twitter, for years—one that Yiannopoulos’s White House stunt foreshadowed. As platforms reckon with their role in amplifying misinformation, abuse, and extreme views, the arguments about content moderation that once lived on the fringes of Twitter’s rules increasingly involve people at the very center of mainstream power.

“Republicans feel that Social Media Platforms totally silence conservatives voices,” Trump tweeted to his 80 million followers this week. “We will strongly regulate, or close them down, before we can ever allow this to happen.” His comments were covered widely in the media, as are many of his more inflammatory or conspiratorial tweets.

Hours before news of the coming executive order broke, Trump advisor Kellyanne Conway went on Fox News and encouraged viewers to hound a Twitter employee, spelling out his account handle and blaming him for the decision to fact-check the president’s tweets. “Somebody in San Francisco go wake him up and tell him he’s about to get a lot more followers,” she said.

Trump himself tagged the employee in a tweet on Thursday, effectively directing supporters to fill his mentions with abusive messages. The Twitter employee is also reportedly receiving death threats.

This cycle has been set off in the past when Twitter has rolled out new policies designed to protect targets of abuse, suspended far-right accounts for rule violations, or stepped up efforts to slow the spread of misinformation. It begins with waves of speculation arguing that Twitter isn’t actually, say, enforcing its new abuse policies but instead implementing a secret anti-conservative agenda that must be stopped. Then there’s a rush to find and target someone responsible for implementing it. The blueprint dates back at least to Gamergate, the harassment campaign championed by Yiannopoulos targeting women in video-game development, whose supporters also claimed instead to be fighting a conspiracy against them ( “It’s actually about ethics in gaming journalism”).

The president uses his own account to continually test Twitter’s boundaries, and now he’s become the catalyst for a new cycle. In just the past week, he’s used his platform to amplify conspiracy theories suggesting that MSNBC host Joe Scarborough murdered a staffer and to spread misinformation about mail-in voting in an earlier series of tweets that were not subject to fact-check labels. He thanked a “Cowboys for Trump” account that tweeted a video where an unidentified man proclaimed that “the only good Democrat is a dead Democrat.” (After cheers from the audience, the speaker then clarifies that he meant the comment “politically.”) The widower of the deceased staffer at the heart of the Scarborough conspiracy theory has begged Twitter to intervene.

The company had not taken any action against those tweets as of Thursday, although it has indicated that it is working to expand the labeling system that was used to flag some of Trump’s tweets about mail-in voting.

Until the fact-checking labels were introduced to two of Trump’s tweets on Tuesday, the platform had scrupulously avoided enforcing its rules against Trump’s account. Some explanations for the enforcement loopholes have cited the newsworthiness of otherwise rule-breaking content and Trump’s status as the head of a government.

But Trump, despite the lack of evidence to support claims of systemic social-media bias against conservatives, has repeatedly promised to take up the issue on behalf of some of his more prominent supporters. In 2018, he accused Google of “rigging” news search results against conservative media, repeating a version of a claim that Trump supporters—including vloggers Diamond and Silk—had circulated in conservative media for a few days earlier. Diamond and Silk (whose real names are Lynnette Hardaway and Rochelle Richardson) claimed at a House Judiciary Committee hearing that April that they were being “censored” by Facebook because of their support for Trump.

In 2019, Trump met with Twitter CEO Jack Dorsey and reportedly took the opportunity to complain about losing Twitter followers. On the same day as that meeting, Trump tweeted that the platform was “very discriminatory.” He later tweeted that his administration was “closely” monitoring conservatives’ complaints of censorship. Later that year, Trump held a “social-media summit” with dozens of his most passionate online supporters to air their collective complaints that Google, Facebook, and Twitter were censoring them.

None of these claims have to be true to be popular, which is something Trump and his online supporters know well. They just need to sound controversial enough to grab attention—or, better yet, redirect it from something else.",Yes,"A potential amplification of misinformation, abuse, and extreme views. This could lead to arguments about content moderation that involve people at the very center of mainstream power.",https://www.technologyreview.com/2020/05/28/1002376/trump-twitter-conservative-bias/,2020-05-28 00:00:00,LABEL_1_relevant,0.9919018149375916,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
86,The Christchurch Shooter and YouTube’s Radicalization Trap,"YouTube, Facebook, and other social media platforms were instrumental in radicalizing the terrorist who killed 51 worshippers in a March 2019 attack on two New Zealand mosques, according to a new report from the country’s government. Online radicalization experts speaking with WIRED say that while platforms have cracked down on extremist content since then, the fundamental business models behind top social media sites still play a role in online radicalization.

According to the report, released last night, the terrorist regularly watched extremist content online and donated to organizations like the Daily Stormer, a white supremacist site, and Stefan Molyneux’s far-right Freedomain Radio. He also gave directly to Austrian far-right activist Martin Sellner. “The individual claimed that he was not a frequent commenter on extreme right-wing sites and that YouTube was, for him, a far more significant source of information and inspiration,” the report says.

The terrorist’s interest in far-right YouTubers and edgy forums like 8chan is not a revelation. But until now, the details of his involvement with these online far-right organizations were not public. Over a year later, YouTube and other platforms have taken steps toward accepting responsibility for white supremacist content that propagates on their websites, including removing popular content creators and hiring thousands more moderators. Yet according to experts, until social media companies open the lid on their black-box policies and even algorithms, white supremacist propaganda will always be a few clicks away.

""The problem goes far deeper than the identification and removal of pieces of problematic content,"" said a New Zealand government spokesperson over email. ""The same algorithms that keep people tuned to the platform and consuming advertising can also promote harmful content once individuals have shown an interest.""

The Christchurch attacker’s pathway to radicalization was entirely unexceptional, say three experts speaking with WIRED who had reviewed the government report. He came from a broken home and from a young age was exposed to domestic violence, sickness, and suicide. He had unsupervised access to a computer, where he played online games and, at age 14, discovered the online forum 4chan. The report details how he expressed racist ideas at his school, and he was twice called in to speak with its anti-racism contact officer regarding anti-Semitism. The report describes him as somebody with “limited personal engagement,” which “left considerable scope for influence from extreme right-wing material, which he found on the internet and in books.” Aside from a couple of years working as a personal trainer, he had no consistent employment.

The terrorist’s mother told the Australian Federal Police that her concerns grew in early 2017. “She remembered him talking about how the Western world was coming to an end because Muslim migrants were coming back into Europe and would out-breed Europeans,” the report says. The terrorist’s friends and family provided narratives of his radicalization that are supported by his internet activity: shared links, donations, comments. While he was not a frequent poster on right-wing sites, he spent ample time in the extremist corners of YouTube.

""They’ve kicked some people off the platform, but they haven’t addressed that underlying issue."" Becca Lewis, Stanford

A damning 2018 report by Stanford researcher and PhD candidate Becca Lewis describes the alternative media system on YouTube that fed young viewers far-right propaganda. This network of channels, which range from mainstream conservatives and libertarians to overt white nationalists, collaborated with each other, funneling viewers into increasingly extreme content streams. She points to Stefan Molyneux as an example. “He’s been shown time and time again to be an important vector point for people’s radicalization,” she says. “He claimed there were scientific differences between the races and promoted debunked pseudoscience. But because he wasn’t a self-identified or overt neo-Nazi, he became embraced by more mainstream people with more mainstream platforms.” YouTube removed Molyneux’s channel in June of this year.

This “step-ladder of amplification” is in part a byproduct of the business model for YouTube creators, says Lewis. Revenue is directly tied to viewership, and exposure is currency. While these networks of creators played off each other’s fan bases, the drive to gain more viewers also incentivized them to post increasingly inflammatory and incendiary content. “One of the most disturbing things I found was not only evidence that audiences were getting radicalized, but also data that literally showed creators getting more radical in their content over time,” she says.",Yes,"It has given rise to a new breed of super-empowered individuals that are capable of inflicting serious harm to society through their online activity.

The Christchurch terrorist’s attack was unplanned; he chose his targets and his weapons based on what he could find online and in his immediate vicinity. He did not have any known criminal",https://www.wired.com/story/christchurch-shooter-youtube-radicalization-extremism/,2020-12-09 00:51:33.762000+00:00,LABEL_1_relevant,0.9958058595657348,Social Media,social_media,Wired,"Information, Discourse & Governance"
87,Did Quora Get Hacked? Top Answer: Yes,"Nearly a month after the midterm elections, details on a hack of the Republican National Congressional Committee reveals that meddling in the midterms was much worse than it seemed on election day. The hack probably should have been the biggest news of the week, but for a little distracting—and important!—thing called the Mueller probe.

In expectation of Robert Mueller making big moves in the investigation before the holidays, Garrett Graff spells out the 14 questions about Trump and Russia that Mueller knows the answer to.

Also this week, veteran Kristofer Goldsmith revealed that foreign trolls are targeting vets on Facebook. Louise Matsakis reported on the ways Facebook’s dominance of the nonprofit sector exposes charities to money—but also hackers. Lily Hay Newman explained that iTunes doesn’t encrypt downloads on purpose, and Brian Barrett warned you about a clever Touch ID scam hitting the App Store.

Australia’s very bad anti-encryption law could be bad for the whole world. A new company wants to solve the problem of email phishing with… the blockchain, obviously. Issie Lapowsky did the dirty work to understand how New York City’s anti-dick pic airdrop rule would even work. And we wrote you a guide to data breaches. You’re welcome!

Of course, that wasn’t all. As always, we’ve rounded up all the news we didn’t break or cover in depth this week. Click on the headlines to read the full stories. And stay safe out there.

In a blog post on Monday, Quora CEO Adam D’Angelo admitted that Quora was breached. The company discovered the problem last Friday, and more than 100 million accounts may have had their data taken. The company says it’s working to contain the problem and has hired independent auditors to figure out what happened. Quora also said that the breach also might affect any “linked networks,” aka Facebook or Google, if you used those to log into Quora. Though no financial information is attached to Quora accounts, there’s a ton of personal and social information available for each account. That kind of information is a juicy target for hackers looking to cause havoc or steal identities.

If you have a Quora account, you probably received an email this week telling you to change your password if it’s one you reuse elsewhere. You should get on that.

There’s something so romantic about a cable car in a city. Riding in one makes you feel like you’re flying above all the troubles of the concrete world below. But there was no romance in what happened to the the brand new free cable-car that opened in Russia’s capital to bring passengers from the tourist district of Vorobyovy Gory to the Luzhniki Stadium. On its first full day of operation, the cable car was hit with ransomware that shut the tramway down. Hackers demanded payment in bitcoin before they would resume service. Police has to turn disappointed passengers away. By the next day, however, they had cable car up and running again.

We told you about the hack that hit 500 million Marriott customers two weeks ago, and how to protect yourself from the fallout. Now, Reuters reports that it was likely a nation-state attack perpetrated by China, according to evidence unearthed by private investigators hired by Marriott. Why would a country want to hack a hotel chain, and steal the reservation records of millions of guests? Because who was staying where and when is very valuable information for a nation that wants to exert almost total control over its people. Investigators caution that they are not 100 percent sure it was China, however.

ZDNet reports that Kaspersky Labs has been brought in to investigate some bank robberies in Eastern Europe, in which robbers stole tens of millions of dollars. They apparently pulled it off by disguising themselves as inspectors, gaining access to the bank networks and leaving malware-laden devices behind. These devices—laptops, Raspberry Pis, and “malicious USB thumb drives known as Bash Bunnies,” according to ZDNet—allowed the hackers to remotely connect to the bank networks, and siphon money away.

More Great WIRED Stories",,"It de-centers the world. In other words, it makes the world about you and your experiences, rather than about the large-scale social trends and changes happening around you. And it’s not just the Russians that are doing this to us. We’re doing it to ourselves.

The Australian government passed a",https://www.wired.com/story/quora-hack-china-marriott-security-news/,2018-12-08 14:00:00+00:00,LABEL_1_relevant,0.9957475066184998,Social Media,social_media,Wired,"Information, Discourse & Governance"
88,"WWIII Memes, Oddly, Prove There's Hope for the Internet","After a US drone strike killed Iranian general Qasem Soleimani last week, the online world saw two nations tumbling toward a war that could consume the globe—the long-prophesied World War III. Some Americans celebrated President Trump taking a stand against an old perceived enemy. Some planned protests. Many, including President Trump and Iranian foreign minister Javad Zarif, made their thoughts known on social media, which may have helped stop a war before it started. Because this is the internet, though, not all of those thoughts were nuanced policy positions. A lot of them were jokes.

Moments after news of Soleimani’s death broke and #WWIII began trending on Twitter, people started to meme. There were jokes about being poor soldiers, about playing dead and being shot again, about how annoyed Queen Elizabeth II must be that she’s going to live through another world war. Many of them featured images and GIFs from pop culture: SpongeBob SquarePants, Friends, Keeping Up With the Kardashians. Especially on TikTok and among young people, the jokes tended to revolve around creative means of dodging the (currently nonexistent) draft, from blocking US military organizations on Twitter to having asthma to accepting the consequences and having a good time in prison. The meme has even made it to some elementary school classrooms: “A former student who teaches fourth and fifth graders told me that his students are all joking about it,” says Viveca Greene, who studies dark humor at Hampshire College.

Jokes about WWIII strike some over the age of 10 as decidedly—offensively—unfunny. It’s easy to see their point: Avoidable death and destruction are generally no laughing matter, and joking about them might trivialize strangers’ real suffering. The cycle of joke and rebuke should be familiar to everyone by now, but in the case of #WWIII, the condemnation has gone, well, worldwide.

“What strikes me as unusual is the sharp rebuke by people in the Middle East who claim to be in far more dangerous situations than Americans who are worried about being drafted,” says Paul Lewis, author of Cracking Up: American Humor in a Time of Conflict. Considering the draft people are worried about doesn’t currently exist, it’s difficult to dispute that. However, these jokes and memes may not always be signs of calloused hearts and internet-maddened minds.

Many people (some meme-makers included) have identified jokes about WWIII and the draft as coping mechanisms, and time-honored ones at that. “There have probably been jokes about wars since there have been wars,” says Lewis, who studies humor and politics at Boston College. “Freud recognized such gallows humor as a last, desperate effort of the ego to distance itself from an inescapable bad fate.” As the world has grown more unpredictable and destabilized, memes have taken on an absurdist ""laugh to keep from crying"" sort of tone. (See: the majority of 2019.)

Other experts think #WWIII jokes are less memetic mental balm than the kind of disaster humor people have observed since the early 1900s, when consumption of media truly came to the masses. “These jokes aren’t like what we know from emergency room workers. Those jokes don’t have the same kind of absurdity,” says Giselinde Kuipers, a cultural sociologist at Catholic University Leuven, Belgium. According to Kuipers, dark topic + absurdity = jokes happens only when there’s a great deal of media-mediated distance between the joker and the crisis, like when the Titanic sank or people outside of New York City learned about the 9/11 attacks.

LEARN MORE The WIRED Guide to Memes

Either way, the motive for making the joke is rather self-serving, but, in aggregate, the shallowness of the act doesn’t negate the potential positive effects. “The sheer number of people reporting that they learned what was going on with WWIII by seeing memes on Reddit is astounding,” says Greene. “It’s not the endpoint, but humor has always been used to draw attention to issues in a way that people aren’t as turned off by as they are if you came with a serious voice and a wagging finger.” Conveying overwhelming information in a comfortable, approachable format is a technique often used by white supremists and terrorist organizations like ISIS to reel in new recruits. WWIII memes do the same, but people are ending up informed about world events rather than radicalized.

Not only are people being informed about events taking place on the other side of the world, they’re able to engage directly with the people whose lives are impacted. “The biggest thing to notice is that this is the globalization of response to humor,” Lewis says. “The global internet is allowing people around the world to see what Americans are joking about, and respond.” The fact that so many of the comments are negative doesn’t mean the discussion isn’t worth having or isn’t a significant improvement over war jokes past. Greene notes that a presumed good–evil binary is all but absent from jokes about WWIII, which was not at all the case in the lead up to the Iraq War. For most people, it’s hard to fully demonize people who can tweet at you.",Yes,"People are now able to see and engage with memes about current events from around the world. Although some of these memes are funny, many people believe that they trivialize the real-life suffering of those affected by the events.",https://www.wired.com/story/wwiii-memes/,2020-01-10 12:00:00+00:00,LABEL_1_relevant,0.9917024970054626,Social Media,social_media,Wired,"Information, Discourse & Governance"
89,Twitter taps crypto developer to lead 'bluesky' decentralized social network effort,"Twitter’s ambitious upstart decentralized social media working group “bluesky” took an important step Monday as the social media company appointed a formal project lead who will direct how the protocol develops moving forward.

Crypto developer Jay Graber was tapped by Twitter to helm the initiative, which the company hopes will eventually create a decentralized social media protocol that a number of social networks including Twitter will operate on. The separate bluesky organization will operate independently but to date has been funded and managed largely by employees at Twitter.

Graber had already been working in a less formal role inside the bluesky team, with Twitter paying her to create a technical review of the decentralized social ecosystem for a working group of developers in the space. Graber previously worked on the developer team behind privacy focused cryptocurrency Zcash and built out her own decentralized social network called Happening, designed to compete with Facebook Events. Graber eventually walked away from the effort after having issues bootstrapping a user base interested in the benefits of decentralization, something that has grown to be a near-insurmountable issue for most upstart networks in the space.

I’m excited to announce that I’ll be leading @bluesky, an initiative started by @Twitter to decentralize social media. Follow updates on Twitter and at https://t.co/Sg4MxK1zwl — Jay Graber (@arcalinea) August 16, 2021

In an interview back in January, Graber told TechCrunch she saw a major opportunity in Twitter entering the decentralized social space due to the hefty user base on the Twitter platform, which will itself eventually migrate to the protocol, the company has said.

“The really powerful thing about Twitter doing a decentralized protocol move is that if you could design a protocol that works in an ideal way, you don’t have to go through the initial effort of finding the niche to bootstrap from because Twitter will bring so many users,” Graber told us.

In January, TechCrunch profiled the initiative as it gathered more attention following Twitter’s permanent ban of former President Donald Trump from its platform. Following Trump’s removal, Twitter CEO Jack Dorsey highlighted the bluesky effort as one of the company’s ongoing initiatives to ensure that social media moderation could be less decentralized in the future. A decentralized social media protocol would allow for individual networks to govern themselves without one company or organization exercising monolithic control over the sphere of online conversations.

“I think a huge focus for everyone involved has been thinking how do we enable better moderation, and not just coming from one source,” Graber told TechCrunch.

The bluesky organization is still in its earliest stages. Graber’s next task is bulking up the team with its first hires, which include a protocol developer and web developer.",Yes,The echo chamber effect.,https://techcrunch.com/2021/08/16/twitter-taps-crypto-developer-to-lead-bluesky-decentralized-social-network-effort/,2021-08-16 00:00:00,LABEL_1_relevant,0.9929665327072144,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
90,In Celebration of the Internet’s True Angels,"When asked why she regularly uses her spare scrolling time to help strangers out, rather than watch Gordon Ramsay TikToks or snoop on enemy Instagram pages like the rest of us, Spalding says that’s exactly it—she helps because other people don’t. “I feel like there just aren’t that many people out there who would take the time, or have the time necessarily. My life is not that busy, I’m not 9-to-5-ing and I don’t have kids,” she says. “It’s kind of more of a selfish thing, because it gives me a sense of satisfaction that I’m able to tie up loose ends for people.”

The Reviewer

After all his work for the day is done and his wife is fast asleep, 43-year-old civil engineering estimator Craig Laws takes out his iPad and writes Amazon reviews. Laws is currently the 25th top reviewer on Amazon UK, a place he earned after other users marked his reviews as “helpful” a total of 10,873 times. The Derby resident has reviewed everything from boxing gloves to air purifiers to dog treats to artist paint palettes, and is currently testing out a laser hair removal device on his shoulders, painstakingly monitoring its progress over the course of a few months. “You do suffer for your art sometimes,” he says.

Laws has been writing Amazon reviews since his first purchase on the site: a Fujifilm digital camera back in 2001. He started leaving reviews because he relied upon them himself. “You get hooked,” he says, “and you start to give back.” Two years ago, Laws was invited to be part of Amazon’s Vine program, meaning he is now regularly offered free products to review. So is all of this less altruistic than it seems? A way to get some free goods? In actual fact, Laws often selects items to review that he doesn’t personally want, so he can give “real, proper impartial advice” (hence the state of his shoulder blades). He also gets “at least one message a day” directly from manufacturers asking him to review their products for money or freebies and ignores “absolutely all of them.” He can’t be bought.

So what keeps this hyper-reviewer going? At any one time, Laws is testing out between five and 10 products—he also takes photos and videos to attach to his reviews in order to make them as detailed as possible. He isn’t remotely motivated by his place in the Amazon reviewer rankings—he likes that his work is recognized, but doesn’t write extra reviews in the hopes of making his way up to number one (unlike some others on the site). Instead, Laws simply likes to help people out, and he has fun doing it.

“It’s nice to have some kind of platform where people listen to you. If I was to say this in a shop corner, no one would listen to me,” he says. Laws doesn’t like using social media because it’s inherently argumentative—“you can just watch people imploding on the internet a lot.” Instead, he uses Amazon to get his voice heard. “You’re never going to change someone’s opinion on the internet,” he says of politics and sport and news. “But if someone is actively looking for an opinion when choosing something, then that’s an opinion that people might listen to.”

The Fixer

There are thousands of YouTube channels dedicated to showing you how to fix this microwave or troubleshoot that computer. The people behind them are nice enough, sure, but they make money and promote their own repair businesses via the videos, so their motives aren’t entirely altruistic. Far more interesting are those who upload one-off videos after they themselves have finally figured out how to repair something that’s been tormenting them. Take, for example, the mystery man behind the wonderfully titled, “HOW TO FIX A SLOW FLUSH. No rambling just a quick answer. *Contains adult language*,” which has been viewed more than 100,000 times. In September 2018, 35-year-old homemaker Jenna Quinlan from California uploaded one such video: “How to Fix an Ironing Board Lever! I Finally figured it out!”

Quinlan didn’t create a YouTube channel in order to post the video—she’d started one earlier that year as a place to share her love of essential oils, and over the years she’s accumulated slightly more than 400 subscribers. She knew an ironing board video wasn’t what her small community wanted or needed, but when she finally fixed her own board after a “super frustrating” couple of months, she just had to share her knowledge with the world. Naturally, she had previously looked on YouTube for tips on how to fix the board, but had found the videos there to have “dark videography, really obscure descriptions, no close-ups to see what’s going on.”",Yes,"We have lost the ability to be helpful.

But Quinlan’s video was none of those things. It had good lighting and Quinlan took her time explaining each step in detail. She even uploaded a second video after someone in the comments section asked her how she fixed the lever in the first place. “I guess I",https://www.wired.com/story/2021-internet-angels/,2021-12-30 12:00:00+00:00,LABEL_1_relevant,0.8351994752883911,Social Media,social_media,Wired,"Information, Discourse & Governance"
91,"Facebook knows Instagram harms teens. Now, its plan to open the app to kids looks worse than ever","Facebook knows Instagram harms teens. Now, its plan to open the app to kids looks worse than ever

Facebook is in the hot seat again.

The Wall Street Journal published a powerful multi-part series on the company this week, drawing from internal documents on everything from the company’s secretive practice of whitelisting celebrities to its knowledge that Instagram is taking a serious toll on the mental health of teen girls.

The flurry of investigative pieces makes it clear that what Facebook says in public doesn’t always reflect the company’s knowledge on known issues behind the scenes. The revelations still managed to shock even though Facebook has been playing dumb about the various social ills it has sown for years. (Remember when Mark Zuckerberg dismissed the notion that Facebook influenced the 2016 election as “crazy?”) Facebook’s longstanding PR playbook is to hide its dangers, denying knowledge of its darker impacts on society publicly, even as research spells them out internally.

That’s all well and good until someone gets ahold of the internal research.

One of the biggest revelations from the WSJ’s report: The company knows that Instagram poses serious dangers to mental health in teenage girls. An internal research slide from 2019 acknowledged that “We make body image issues worse for one in three teen girls” — a shocking admission for a company charging ahead with plans to expand to even younger and more vulnerable age groups.

As recently as May, Instagram’s Adam Mosseri dismissed concerns around the app’s negative impact on teens as “quite small.”

But internally, the picture told a different story. According to the WSJ, from 2019 to 2021, the company conducted a thorough deep dive into teen mental health, including online surveys, diary studies, focus groups and large-scale questionnaires.

According to one internal slide, the findings showed that 32% of teenage girls reported that Instagram made them have a worse body image. Of research participants who experienced suicidal thoughts, 13% of British teens and 6% of American teens directly linked their interest in killing themselves to Instagram.

“Teens blame Instagram for increases in the rate of anxiety and depression,” another internal slide stated. “This reaction was unprompted and consistent across all groups.”

Following the WSJ report, Senators Marsha Blackburn (R-TN) and Richard Blumenthal (D-CT) announced a probe into Facebook’s lack of transparency around internal research showing that Instagram poses serious and even lethal danger to teens. The Senate Subcommittee on Consumer Protection, Product Safety, and Data Security will launch the investigation.

“We are in touch with a Facebook whistleblower and will use every resource at our disposal to investigate what Facebook knew and when they knew it – including seeking further documents and pursuing witness testimony,” Senators Blackburn and Blumenthal wrote. “The Wall Street Journal’s blockbuster reporting may only be the tip of the iceberg.”

Blackburn and Blumenthal weren’t the only U.S. lawmakers alarmed by the new report. Sen. Ed Markey (D-MA), Rep. Kathy Castor (D-FL), and Lori Trahan (D-MA) sent Facebook their own letter demanding that the company walk away from its plan to launch Instagram for kids. “Children and teens are uniquely vulnerable populations online, and these findings paint a clear and devastating picture of Instagram as an app that poses significant threats to young people’s wellbeing,” the lawmakers wrote.

Big Tech has become the new Big Tobacco. Facebook is lying about how their product harms teens. https://t.co/85oo3B9oO0 — Rep. Ken Buck (@RepKenBuck) September 14, 2021

Facebook gobbled up Instagram because they were too chicken to compete against them fair & square for younger users. When there’s one big game in town, there’s a whole lot less pressure to offer the best service—or do the least damage. #BreakUpBigTechhttps://t.co/mIyHQ2iPs8 — Elizabeth Warren (@ewarren) September 16, 2021

In May, a group of 44 state attorneys general wrote to Instagram to encourage the company to abandon its plans to bring Instagram to kids under the age of 13. “It appears that Facebook is not responding to a need, but instead creating one, as this platform appeals primarily to children who otherwise do not or would not have an Instagram account,” the group of attorneys general wrote. They warned that an Instagram for kids would be “harmful for myriad reasons.”

In April, a collection of the same Democratic lawmakers expressed “serious concerns” about Instagram’s potential impact on the well-being of young users. That same month, a coalition of consumer advocacy organizations also demanded that the company reconsider launching a version of Instagram for kids.

According to the documents obtained by the WSJ, all of those concerns look extremely valid. In spite of extensive internal research and their deeply troubling findings, Facebook has downplayed its knowledge publicly, even as regulators regularly pressed the company for what it really knows.

Instagram’s Mosseri may have made matters worse Thursday when he made a less than flattering analogy between social media platforms and vehicles. “We know that more people die than would otherwise because of car accidents, but by and large, cars create way more value in the world than they destroy,” Mosseri told Peter Kafka on Recode’s media podcast. “And I think social media is similar.”

Mosseri dismissed any comparison between social media and drugs or cigarettes in spite of social media’s well-researched addictive effects, likening social platforms to the auto industry instead. Naturally, the company’s many critics jumped on the car comparison, pointing to their widespread lethality and the fact that the auto industry is heavily regulated — unlike social media.",Yes,It has negative impacts on the mental health of teen girls.,https://techcrunch.com/2021/09/16/facebook-instagram-for-kids-mosseri-wsj-teen-girls/,2021-09-16 00:00:00,LABEL_1_relevant,0.9960582256317142,Social Media,social_media,TechCrunch,Equality & Justice
92,Inside Facebook's Plan to Safeguard the 2018 Election,"Deep in the bowels of Facebook's serpentine campus in Menlo Park, California, is a room about 25 feet square that may have a lot to do with how the world thinks about the company in the coming months. It looks like a Wall Street trading floor, with screens on every wall and every desk. And 20 hours a day---soon to be 24 hours a day---it's jammed with about two dozen geeks, spooks, hackers, and lawyers trying to spot and quash the next bad thing to happen on the company's networks.

It's known appropriately as the War Room, and it was set up just a month ago---in advance of the Brazilian presidential election and US midterm elections---as perhaps Facebook’s most dramatic and visual step to ensure that the fraud and manipulation that was rampant on Facebook's networks during the 2016 US presidential election don't recur.

In years past Facebook would have worked hard to keep an effort like this under wraps for fear of letting competitors know what it was up to or of signaling an imperfection in one of Silicon Valley's biggest success stories. But Facebook's reputation has been damaged by the manipulation it did not detect in 2016, its arrogant response after the vote, and this year’s Cambridge Analytica scandal. So Wednesday morning it invited about two dozen journalists to take a look, to ask questions, and to hopefully tell the world that Facebook is at least trying to get things right this time.

Inside Facebook's War Room for detecting and preventing election interference. WIRED

The room doesn't look like much from the hallway of Building 20. In fact, blackout paper on the windows makes it look like an unlit empty conference room. But inside are screens monitoring viral content, spam, hate speech, and voter suppression across all Facebook's networks, as well as external sites like Twitter and Reddit. “We've been doing all this work virtually for two years. But when stuff needs to be done fast, there is no substitute for face-to-face contact,” said Samidh Chakrabarti, Facebook’s elections and civic engagement boss.

Spokesperson Tom Reynolds said the two dozen teams represented inside the War Room are backed up by 20,000 people Facebook has dedicated to better policing its platforms. The teams include specialists in threat intelligence, data science, engineering, research, operations, legal policy, and communications. For now, it is staffed about 20 hours a day. That will increase to 24 hours five days before Brazil’s October 28 vote and again five days before the November 6 US midterm elections.

The room is set up as an acknowledgment that Facebook is in an arms race against the tricksters and manipulators, and that threats can arrive from anywhere. They can start with someone creating a fake account, or a legitimate account launching a fake news campaign, or as spammers out to make a quick buck, and they can start in any language in any country. With representatives of so many teams working within ear shot, the hope is that the threats can be addressed in a matter of minutes before they get traction on the platform.

On Wednesday, executives wouldn’t commit to using the War Room beyond the US midterms. But they talked so effusively about its work that it’s hard to imagine why they’d consider shutting it down.",Yes,"It can be used to spread false information or hate speech cheaply and quickly to a large audience. This is what happened in the 2016 US presidential election, when Russian agents used Facebook to spread disinformation in an attempt to influence the election. In response, Facebook set up the War Room, a physical space where a team of employees work to detect and prevent",https://www.wired.com/story/inside-facebooks-plan-to-safeguard-2018-election/,2018-10-18 10:00:00+00:00,LABEL_1_relevant,0.9951708912849426,Social Media,social_media,Wired,Politics
93,The War in Ukraine Is Keeping Chinese Social Media Censors Busy,"“Artillery fire lights up the sky and breaks my heart. I hope my compatriots in Ukraine are taking care of themselves and their families,” said a user on Weibo, often called China’s Twitter, on February 27. The message was quickly blocked, according to Free Weibo, a service of Great Fire, which tracks Chinese censorship online.

Two days later, a very different message appeared on Weibo: “I support fighting! America and Taiwan have gone too far.” That, too, was blocked, according to Free Weibo.

The messages—and their quick disappearance—show how Chinese social media platforms find themselves in the crosshairs of the Russia-Ukraine war. The platforms must be sure to toe the official line amid subtle shifts in China’s position. Their responses could be an early test of new rules governing how companies use algorithms, which may make them responsible for trending topics and fake news appearing on their sites.

In general, Chinese online platforms receive daily guidance from the government about what sort of content to remove, says Yuqi Na, a researcher in media and communications at the University of Westminster.

A hint of how that works emerged in the days leading up to the invasion. On February 22, a Chinese outlet called Horizon News briefly posted, probably by accident, what appear to be internal instructions for how to spin the Ukraine crisis on its official Weibo account. Among the supposed rules: “Do not post anything unfavorable to Russia or pro-Western.”

The instructions also said to monitor comments and only use hashtags started by state outlets Xinhua, CCTV, or People’s Daily, according to China Digital Times. That sort of direction to follow the lead of major state outlets is common, says Maria Repnikova, an assistant professor in global communication at Georgia State University and author of Chinese Soft Power.

Prior to the invasion, Chinese state media outlets and officials’ Twitter accounts repeated a drumbeat of US warmongering, and brushed off the possibility of an invasion. Once the assault began, China was put in the awkward position of having to reconcile its long-standing policy of noninterference and respecting national sovereignty with its ties to Russia. Just weeks earlier, the two countries reaffirmed their relationship when Russian president Vladimir Putin was a VIP guest at the Beijing Winter Olympics.

In the early days of the war, Chinese state media seemed surprised and took a cautious approach. The relatively sparse coverage largely echoed Russian outlets, calling the conflict a “special military operation” and placing the blame on the US and NATO. “It’s quite intentional,” says Na. “A lot of internet users buy into that narrative when it's their main information source.”

“You’ve got the official narrative, but you’ve got a lot of counternarratives circulating even if they get shut down as quickly as they go up.” Anthony Saich, director, Ash Center for Democratic Governance and Innovation

In that environment, pro-Russia and nationalist discourse flourished. Putin’s February 24 speech justifying the invasion went viral, says Aliaksandr Herasimenka, director of research at the Oxford Internet Institute’s Program on Democracy and Technology. Social media companies let pro-war posts thrive, as well as some sexist posts by Chinese men pining for Ukrainian refugee brides.

At the same time, there was an outpouring of sympathy for Ukrainians on platforms such as Weibo and Weixin, a widely used chat and news app—but many of those posts disappeared once they became popular. Twitter users documented posts that were taken down, including posts depicting antiwar protests in other countries.",Yes,"It led to a false sense of democracy and a false sense of community. Social media has been shown to increase feelings of loneliness and isolation, and to lead to negative mental health consequences, such as anxiety and depression.",https://www.wired.com/story/war-ukraine-chinese-social-media-censors-busy/,2022-03-04 12:00:00+00:00,LABEL_1_relevant,0.9955621957778932,Social Media,social_media,Wired,"Information, Discourse & Governance"
94,Move fast and break Facebook: A bull case for antitrust enforcement,"This is the second post in a series on the Facebook monopoly. The first post explored how the U.S. Federal Trade Commission should define the Facebook monopoly. I am inspired by Cloudflare’s recent post explaining the impact of Amazon’s monopoly in its industry.

Perhaps it was a competitive tactic, but I genuinely believe it more a patriotic duty: guideposts for legislators and regulators on a complex issue. My generation has watched with a combination of sadness and trepidation as legislators who barely use email question the leading technologists of our time about products that have long pervaded our lives in ways we don’t yet understand.

I, personally, and my company both stand to gain little from this — but as a participant in the latest generation of social media upstarts, and as an American concerned for the future of our democracy, I feel a duty to try.

Mark Zuckerberg has reached his Key Largo moment.

In May 1972, executives of the era’s preeminent technology company — AT&T — met at a secret retreat in Key Largo, Florida. Their company was in crisis.

At the time, Ma Bell’s breathtaking monopoly consisted of a holy trinity: Western Electric (the vast majority of phones and cables used for American telephony), the lucrative long distance service (for both personal and business use) and local telephone service, which the company subsidized in exchange for its monopoly.

Over the next decade, all three government branches — legislators, regulators and the courts — parried with AT&T’s lawyers as the press piled on, battering the company’s reputation in the process. By 1982, a consent decree forced AT&T’s dismantling. The biggest company on earth withered to 30% of its book value and seven independent “Baby Bell” regional operating companies. AT&T’s brand would live on, but the business as the world knew it was dead.

Mark Zuckerberg is, undoubtedly, the greatest technologist of our time. For over 17 years, he has outgunned, outsmarted and outperformed like no software entrepreneur before him. Earlier this month, the U.S. Federal Trade Commission refiled its sweeping antitrust case against Facebook.

Its own holy trinity of Facebook Blue, Instagram and WhatsApp is under attack. All three government branches — legislators, regulators and the courts — are gaining steam in their fight, and the press is piling on, battering the company’s reputation in the process. Facebook, the AT&T of our time, is at the brink. For so long, Zuckerberg has told us all to move fast and break things. It’s time for him to break Facebook.

If Facebook does exist to “make the world more open and connected, and not just to build a company,” as Zuckerberg wrote in the 2012 IPO prospectus, he will spin off Instagram and WhatsApp now so that they have a fighting chance. It would be the ultimate Zuckerbergian chess move. Zuckerberg would lose voting control and thus power over all three entities, but in his action he would successfully scatter the opposition. The rationale is simple:

The United States government will break up Facebook. It is not a matter of if; it is a matter of when. Facebook is already losing. Facebook Blue, Instagram and WhatsApp all face existential threats. Pressure from the government will stifle Facebook’s efforts to right the ship. Facebook will generate more value for shareholders as three separate companies.

I write this as an admirer; I genuinely believe much of the criticism Zuckerberg has received is unfair. Facebook faces Sisyphean tasks. The FTC will not let Zuckerberg sneeze without an investigation, and the company has failed to innovate.

Given no chance to acquire new technology and talent, how can Facebook survive over the long term? In 2006, Terry Semel of Yahoo offered $1 billion to buy Facebook. Zuckerberg reportedly remarked, “I just don’t know if I want to work for Terry Semel.” Even if the FTC were to allow it, this generation of founders will not sell to Facebook. Unfair or not, Mark Zuckerberg has become Terry Semel.

The government will break up Facebook

It is not a matter of if; it is a matter of when.

In a speech on the floor of Congress in 1890, Senator John Sherman, the founding father of the modern American antitrust movement, famously said, “If we will not endure a king as a political power, we should not endure a king over the production, transportation and sale of any of the necessities of life. If we would not submit to an emperor, we should not submit to an autocrat of trade with power to prevent competition and to fix the price of any commodity.”

This is the sentiment driving the building resistance to Facebook’s monopoly, and it shows no sign of abating. Zuckerberg has proudly called Facebook the fifth estate. In the U.S., we only have four estates.

All three branches of the federal government are heating up their pursuit. In the Senate, an unusual bipartisan coalition is emerging, with Senators Amy Klobuchar (D-MN), Mark Warner (D-VA), Elizabeth Warren (D-MA) and Josh Hawley (R-MO) each waging a war from multiple fronts.

In the House, Speaker Nancy Pelosi (D-CA) has called Facebook “part of the problem.” Lina Khan’s FTC is likewise only getting started, with unequivocal support from the White House that feels burned by Facebook’s disingenuous lobbying. The Department of Justice will join, too, aided by state attorneys general. And the courts will continue to turn the wheels of justice, slowly but surely.

In the wake of Facebook co-founder Chris Hughes’ scathing 2019 New York Times op-ed, Zuckerberg said that Facebook’s immense size allows it to spend more on trust and safety than Twitter makes in revenue.

“If what you care about is democracy and elections, then you want a company like us to be able to invest billions of dollars per year like we are in building up really advanced tools to fight election interference,” Zuckerberg said.

This could be true, but it does not prove that the concentration of such power in one man’s hands is consistent with U.S. public policy. And the centralized operations could be rebuilt easily in standalone entities.

Time and time again, whether on Holocaust denial, election propaganda or vaccine misinformation, Zuckerberg has struggled to make quick judgments when presented with the information his trust and safety team uncovers. And even before a decision is made, the structure of the team disincentivizes it from even measuring anything that could harm Facebook’s brand. This is inherently inconsistent with U.S. democracy. The New York Times’ army of reporters will not stop uncovering scandal after scandal, contradicting Zuckerberg’s narrative. The writing is on the wall.

Facebook is losing

Facebook Blue, Instagram and WhatsApp all face existential threats. Pressure from the government will stifle Facebook’s efforts to right the ship.

For so long, Facebook has dominated the social media industry. But if you ask Chinese technology executives about Facebook today, they quote Tencent founder Pony Ma: “When a giant falls, his corpse will still be warm for a while.”

Facebook’s recent demise begins with its brand. The endless, cascading scandals of the last decade have irreparably harmed its image. Younger users refuse to adopt the flagship Facebook Blue. The company’s internal polling on two key metrics — good for the world (GFW) and cares about users (CAU) — shows Facebook’s reputation is in tatters. Talent is fleeing, too; Instacart alone recently poached 55 Facebook executives.

In 2012 and 2014, Instagram and WhatsApp were real dangers. Facebook extinguished both through acquisition. Yet today they represent the company’s two most promising, underutilized assets. They are the underinvested telephone networks of our time.

Weeks ago, Instagram head Adam Mosseri announced that the company no longer considers itself a photo-sharing app. Instead, its focus is entertainment. In other words, as the media widely reported, Instagram is changing to compete with TikTok.

TikTok’s strength represents an existential threat. U.S. children 4 to 15 already spend over 80 minutes a day on ByteDance’s TikTok, and it’s just getting started. The demographics are quickly expanding way beyond teenagers, as social products always have. For Instagram, it could be too little too late — as a part of Facebook, Instagram cannot acquire the technology and retain the talent it needs to compete with TikTok.

Imagine Instagram acquisitions of Squarespace to bolster its e-commerce offerings, or Etsy to create a meaningful marketplace. As a part of Facebook, Instagram is strategically adrift.

Likewise, a standalone WhatsApp could easily be a $100 billion market cap company. WhatsApp has a proud legacy of robust security offerings, but its brand has been tarnished by associations with Facebook. Discord’s rise represents a substantial threat, and WhatsApp has failed to innovate to account for this generation’s desire for community-driven messaging. Snapchat, too, is in many ways a potential WhatsApp killer; its young users use photography and video as a messaging medium. Facebook’s top augmented reality talents are leaving for Snapchat.

With 2 billion monthly active users, WhatApp could be a privacy-focused alternative to Facebook Blue, and it would logically introduce expanded profiles, photo-sharing capabilities and other features that would strengthen its offerings. Inside Facebook, WhatsApp has suffered from underinvestment as a potential threat to Facebook Blue and Messenger. Shareholders have suffered for it.

Beyond Instagram and WhatsApp, Facebook Blue itself is struggling. Q2’s earnings may have skyrocketed, but the increase in revenue hid a troubling sign: Ads increased by 47%, but inventory increased by just 6%. This means Facebook is struggling to find new places to run its ads. Why? The core social graph of Facebook is too old.

I fondly remember the day Facebook came to my high school; I have thousands of friends on the platform. I do not use Facebook anymore — not for political reasons, but because my friends have left. A decade ago, hundreds of people wished me happy birthday every year. This year it was 24, half of whom are over the age of 50. And I’m 32 years old. Teen girls run the social world, and many of them don’t even have Facebook on their phones.

Zuckerberg’s newfound push into the metaverse has been well covered, but the question remains: Why wouldn’t a Facebook serious about the metaverse acquire Roblox? Of course, the FTC would currently never allow it.

Facebook’s current clunky attempt at a hardware solution, with an emphasis on the workplace, shows little sign of promise. The launch was hardly propitious, as CNN reported, “While Bosworth, the Facebook executive, was in the middle of describing how he sees Workrooms as a more interactive way to gather virtually with coworkers than video chat, his avatar froze midsentence, the pixels of its digital skin turning from flesh-toned to gray. He had been disconnected.”

This is not the indomitable Facebook of yore. This is graying Facebook, freezing midsentence.

Facebook will generate more value for shareholders as three separate companies

Zuckerberg’s control of 58% of Facebook’s voting shares has forestalled a typical Wall Street reckoning: Investors are tiring of Zuckerberg’s unilateral power. Many justifiably believe the company is more valuable as the sum of its parts. The success of AT&T’s breakup is a case in point.

Five years after AT&T’s 1984 breakup, AT&T and the Baby Bells’ value had doubled compared to AT&T’s pre-breakup market capitalization. Pressure from Japanese entrants battered Western Electric’s market share, but greater competition in telephony spurred investment and innovation among the Baby Bells.

AT&T turned its focus to competing with IBM and preparing for the coming information age. A smaller AT&T became more nimble, ready to focus on the future rather than dwell on the past.

Standalone Facebook Blue, Instagram and WhatsApp could drastically change their futures by attracting talent and acquiring new technologies.

The U.K.’s recent opposition to Facebook’s $400 million GIPHY acquisition proves Facebook will struggle mightily to acquire even small bolt-ons.

Zuckerberg has always been one step ahead. And when he wasn’t, he was famously unprecious: “Copying is faster than innovating.” If he really believes in Facebook’s mission and recognizes that the situation cannot possibly get any better from here, he will copy AT&T’s solution before it is forced upon him.

Regulators are tying Zuckerberg’s hands behind his back as the company weathers body blows and uppercuts from Beijing to Silicon Valley. As Zuckerberg’s idol Augustus Caesar might have once said, carpe diem. It’s time to break Facebook.",Yes,The echo chamber effect.,https://techcrunch.com/2021/08/29/move-fast-and-break-facebook-a-bull-case-for-antitrust-enforcement/,2021-08-29 00:00:00,LABEL_1_relevant,0.995303511619568,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
95,FourFront nabs funding to build an interconnected network of fictional TikTok stars,"Every popular social media platform has opened up a new type of storytelling to a new generation of users, but TikTok’s impact has perhaps been the swiftest as the short-form video platform’s user base has quickly swelled past 1 billion users and transformed how social media stars are made.

FourFront is a media startup that’s looking to define a new type of storytelling on the platform, aiming to popularize a network of individual actors performing scripted short-form serialized stories that feel native to TikTok. Fictionalized storytelling in vlog form is clearly not a new development for social media, but FourFront is hoping that it can capitalize on the discoverability opened up by TikTok’s For You Page (FYP) to steadily build new audiences.

The startup tells TechCrunch that they’ve raised $1.5 million in seed funding from Bam Ventures, Slow Ventures, BDMI, Alumni Ventures Group and HustleFund.

The startup has a couple dozen characters on TikTok, with a handful racking up several hundreds of thousands of followers. Not all of the characters have been hits, and FourFront’s team of writers and social media strategists have settled on nine characters who they’re looking to create an interconnected “universe” around where the actors organically cross paths with each other. While FourFront’s team plans out the character arcs, it’s on the actors to shoot the videos themselves.

The scripted content is often pretty soap opera-esque, but follows the format of popular videos on the platform; “watch us confront my sister’s cheating fiancé LIVE” implores one video from FourFront’s most popular character “Sydney,” who has nearly 500,000 followers after debuting in June. In addition to helping catch her sister’s cheating fiancé, Sydney has also discussed the stresses of her roommate breaking her lease early and the lessons she has learned working in customer support for a dating app.

FourFront co-founder Ilan Benjamin tells TechCrunch that his startup isn’t trying to mislead anyone into thinking that their network of characters are real, noting that their profile bios highlight the fictional nature of the stories and that each of the videos include the #fictional tag. “We don’t want to confuse audiences or trick them, we want them to be entertained,” Benjamin says.

Building content for TikTok means catering to the whims of the FYP, which means plenty of viewers are catching characters like Sydney in the middle of their storylines, something that can create storytelling challenges when it comes to balancing the patience of existing fans while capturing the curiosity of new viewers.

“The plot mechanics might be repetitive,” Benjamin says. “It’s a constant balancing act; every video has to stand on its own, and every single video has to be evergreen.”

For now, the company is largely focused on building out its network of stars and their audiences while looking to push boundaries in how audiences interact with characters using tech products like conversational AI chatbots and polls to help direct the stories themselves.",Yes,"It has created a new generation of ""social media stars"". These stars are made through platforms such as TikTok, which has a user base of over 1 billion people. FourFront is a media startup that is looking to create a network of individual actors who perform scripted short-form serialized stories that are native to TikTok. The company has",https://techcrunch.com/2021/10/12/fourfront-nabs-funding-to-build-an-interconnected-network-of-fictional-tiktok-stars/,2021-10-12 00:00:00,LABEL_1_relevant,0.9951062202453612,Social Media,social_media,TechCrunch,User Experience & Entertainment
96,Trump's social network has 30 days to stop breaking the rules of its ...,"The Software Freedom Conservancy (SFC) says former President Donald Trump’s new social network violated a free and open-source software licensing agreement by ripping off decentralized social network Mastodon. The Trump Media and Technology Group (TMTG) has 30 days to comply with the terms of the license before its access is terminated — forcing it to rebuild the platform or face legal action.

TMTG launched a special purpose acquisition company fundraising effort yesterday with promises to build a sweeping media empire. Its only product so far is a social network called Truth Social that appears strongly to be forked from Mastodon. While anyone can freely reuse Mastodon’s code (and groups like right-wing social network Gab have already done so), they still have to comply with the Affero General Public License (or AGPLv3) that governs that code, and its conditions include offering their own source code to all users.

Truth Social doesn’t comply with that license and, in fact, refers to its service as “proprietary.” Its developers apparently attempted to scrub references that would make the Mastodon connection clear — at one point listing a “sighting” of the Mastodon logo as a bug — but included direct references to Mastodon in the site’s underlying HTML alongside obvious visual similarities.

Truth Social must offer its source code to the pranksters who defaced it

TMTG’s strategy hasn’t sat well with the SFC, an organization that enforces free and open-source software licenses. “The license purposefully treats everyone equally (even people we don’t like or agree with), but they must operate under the same rules of the copyleft licenses that apply to everyone else,” SFC policy fellow Bradley Kuhn wrote in a blog post. “Today, we saw the Trump Media and Technology Group ignoring those important rules — which were designed for the social good.”

Truth Social hasn’t officially launched. But users could access a test version of the platform, where many of them created prank accounts that flooded the service with false company announcements and even fake Donald Trump posts. (The platform has since been replaced by a waitlist.) The SFC demands that TMTG offer all these users access to the Truth Social source code. “If they fail to do this within 30 days, their rights and permissions in the software are automatically and permanently terminated,” Kuhn says.

If Truth Social fails to make the source code available, the SFC could sue it for violating the terms of the license it used. Earlier this year, the group sued electronics maker Vizio for “repeated failures to fulfill even the basic requirements” of free software licensing. “We will be following this issue very closely and demanding that Trump’s Group give the corresponding source to all who use the site,” Kuhn writes.

Mastodon founder Eugen Rochko also said yesterday that he intended to seek legal counsel about the situation, although he didn’t discuss a specific course of action. “Compliance with our AGPLv3 license is very important to me as that is the sole basis upon which I and other developers are willing to give away years of work for free,” he told Talking Points Memo.",Yes,The echo chamber effect. This occurs when people only see information that reaffirms their own beliefs.,https://www.theverge.com/2021/10/22/22740354/trump-truth-social-network-spac-mastodon-license-software-freedom-conservancy,2021-10-22 00:00:00,LABEL_1_relevant,0.9948703050613404,Social Media,social_media,The Verge,"Information, Discourse & Governance"
97,Stop Meme-ing Taylor Swift,"At this point, thinking sober thoughts about a new Taylor Swift song is painfully predictable. It's a here we go again exercise that think-piece writers and social media pundits have come to dread and celebrate in equal measure. Yet, no matter how formulaic the Swift responses are, the internet must be fed. Luckily for all involved, her latest single, ""You Need to Calm Down,"" is irresistible hot-take fodder: It's been hailed as revolutionary, slammed as queerbait, praised as inclusive, and condemned as intolerant. The lyrics and music video nod to Swift's many celebrity feuds, but also include a GLAAD endorsement and a petition to ratify the Equality Act. There is homophobia, and also a food fight. It's a Tayfecta of Swiftian contradiction, and if there's one thing no one is feeling about it, it's calm.

Now, genuine reasons to celebrate or scorn ""You Need to Calm Down"" do exist. Many are celebrating the video because it approaches something like allyship, especially for a pop song. It's packed with queer celebrities of many racial, sexual, and gender identities; it highlights real activism and the Equality Act, and, because of Swift's platform, makes news stories of them both. (Donations to GLAAD have spiked since the song's release.) Swift and erstwhile nemesis Katy Perry buck tradition and do not literally kiss to make up. Some of its lines—like ""shade never made anybody less gay""—are already being printed on T-shirts, and will be belted all Pride month long, as they are clearly meant to be.

The candy-colored bits of progress are not without tonal issues, though. Critics point out that homophobia isn't shade. That putting Swift's struggles with online haters and the LGBTQ+ community’s struggles with systemic bigotry under the same lyrical umbrella is reductive. That the video's homophobic protesters are unkempt, cowboy-hatted, blue-collar stereotypes, which is both classist and ignores prejudice's pervasiveness at every level of American society. Some read Swift's blue-purple-pink wig as a nod to the bisexual flag and hence as queerbait. Other people think it's unfair that Swift is only telling the protesters, and not the LGBTQ+ community, to calm down. (Those people are wrong. My patience for folks who think a glittery parade is tantamount to hate speech can't come to the phone right now. It's dead.)

[#video: https://www.youtube.com/embed/Dkk9gvTmCXY

The song has spawned more opinions than it has words. A still of Swift, dressed in a french fry costume, embracing Perry, who is dressed as a hamburger, is on its way to becoming a submeme of its own. Depending on who you are, it could be a cutesy symbol of happiness and buried hatchets, of Swift and Perry's vapidity, of straight white women centering themselves in a social movement that isn't theirs, or the bizarro world of performative celebrity relationships calibrated for maximum engagement. Just like the song as a whole. That's a lot of weight for a three-and-a-half minute pop song to carry, and putting that kind of burden on it does a, well, queer thing: It proves Swift right.

If there's anything to say about ""You Need to Calm Down,"" it's that the song is more evidence that Taylor Swift is extremely good at her job: being the pop star people deserve, if not the one they say they want. People claimed to despise Swift's lack of politics, and now she is overtly political and they still hate it. People claimed to dislike her petty feuding, but then spent thousands of hours treating her lyrics, tweets, and Instagram photos like a scandal scavenger hunt. ""You Need to Calm Down"" continues to sate internet sleuths with its references to snakes (an obvious allusion to her beef with Kanye West and Kim Kardashian) and gowns (allegedly a nod to a dustup with Aretha Franklin). She even managed to iterate on a strategy that proved successful with ""Look What You Made Me Do"" and even ""Shake It Off"": blaming the listener, placing the controversy at the feet of the ""haters,"" and accepting little culpability. No matter what you're feeling about ""You Need to Calm Down,"" your hot take is still playing directly into Swift's hands.

Given the circumstances, there is only one solution: Stop meme-ing Taylor Swift. Not because she's scarily capable of manipulating the internet, not because her spotty feminism and allyship means she should be canceled, and certainly not because talking about her is boring. Do it because everyone's energy can be better spent. Sometimes a pop song is just a pop song. If you disagree, maybe you need to calm down.

More Great WIRED Stories",Yes,"It's created a more polarized society.

The internet's most important people are far from anonymous. They're the celebrities of the internet.

Is the future of the internet more like Reddit, or Facebook?

The chaos and disruption of the 2016 election was just a dress rehearsal. The real thing is coming.

The",https://www.wired.com/story/stop-meme-ing-taylor-swift/,2019-06-19 15:25:13.649000+00:00,LABEL_1_relevant,0.995988667011261,Social Media,social_media,Wired,Politics
98,Startup lets landlords scan tenants' Facebook to check if they can ...,"A UK startup has developed software for landlords that lets them analyze potential tenants using data from their social media accounts. Score Assured claims its service is simply another way for renters to secure a house in competitive markets, but following coverage of the business by The Washington Post this week — and negative reaction from many users on social media — the company now says it may reconsider how its software operates.

The software scans for keywords like ""poor"" and ""no money""

Landlords use the company's Tenant Assured program to send requests for profiles to would-be tenants. These then grant the program access to data from one or more social media networks (including Facebook, Twitter, LinkedIn, and Instagram), which it uses to create a one-time report on the individual. This process scans private conversations and public posts to record information about the user's personality, life events (like giving birth or getting married), and even their ""financial stress level"" — a measure of how easy it is for them to pay their rent, based on the frequency with which keywords like ""no money,"" ""poor,"" and ""staying in,"" appear in their posts.

Understandably, the reaction to this sort of analysis being used as a tool by landlords has not been positive. Simply scanning for keywords is a pretty crude measure of anything, especially something as fluctuating and relative and ""financial stress."" The Post noted out that some information collected by the startup has a protected status under US housing discrimination law, and a lawyer specializing in this topic told Gawker: ""The designer of [Tenant Assured] may be legally exposed, despite the claim that it is only passing information along.""

Speaking to The Verge, Score Assured's co-founder Steve Thornhill said that he'd expected this sort of reaction, but was adamant that the company's software is more benign than it seems. ""It's about giving the tenant more opportunity to get the property they want,"" he says. ""A lot of people now, millennials, for example, don't have credit scores — so how they can get a property when the answer from the traditional credit score is going to be no?""

Thornhill points out that the report produced from users' social media is a one-time file, and that there's no constant surveillance and no access without the tenant's permission. And he admits that while keyword-scraping can be a crude metric by itself, it's meant to be used alongside the landlord's own judgement. As for violating US housing discrimination laws, Thornhill points out that the company has no plans to enter the US market so this is not a consideration for them. And as for UK discrimination laws? ""I believe it [complies] but we’re going to double check that,"" he says.

A sample report. (Image credit: Score Assured)

There's certainly an argument to be made for the utility of the software and that it is, by itself, neutral. But Tenant Assured shouldn't ignore the fact that its program has the power to enable and amplify prejudice. This is especially true in a housing market like London, where landlords already wield disproportionate power over tenants simply because housing stock is in short supply.

the old argument: if you've got nothing to hide, you've got nothing to fear

And while the system may be opt-in, if it becomes popular it could become a de facto requirement from landlords. This prospect brings up all sorts of depressingly familiar arguments about privacy and personal data, where those in power demand access simply because the information is there. Imagine being told that you have to complete a social media profile before you can rent a house. After all, says the landlord, if you've got nothing to hide, you've got nothing to fear.

Score Assured even thinks its algorithms could be used by employers and private citizens. In the digital age, says Thornhill, we need new ways to trust strangers. ""Forty or fifty years ago, the landlord knew all his potential tenants,"" he says. ""We're trying to bring back a level of personal relationship to the digital world so the right judgement can be made for the right reason."" But is an automated report based on social media anymore personal? I'd say no, but I imagine landlords would want to know, just the same.",Yes,Private individuals or companies may use it to violate users' privacy or housing discrimination laws.,https://www.theverge.com/2016/6/10/11903082/landlord-social-media-credit-check,2016-06-10 00:00:00,LABEL_1_relevant,0.9962394237518308,Social Media,social_media,The Verge,Security & Privacy
99,One Woman’s High-Touch Bid to Upend the Sex-Toy Industry,"Others, too, had “seized the yes” and found it somewhat lacking. Mashable, one of the few press outlets that reviewed the device, awarded it 2.5 out of 5 stars, a score boosted by some extra points awarded for its “cool factor.” The handful of reviewers who have shared their opinion on ProductHunt were almost uniformly displeased. One suggested that the company's attempt at biomimesis had, perhaps, been too successful for its own good: “Overall this experience was worse than a toss with a boring inexperienced man.”

In late April, a few weeks after I'd received my Osé, DiCarlo and I talked on Zoom. Brand Lora was in full effect: She looked fantastic, with her hair carefully tousled and her neck adorned with jewelry. But her tech-visionary hoopla was more subdued than I'd expected. Where in other interviews she'd boasted of her product's ability to remake the female orgasm, now she positioned the Osé as a baby step. “We're still a startup,” she explained. “Nobody ever gets their first product to market perfect. Nobody ever gets it even close to perfect.” A few minutes later, she pivoted to telling me about the company's newer offering, a sex-education and coaching platform called WellSX that will eliminate shame around sex by providing users with a “high-touch human experience.”

Could this be sex tech’s Juicero—a humdrum gadget bested by your own two hands?

Lora DiCarlo was on to other sex toys too. At CES 2020, the company debuted the Onda and the Baci—effectively the Osé's G-spot stimulator and clitoral suction device split in two. In February, Lora DiCarlo filed trademarks for two more unreleased devices, the Filare and the Carezza; and a redesigned Osé 2 is now on sale. As a way of doing business, this would be pretty normal for a tech firm, says Janet Lieberman-Lu, the engineer and cofounder of Dame Products. (Lieberman-Lu left that company earlier this year.) The tech-world mindset and funding strategy “pushes you into a trajectory where you have to grow really fast,” she says. “You're building up the bubble, and then you're trying to build the structure in place in the hopes that when that bubble pops there's something there to catch your company.”

That's not how things typically work in the sex-toy industry. For a company like Dame Products—or even Brian Sloan's Very Intelligent Ecommerce—consumers, not investors, provide most of the cash from the start. Not many adult companies have relied heavily on venture capital, and the handful of exceptions—like the hedge-fund-backed Jimmyjane—have tended to get their major investments years after their products have proved successful in the marketplace. Lieberman-Lu was struck by how things played out with the Osé, though, which was picking up awards before anyone had even held it in their hands. “It's a little bit jarring for people who work in the industry to see the artifice,” she says. “It's not just a different way of doing things. It's an unhealthy way of doing things.”

Indeed, once I saw the Osé up close, another fragile “health and wellness” startup came to mind: Juicero, the company that saw an opening in 2016 for its $400, Wi-Fi-enabled juicers in the fruit-smoothie-tech space and decided to fill it. In spite of $120 million from Silicon Valley investors, Juicero's product—described as a new “platform” for food delivery—turned out to be about as good at pressing the company's proprietary produce packets as a human's grip.

Content This content can also be viewed on the site it originates from.

Could the Osé be sex tech's version of the same—a humdrum gadget potentially bested by your own two hands, dressed up as innovation and sold at twice the price of competing products? “We don't pretend that we know 100 percent exactly what we're doing,” DiCarlo told me as we wrapped up our interview. “We're a young company. We're learning as we go.” It was both the most honest and the most off-brand thing she said to me during our time together.

Her award-winning product may have been a dud, but it was clear that was only half the story. From DiCarlo's first moment in the spotlight, she'd been celebrated for her mastery of tech—for the way she'd used it to redefine the sex toy in service of her mission of empowerment. But the noisy, undistinguished vibrator that's now collecting dust under my bed was not, perhaps, her main invention. The company's success came not from the Osé but from the way it was promoted. DiCarlo had built a hype machine, precision-engineered with all the tools of startup culture, and there's no denying that it worked. It was Brand Lora, not the Osé, that recast a low-level health care worker with no background in either sex or tech as a thought leader in both spaces. It was Brand Lora, not the Osé, that helped the world accept that a sensual massager could be on par with Apple's AirPods, and that an industry long neglected and belittled should at last be taken seriously.",Yes,"It can be used to easily spread false information.

Social media platforms like Twitter and Facebook are designed to be addictive and to keep users engaged for as long as possible. This is done by using techniques such as showing users content that is controversial or that reinforces their existing beliefs. As a result, it's very easy for false information to spread",https://www.wired.com/story/lora-dicarlo-ose-sex-toy-industry/,2020-10-16 10:00:00+00:00,LABEL_1_relevant,0.9964845180511476,Social Media,social_media,Wired,"Information, Discourse & Governance"
100,Why Is Facebook So Afraid of Checking Facts?,"A video laden with falsehoods about Covid-19 emerged on Facebook last week, and has now been viewed many millions of times. The company has taken steps to minimize the video’s reach, but its fact-checks, in particular, appear to have been applied with a curious—if not dangerous—reticence. The reason for that reticence should alarm you: It seems that the biggest social network in the world is, at least in part, basing its response to pandemic-related misinformation on a misreading of the academic literature.

At issue is the company’s long-standing deference to the risk of so-called “backfire effects.” That is to say, Facebook worries that the mere act of trying to debunk a bogus claim may only help to make the lie grow stronger. CEO and founder Mark Zuckerberg expressed this precise concern back in February 2017: “Research shows that some of the most obvious ideas, like showing people an article from the opposite perspective, actually deepen polarization,” he said. The company would later cite the same theory to explain why it had stopped applying “red flag” warnings to fallacious headlines: “Academic research on correcting misinformation,” a Facebook product manager wrote, has shown that such warnings “may actually entrench deeply held beliefs.”

WIRED OPINION ABOUT Ethan Porter is an assistant professor at George Washington University. Thomas J. Wood is an assistant professor at Ohio State University.

Facebook’s fear of backfire hasn’t abated in the midst of this pandemic, or the infodemic that came with it. On April 16, the company announced a plan to deal with rampant Covid-19 misinformation: In addition to putting warning labels on some specific content, it would show decidedly nonspecific warnings to those who’d interacted with a harmful post and nudge them toward more authoritative sources. The vagueness of these latter warnings, Facebook told the website STAT, was meant to minimize the risk of backfire.

But here’s the thing: Whatever Facebook says (or thinks) about the backfire effect, this phenomenon has not, in fact, been “shown” or demonstrated in any thorough way. Rather, it’s a bogeyman—a zombie theory from the research literature circa 2008 that has all but been abandoned since. More recent studies, encompassing a broad array of issues, find the opposite is true: On almost all possible topics, almost all of the time, the average person—Democrat or Republican, young or old, well-educated or not—responds to facts just the way you’d hope, by becoming more factually accurate.

Yes, it’s possible to find exceptions. If you follow all this research very carefully, you’ll be familiar with the rare occasions when, in experimental settings, corrections have failed. If you have a day job, though, and need a rule of thumb, try this: Debunkings and corrections are effective, full stop. This summary puts you much closer to the academic consensus than does the suggestion that backfire effects are widespread and pose an active threat to online discourse.

We’ve demonstrated this fact about facts many times ourselves. Our peer-reviewed book and multiple academic articles describe dozens of randomized studies that we’ve run in which people are exposed to misinformation and fact-checks. The research consistently finds that subjects end up being more accurate in their answers to factual questions. We’ve shown that fact-checks are effective against outlandish conspiracy theories, as well as more run-of-the-mill Trump misstatements. We even partnered up with the authors of the most popular academic article on the backfire effect, in the hopes of tracking it down. Again, we came up empty-handed.",Yes,"It provides a way for individuals to share information with others and also to collect information from others with minimal effort. This allows for information to spread virally, which can lead to the formation of false beliefs. While social media has some benefits, such as providing a way for people to connect with others and share information, it also has some drawbacks,",https://www.wired.com/story/why-is-facebook-so-afraid-of-checking-facts/,2020-05-14 13:00:00+00:00,LABEL_1_relevant,0.9963098168373108,Social Media,social_media,Wired,"Information, Discourse & Governance"
101,Instagram's Newest Star Is ... a Tree?,"In recent days an Instagram post about planting trees has been making its way, like the darkest of dark horses, up the list of most liked posts in the platform’s history. As of this writing, it’s tallied 14.3 million likes, more than Justin Bieber’s engagement photo with Hailey Baldwin, and more than any picture Kylie Jenner has ever posted of her daughter, other than her first photo. (Duh—look at that tiny well-manicured hand!) The Great Instagram Tree Post of 2019 currently sits in fifth place, just behind a picture Selena Gomez took with her friends.

The meteoric post is from the eco-friendly apparel company Tentree. Peppered with a healthy helping of tree emoji, it promises that for “every TEN LIKES this post gets, we will plant ONE TREE” in Indonesia. And who’d turn down an offer to help the planet when all it takes is to read a few words and tap a little heart.

Tentree, of course, has a business interest in going viral: It gains followers, exposure, and presumably sales. That said, if a company is volunteering to plant trees, then by all means they should help reforest the world. Except that reforestation isn’t so easy as liking an Instagram post, and it’s not so easy as just planting lots of trees and walking away. Because as with any celebrity’s carefully orchestrated photo on social media, the reality is much more complicated.

Instagram content This content can also be viewed on the site it originates from.

Conservationists have a few ways of restoring forests. One is known as natural regeneration, in which some deforested areas regrow on their own if protected from further logging. “You don't need to plant anything, you just stop whacking the system,” says Bronson Griscom, director of forest carbon science at the Nature Conservancy. “Stop burning it, stop plowing it, stop cutting it.”

A variation on this strategy is called assisted natural regeneration, in which conservationists strategically plant certain species, like fruit trees, to kickstart the system. “Then the birds will start coming and they'll bring species from elsewhere,” says Griscom. Their poop carries seeds that can over time add variety to the area. “You don't have to plant all the species—you just plant some key species that bring the system back into a diverse form.”

But that’s just the beginning: Deforestation will keep happening in a given area unless there’s a bigger intervention. This means reining in unsustainable logging operations, which also isn’t that straightforward—the livelihood of local peoples has to factor in too.

Matt Simon covers cannabis, robots, and climate science for WIRED.

Take, for instance, work that the Nature Conservancy does in Brazil. A major threat to Brazil’s forests is ranching, both from big and little players: Smaller outfits raise calves, which they pass on to larger ranchers to raise as adult cattle. Conservationists can work relatively easily with the big ranchers and convince them to better mind the forests, “but it's this larger group of people that have smaller amounts of land that's hard, because they don't have as many options,” says Griscom.

So the Nature Conservancy is helping them switch to a more forest-friendly livelihood with cacao farming. The beauty of cacao is that it does well in the shade, which means you don’t need to clear-cut forests to grow it. Instead, you integrate the farm right into the forest. Bonus: Studies have shown this kind of cultivation can actually increase biodiversity and improve soil fertility.

Reforesting by planting a bunch of trees, then, is just the start. “It's a stop doing thing, rather than a doing thing,” says Griscom. “Having said that, in our experience in order to stop doing something it is still a doing story, in the sense that, how do you change someone's livelihood so that they're doing something else?”",Yes,It can give people a false sense of accomplishment.,https://www.wired.com/story/instagrams-newest-star-is-a-tree/,2019-04-25 23:10:14.396000+00:00,LABEL_1_relevant,0.9945525527000428,Social Media,social_media,Wired,"Information, Discourse & Governance"
102,This tech millionaire went from covid trial funder to misinformation superspreader,"His efforts became more focused on medical research when, in 2007, he was diagnosed with a rare blood cancer. His foundation shifted focus to one goal—curing Steve Kirsch—by supporting one of the few scientists looking at the disease. After several failed attempts to stop the progression of his disease, he designed his own protocol for chemotherapy and doctor-shopped to find an oncologist who would give it to him. He’s now outlived his initial prognosis by several years.

“He’s a genuinely good guy. I mean, he really, truly has a heart of gold,” Char told me. “He’s spending his own money to do what he thinks is right. It’s motivated out of his sense of keeping people safe and advancing health care.”

But Kirsch is also motivated by an unsatisfied competitive streak. In that same IEEE Spectrum story about his then-new startup, Propel Software, he said he felt successful, but not famous.

“Mouse Systems is not a household word,” he told the journalist. “We didn’t come up with better mouse technology than Microsoft did. Infoseek lost out to Yahoo; it had a chance to grow bigger, but it didn't. And FrameMaker is still a niche product. Yes, these were successes, but the successes could have been bigger if we had really paid attention to marketing. I’m not going to make the same mistake again.”

“Now we’ve lost the high ground”

It is not unusual to be wary of developing science, or wrong to be skeptical of pharmaceutical companies. These huge businesses do often prioritize profits over human health: in 2009, Pfizer paid a $2.3 billion settlement over kickbacks and fraudulent marketing, including a $1.3 billion felony fine.

In 2013, Johnson & Johnson paid $2.2 billion for its own kickback and fraud scandal, including a specific $400 million fine for its subsidiary Janssen, which manufactures the covid vaccine. The US government accused Janssen of improperly promoting the antipsychotic drug Risperdal to dementia patients despite the drug increasing deaths in the elderly. The man who ran Risperdal sales, Alex Gorsky, is now CEO of Johnson & Johnson.

As a health care journalist, I started off firmly in the wait-and-see camp on mRNA vaccines. Thanks to the volumes of data and information provided by pharmaceutical companies and regulators, as well as large numbers of trials from independently funded research groups around the world, I now trust that they’re safe for the vast majority of adults.

I also think it makes a lot of sense to look for pre-existing drugs that can help treat covid symptoms. Over the next few years, millions of unvaccinated people are going to get covid; it’s vital to try to mitigate their suffering, as well as lessen pressure on the health care system.

But the best way to help people is through rigorous trials that show what drugs help which people, and at what doses and times—not by basing entire protocols on incredibly limited evidence.

Unfortunately, as Jeffrey Morris at UPenn points out, public health officials and scientists have done plenty to undermine their own authority, like claiming masks don’t work, downplaying the natural immunity conveyed by previous covid infections, and not doing enough public communication about vaccine safety surveillance systems.

”We don’t want to feed the anti-vaccine trolls, so we actively suppress clear scientific data. Now we’ve lost the high ground,” Morris told me.

And that is what has allowed Kirsch, and people like him, to become so influential. It’s a cycle that feeds mistrust and boosts the profiles of influencers who present themselves in opposition to official authorities.

“The collateral damage is that, now, a lot of people don’t trust scientific leaders or the scientific community. They’re finding alternative leaders to follow,” Morris said. “That’s what creates some of these heroes.”

This story is part of the Pandemic Technology Project, supported by The Rockefeller Foundation.",Yes,"It has amplified the voices of a small number of influencers, who are often not experts in the field, at the expense of more qualified people. This has led to a mistrust of scientific authorities and a decrease in the public's willingness to accept new information.",https://www.technologyreview.com/2021/10/05/1036408/silicon-valley-millionaire-steve-kirsch-covid-vaccine-misinformation/,2021-10-05 00:00:00,LABEL_1_relevant,0.99649578332901,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
103,"Europe's digital rules reboot could tame Facebook, whistleblower Frances Haugen tells EU Parliament","In her latest turn in front of a phalanx of lawmakers, Facebook whistleblower Frances Haugen gave a polished testimony to the European Parliament on Monday — following similar sessions in front of U.K. and U.S. legislators in recent weeks.

Her core message was the same dire warning she’s sounded on both sides of the Atlantic: Facebook prioritizes profit over safety, choosing to ignore the amplification of toxic content that’s harmful to individuals, societies and democracy. And that regulatory oversight is thus essential to rein in and make such irresponsibly operated platform power accountable — with no time for lawmakers to lose in imposing rules on social media.

The (to date) highest profile Facebook whistleblower got a very warm reception from the European Parliament, where MEPs were universally effusive in thanking her for her time — and what they couched as her “bravery” in raising her concerns publicly — applauding Haugen before she spoke and again at the end of the nearly three hour presentation plus Q&A session.

They questioned her on a range of issues — giving over the largest share of their attention to how incoming pan-EU digital regulations can best deliver effective transparency and accountability on slippery platform giants.

The Digital Services Act (DSA) is front of mind for MEPs as they are considering and voting on amendments to the Commission’s proposal that could seriously reshape the legislation.

Such as a push by some MEPs to get an outright ban on behavioral advertising added to the legislation in favor of privacy-safe alternatives like contextual ads. Or another amendment that’s recently gained some backing — pushing to exempt news media from platform content takedowns.

Turns out Haugen isn’t a fan of either of those potential amendments. But she spoke up in favor of the regulation as a whole.

The general thrust of the DSA is aimed at achieving a trusted and safe online environment — and a number of MEPs speaking during today’s session spied a soapboxing opportunity to toot the EU’s horn for being so advanced as to have a digital regulation not just on the table but advancing rapidly toward adoption slap-bang in the midst of (yet) another Facebook publicity crisis — with the glare of the global spotlight on Haugen speaking to the European Parliament.

The Facebook whistleblower was happy to massage political egos, telling MEPs that she’s “grateful” the EU is taking platform regulation seriously — and suggesting there’s an opportunity for the bloc to set a “global gold standard” with the DSA.

Although she used a similar line in the U.K. parliament during another evidence session last month, where she talked up domestic online safety legislation in similarly glowing tones.

To MEPs, Haugen repeated her warning to U.K. lawmakers that Facebook is exceptionally adept at “dancing with data” — impressing on them that they too must not pass naive laws that simply require the tech giant to hand over data about what’s happening on its platform. Rather Facebook must be made to explain any data sets it hands over, down to the detail of the queries it uses to pull data and generate oversight audits.

Without such a step in legislation, Haugen warned that shiny new EU digital rules will arrive with a massive loophole baked in for Facebook to dance through by serving up selectively self-serving data — running whatever queries it needs to paint the picture to get the tick in the box.

For regulation to be effective on platforms as untrustworthy as Facebook, she suggested it must be multitiered, dynamic and take continuous input from a broader ecosystem of civil society organizations and external researchers — to stay on top of emergent harms and ensure the law is actually doing the job intended.

It should also take a broad view of oversight, she urged — providing platform data to a wider circle of external experts than merely just the “vetted academics” of the current DSA proposal in order to really deliver the sought-for accountability around AI-fuelled impacts.

“Facebook has shown that they will lie with data,” she told the European Parliament. “I encourage you to put in the DSA; if Facebook gives you data they should have to show you how they got it … It’s really, really important that they should have to disclose the process, the queries, the notebooks they used to pull this data because you can’t trust anything they give you unless you can confirm that.”

Haugen didn’t just sound the alarm; she layered on the flattery, too — telling MEPs that she “strongly believe[s] that Europe has a critical role to play in regulating these platforms because you are a vibrant, linguistically diverse democracy.”

“If you get the DSA right for your linguistically and ethnically diverse, 450 million EU citizens you can create a game-changer for the world — you can force platforms to price in societal risk to their business operations so that the decisions about what products to build and how to build them is not purely based on profit maximization. You can establish systemic rules and standards that address risks while protecting free speech and you can show the world how transparency, oversight and enforcement should work.”

“There’s a deep, deep need to make sure that platforms must disclose what safety systems they have, what languages those safety systems are in and a performance per language — and that’s the kind of thing where you can put in the DSA,” she went on, fleshing out her case for comprehensive disclosure requirements. “You can say: You need to be honest with us on is this actually dangerous for a large fraction of Europeans?”

Such an approach would have benefits that scale beyond Europe, per Haugen — by forcing Facebook “toward language-neutral content-neutral solutions,” which she argued are needed to tackle harms across all the markets and languages where the platform operates.

The skew in how much of Facebook’s (limited) safety budget gets directed toward English-speaking markets — and/or to the handful of markets where it’s afraid of regulation — is one of the core issues amplified by her leaking of so many internal Facebook documents. And she suggested Europe could help tackle this lack of global equity around how powerful platforms operate (and what they choose to prioritize or de-prioritize) by enforcing context-specific transparency around Facebook’s AI models — requiring not just a general measure of performance but specifics per market; per language; per safety system; even per cohort of heavily targeted users.

Forcing Facebook to address safety as a systemic requirement would not only solve problems the platform causes in markets across Europe but it would “speak up for people who live in fragile places in the world that don’t have as much influence,” she argued, adding: “The places in the world that have the most linguistic diversity are often the most fragile places and they need Europe to step in — because you guys have influence and you can really help them.”

While many of Haugen’s talking points were familiar from her earlier testimony sessions and press interviews, during the Q&A a number of EU lawmakers sought to engage her on whether Facebook’s problem with toxic content amplification might be tackled by an outright ban on microtargeted/behavioral advertising — an active debate in the parliament — so that the adtech giant can no longer use people’s information against them to profit through data-driven manipulation.

On this, Haugen demurred — saying she supports people being able to choose ad targeting (or no ad targeting) themselves, rather than regulators deciding.

Instead of an outright ban she suggested that “specific things and ads … really need to be regulated” — pointing to ad rates as one area she would target for regulation. “Given the current system subsidizes hate — it’s 5x to 10x cheaper to run a political ad that’s hateful than a non-hateful ad — I think you need to have flat rates for ads,” she said on that. “But I also think there should be regulation on targeting ads to specific people.

“I don’t know if you’re aware of this but you can target specific ads to an audience of 100 people. And I’m pretty sure that is being misused because I did an analysis on who is hyperexposed to political ads and unsurprisingly the people who are most exposed are in Washington, D.C. and they are radically overexposed — we’re talking thousands of political ads a month. So I do think having mechanisms to target specific people without their knowledge … is unacceptable.”

Haugen also argued for a ban on Facebook being able to use third-party data sources to enrich the profiles it holds on people for ad targeting purposes.

“With regard to profiling and data retention I think you shouldn’t be allowed to take third-party data sources — something Facebook does, they work with credit card companies, other forms — and it makes their ads radically more profitable,” she said, adding: “I think you should have to consent to every time you hook up more data sources. Because I think people would feel really uncomfortable if they knew that Facebook had some of the data they do.”

But on behavioral ad targeting she studiously avoided supporting an outright ban.

It was an interesting wrinkle during the session, given there is momentum on the issue within the EU — including as a result of her own whistleblowing amplifying regional lawmakers’ concerns about Facebook — and Haugen could have helped stoke that (but opted not to).

“With regard to targeted ads, I’m a strong proponent that people should be allowed to make choices with regard to how they are targeted — and I encourage prohibiting dark patterns that force people into opting into those things,” she said during one response (but without going into detail on exactly how regulators could draft a law that’s effective against something as cynically multifaceted as “dark pattern design”).

“Platforms should have to be transparent about how they use that data,” was all she offered, before falling back on reiterating: “I’m a big proponent that they should also have to publish policies like do they give flat ad rates for all political ads because you shouldn’t be subsidizing hate in political ads.”

Her argument against banning behavioral ads seemed to boil down to (or rather hinge on) regulators achieving fully comprehensive platform transparency — that’s able to provide an accurate picture of what Facebook (et al.) actually does with people’s data — i.e., in order that users can then make a genuine choice over whether they want such targeting or not. So it hinges on full-picture accountability.

Yet during another point in the session — after she had been asked whether children can really consent to data processing by platforms like Facebook — Haugen argued it’s doubtful that adults can (currently) understand what Facebook is doing with their data, let alone kids.

“With regard to can children understand what they’re trading away, I think almost certainly we as adults — we don’t know what we’ve traded away,” she told MEPs. “We don’t know what goes in the algorithms, we don’t know how we’re targeted so the idea that children can given informed consent — I don’t think we give informed consent and they have less capability.”

Given that, her faith that such comprehensive transparency is possible — and will paint a universally comprehensible picture of data-driven manipulation that allows all adults to make a truly informed decision to accept manipulative behavior ads (or not) — looks, well, rather tenuous.

If we follow Haugen’s logic, were the suggested cure of radical transparency to fail — including by regulator’s improperly/inaccurately communicating everything that’s been found to users and/or failing to ensure users are appropriately and universally educated regarding their risks and rights — well the risk is, surely, that data-drive exploitation will continue (just now with a free pass baked into legislation).

Her argument here felt like it lacked coherence. As if her opposition to banning behavioral ads — and, therefore, to tackling one core incentive that’s fuelling social media’s manipulative toxicity — was rather more ideological than logical.

(Certainly it looks like quite the leap of faith in governments around the world being able to scramble into place the kind of high functioning, “full-fat” oversight Haugen suggests is needed — even as, simultaneously, she’s spent weeks impressing on lawmakers that platforms can only be understood as highly contex- specific and devilishly data-detailed algorithm machines; Not to mention the sheer scale of the task at hand, even just given Facebook’s “amazing” amounts of data, as she put it in the Q&A today, suggesting that if regulators were handed Facebook data in raw form it would be far too overwhelming for them.)

This is also perhaps exactly the perspective you’d expect from a data scientist, not a rights expert.

(Ditto her quick dismissal of banning behavioral ads is the sort of trigger reaction you’d expect from a platform insider whose expertise comes from having been privy to the black boxes and focused on manipulating algorithms and data versus being outside the machine where the harms flow and are felt.)

At another point during the session Haugen further complicated her advocacy for radical transparency as the sole panacea for social media’s ills — warning against the EU leaving enforcement of such complex matters up to 27 national agencies.

Were the EU to do that she suggested it would doom the DSA to fail. Instead she advised lawmakers to create a central EU bureaucracy to deal with enforcing the highly detailed, layered and dynamic rules she says are needed to wrap Facebook-level platforms — going so far as to suggest that ex-industry algorithm experts like herself might find a “home” there, chipping in to help with their specialist knowledge and “giv[ing] back by contributing to public accountability.”

“The number of formal experts in these things — how the algorithms really work and the consequences of them — there are very, very few in the world. Because you can’t get a master’s degree in it, you can’t get a Ph.D. in it, you have to go work for one of these companies and be trained up internally,” she suggested, adding: “I sincerely worry that if you delegate this functionality to 27 member states you will not be able to get critical mass in any one place.

“It’ll be very, very difficult to get enough experts and distribute them that broadly.”

With so many warnings to lawmakers about the need to nail down devilish details in self-serving data sets and “fragile” AIs, in order to prevent platforms from simply carrying on pulling the wool over everyone’s eyes, it seems instructive that Haugen should be so opposed to regulators actually choosing to set some simple limits — such as no personal data for ads.

She was also asked directly by MEPs on whether regulators should put limits on what platforms can do with data and/or limits on the inputs it can use for algorithms. Again her preference in response to the questions was for transparency — not limits. (Although elsewhere, and as noted above, she did at least call for a ban on Facebook buying third-party data sets to enrich its ad profiling.)

Ultimately, then, the ideology of the algorithm expert may have a few blind spots when it comes to thinking outside the black box for ways to come up with effective regulation for data-driven software machines.

Some hard stops might actually be just what’s needed for democratic societies to wrest back control from data-mining tech giants.

Haugen’s best advocacy may therefore be her highly detailed warnings around the risk of loopholes fatally scuttling digital regulation. She is undoubtedly correct that here the risks are multitudinous.

Earlier in her presentation she raised another possible loophole — pushing lawmakers not to exempt news media content from the DSA (which is another potential amendment MEPs are mulling). “If you’re going to make content neutral rules, then they must really be neutral,” she argued. “Nothing is singled out and nothing is exempted.

“Every modern disinformation campaign will exploit news media channels on digital platforms by gaming the system,” she warned. “If the DSA makes it illegal for platforms to address these issues we risk undermining the effectiveness of the law — indeed we may be worse off than today’s situation.”

During the Q&A, Haugen also faced a couple of questions from MEPs on new challenges that will arise for regulators in light of Facebook’s planned pivot to building the so-called “metaverse.”

On this she told lawmakers she’s “extremely concerned” — warning of the increased data gathering that could flow from the proliferation of metaverse-feeding sensors in homes and offices.

She also raised concerns that Facebook’s focus on building workplace tools might result in a situation in which opting out is not even an option, given that employees typically have little say over business tools — suggesting people may face a dystopic future choice between Facebook’s ad profiling or being able to earn a living.

Facebook’s fresh focus on “the metaverse” illustrates what Haugen dubbed a “meta problem” for Facebook — aka: That its preference is “to move on” rather than stop and fix the problems created by its current technology.

Regulators must throw the levers that force the juggernaut to plot a new, safety-focused course, she urged.",Yes,"The people who are most engaged and active on the platforms are the ones who see the most content, and this content is tailored to their interests. As a result, people are becoming more and more entrenched in their own echo chambers, only seeing content that reinforces their existing beliefs. This can lead to an increase in political polarization and the spread of misinformation",https://techcrunch.com/2021/11/08/facebook-whistleblower-frances-haugen-european-parliament/,2021-11-08 00:00:00,LABEL_1_relevant,0.9963573813438416,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
104,Junk news on social media is shared predominantly by the right wing,"Analyzing the dynamics of online political news sharing shows that there’s an ideological pattern to who’s spreading the bad stuff.

The study: Researchers at the Oxford Internet Institute monitored 13,500 politically active US Twitter users and 48,000 public Facebook pages for the three months ahead of the recent State of the Union address. They then studied how political news that was extremist, sensationalist, conspiratorial, fake, or otherwise characteristic of “junk” got shared.

The results: On Twitter, a core of Trump supporters shared “the widest range of known junk news sources” and circulated “more junk news than all the other groups put together.” On Facebook, extreme hard-right pages (not Republican pages) achieved a similar feat.

Why it matters: Fake news remains a huge headache for social networks and political leaders. A better understanding of how it’s shared could help clean it up.",yes,"The way it has allowed fake news to spread quickly and easily. A study has shown that a core group of Trump supporters on Twitter are responsible for sharing the widest range of known fake news sources. On Facebook, extreme hard-right pages are also responsible for sharing a lot of fake news.",https://www.technologyreview.com/2018/02/06/241313/junk-news-on-social-media-is-shared-predominantly-by-the-right-wing/,2018-02-06 00:00:00,LABEL_1_relevant,0.9931371212005616,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
105,Here’s How Mark Zuckerberg Sees Facebook’s New Era of Privacy,"Mark Zuckerberg once promised Facebook would move fast and break things. Now Zuckerberg says Facebook is trying to fix the things it broke.

Standing on stage before an audience of developers at the annual F8 Conference on Tuesday, Zuckerberg—the same guy who spent years convincing billions of people to share their every thought and action with the world—explained all the ways Facebook is going to help people keep that same information under wraps.

“I believe the future is private,” the CEO said, almost as soon as he began, setting the tone for a day of product announcements across Facebook, Instagram, Messenger, and WhatsApp.

In his much-anticipated keynote address, Zuckerberg readily admitted he’s an odd champion for the cause of privacy, particularly after the year Facebook has had. The social networking giant now faces more than a dozen international investigations into its history of privacy violations, from its years of willy-nilly data sharing to several recent data breaches.

“I know that we don't exactly have the strongest reputation on privacy right now, to put it lightly,” Zuckerberg said through nervous laughter. But he seems to believe a Facebook redesign and a litany of new products focused on messaging and groups could turn that reputation around. “At the end of the day, this isn't just about building some new products,” Zuckerberg said. “It's a major shift in how we run this company.”

To mark this supposedly new era, Zuckerberg unveiled a subtle redesign for Facebook that places more emphasis on Groups. There’s now a Groups tab at the center of the app, content from Groups will appear more often in News Feed, and Facebook will give users more prompts to discover and join new Groups. Facebook is also adding new features for specific types of communities. Groups related to jobs, for instance, will now accept job postings from employers, while health support Groups will enable users to ask administrators to post on their behalf, to protect their privacy.

“Basically, now, everywhere where you're going to be able to see and connect with your friends you're also going to be able to see and connect with groups that you care about,” Zuckerberg said. “It all adds up to this feeling that groups are now at the heart of the experience, just as much as your friends and family are.”

Other announcements were tailor-made to get people out from behind their computer screens and meeting up in person. One such product, called Secret Crush, gives Facebook users a way to indicate which of their Facebook friends they’re romantically interested in and get notified if the feeling is mutual. Facebook has also revamped its Events tab to make it easier to find nearby events.

These announcements are, in many ways, a continuation of a multiyear shift underway at Facebook. For more than two years, the company’s leaders have expressed a desire to reorient Facebook around smaller communities. Zuckerberg first indicated this switch in 2017, when he published a 5,000-world manifesto on making Facebook a platform for enabling real-world connections. It was a departure from the social network’s long-stated vision of connecting everyone in the world to the most possible people. The flaws in that vision became starkly apparent around 2016, when propagandists and bad actors abused so many thin, anonymous connections to manipulate and divide people for political gain.",Yes,It can be used to manipulate and divide people for political gain.,https://www.wired.com/story/f8-zuckerberg-future-is-private/,2019-04-30 20:32:15.570000+00:00,LABEL_1_relevant,0.9758919477462769,Social Media,social_media,Wired,"Information, Discourse & Governance"
106,"Amid Social Distancing, Neighbors Mobilize Over Facebook","A few weeks ago, David Perez’s brother handed him a package of face masks and warnings about a new coronavirus spreading around the United States. Perez had heard about Covid-19, the disease caused by the virus, but he wasn’t sure how serious it was, so he started doing some research. Online, he found startling reports, bizarre conspiracy theories, and a whole lot of questions about what people were supposed to do before the outbreak reached them.

The official line from the Centers for Disease Control is for people to wash their hands, cover their coughs, and avoid large gatherings. “Social distancing,” or the practice of self-isolating, has been understood as key to preventing community spread of the virus. Keeping people out of shared, public spaces could be the key to keeping the spread low enough to meet hospital capacity. That will be critical in the days and weeks ahead.

But that advice isn’t always easy to decipher. Should you still send your kid to school? Cancel a vacation months in advance? And while you’re home alone for the foreseeable future, what else are you supposed to do to make sure your community can survive?

Content This content can also be viewed on the site it originates from.

The responsible amount of social distancing is often context-dependent, which has made it difficult for experts to offer yes-or-no answers. (This guide, from The Atlantic, gets into some of those nuances.) The guidance can differ between people, cities, and specific locations within those cities, making it that much more important for neighborhoods and communities to band together.

Perez lives in San Bernardino, California, which as of Friday still has no reported cases of Covid-19. Even still, the city has preemptively declared a public health emergency and encouraged residents to begin practicing social distancing. Perez wanted to figure out what else he could do to prevent the worst from reaching his community. So he created a Facebook group, California Coronavirus Alerts, for locals to huddle up about what was going on.

California Coronavirus Alerts is just one of many Facebook groups that have sprung up in response to growing concerns about the coronavirus. Some focus on emergency preparedness, while others exist to share news about the virus. Perez’s group specifically coordinates people in the Inland Empire, the region of California where he lives. Some are using it to ask for region-specific advice (is it still OK to go to Disneyland?) while others have shared notices and practices from local colleges, libraries, other spaces.

Drew Harris, who studies population health at the Thomas Jefferson University in Philadelphia, says people should be concerned first and foremost with social distancing—it’s the only way to “flatten the curve” and ensure that hospitals don’t exceed their capacity. But next, it’s worth considering who in a community might be impacted most in a crisis. Harris is specifically concerned with what he calls second- and third-order effects of the virus. If schools get closed, for example, what happens to kids who rely on school lunch for a meal? What happens to the homeless? “The blood centers I’m part of are saying, ‘Let’s cancel all of our blood drives,’” says Harris. “And now we have a blood shortage.”",Yes.,"It can be used to spread misinformation. This can have a number of negative effects, including causing people to make poor decisions about their health, and leading to panic and anxiety. In the case of the coronavirus, it is especially important for people to get accurate information from reliable sources, and to avoid panicking.",https://www.wired.com/story/coronavirus-social-distancing-neighbors-mobilize-facebook/,2020-03-14 12:00:00+00:00,LABEL_1_relevant,0.9962771534919742,Social Media,social_media,Wired,"Information, Discourse & Governance"
107,Can Twitter Fix Its Harassment Problem without Losing Its Soul?,"Clay Shirky, an associate professor at New York University who has written two books about social media, thinks the problem has reached a point where Twitter needs to remind users about what is and isn’t okay, and devise some consequences for misbehavior. Even though it may be labor intensive and therefore expensive to keep and enforce such rules, he says, the Twitter community would benefit from it.

“There’s a bigger threat to not taking on this problem than taking on this problem, simply because public sympathy is going to go more in the direction of the abused than the abusers,” Shirky says.

But while the time seems right for Twitter to act, it is far from clear how best to discourage such behavior. It does have methods in place to help deal with abuse, such as the ability to block and report a user who’s bothering you. Yet while that may help if you’re dealing with one or even a few bothersome tweeters, it cannot stop a deluge of nasty posts, and a determined harasser can always just make a new user profile and start the harassment anew.

How to make Twitter safer without turning off some existing users may be an even trickier question. The freewheeling exchange of views and opinions that characterizes Twitter is part of its appeal; if it were to enforce strict controls over who could talk to whom, or—like Facebook—require people to disclose their real identities, that might cut down on the flow of information.

Improvements may come if Twitter carefully combines increased communication with users and technology for automatically identifying harassers.

Twitter might reduce the burden on its own staff by involving its users more directly. Justin Patchin, a professor of criminal justice at the University of Wisconsin-Eau Claire and co-director of the school’s Cyberbullying Research Center, says Twitter could ask its community to regulate itself—perhaps by allowing users to volunteer to vote on whether or not content is abusive.

Kate Crawford, a visiting professor at the MIT Center for Civic Media and a principal researcher at Microsoft Research, suggests that people experiencing harassment could share lists of users they have blocked with others having the same problem. “There is no quick technical fix for a social problem,” says Crawford. “What there can be is a broadening of the understanding of what the problem is.”

Twitter doesn’t preëmptively spot and wipe out offensive content, yet Jerry Zhu, an associate professor of computer science at the University of Wisconsin-Madison who has studied the use of machine learning to track abusive posts on Twitter, says artificial intelligence can help spot and censor nasty posts by looking out for key words and phrases. This is hard to do with certainty if bullying is not explicit, though; sometimes people are being nasty without using obviously mean language. “This is where the current technique is hitting the AI limit in some ways in that computers cannot be that subtle and parse that meaning out of it accurately,” he says.

While any changes will risk upsetting some users, that may be a price that Twitter is willing to pay in the short term to create a more hospitable service over the long run. Says Robin Kowalski, a psychology professor at Clemson University who studies cyberbullying: “If they lose a few users, they’re going to gain a few more.”",Yes,"Online harassment. Twitter is considering ways to reduce online harassment, but it is difficult to do so without also limiting freedom of expression.",https://www.technologyreview.com/2015/02/12/169381/can-twitter-fix-its-harassment-problem-without-losing-its-soul/,2015-02-12 00:00:00,LABEL_1_relevant,0.9963067770004272,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
108,The Internet Made Dumbledore Gay,"Just imagine the fanfiction now. It’s the kind of thing people write on social media after a tense scene between two heavily “shipped” characters, or when fan-favorite actors get cozy off set. It’s also what Potterverse creator J.K. Rowling said to a shocked Carnegie Hall in 2007, moments after she revealed that she’d “always thought of” Albus Dumbledore, high wizard of Hogwarts, as gay. Slashfic indeed followed.

That day, Rowling’s announcement was mostly met with applause. These days, Rowling’s social-justice-flavored extratextual canonical embellishments—not only about Dumbledore’s sexuality, which she elaborated on over the weekend, but also about Hermione’s race, Harry’s views on Israel, and Nagini’s hidden origins as a cursed Asian woman trapped in the body of a snake kept as a pet by a white supremacist—are mostly met with boos. Over the past decade-plus, Rowling seems to have lost touch with, if not downright alienated, the portion of the internet she hopes to engage with and delight with her cutesy, random-seeming new factoids. Now fans must ask themselves: Do they share some of the blame?

You may not be a fanfiction writer or reader or even a Potterhead, but you almost certainly dive into some fictional world on occasion, and those dives are likely longer and deeper than they used to be. Movie studios and companies like Netflix have become experts in stretching your attention: optimizing a show’s bingeability, engineering viral moments and social media speculation, leaning into easter eggs and online exclusives, buying up superfranchises so vast and rich you can make an unlimited numbers of books, movies, TV shows, videogames, VR experiences, and theme park rides out of them. (It takes over 11 days to watch the entirety of the Marvel Cinematic Universe. You’ve probably done it at least 11 times already.)

For them, it’s a matter of profit. For you, it’s more Pavlov. When you immerse yourself in pleasurable worlds and stories for long periods, you’re practically mainlining dopamine, which increases the likelihood that you’ll not only like the characters you’re watching—you’ll begin to identify with them. That leaves you mildly Netflix- or book-addicted and heavily invested in the people you’ve come to know during your binge. Your totally reasonable response when it all ends: more, longer, and right now, thanks. These are my friends!

A version of this voracity has been happening probably forever. People used to line the docks waiting for the next installment of Charles Dickens stories, screaming questions about the characters to the crew as the boats drew near. Lord of the Rings developed a cultlike following in 1960s counterculture—they scrawled messages like “Frodo Lives!” or “Gandalf for President” on the walls of subway stations, the Twitter of their day. But there’s only so much you can communicate in something as small and impermanent as a graffito, and writers at the time were far removed from fan pressure for more more more. Tolkien didn’t publish his book-length world-building exercise, The Silmarillion, in his lifetime. Had he been on Twitter, he probably would have.

J.K. Rowling is very much on the internet, and she knows its power and potential. When she asked her audience in 2007 to imagine the fanfiction—to imagine the ways fans would react to this extension of the Harry Potter universe with their own versions and elaborations—she wasn’t condemning the practice. She was encouraging it. Fanfiction, and, if my teenage memory serves, especially Harry Potter fanfic circa 2007, has always been a space for queer storytelling. Rowling was adding canonized kerosene to a spark that was already there. She’d keep up the practice for years to come, blurring the boundary between her sacred text and fannish wish fulfillment.

How fans consume stories—and what they’ve come to expect—necessarily changes how they’re created and told. Pissing off the fandom can hinder the success of your story, and fans, through social media, now have a more direct line to their favorite stories’ creators than their Dickensian counterparts. More strategic handlers of stories with large and vocal fandoms have taken Rowling’s tactic—if you can’t beat them, feed them—and carefully, calculatingly run with it. At this point, Rowling herself seems to be running with scissors, ready to slice up your childhood. The fact is, her “revelations” have come to seem as remote and unnatural as bad fanfic, and many fans have begun to view them as such.",Yes,"It has allowed creators to directly engage with their fan bases to a degree that was previously impossible. This has led to a situation where creators are under immense pressure to please their fans, which can result in bad story-telling as creators make choices based on what they think fans will want to see, rather than what makes for a good story. In",https://www.wired.com/story/the-internet-made-dumbledore-gay/,2019-03-19 21:56:08.325000+00:00,LABEL_1_relevant,0.9959371089935304,Social Media,social_media,Wired,"Information, Discourse & Governance"
109,"YouTube geoblocks Russia Today, Sputnik channels in Europe","Google has just announced that YouTube will geoblock the Kremlin-affiliated media outlets Russia Today (RT) and Sputnik in Europe, following pressure from regional lawmakers to choke off Russian war propaganda.

In a tweet announcing the geoblocks today, Google’s Europe policy team writes: “Due to the ongoing war in Ukraine, we’re blocking YouTube channels connected to RT and Sputnik across Europe, effective immediately.”

We’ve reached out to the Commission for a response to YouTube’s announcement.

As TechCrunch reported earlier, the EU confirmed last night that the ban on RT, Sputink and their subsidiaries will cover all distribution channels, including online platforms.

Due to the ongoing war in Ukraine, we’re blocking YouTube channels connected to RT and Sputnik across Europe, effective immediately. It’ll take time for our systems to fully ramp up. Our teams continue to monitor the situation around the clock to take swift action. — Google Europe (@googleeurope) March 1, 2022

On Monday the EU’s internal market commissioner, Thierry Breton, held a video call with the CEOs of Google and YouTube to press them to step up their efforts.

YouTube hosts thousands of videos by the two Russian state-affiliated media firms and — as we noted yesterday — RT’s own channel marketing claims it’s “the most watched news network on YouTube”.

Now Europeans who try to browse to RT’s YouTube channel are met with a message that “this channel is not available in your country”:

Yesterday Facebook, Microsoft, TikTok and Twitter announced similar restrictions.

But Google took a little longer to consider its response. The tech giant has not offered an explanation for the delay.

In its tweet announcing the geoblocking this morning, Google also warned: “It’ll take time for our systems to fully ramp up. Our teams continue to monitor the situation around the clock to take swift action.”

So the company appears to be signalling that it expects some content from Kremlin-linked channels to continue to be accessible in the near term — as its systems “ramp up”.

With YouTube only geoblocking RT and Sputnik, rather than banning or suspending the accounts, Russia’s propaganda can of course continue to spread outside Europe — including still being available inside Russia itself.

EU #sanctions stopping Russian propaganda. But in the rest of the world #Google won’t stop making money with Russian lies about Ukrainans fighting for their lives and their freedom. Shame on you. #StandingWithUkraine #UkraineKrieg #Zelenskiy #DSA https://t.co/2bEnwVD5zu — Alexandra Geese (@AlexandraGeese) March 1, 2022

Google is not alone in opting for this compromise, though.

Facebook’s parent Meta and TikTok also opted to geoblock Russian-state affiliated media accounts, rather than suspending or banning their accounts entirely.

Twitter and Microsoft have also taken a slightly different tack, reflecting the difference nuances of their platforms — saying they would take measures to reduce visibility of state-sponsored RT and Sputnik content, so essentially limiting their freedom of reach.

Twitter also expanded its labelling policy yesterday — adding notifications to tweets that link to Russian state-affiliated media and warning users to “stay informed”.

Given the latter is far more an information network than a social (or broadcast) network, labelling and contextualizing seems an appropriate response — although it remains to be seen whether the EU’s incoming ban on RT and Sputnik will require web platforms to go further.",Yes,"The spread of false information. This can be in the form of fake news, conspiracy theories, and other forms of misinformation. Social media platforms have been criticized for not doing enough to prevent the spread of false information. In response, some platforms have implemented fact-checking measures, while others have banned or limited the reach of state-affiliated media outlets.",https://techcrunch.com/2022/03/01/youtube-geoblocks-rt/,2022-03-01 00:00:00,LABEL_1_relevant,0.9954013824462892,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
110,It's Time to Defund Social Media,"Political speech has always been tethered to public health. The mass protests that erupted following George Floyd’s murder foreground this overlap: White supremacy is a public health disaster. Climate denialism and anti-vaxx activism similarly threaten the lives and safety of citizens around the globe. With Covid-19, the line between political speech and public health has eroded in even more distressing ways. Objects of science have, for many, been reduced to matters of opinion—or even outright conspiracy. In those cases, masks aren’t just masks; they’re symbols of oppression. Anthony Fauci isn’t just the country’s most prominent infectious disease expert; he’s part of a Deep State cabal seeking to undermine the Trump administration from within. The virus itself isn’t ripping through our neighborhoods, forcing cities like Houston to use backup morgue space; it’s a hoax, or something the Democrats are exploiting to goose their election odds. The inability to cordon off the basic facts of public health from reactionary propaganda threatens people’s lives. And, like everything in this godforsaken pandemic, things are on track to get much worse.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

While it might be tempting to blame the people who refuse to wear masks, or the politicians who would rather discuss individual rights than public health, or the president who would rather tweet about beans, our present crisis runs much deeper than any single individual or group. The structures and assumptions we take for granted are themselves part of the problem. Calls to defund the police provide a helpful analogue. Until we fundamentally reimagine what law enforcement means and does, lasting change will not be possible; all the injustice baked into the system will continue emerging from that system. Something similar could be said of social media: Until we fundamentally reimagine our information ecosystem and our respective roles within it, we’ll keep repeating the same patterns over and over—not as a bug of the system, not as a feature of the system, but as the system itself.

One force we must confront is the attention economy, an incentive structure designed to reward the most uncompromising, polarized, clickable minority. (Ironically, this minority is very often part of the white majority; see breathless, disproportionate coverage of white nationalists and supremacists following the 2016 election.) The resulting tyranny of the loudest presents an algorithmically-warped view of what’s happening in the rest of the United States.

The debate around masks is a textbook example. The articles that trend and videos that go viral don’t feature the majority of Americans who are perfectly fine wearing masks; who do so without once throwing a temper tantrum in Costco. Without question, there are people who cannot stop throwing temper tantrums in Costco; it’s become its own genre of performance art. It’s also true that many Republican politicians have flouted CDC guidance on masks or refused to issue mask mandates; Georgia governor Brian Kemp is even suing the city of Atlanta to prevent enforcement of a mandate. All of this is unquestionably dangerous; in a global pandemic, it doesn’t take a very high percentage of the population to put everyone else at risk—with particularly dire consequences for Black and Latino communities, making mask recalcitrance (indeed, any Covid-related recalcitrance) as much a threat to civil rights as to public health. Still, it just isn’t the case that, across the country, everyone is screaming at each other about masks. Most people shut up and wear them.

If we could emphasize the common-sense consensus on masks, it wouldn’t just affirm our faith in humanity. (Though it certainly would do that: I’d spend hours on a YouTube channel dedicated to groups of people being reasonable.) It would also undercut the attention economy, in two ways.

First, it would minimize the incentive to be an asshole. If you’re not rewarding people with clicks and likes for antagonistic behaviors, there’s less reason for them to keep doing it. This is a dynamic as old as trolldom. As long as something generates capital—whether economic or social—there’s no reason to stop. In fact, one’s livelihood might depend on keeping it up, and doing it even worse the next time.",Yes,"It has become a tool for authoritarian leaders to get people to believe lies. 

The second way that emphasizing the common-sense consensus on masks would undercut the attention economy is by affirming a shared reality—one that isn’t subject to the whims of a 24-hour news cycle or the algorithm’s insatiable",https://www.wired.com/story/its-time-to-defund-social-media/,2020-07-21 12:00:00+00:00,LABEL_1_relevant,0.9939274787902832,Social Media,social_media,Wired,"Information, Discourse & Governance"
111,Twitch’s DMCA Takedowns Threaten to Drive Musicians Away,"As a fan of underground electronic music, the music producer, Twitch streamer, and label owner Chris Reed, who goes by the stage name Plastician, was used to going the extra mile to catch a show.

“I literally used to have to climb through tower block building windows and hide from the police.” Today, he’s a DJ and runs a music label called Terrorhythm. He goes on, “This is like nothing compared to the shit we've had to put up with before. A lot of us have been through shit like this in the past.”

The shit in question is his and other DJs’ ongoing struggle with the DMCA. While Plastician no longer finds himself climbing through windows or ducking the police, he navigates a more abstract, virtual obstacle course: Twitch.

As Twitch grew, so did the diversity of content. Initially a site dominated by video game streamers, in 2015, the website created a new category of content called Creative and IRL. Then, by 2018, the company announced that this category had “exploded in growth,” and as a result, it created a new category for music.

For Dave Eckblad and many other DJs, Twitch became a way to maintain their local music scene during the coronavirus pandemic. Before the pandemic, he worked in music promotion and co-owned a record store. Now, he produces the Twitch stream for the Minneapolis-based music collective Intellephunk.

""After lockdown we had nowhere to do what we do.” Eckblad says He explains that one of the members of his collective “had to pivot and figure out some new stuff to do. And I was able to help him get that going quick. I had already been doing stuff on Twitch, on and off, so we worked to put a show together.” It wasn’t as much money as a live event, but the stream allowed Eckblad and Intellephunk to continue to engage their community and make some money off of tips from viewers watching the stream.

Intellephunk wasn’t the only music group to take its music to Twitch. At the outset of the pandemic, musicians flooded the platform with content to make up for canceled live events. Entire concerts and music festivals went completely online to Twitch and streaming platforms like it. A report from StreamElements noted that the number of hours users spent watching music and performing arts rose from 3.6 million to 17.6 million hours over the course of one year.

But that rosiness and excitement soon started to dissolve.

Last October, Twitch sent a letter to streamers regarding Digital Millennium Copyright Act (DMCA) violations. Any channel that had video content with copyrighted music must take it down or risk having their channel deleted. The public backlash was so strong that Twitch issued an apology within 24 hours. The company stated that the frustration of streamers was “completely justified,” and that it was just as surprised as content creators were by the volume of DMCA requests it had received. But Twitch maintained its warning: “If you play recorded music on your stream, you need to stop.”

The Threat of the DMCA

DJs already know the threat of copyright abuse. Twitch’s DMCA takedowns represent another setback in a long history of push and pull between creators and copyright enforcement. What is new, however, is how these stories illustrate the larger issue of uneven and confusing copyright enforcement on the internet. As Twitch doubles down, DJs cast doubt on the platform’s future with music.

Sarvesh Ramprakash, also known as Icarus Redux, started in Los Angeles’ electronic music scene in 2014. Today he spins at shows across the midwest and organizes music communities for artists of color. He told me he hasn’t performed recently on Twitch, specifically because of the DMCA issue.",Yes,"It's not easy to control what people share or where they share it. This makes it difficult to protect the intellectual property of content creators, like musicians.

The 1998 Digital Millennium Copyright Act was designed to help content creators protect their work online. The DMCA made it illegal to circumvent digital rights management (DRM) technology, and it established",https://www.wired.com/story/twitch-coronavirus-musicians-dmca-takedowns/,2021-02-24 12:00:00+00:00,LABEL_1_relevant,0.9964696168899536,Social Media,social_media,Wired,"Information, Discourse & Governance"
112,Making Money with Social Media,"The goal, says Social Agency cofounder Scott McCaskill, is to let companies see “whether all the time put into doing those things is really helping build brand or product awareness, which kinds of content are most successful, what days and even times of day result in the most traffic or new followers/friends.”

A free version allows a company to manage a single identity or “voice” across each platform. Paid versions let companies coordinate multiple users and voices, and provide a longer data history. McCaskill says the software has had the most success with units of large companies and marketing agencies.

Media metrics: With Spredfast, companies can evaluate how people read, pass along, or comment on content on social media websites over time.

Spredfast gives companies a way to plan and manage content deployment. For instance, users can write blog entries, tweets, or Facebook updates ahead of time and then schedule when they will be posted. A store that might offer an online coupon code or one-day sale could, with Spredfast, have Twitter push that code out several times a day to increase the number of site visitors. The software’s metrics, McCaskill says, let marketers figure out the best times to post updates. Spredfast also makes it easy for them to test different strategies.

The company launched a year ago as a maker of custom Facebook applications. When Facebook redesigned its home page, says McCaskill, Social Agency’s business model was effectively torpedoed. As part of its sales strategy, the company had spent a lot of time helping clients plan their social media strategies. So the founders retooled and used their expertise to start building Spredfast about nine months ago. The software launched in private beta in September, public beta in October, and had its “official” launch on December 2.

Social Agency plans to introduce a feature by the end of January that will help users design a social media campaign based on their objectives. McCaskill says that Spredfast will most likely present users with a list of common marketing goals that they can check off. The software will suggest a template for a campaign based on what’s worked best for clients with similar goals.",Yes,It can present a negative image of the company to potential consumers.,https://www.technologyreview.com/2009/12/29/206864/making-money-with-social-media/,2009-12-29 00:00:00,LABEL_1_relevant,0.9944847226142884,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
113,These Companies Are Already Living in Zuckerberg’s Metaverse,"Facebook’s metaverse, or Meta’s metaverse, isn’t just being touted as a better version of the internet—it’s being hailed as a better version of reality. We will, apparently, “socialize, learn, collaborate, and play” in an interconnected 3D virtual space that Facebook cofounder and CEO Mark Zuckerberg describes as an “embodied internet.” This space, Zuckerberg claims, won’t be created by one single company, but rather by a network of creators and developers. First problem: 91 percent of software developers are male. Second problem: You’ve been living in a version of metaverse for years—and, having taken over video games, it’s now coming for the world of work.

Companies big and small have been testing avatar-based platforms for remote and hybrid working since Covid-19 lockdowns began. Using Oculus VR headsets, Facebook’s Horizon Workrooms envisages a near future in which people meet virtually in a soulless, floaty virtual world. Microsoft’s Mesh for Hololens 2 hopes to facilitate similarly corporate mixed reality meetups, and Canadian ecommerce platform Shopify just launched its browser-based game Shopify Party, in which employees appear as their chosen avatars to spice up one-to-ones, icebreakers, standups, and other team events.

Many have already pointed out how boring the Zuckerverse looks. Most of us have already been living in that future, be it through the organized fun of workplace social apps or through video games like Fortnite. And while the video game metaverse offers plenty of room for imagination and connection, the corporate metaverse risks repeating and potentially magnifying the flaws of the real world.

Whether a company adopts certain aspects of the corporate metaverse, or uses it for every aspect of remote work, there’s nothing to stop unconscious biases from seeping in. “It’s easy for companies to just invest in the technology, but businesses need to understand the psychology driving people to use it,” says Roshni Raveendhran, a professor at the University of Virginia’s Darden School of Business. “Can employees thrive within that sort of universe? What would allow them to thrive?”

Unfettered by the risks of mass metaverse adoption—or, less excitingly, lots of companies using remote collaboration tools—a gaggle of startups are piling in to sell the future. The most well-known virtual workspace tool is Gather.town, which amassed 4 million users in just over a year as the pandemic took hold. Its retro, pixelated design is intentionally basic, while Roblox’s Loom.ai and Teeoh use sophisticated graphics for more realistic virtual worlds. The preeminent simulation platform Second Life was adopted by Cisco and IBM over a decade ago. Despite virtual and augmented reality companies consistently promising the world and failing to deliver, a 2020 report from consultancy PwC predicts that nearly 23.5 million jobs worldwide will use AR and VR by 2030 for tasks such as employee training, meetings, and customer service.

For businesses, the most interesting benefit of avatars, a video game staple pioneered by NASA employees in the 1970s, is the sense of digital proximity, without needing to focus on facial expressions—the cause of the much-maligned Zoom fatigue. And while self-expression is the allure of video game avatars, it’s not yet clear what employees gain from being asked to exist in the corporate metaverse.",Yes,"It can lead to internet addiction. This is especially true for young people, who are the most active users of social media. Internet addiction can lead to a number of problems, including sleep deprivation, anxiety, and depression.",https://www.wired.com/story/these-companies-already-live-in-zuckerberg-metaverse/,2021-11-02 11:00:00+00:00,LABEL_1_relevant,0.9879339933395386,Social Media,social_media,Wired,Health & Wellbeing
114,Netflix v Modi and the battle for Indian cinema’s soul,"In 2018, Netflix hired Arya to commission feature-length content. That year, the company made more than 20 original films and five original series in Hindi. But this did little to alter its public persona. In a country with more than 24 major languages, Netflix was still viewed as an English-language platform for westernized Indians. And this is where Arya, who knew everyone who mattered in Hindi film, fit into the picture. She had worked in advertising, and then as an actor and a writer, before moving on to TV production.

Soon she enlisted many of her childhood friends, who had grown up to become some of the most powerful people in the Hindi film industry, to work for Netflix. She signed on Zoya Akhtar, whose last feature film was India’s official entry to the Academy Awards, to direct a short film. Like Arya, Akhtar comes from a film family, but because Bollywood is a male-dominated industry, it’s still almost impossible for a female filmmaker or female-oriented films to raise capital. By contrast, several women helmed projects at Netflix. The platform’s biggest star is Radhika Apte, a Bollywood actress who has appeared in so many Netflix productions that online wags joke she’s in all of them.

Srishti Behl Arya, who runs Netflix's division of Indian original films. NETFLIX

But working with Bollywood meant dealing with its shortcomings. Netflix held several workshops in Mumbai to train Indian content creators. It taught them how to develop a major series, but also helped them brush up on basics such as how to write, schedule, and budget. “That’s how we can add value to the industry,” Arya told me. “By helping it get more organized.”

On my last day in Mumbai, I went to visit Red Chillies Entertainment, a towering production house owned by Shah Rukh Khan, which produces shows for Netflix. Back in 2017, Hastings and Khan had appeared together in a stilted promotional skit announcing a new spy thriller called Bard of Blood.

The foyer was deserted on the day I arrived, except for a beautiful sculpture of Ganesha, a Hindu god who is viewed as the patron of the arts. It was wrapped in plastic to protect it from construction dust. Around it some barefoot workmen were operating power tools without any protective gear. On the fourth floor, an exhausted-looking man with slippers on his feet and salt in his dark hair emerged from an editing studio. Several years ago, newly graduated from the London School of Film, Patrick Graham had been struggling to land projects when a friend suggested he try Bollywood. He floundered at first, stifled by censorship. But then, in 2018, Netflix India gave Graham the budget to produce a fictional series in which Muslims are rounded up in internment camps. They also brought him in to co-write the screenplay for Leila. When we met, he was wrapping up production on Betaal, a four-episode zombie series that would be released the next year. Months earlier, in a conversation on the phone, Graham had seemed pumped at the opportunity. “It’s massive,” he’d said. But in person, in Mumbai, he was downcast. “I have to go through the series and remove anything that might offend,” he told me, gloomily. “The oversensitive people are winning.”

In November 2020, Hindu nationalists went after Netflix again. Mira Nair’s critically acclaimed adaptation of Vikram Seth’s novel A Suitable Boy showed a Muslim boy kissing a Hindu girl. A leader of the BJP’s youth wing filed a police complaint about the series for “shooting kissing scenes under temple premises.” The leader accused the show of promoting “love jihad”—a conspiracy theory that claims Muslim men are seducing Hindu women in order to convert them to Islam.",Yes,People are now more tribal and echo chamber effects are more pronounced.,https://www.technologyreview.com/2021/03/24/1021139/netflix-india-modi-kashyap-sacred-games-censorship/,2021-03-24 00:00:00,LABEL_1_relevant,0.9838780164718628,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
115,The Life and Viral Fame of Virginia's Two-Headed Snake,"Late last summer in Woodbridge, Virginia, a woman wandered into her yard and found an eastern copperhead slithering through her flower bed. That's not so unusual where she lives, as the region is home to a plethora of ophidians, from harmless corn snakes to venomous rattlers. But this one was different: It had two heads.

It's called dicephaly, a mysterious disorder occurring in just one out of every 100,000 snakes born in the wild and one out of 10,000 born in captivity. Affected snakes possess two brains with distinct personalities, though one head typically dominates the other, which might lack a trachea, esophagus, or even eyes. Scientists suspect it happens when an embryo in the early stages of development divides—possibly induced by sudden temperature changes, environmental pollution, or inbreeding. Whatever the cause, these unlucky creatures don't live long. Nearly half are dead on arrival, and few survive beyond the first few months.

The Woodbridge serpent—er, serpents?—were at most three weeks old, no longer than a Penguin paperback, but it (they?) caused a stir. Naturally, pictures made their way onto Facebook, then inevitably CNN, The New York Daily News, and even Snapchat. Calls began flooding the Virginia Department of Game and Inland Fisheries—which removed the critter from the woman's property—from people curious to see it up close and zoos eager to take it off their hands.

""After about 48 hours of that madness, I was like, I'm done,"" says state herpetologist John D. Kleopfer. ""I don't know how these celebrities, like the Kardashians, live.""

Kloepher enlisted the help of Cooper Sallade, a respected viper breeder in Richmond, who agreed to raise the bifurcated snake on a strictly confidential basis. Sallade, 27, has been handling ophidians since childhood, when he'd catch them outside and keep them in jars under his bed. Now he has a nondescript warehouse full of them. He keeps up to 300 at any given time locked away in temperature-regulated containers on PVC rack systems that appear, at a glance, like filing cabinets.

""Reptiles are actually not my favorite animals,"" Sallade says. ""I prefer birds and large mammals, but you can't keep a whole house full of bears.""

'After about 48 hours of that madness, I was like, I'm done. I don't know how these celebrities, like the Kardashians, live.' Herpetologist John D. Kleopfer

At the Game and Fisheries' office outside Richmond, Sallade coaxed the copperhead into a Rubbermaid food container with air holes drilled into the sides, placed that into a wooden box labeled ""venomous snake,"" screwed it shut, then drove it to his facility. For the next two and a half months, he quarantined it in a special room with a separate HVAC system to stop the spread of any possible pathogens to his other reptiles. The snake didn't move much or eat. So once a week, Sallade force fed it a euthanized baby mouse, gently pinning down both heads with a foam hobby brush while using tweezers to slowly massage the rodent into the less developed head, which happened to have the most developed gullet and windpipe. Soon it was regularly defecating and shedding its skin. Sallade felt hopeful.

""Since the snake had such an incomprehensible amount of media attention, there was a lot of pressure on me to keep that thing alive,"" he says.

But alas, one December morning, Sallade went in to check on it and found the snake had died. He was sad, but not exactly surprised. ""If it had been a snake that was born in my collection, I wouldn't have told anybody about it,"" he says. ""Honestly, I would probably have euthanized it myself, because it was so hard for the snake, just being alive.""",Yes.,It can lead to a lot of pressure and stress on people who would otherwise not have to deal with it.,https://www.wired.com/story/two-headed-snake/,2019-06-13 21:53:03.510000+00:00,LABEL_1_relevant,0.9962748289108276,Social Media,social_media,Wired,"Information, Discourse & Governance"
116,The Battle for Union Representation on YouTube,"Without video creators, YouTube wouldn’t be one of the world’s biggest social platforms. Without the platform, YouTubers wouldn’t be, well, YouTubers. But video creators are regularly facing new policy changes from YouTube that could impact their ability to make money from their work—and it’s not always clear what these changes are or why YouTube is making them.

Now, as part of a push for fair treatment, YouTubers are looking to collective action. The effort is being led, in part, by an unlikely character: a creator in Germany who makes high-powered slingshots for his audience of 2.3 million subscribers. This week on the Gadget Lab podcast, we talk with WIRED staff writer Emma Grey Ellis about what YouTubers hope to get out of their unionization efforts and what the movement means for the video giant.

Content This content can also be viewed on the site it originates from.

Also in the news: Mark Zuckerberg gets grilled by the House Financial Services Committee about Libra, Facebook’s proposed cryptocurrency … except the hearing was about much more than just Libra.

Show Notes:

Read Emma’s story about the YouTubers union here. And here’s Steven Levy’s story about the Libra hearing in Washington. Read Lauren's review of the Samsung Galaxy Fold here.

Recommendations:

Emma recommends the science and comedy podcast Ologies With Alie Ward. Mike recommends the book I Like to Watch by Emily Nussbaum. Lauren recommends the book Catch and Kill by Ronan Farrow. Arielle recommends Google’s experimental Digital Wellbeing features, like the one that batches your notifications for you.

Emma Grey Ellis is on Twitter @EmmaGreyEllis. Lauren Goode can be found @LaurenGoode. Arielle Pardes is @pardesoteric. Michael Calore is @snackfight. Boone Ashworth, who edits the show, can be found at @BooneAshworth. Bling the main hotline at @GadgetLab. Our theme song is by Solar Keys.",Yes,It can lead to echo chambers and filter bubbles.,https://www.wired.com/story/gadget-lab-podcast-428/,2019-10-25 12:00:00+00:00,LABEL_1_relevant,0.7018484473228455,Social Media,social_media,Wired,"Information, Discourse & Governance"
117,TikTok’s Black Box Obscures Its Role in Russia’s War,"Ten days into Russia’s invasion of Ukraine, TikTok announced it had suspended new posts from Russian accounts due to the country’s new “fake news” law. But the company was quieter about a second policy shift—one that blocked TikTok users in Russia from seeing any content posted by accounts located outside the country.

Findings by social media research collective Tracking Exposed suggest that TikTok enfolded its Russian users in a vast echo chamber intended to pacify president Vladimir Putin’s government. Inside that digital enclave, a network of Russian accounts posting pro-invasion content somehow kept operating. “There was clear manipulation of the information ecosystem on TikTok,” says Salvatore Romano, head of research at Tracking Exposed.

TikTok spokesperson Jamie Favazza declined to comment on Tracking Exposed’s findings and repeated a previous statement that the company had blocked new uploads from Russia. But the platform, owned by Chinese startup ByteDance, has been less critical of Russia than US rivals and has been treated less harshly by Russia’s government. TikTok complied with EU sanctions forcing platforms to block access to Russian state-backed media from Europe. Meta, Google, and Twitter have also adjusted their algorithms to make content or links to those outlets less visible. In apparent retaliation, Facebook and Twitter were both blocked by Russian internet censors. On March 21, a Moscow court banned Facebook and Instagram from Russia, accusing parent company Meta of “extremist activities.”

TikTok’s actions in Russia and its central role in spreading video and rumor from the war in Ukraine add urgency to open questions about how truth and mistruth circulate on the platform, Romano and other researchers say. TikTok’s geopolitical moment also highlights the challenges faced by researchers trying to answer such questions. The app, launched in 2017, surpassed 1 billion monthly users in September 2021, but it is less well studied, and more difficult to study, than its older rivals.

Most work on the dynamics and downsides of social media has focused on Facebook and Twitter. Tools and techniques developed for those platforms have shone revealing light on the spread of misinformation about Covid-19 and uncovered online manipulation campaigns linked to governments, including Russia, China, and Mexico. Meta and Twitter provide APIs to help researchers see what is circulating on their platforms.

TikTok does not provide a research API, making it hard to answer questions about its role in spreading accurate or inaccurate information around the Ukraine war or other topics. And while researchers might like to see Meta and Twitter provide broader data access, these platforms at least offer something, says Shelby Grossman, a researcher who has been monitoring pro-Russian posts about Ukraine at Stanford’s Internet Observatory. “It’s tough to look systematically at what’s happening on TikTok,” she says. Researchers have also scrambled to monitor content about Ukraine on messaging app Telegram, which also lacks a researcher API and is much less studied than US networks.

TikTok spokesperson Favazza says that although it does not currently provide a research API, “we strongly support independent research,” citing a program that briefs lawmakers and experts in online harms on its moderation and recommendation systems. TikTok has previously claimed the war in Ukraine prompted it to increase moderation and speed up a pilot project labeling state-controlled media accounts but did not specify exactly how its operations have changed. On March 24, two TikTok moderators filed a lawsuit against the company alleging psychological harm from “exposure to highly toxic and extremely disturbing images.”

One of the biggest challenges to outside researchers interested in what circulates on TikTok stems from the power and influence of its recommendation algorithm, which plays an outsize role compared to older social networks. The app and its rapid growth are built on the For You page, which shows an endless feed of videos curated by TikTok’s algorithm and drawn largely from accounts a user does not follow. As a result, different people see wildly different videos, with the feed based on past viewing and other signals.",Yes,"It can be used to spread misinformation and manipulate public opinion. This is especially true for TikTok, which has been used to spread pro-Russian propaganda about the war in Ukraine. While TikTok has taken some steps to address this problem, it is difficult to study the platform due to a lack of data access.",https://www.wired.com/story/tiktok-algorithm-russia-war/,2022-03-28 11:00:00+00:00,LABEL_1_relevant,0.9953193068504332,Social Media,social_media,Wired,"Information, Discourse & Governance"
118,Think FaceApp Is Scary? Wait Till You Hear About Facebook,"FaceApp is a viral lark that takes a convincing guess at what you’ll look like when you’re old. FaceApp is also the product of a Russian company that sends photos from your device to its servers, retains rights to use them in perpetuity, and performs artificial intelligence black magic on them. And so the FaceApp backlash has kicked into gear, with anxious stories and tweets warning you off of its charms. Which, fine! Just make sure you save some of that ire for bigger targets.

The response to FaceApp is predictable, if only because this cycle has happened before. FaceApp went viral when it launched in 2017, and prompted a similar—if far more muted—privacy kerfuffle. But compared to Meitu, that year’s other viral face manipulator, which is quite a phrase to type, FaceApp was downright saintly in its data collection. At least FaceApp didn't access your GPS and SIM card information. More energy was directed at bigger problems, like FaceApp’s blackface filter. (Yep!)

""This is definitely not a unique FaceApp problem. FaceApp is part of a larger privacy problem."" Christine Bannan, EPIC

The latest frenzy appears to have been kicked off by a since-deleted tweet that claimed FaceApp uploads all of your photos to the cloud. That certainly would be alarming. But FaceApp has denied the claim, and multiple security researchers have confirmed that it’s not so. FaceApp takes only the photo you ask it to manipulate. The company also says it deletes “most images” from its servers within 48 hours of uploading, although admittedly there’s no way to confirm that it does so in practice. If you want FaceApp to remove all of your data from its servers, you can send a request within the app, by going to Settings > Support > Report a bug and putting “Privacy” in the subject line. “Our support team is currently overloaded, but these requests have our priority,” FaceApp founder Yaroslav Goncharov said in a statement. “We are working on the better UI for that.”

Those measures don’t make FaceApp some paragon of data privacy. While the way it manages photos is kosher under Apple rules, FaceApp doesn't make it clear enough to users that it's sending them to a server. “I cannot think of any situation where an app should not be very painfully clear about a photo being uploaded to a remote server,” says Will Strafach, security researcher and developer of Guardian, an iOS firewall app. “Users always have the right to know this.”

Still, it’s important to note that while FaceApp calls St. Petersburg home, its servers are based in the US. The company said in a statement that ""the user data is not transferred to Russia."" Like almost everyone else, FaceApp uses Amazon’s cloud. And it has at least a plausible reason for doing so: The processing power required to apply a Methuselahn filter on your face is more manageable there than on your device. More recent iPhones and Android devices have machine learning capabilities baked into their hardware, but it’s safe to assume that plenty of FaceApp’s reported 80 million users are on older models.

So what’s changed since 2017? On the FaceApp side, not much. But the world around it looks markedly different. Russia has become synonymous with nefarious online meddling, to the point that any company—even a silly filter app—becomes a bogeyman. Awareness of facial recognition’s perils has reached something close to critical mass. And the idea that one’s personal data might be worth protecting has gained real, immutable traction.",Yes,"People who are not well-informed share information that is false, which causes problems because people believe the information to be true.",https://www.wired.com/story/faceapp-privacy-backlash-facebook/,2019-07-17 19:08:04.979000+00:00,LABEL_1_relevant,0.9956989288330078,Social Media,social_media,Wired,"Information, Discourse & Governance"
119,Facebook and Twitter Finally Respond to Trump,"And so it didn't really go over that well among the crowd who had been calling on him to say something. Because even as he was saying go home, he was repeating the same theory that underlay all the chaos in the first place. And so not long after that, I confess I don't remember the precise sequence, but it didn't take long for Facebook and YouTube to pull that video down and for Twitter, at first, to basically freeze it so you couldn't retweet, reply to, or like it, and then eventually Twitter took it down as well.

MC: Right, and in that period of time when Twitter froze it, you could still quote-tweet it, right?

GE: You could still quote-tweet it. You could still view it. So if you went to Trump's Twitter feed, you could view it, and it had like 12 million views. But this was sort of a bridge moment in Twitter's evolving stance toward content that it deems violates its rules but comes from public figures. Twitter has been really loathe to take stuff actually down, because it makes the argument that there is news value in knowing what a world figure or a national political figure like Trump has said. And so what they've tried to do are things to slow the virality while allowing people to see it. So they were sort of in a way station on that, but they quickly ratcheted up to “No, we're pulling this down.”

LG: What rationale did they give for taking the video down?

GE: So, at first, it was a little bit unclear. We were just told that the videos violated policies. And then Facebook said they had judged that it was more likely to encourage further violence than tamp it down. So, basically, they looked at this video in which Trump says, ""Go home, be peaceful,"" but also says, ""We were robbed, the election was stolen,"" and decided that, on balance, it was likely to provoke more violence. Now, personally, as this was unfolding and I'm kind of watching it happen in real time, I thought that was kind of weird. Because if you watch the video as I did, and if you read it—I've got the whole text of it in my story that's up on WIRED now if people want to read it—the “election was stolen” stuff, that's not new. Right? Trump's supporters already buy into that, but he did tell them several times in that little video, ""Go home.""

And so it seemed a little bit like a stretch to me to treat that as inciting violence insofar as the bad part was the stuff that Trump has always been doing. And so it kind of raised the question, well, wait a minute, if you're going to ban Trump from saying the election was stolen in this video, don't you have to ban all the other times when he says it? And then that's exactly what happened. So, the die was kind of cast there, because shortly after they pulled the video down, they start pulling other stuff down. Trump was tweeting things that were kind of in the same spirit as the video where it was like, ""Hey, be peaceful, don't be violent. The election was stolen from us."" And so pretty quickly, Twitter and Facebook start taking those posts down. And then you start wondering, well, I mean, we kind of know what all his future ones are going to be, and then, lo and behold, they just decided to freeze his ability to post.",Yes,"It has allowed for the easy and quick spread of information and ideas, which has led to a number of problems. These problems include the spread of false information, the amplification of hateful or violent speech, and the creation of echo chambers.",https://www.wired.com/story/gadget-lab-podcast-486/,2021-01-08 13:00:00+00:00,LABEL_1_relevant,0.9946081638336182,Social Media,social_media,Wired,"Information, Discourse & Governance"
120,Data on Content Moderation Won't Silence Facebook’s Critics,"Faced with escalating accusations of bias from the right and an onslaught of calls to break up Facebook on the left, the social media juggernaut released a data dump Thursday that its leaders hope will help the public better understand how it moderates content—and remind them that the bigger Facebook is, the more it can invest in fending off these threats.

In its third Content Standards Enforcement Report, Facebook laid out the sheer amount of prohibited content that appears on its platform, including fake accounts, spam, terrorist propaganda, child sexual exploitation, hate speech, bullying, nudity, violence, and the sale of regulated goods. In most, though not all categories, the company explained how prevalent views of that content were, how many pieces of content Facebook took action on, how much of it Facebook found before users reported it, how many enforcement decisions Facebook users appealed, and how much content was restored after that appeal.

Some of the numbers Facebook reported leap off the screen. Over the last six months, for instance, Facebook says it saw a ""steep increase"" in abusive, fake accounts, which accounted for 5 percent of its monthly active users during the last two quarters. Between October 2018 and March 2019 alone, the company removed 3 billion fake accounts, almost all of them within minutes of creation. The report also includes some never-before-shared insights into certain categories of banned content, including child sexual exploitation and terrorist propaganda, which Facebook says accounted for .03 percent of all views in Q1 of 2019. According to the report, Facebook catches more than 99 percent of those posts before a single user reports them.

Issie Lapowsky covers the intersection of tech, politics, and national affairs for WIRED.

Just hours before these numbers came out, a separate report was published by the Data Transparency Advisory Group, an independent group Facebook established last year to audit its content moderation efforts. This report offers an analysis of Facebook’s internal review processes and the metrics tracked with regard to prohibited content, while offering recommendations for how Facebook might improve.

Taken together, the reports serve as Facebook’s answer to several ongoing debates about the company’s size and tremendous power. Whether such an influx of information will ultimately satisfy Facebook’s critics or be used against it remains to be seen.

In a press call introducing the reports, CEO and founder Mark Zuckerberg argued that Facebook's size is precisely what makes the company a responsible watchdog for the internet. ""The amount of our budget that goes toward our safety systems, I believe, is greater than Twitter's whole revenue this year. So we're able to do things that I think are just not possible for other folks,"" Zuckerberg said. Breaking Facebook up, he claimed, would make enforcement tougher. ""We really need to decide what issues we think are the most important to address and to focus on, because in some ways, some of the remedies cut against each other in terms of making progress.""

Facebook only began releasing these content moderation reports a year ago, in response to calls for more information about how it sets and enforces its policies. The numbers included in the reports certainly underscore the scope of the problem Facebook is up against. In just the first three months of this year, for instance, the company took action against 5.4 million pieces of content that included child sexual exploitation, 4 million pieces of hate speech, 6.4 million pieces of terrorist propaganda, and 1.76 billion pieces of spam, among other banned content.",Yes,"It leads to the formation of echo chambers, where people only see information that conforms to their existing beliefs. This can have dangerous consequences, as it can lead to the spread of misinformation and the reinforcement of people's biases.",https://www.wired.com/story/facebook-community-standards-report/,2019-05-23 21:09:55.193000+00:00,LABEL_1_relevant,0.9963891506195068,Social Media,social_media,Wired,"Information, Discourse & Governance"
121,Google Bans Infowars Android App Over Coronavirus Claims,"On Friday, Google removed the Infowars Android app from its Play Store, extinguishing one of the last mainstream strongholds of infamous conspiracy theorist Alex Jones. The takedown comes on the heels of a video, posted in the Infowars app last week and viewed by WIRED, in which Jones disputed the need for social distancing, shelter in place, and quarantine efforts meant to slow the spread of the novel coronavirus.

Google confirmed to WIRED that it removed the app on Friday. The app had more than 100,000 downloads according to Google Play's published metrics, and was rated ""E10+,"" meaning safe for all users 10 and older. The Infowars app sold products like supplements and protein powder, broadcast The Alex Jones Show live, and posted videos and articles from Jones and others.

""Now more than ever, combating misinformation on the Play Store is a top priority for the team,"" a Google spokesperson told WIRED. ""When we find apps that violate Play policy by distributing misleading or harmful information, we remove them from the store.""

Alex Jones and Infowars did not immediately respond to a request for comment. In the video in question, Jones said that ""everybody dies under the new world order except maybe one tenth of one percent that believe they're going to merge with machines and have made deals with this inter-dimensional thing that gave them all the technology ... You can't make a deal with these aliens, OK, that the Bible tells you about and ever get off the planet."" Elsewhere in the video, Jones claims that natural antivirals exist to treat the novel coronavirus.

Google is far from the first tech giant to block or ban Infowars content. In fact, Google-owned YouTube suspended The Alex Jones Channel in August 2018, after mounting pressure from critics that platforms should enforce their anti-hate speech policies. Facebook took action against several Jones-operated pages that same day, as did Twitter with Infowars-related accounts the next month. Facebook instituted a full ban of Alex Jones and Infowars content across all of its platforms in May 2019. Apple's iOS App Store, Google Play's main competitor, already banned the Infowars app more than a year ago during the late summer 2018 blitz.

In the wake of all the mainstream bans, Jones and his supporters have repeatedly maintained that blocking Infowars and Jones' content only spreads it farther and makes it more popular. But research indicates that so-called ""deplatforming"" actions do effectively reduce the spread of hate speech, misinformation, and conspiracy theories. The bigger question is why Google waited so long to take action against Infowars given that its peer organizations did so a year and a half ago.

Tech services and social networks have long touted their role as defenders of free speech. But rampant abuse, misinformation, and coordinated disinformation campaigns carried out by nation states on the platforms have brought the industry to a new understanding in recent years of its necessary role in policing violent and dangerous content. The industry has also specifically moved to crack down on misinformation about the Covid-19 pandemic in the last few weeks. A joint statement from Facebook, Google, LinkedIn, Microsoft, Reddit, Twitter, and YouTube last week said that the companies are pledging to help ""millions of people stay connected while also jointly combating fraud and misinformation about the virus.""",Yes,It has amplified conspiracy theories and fake news.,https://www.wired.com/story/google-bans-infowars-android-app-alex-jones-coronavirus/,2020-03-27 16:07:38.229000+00:00,LABEL_1_relevant,0.9969396591186525,Social Media,social_media,Wired,"Information, Discourse & Governance"
122,Judge blocks Florida's social media law - The Verge,"A Florida internet law is on hold while a lawsuit contesting it continues. Judge Robert Hinkle has issued a preliminary injunction blocking nearly all of SB 7072, a controversial and sweeping regulation of social media platforms.

“The legislation now at issue was an effort to rein in social-media providers deemed too large and too liberal. Balancing the exchange of ideas among private speakers is not a legitimate governmental interest,” Hinkle wrote his order. Moreover, the law “discriminates on its face among otherwise identical speakers,” partly thanks to an eyebrow-raising exemption for companies that operate a theme park. That raises the bar for evaluating whether the law violates the First Amendment — and in Hinkle’s estimation, it probably does.

“The legislation compels providers to host speech that violates their standards ... and forbids providers from speaking as they otherwise would”

SB 7072, signed by Governor Ron DeSantis in May, limits when web services can kick off users. It includes a ban on suspending or annotating the posts of political candidates, a requirement that users can “opt out” of algorithmic sorting systems, and an “antitrust violator blacklist” for companies that break the rules. As mentioned above, the rules don’t apply to “a company that owns and operates a theme park or entertainment complex,” placating media conglomerates like Disney.

Industry groups NetChoice and the CCIA sued to stop the law’s enforcement, saying it would compel social media platforms to host offensive speech that violated their editorial policies. Judge Hinkle heard arguments from both groups and the DeSantis administration on Monday, when he appeared dubious of the law, calling the theme park exemption in particular a “major constitutional issue.”

The injunction is similarly skeptical, summarizing the law as follows:

“The state of Florida has adopted legislation that imposes sweeping requirements on some but not all social-media providers. The legislation applies only to large providers, not otherwise-identical but smaller providers, and explicitly exempts providers under common ownership with any large Florida theme park. The legislation compels providers to host speech that violates their standards — speech they otherwise would not host — and forbids providers from speaking as they otherwise would. The Governor’s signing statement and numerous remarks of legislators show rather clearly that the legislation is viewpoint-based. And parts contravene a federal statute.”

The federal statute Hinkle mentions is Section 230 of the Communications Decency Act, a rule that gives websites and apps wide latitude to decide what material they host. Beyond that, Hinkle says social networks regularly use editorial judgment to prioritize, remove, label, and otherwise sort content shown to users — and much of that editorial judgment is likely protected by the First Amendment. “The state has asserted it is on the side of the First Amendment; the plaintiffs are not. It is perhaps a nice sound bite. But the assertion is wholly at odds with accepted constitutional principles,” Hinkle writes.

Federal lawmakers, like state ones, have an appetite for regulating social media. That includes a package of bills aimed at reducing the potential monopoly power of companies like Facebook and Google, as well as several proposals for changing Section 230. In April, Supreme Court Justice Clarence Thomas also made a case for regulating social media. But Florida’s law was one of the first, most sweeping rules governing how companies can moderate web platforms — and it’s not surprising that it’s been stopped short.",Yes.,The way it can spread misinformation.,https://www.theverge.com/2021/6/30/22558086/florida-social-media-theme-park-law-preliminary-injunction-lawsuit,2021-06-30 00:00:00,LABEL_1_relevant,0.9931055903434752,Social Media,social_media,The Verge,"Information, Discourse & Governance"
123,"Online Meetups Are Sad, but What If You Were a Cute Animal?","Andy Baio is over online meetups. Zoomed out. VR skeptical. Serendipity deficient. A lot of us probably feel the same way.

But Baio has an advantage: He’s a technologist, a blogger, and the cofounder of the XOXO festival, which first launched in 2012 as an event for creators minus the soulless marketing of SXSW. He’s experienced conferences and virtual meetups from all angles: IRL, 2D, sort-of-3D. So after nearly a year of mostly lackluster online events, he decided to build a new kind of meeting space, one that, he says, would fix the more depressing elements of online meetings while welcoming in “bright spots of creative experimentation.”

The result is Skittish, a game-like meetup space that runs in the web browser. People appear as animal avatars; when I met up with Baio in Skittish, I appeared as a pony, and he was an owl. Users communicate with each other via live audio chat, and who you’re chatting with depends on your position in the cartoon-like 3D world. Move closer to a group of animated avatars, and their conversation gets louder; move away from them and the audio recedes. The look and feel of Skittish’s virtual world is reminiscent of Animal Crossing, which Baio says was an intentional design choice, to get close to the game’s “playfulness” and social elements.

Users in Skittish appear as cute 3D animals. Courtesy of Skittish

Skittish is still in beta. Baio was, well, skittish to say exactly when it will launch to the public, since the team behind it is still working on supporting potentially thousands of users and formulating its content moderation policies. At some point, he plans to charge for the service. For now, Baio agreed to talk to WIRED (where he has also contributed as a writer) about his new creation, his ideas for online conferences, and what the future of events like XOXO might be. The conversation has been edited for length.

WIRED: What are you hoping to accomplish in building Skittish?

Andy Baio: This entire project came out of a couple things. First, my experience running XOXO and then having the pandemic cancel that and upend my entire event organizing career. And by extension, seeing every other event organizer in the world struggle with trying to adapt to that. I started going to a bunch of remote events and I just didn't feel that they were right for me. A lot of them feel like you're just watching a YouTube live stream, or you're in a Zoom call. That just wasn't super compelling to me. The kind of events that I love—that I love attending and the ones that I try to organize—have the experience of getting a bunch of like-minded people into the same space at the same time, and then all of the the magic that happens after that when you allow them to interact in different ways, as they're moving between programming or as they just run into each other in line or in the hallway.

But over the last year we did start seeing some really interesting experimentation. I mentioned in my post, the environments created for Roguelike Celebration and LIKELIKE events were kind of borrowed from games. And then there’s experimentation around what we're doing right now—proximity chat, which allows you to talk to people that are nearby [in a virtual world]. I mean, there's dozens of these now, like that sort of spatial audio has been around for 15 years at least with Second Life, but all of these platforms are now kind of using this idea to help scale up a community, because with Zoom only one person can really speak at a time, right? And, you can’t see this right now, but it’s worth mentioning that Skittish supports an emerging standard for streaming payments to creators over the web.",Yes,"We're more divided than ever. And I think there's a lot of evidence that that's true. And I think that events like XOXO and the heart of the festival movement—the one that I'm a part of—is really about rebuilding bridges between people, between communities. And the way I see Skittish being used is",https://www.wired.com/story/skittish-andy-baio/,2021-02-18 12:00:00+00:00,LABEL_1_relevant,0.9956812858581544,Social Media,social_media,Wired,User Experience & Entertainment
124,Twitter Used to Predict Box Office Revenues,"The results are a fascinating insight into the power and limitations of Twitter. Asur and Huberman found that the rate at which people produce tweets about movies can accurately forecast the box office revenue of the film, but only after it is released.

And the predictions from tweets are more accurate than any other method of forecasting.

Film studios and others do this in variouys ways to gauge the popularity of their products. One indicator is the amount of money they spend on advertising. Asur and Huberman say this may be linked to rate of twittering before a movie is released. Others methods of forecasting include focus groups and exit polls of people who have seen the film.

But the most accurate is the Hollywood Stock Exchange, a market in which people can buy and sell virtual shares in actors, directors and individual movies. Researchers have long known that these kinds of virtual stock markets appear to tap into the wisdom of the crows to make uncannily accurate predictions about everything from the outcome of elections to price of real stocks. The Hollywood Stock Exchange produces unusually accurate predictions about the popularity of films.

However, when it comes to the accuracy of forcecasts, counting the rate of tweeting trounces the Hollywood Stock Exchange, say Asur and Huberman: “Our predictions are consistently better than those produced by an information market such as the Hollywood Stock Exchange, the gold standard in the industry.”

That will delight the founders of Twitter and the ecosystem of websites and services that have sprung up around it. These services have enjoyed enormous popularity but have yet to demonstrate their commercial worth.

However, it also raises an interesting new question for advertisers and marketing executives. Can they change the demand for their film, product or service buy directly influencing the rate at which people tweet about it? In other words, can they change the future that tweeters predict?

There’s only one way to find out and it’ll require a substantial investment in marketing and advertising. Tweeters be warned!

Ref: arxiv.org/abs/1003.5699: Predicting the Future With Social Media",Yes,It can be used to predict the future.,https://www.technologyreview.com/2010/04/01/204984/twitter-used-to-predict-box-office-revenues/,2010-04-01 00:00:00,LABEL_1_relevant,0.9953961968421936,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
125,You Can Now See All the Ads Facebook Is Running Globally,"Facebook is debuting a raft of new features aimed at bringing more transparency to advertising on the platform, in the wake of an ongoing scandal in which Russian trolls were able to purchase ads targeting the US electorate in the run-up to the 2016 election. The new features include a tool called View Ads, which allows users to see every active ad purchased on Facebook, Instagram, Messenger, and its partners. They were announced earlier this year and vetted in Canada. Now, the company is rolling View Ads out around the world.

“We’re providing much more transparency than any other advertising platform,” said Facebook’s chief operating officer Sheryl Sandberg in a meeting with reporters Thursday.

""The hope of the announcement today, making [Facebook] much more transparent than it was before, is that it will hold us accountable, it will hold advertisers accountable, but it will also give people the ability to find things that maybe shouldn't be up or find things that might be misleading so we can take action,"" she said.

The company is also expanding a feature it announced in April that allows users to see information on every political ad bought on its platforms, as well as so-called “issue ads” that touch on topics that might be construed as political, such as abortion or immigration. This tool gives users access to information on how much the ad cost, who was behind the ad, and the demographics of the audience it reached. Advertisers must also verify their identity and location before purchasing political or issue ads. Facebook rolled this political ad portal out in the United States in May, and now it’s launching in Brazil, as well, ahead of elections this fall.

“We have to take a deliberate approach to expanding to other countries,” said Rob Leathern, Facebook’s director of product management.

The company's political ad tool leaves something to be desired. While it offers access to metrics on each individual ad, it's still impossible to look at an advertiser's overall purchasing behavior on Facebook to see who they're targeting in general and how. But Leathern said the company will release an API later this year that makes that kind of data crunching easier.

For years, Facebook resisted abiding by the same Federal Election Commission political advertising regulations that traditional forms of media, like radio and television, are required to follow. But the revelation last fall that Facebook and other tech platforms had sold ads to Russian propagandists has forced the companies that were manipulated by the Russian actors to change their tune. Facebook, Google, and Twitter have all unveiled a variety of transparency measures aimed at heading off what they fear could be more draconian legislation if Congress is left to write the rules.

Already, Facebook has struggled to determine what types of ads qualify under its new policy. A number of media outlets have criticized the platform over the last month for labeling their ads political, despite only containing links to news stories. In response, Campbell Brown, Facebook’s head of news partnerships, said in a blog post that the company would soon update its ad transparency database to distinguish between journalism content and genuine political advocacy, like ads for a specific candidate.",Yes,It's very easy to spread false information and rumors.,https://www.wired.com/story/facebook-aims-more-transparency-view-ads-feature/,2018-06-28 19:15:17.587000+00:00,LABEL_1_relevant,0.9922967553138732,Social Media,social_media,Wired,"Information, Discourse & Governance"
126,"Facebook Changed the World, but Was It for the Better?","LG: Thanks for being here. I'm thrilled.

SL: It's falling apart already. Unbelievable.

[Intro theme music]

LG: Hi everyone. Welcome to Gadget Lab. I'm Lauren Goode, a senior writer at WIRED, and I'm here with my Gadget Lab cohost, WIRED senior editor, Michael Calore.

MC: Hello.

LG: Hello. And today we are also joined by WIRED editor at large, Steven Levy. Steven, thank you so much for being here.

SL: Well thanks. Thanks for finally letting me in this podcast room.

LG: Yeah, we hear that a lot from our colleagues at WIRED. They remember, with great detail, the last time they were on the show. Such as when Brian Barrett comes back on, he's like, ""It's been 17 months since I was on this show."" But that's true. This is that we always want to have our colleagues on this show, and we're so happy that you're here. And when was the last time you were on the show, by the way?

SL: The late 50s.

LG: Okay. When we were very, very early to podcasting. I mean, we were doing podcasts back when some people weren't even listening to terrestrial radio. Okay, Steven, let's get right to it. You are here because you wrote a book. It's called Facebook: The Inside Story. I have it right here in front of me. Folks, this is a tome. I don't know if you just heard that thud on the table, but this is like, you could lift weights with this thing. You spent years on this book, Steven, and you had a fair amount of access to people like Mark Zuckerberg, and other executives like Cheryl Sandberg in the process. It's a fantastic book. What compelled you to start writing this book when you did?

SL: So, I could even pinpoint the date. It was August 27th, 2015 when Mark Zuckerberg wrote on his feed, he put a story up, saying a billion people had been on Facebook the day before. And this wasn't how many people signed up for the services. It's like a billion people, in 24 hours, have been on Facebook. And I thought about that. Had that ever happened before? Like the World Cup gets a billion people, but that's not an interactive network, where someone could post something, and in theory you can get to everybody. And all the people's individual networks were intertwined there. So, I'd known that his ambitions were huge, and that Facebook was doing very well, but the reality of it made me think, ""Wow, this is something new. How do I tell this story? I've got to tell this story. This is my story."" And to tell who did it, how they did it, and what it means.

LG: And you had written a book about Google previously. Talk a little bit about that experience, and how it compared to your writing of the Facebook story?

SL: Yeah. My previous book was called In the Plex, it was about Google. And the process was similar in that I went to them, and said, ""I want to write a book, give me access to your people. You don't get to say anything about the contents. You'll see it when it's done."" And, I thought it would be pretty much a similar process. I would write this, I would try to put together a narrative of the story of Google, where people would be able to understand Google after they read the book. And the story, almost like a novel, would have a climax, and would have tension, and the tension in this story, the Google story, was its experience in China, where it went through this moral dilemma. And I thought, ""This'll be something similar.""",Yes,"It's polarizing. And the main reason that's happening is that it's creating what are called echo chambers, where you only get your news, your opinions, your ideas, and your beliefs reinforced. And that's how you get to a world where people think of each other as enemies.

LG: Mm-hmm (",https://www.wired.com/story/gadget-lab-podcast-445/,2020-03-06 13:00:00+00:00,LABEL_1_relevant,0.9932498931884766,Social Media,social_media,Wired,"Information, Discourse & Governance"
127,An AI Used Facebook Data to Predict Mental Illness,"It’s easy to do bad things with Facebook data. From targeting ads for bizarrely specific T-shirts to manipulating an electorate, the questionable purposes to which the social media behemoth can be put are numerous. But there are also some people out there trying to use Facebook for good—or, at least, to improve the diagnosis of mental illness. On December 3, a group of researchers reported that they had managed to predict psychiatric diagnoses with Facebook data—using messages sent up to 18 months before a user received an official diagnosis.

The team worked with 223 volunteers, who all gave the researchers access to their personal Facebook messages. Using an artificial intelligence algorithm, the researchers leveraged attributes extracted from these messages, as well as the Facebook photos each participant had posted, to predict whether they had a mood disorder (like bipolar or depression), a schizophrenia spectrum disorder, or no mental health issues. According to their results, swear words were indicative of mental illness in general, and perception words (like see, feel, hear) and words related to negative emotions were indicative of schizophrenia. And in photos, more bluish colors were associated with mood disorders.

To evaluate how successful their algorithm was, the researchers used a common metric in artificial intelligence that measures the trade-off between false positives and false negatives. As the algorithm categorizes more and more participants as positive (say, as having a schizophrenia spectrum disorder), it will miss fewer participants who really do have schizophrenia (a low false negative rate), but it will mislabel some healthy participants as having schizophrenia (a high false positive rate). A perfect algorithm can have no false positives and no false negatives at the same time; such an algorithm would be assigned a score of 1. An algorithm that guessed randomly would have a score of 0.5. The research team achieved scores ranging from 0.65 to 0.77, depending on the specific predictions they asked the algorithm to make. Even when the researchers restricted themselves to messages from over a year before the subjects received a diagnosis, they could make these predictions substantially better than would have been expected by chance.

According to H. Andrew Schwartz, an assistant professor of computer science at Stony Brook University who was not involved in the study, these scores are comparable to those achieved by the PHQ-9, a standard, 10-question survey used to screen for depression. This result raises the possibility that Facebook data could be used for mental illness screening—potentially long before a patient would otherwise have received a diagnosis.

Michael Birnbaum, an assistant professor at the Feinstein Institutes for Medical Research in Manhasset, New York, who led the study, believes that this sort of AI tool could make an enormous difference in the treatment of psychiatric illnesses. “We now understand this idea that cancer has many different stages,” Birnbaum says. “If you catch cancer at Stage I, it’s drastically different than if you catch it once it metastasizes. In psychiatry, we have a tendency to start working with people once it’s already metastasized. But there’s the potential to catch people earlier.”

Birnbaum is far from the first researcher to have used social media data to predict the presence of mental illness. Previously, researchers have used Facebook statuses, tweets, and Reddit posts to identify diagnoses ranging from depression to attention deficit hyperactivity disorder. But he and his team broke new ground by working directly with patients who had existing psychiatric diagnoses. Other researchers haven’t, in general, been able to work off of clinically confirmed diagnoses—they have taken subjects’ word for their diagnoses, asked them for self-diagnoses, or had them take questionnaires like the PHQ-9 as a proxy for diagnosis. Everyone in Birnbaum’s study, in contrast, had an official diagnosis from a psychiatric professional. And since the researchers had definitive dates for when these diagnoses were made, they could try to make predictions from messages sent before the patients knew about their mental illnesses.",Yes,"It can be used to predict psychiatric diagnoses with Facebook data. The team worked with 223 volunteers, who all gave the researchers access to their personal Facebook messages. Using an artificial intelligence algorithm, the researchers leveraged attributes extracted from these messages to predict whether they had a mood disorder (like bipolar or depression), a schizophrenia spectrum disorder, or no mental health",https://www.wired.com/story/an-ai-used-facebook-data-to-predict-mental-illness/,2020-12-14 13:00:00+00:00,LABEL_1_relevant,0.996542751789093,Social Media,social_media,Wired,Health & Wellbeing
128,"The latest on Facebook’s data scandal: Zuckerberg speaks, lawsuits, and ignored whistleblowers","And all the while the firm’s CEO remains hugely conspicuous by his absence.

Backstory: In case you missed it, Facebook is embroiled in a huge scandal because of the way its users’ data was shared with Cambridge Analytica, a firm that provided data to the Trump election campaign in 2016.

Overlooked whistleblowers: Ex-Facebook staffer Sandy Parakilas told British politicians today that his warnings about the firm’s lax data protection standards were ignored, and that some of the executives he told still work at the social network. To this point, today’s Bloomberg Businessweek cover story makes a compelling argument: maybe we need a Data Protection Agency?

The legal backlash begins: It was only a matter of time, but the first legal complaints against Facebook and Cambridge Analytica have now been filed. Expect more to follow in the coming days.

#DeleteFacebook: Brian Acton, the cofounder of WhatsApp (who made billions by selling his startup to, ahem, Facebook in 2014), has been a very vocal part of a campaign urging people to quit the social network. “It is time. #deletefacebook,” he tweeted. (Or you could manipulate Facebook instead of letting it manipulate you.)

Where’s Zuck? The CEO was a no-show at a staff meeting yesterday. The Daily Beast says he’s “working around the clock.” The American and British governments want him to give evidence, but he sent “mid-level staffers” to testify to Congress today.

Here’s Zuck! Late Wednesday afternoon Mark Zuckerberg issued a long apology on his Facebook page (naturally), giving a timeline of events from Facebook's point of view and plans to investigate data access for outside developers.

Lots to lose: Media analysts say that Facebook has made a huge mess of handling the situation so far. (See: the firm’s stock price.)

Is it different this time? Every Facebook scandal feels like the one that’s going to bring about radical change, but it hasn’t—yet. Gadfly proposes that Facebook is bigger, and lawmakers more suspicious, than ever this time, so it could be different. But that’s a very big “could.”",Yes,"It can give people false information to believe in.

Whether it is a political candidate or a product, people can use social media to spread false information quickly and easily. This can have serious consequences, as people may make decisions based on false information.",https://www.technologyreview.com/2018/03/21/144526/the-latest-on-facebooks-data-scandal-lawsuits-calls-to-quit-and-ignored/,2018-03-21 00:00:00,LABEL_1_relevant,0.9959316849708556,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
129,A Reddit Community Has Become a Memorial for Covid-19 Victims,"When it launched in 2013, Reddit’s r/LastImages community was a place for people to document the final photographs taken of celebrities and other prominent figures, and even historical objects like the Titanic. But over time, it has morphed into something else entirely.

WIRED UK This story originally appeared on WIRED UK.

Now it’s a place where people post the last photos taken of their loved ones before they died, and since the Covid-19 pandemic began, it’s seen a spike in activity from both posters and commenters as they remember loved ones lost to the disease.

At the beginning of May, Connecticut-based Manny Garcia posted the last ever picture he took with his dad at a New York Giants game before he died from Covid-19. “Earlier this year, I landed my dream job, and I told myself that I would go all out for that man this year. He deserves it,” Garcia remembers. On his dad’s birthday, Garcia and his sister drove down to New York, both dressed head to toe in Giants gear, picked him up, and drove to the stadium in New Jersey.

Garcia remembers how his dad would announce the names and numbers of the players that people were wearing on their jerseys. “He was so happy to be there with his kids. I don’t think I’ll ever meet another person who enjoys and knows more about random New York Giants or New York Yankees trivia than him,” he says.

In late April, Pennsylvania-based Daniel Tweedle posted the last photo of him and his brother with their mother, grinning widely, taken shortly after the birth of his twin sons. His mother, who lived in North Carolina, traveled to Western Pennsylvania to meet her grandchildren. The photograph was taken in the summer, before Tweedle’s mother died from Covid-19. “My brother and his wife came from eastern Pennsylvania to meet my boys. It was the first time in 10 years we were together at the same time,” he says. “My mom was thrilled to meet my boys, and she would constantly demand pictures of the boys. She would buy them books and toys and clothes. We literally have winter jackets for them for the next four or five years.”

The motivation each person has for posting their loved one’s last image varies. Garcia says he hoped it would help him with the grieving process. “In my case, it did,” he says. Tweedle says he posted to the subreddit because he was angry, feeling bitter about losing his mother to coronavirus and not being able to be at her side. “I wanted people to see that those 50,000 people—at the time in the US—who died weren't just numbers. They were real moms and grandmothers, dads and grandpas. People that we love and need,” Tweedle says. “There just seems to be a detachment from the general public. I wanted to add a face so people could see what we were losing every day to Covid.”

Debra Bassett, who researches death and dying at the University of Warwick in Coventry, England, says places like r/LastImages have never been more essential. “It’s even more important now that these memorial platforms are allowing people to make their loved one important rather than making them just a statistic or a number,” she says.",Yes,It can lead to people feeling more isolated and lonely.,https://www.wired.com/story/a-reddit-community-has-become-a-memorial-for-covid-19-victims/,2020-06-02 13:30:00+00:00,LABEL_1_relevant,0.9954387545585632,Social Media,social_media,Wired,"Information, Discourse & Governance"
130,‘The Internet Remains Undefeated’ Must Be Defeated,"To a wide-ranging group of social media users like Hall, “The internet remains undefeated” is, on its face, a simple expression of joy, or nostalgia for a more joyous era of the internet. Ryan Milner, a professor of internet culture at the College of Charleston and author of The World Made Meme, says the phrase harkens back to a time, between roughly 2003 and 2013, when the internet was “still kind of this other place that didn’t operate by and could maybe transcend real-world rules.” This was the heyday of early YouTube and message boards like Something Awful, 4chan, and Reddit, “when you saw a flurry of subcultural activity and content creation that became kind of a tone setter for people who are still extremely online.” So in 2021, people comment “The internet remains undefeated” to a flourishing of memes about Bernie Sanders and his mittens or the discord between your fall plans and the Delta variant, because it recalls when life online seemed less about livestreamed mass murders and the algorithmically driven death of democracy and more about rickrolling and lolcats. At the surface level, says Milner, the phrase “is a way to kind of appreciate when the early spirit of collective creativity online resurfaces.”

People also use the phrase, Milner adds, as a way of “reacting to the randomness of what they encounter online.” Every piece of content “is made by a real person at the other end of the tubes. But we just see the funny picture. So instead of saying ‘Tim from Madison, Wisconsin, remains undefeated,’ we tend to collapse everything from everyone as being from ‘the internet,’ as if it’s this singular mystical being.” In that sense, the saying is a collectivist antivenom to unhinged individualism online.

“The internet remains undefeated” glorifies the removal of context, nuance, and thought.

But exuberant and egalitarian as the expression may appear, its undertones are much darker. For one thing, “The internet remains undefeated” is also a symptom of what Milner and fellow internet culture scholar Whitney Phillips call fetishistic flattening. This is the tendency for internet users to fixate on a meme or tweet itself, and not consider how or why it was created, the backstory of who or what’s being depicted and shared, or who may be harmed in the process. (The “Hide your kids, hide your wife” song, which belittles the man in the original clip, and deepfaked drunk Nancy Pelosi are all standard examples of fetishization.) In this way, “The internet remains undefeated” glorifies the removal of context, nuance, and thought. “Undefeated” in particular also captures how on social media, context is subsumed by combativeness. Beneath the surface, says Milner, the phrase is often “antagonistic and barbed,” and of “an atmosphere where how funny you are about what you produce and say, and how many people respond no matter what you say, is seen as a competition.”

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

Of course, context removal and ruthless competitiveness are embedded in dozens of other popular memes and replies to memes: Distracted Boyfriend, Galaxy Brain, Swole Doge vs. Meek Doge, so and so “woke up and chose violence.” But whereas those all celebrate the defeat of a single common enemy or idea lampooned in the meme itself, what makes “The internet remains undefeated” so deflating is that it celebrates our own collective defeat of ourselves. The internet’s unstated, vanquished opponent is us, the users who both consume and are the butts of the memes that phrase is often a response to. But deep down we all understand that we are also the internet, as the ones who populate it, generate its content, and created it in the first place. As Jeffrey Bloechl, a philosophy professor and phenomenologist at Boston College, told me, any problems that appear on the internet “can be traced back to things we human beings either did or failed to do when we made the thing.” After all, he adds, humans designed the internet to be boundless. “If the internet, strictly as internet, is fundamentally mathematical, it cannot itself be the source of any limits.” By that logic, “there is no way not to wonder whether in unleashing a power that is undefeated,” one that can transcend the limits of our own bodies and minds, we’ve also unleashed “a power to change what we are,” a power to defeat the human condition.

That is the horrifying economy of Those Four Words: There is no more haunting a distillation of the unstoppable seepage of technology into every fabric of our being than “The internet remains undefeated.” These words are a glaring reminder that the internet, of which I am a part, is defeating me. That in the moment I am reading them, I am devoting my attention not to my wife, infant daughter, friends, family, colleagues, wind rattling the window pane, or my breathing, but to what faceless strangers are saying about Nicki Minaj’s cousin’s friends’ balls, and to what quippy things I should be saying to faceless strangers about Nicki Minaj’s cousin’s friends balls. That gif of the Teletubbies having tantric sex? It exists only in my smooth, broken brain, a brain the internet broke so that I think in the way the internet wants me to think.",Yes,It isolates people from real human connection and makes them more likely to believe things that aren't true.,https://www.wired.com/story/internet-remains-undefeated/,2021-09-24 12:00:00+00:00,LABEL_1_relevant,0.9938687682151794,Social Media,social_media,Wired,"Information, Discourse & Governance"
131,Far-Right Propaganda Floods Facebook Ahead of EU Vote,"In Italy, the perpetrators spread a movie clip of a car being destroyed and pretended it was news footage of migrants wrecking a police vehicle. In Poland, they disseminated a fake news story about migrant taxi drivers raping European women. In Spain, they shared lies about Catalan separatists shutting down a child cancer center. In the UK, they shared a blog post with a beheading photo and a sensationalist headline, claiming ""A Billion Muslims Want Sharia Law.""

These are just a few examples of the ugly far-right propaganda and disinformation that has flooded Facebook ahead of parliamentary elections in the European Union this week. According to a new report by online activist group Avaaz, networks of fake accounts, pages, and groups have been spreading divisive, white-nationalist, anti-immigrant content throughout Germany, the UK, France, Italy, Poland, and Spain. In some cases, they posed as politicians themselves. In others, they created fan pages for political parties or alternative media organizations. Together, they amassed followings several times larger than the actual far-right groups operating within these countries.

As part of its investigation, Avaaz's global network of volunteers reported more than 500 ""suspicious"" pages and groups to Facebook. The company has since taken action against 77 of them, the report states, and has removed more than 200 accounts. Avaaz says those pages had nearly 6 million followers among them, three times as many followers as the actual far-right groups in those countries combined. The content they spread was viewed 533 million times over the past three months, according to the report. Just who exactly is behind these operations remains a mystery.

Facebook confirmed that it had taken some accounts and pages down, but did not provide specific numbers. “We thank Avaaz for sharing their research for us to investigate. As we have said, we are focused on protecting the integrity of elections across the European Union and around the world,"" a Facebook spokesperson said. ""We have removed a number of fake and duplicate accounts that were violating our authenticity policies, as well as multiple pages for name change and other violations. We also took action against pages that repeatedly posted misinformation. We will take further action if we find additional violations.""

Issie Lapowsky covers the intersection of tech, politics, and national affairs for WIRED.

The report underscores a persistent dilemma for Facebook. The company has gotten better at recognizing and removing election interference operations on its platform after coming under fire for how Russian operatives were able to target American voters during the 2016 presidential campaign. But despite efforts to protect elections in the EU and around the world from further interference, Facebook still hasn't found a way to keep all this toxic and manipulative content from spreading in the first place. Of course, it's not alone among social media companies. But Facebook's unparalleled size makes it a particularly powerful tool.

""With days to go until EU elections, Europe is drowning in disinformation. The size and sophistication of these networks makes them weapons of mass destruction for democracy, and right now they are pointed squarely at Europe,"" Christoph Schott, campaign director at Avaaz, said in a statement. ""The most worrying thing is we’ve just scratched the surface. There could be much, much more out there.""

Avaaz has been on a mission recently to get tech companies to take a more aggressive stance against disinformation. Earlier this month, the organization led a group of so-called fake news victims to Silicon Valley to meet with Facebook and Twitter. This report is part of that work. Over the course of a three-month investigation, Avaaz researchers and volunteers around the globe reviewed thousands of pieces of suspected disinformation, hate speech, and fake news, much of it submitted by the public through a crowdsourcing site that Avaaz created. They looked for signs that the pages, groups, and accounts connected to this content might be violating Facebook's policies around authenticity, violence, or hate speech, and shared their findings with Facebook. Facebook took action against some, but not all, of the accounts, pages, and groups Avaaz identified.

""With days to go until EU elections, Europe is drowning in disinformation."" Christoph Schott, Avaaz

In Germany, for example, Avaaz says Facebook took action against 131 fake accounts, many of which propped up the far-right party Alternative for Germany, or AfD. In France, Facebook removed a page called Suavelos, which promoted white-nationalist ideas, something Facebook only recently prohibited. Facebook shut down 23 pages Avaaz reported in Italy for a variety of infractions, including changing the name of a page to something unrelated to the page's origins; one such page changed its name eight times, according to Avaaz, starting as a sports page and eventually morphing into a political page. In Poland, Avaaz found a network of pages sharing a fake news story almost simultaneously about migrant drivers raping European women. Of the 26 pages Avaaz found that shared the article, Facebook took down 11. Avaaz was also behind a recently reported takedown of pages and groups in Spain just before the election there in April. Finally, in the UK, Avaaz says Facebook removed 132 posts, pages, and groups listed in its report.

These takedowns are different from the ones Facebook has traditionally announced publicly, in that none of them appear to be what the company calls ""coordinated inauthentic behavior"" driven by a nation-state or single organization. Facebook has previously announced these types of influence operations associated with entities in Russia, Iran, and Israel. Instead, the Facebook spokesperson said, these removals are part of actions the company takes ""regularly and routinely"" against content that violates its policies.",Yes,"People tend to share things that align with their worldview, whether those things are true or not. As a result, people are often exposed only to information that reinforces their existing beliefs, which can make it difficult for them to understand and empathize with others.",https://www.wired.com/story/far-right-propaganda-facebook-eu-elections/,2019-05-22 05:00:00+00:00,LABEL_1_relevant,0.9958884119987488,Social Media,social_media,Wired,"Information, Discourse & Governance"
132,The mass shooting in New Zealand shows how broken social media is,"A gunman live-streamed the murder of dozens of innocents in two mosques in Christchurch, New Zealand, on Friday—and the world got a terrible reminder of how flawed existing social-media policies and algorithms are for policing violent and offensive content.

In the days before the shooting, the perpetrator apparently boasted of his plans and posted an online manifesto. He then broadcast the horrific act live on Facebook. The attack left 49 people dead and dozens more injured.

Live stream: Over the past 18 months, following harassment and fake-news scandals, social-media companies have invested heavily in content moderators. But this did little to stop video of the shooting from spreading. Not only was the live stream reportedly up for 20 minutes, but the resulting video was then reposted on YouTube, with some clips remaining up for over an hour.

Several factors contributed to letting the footage slip through the filters, according to experts.

Real-time challenge: It’s vital to catch a video quickly, so that it doesn’t spread onto other platforms. But social-media moderation simply isn’t geared toward catching content in real time. It is impossible to automate the process effectively, and identifying live streams that need to be shut down manually is “like finding a needle in the haystack of data that’s flowing over the network all the time,” says Charles Seife, a professor at NYU’s School of Journalism. He adds that Facebook could require users to build up a reputation before letting them live-stream content, to reduce the risks.

Whack-a-mole: Moderators are overwhelmed at the best of times. Video of the shooting hosted on YouTube most likely spread so quickly that the humans employed to check for inappropriate content didn’t have time to catch everything. These workers typically have a few seconds to make a call. The process can be partly automated, but those who reposted the footage apparently clipped it and introduced distortions to avoid these algorithms.

Algorithmic failure: Social-media companies also use algorithmic tweaks to de-prioritize suspicious content. But Mike Ananny, an associate professor at the University of Southern California, says these algorithms were probably thrown by the popularity of the offending videos.

Not our problem: These factors reflect the key systemic problem: Facebook, YouTube, and other big social platforms do not see themselves as the arbiters of content in the first place. Research has shown that far-right sources of information can be policed more proactively to prevent violent or hateful material from spreading. “They have this attitude of being post hoc,” says Ananny. “It’s a deep cultural thing.”",Yes,"It can spread violent and offensive content quickly and easily. There are a number of factors that contribute to this, including the fact that social media platforms are not designed to police content in real time, and that their algorithms can be easily circumvented by those who wish to do so.",https://www.technologyreview.com/2019/03/15/65970/the-mass-shooting-in-new-zealand-shows-how-broken-social-media-is/,2019-03-15 00:00:00,LABEL_1_relevant,0.9960516095161438,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
133,6 Lingering Questions From the Times' Facebook Bombshell,"On Wednesday afternoon, The New York Times published a blockbuster report—with five bylines, 50 sources, and 5,000 words—on the failures of Facebook’s management team during the past three years. It begins with Sheryl Sandberg yelling at one of her employees; it ends with her notes to self, captured by a photographer, as she sat before the Senate: “Slow, Pause, Determined.” The story, in other words, is not very flattering (and you should definitely read it) We did, and we have six follow-up questions that merit more investigation.

1) What is Sandberg’s future at Facebook?

For most of the social media company's history, Sandberg has avoided criticism. During the past year, most of the anger at Facebook has been directed at Zuckerberg. That has started to change recently. The Wall Street Journal reported, for example, on a “swat team” that Sandberg runs, tasked with identifying and preventing future catastrophes. The Times story, though, is the first to cast her as the central antagonist.

It was Sandberg, the story says, who seethed after security executive Alex Stamos (who later left the company) disclosed to a Facebook board committee that the extent of Russian interference was still unknown and unchecked. It was Sandberg who chastised Stamos for devoting time and effort to look into the Russian campaign without company approval. It was Sandberg who sided with Joel Kaplan, vice president for public policy, about leaving the Russians out of its white paper on election interference, and it was Sandberg who encouraged Stamos to be less specific in his initial posting about Russia’s propaganda campaign. Sandberg appealed to Senator Amy Klobuchar, the Democrat from Minnesota, to dial down her attacks on Facebook. And Sandberg was the one who came out in support of the Stop Enabling Sex Trafficking Act, a decision the Times asserts was motivated in part to make other tech giants like Google look bad.

In a post responding to the newspaper, Facebook rejects that assertion. “Sheryl championed this legislation because she believed it was the right thing to do, and that tech companies need to be more open to content regulation where it can prevent real world harm,” the company wrote.

The question now is whether the woman charged with solving Facebook’s hardest problems has caused a few too many of her own.

2) What other tech company has been hiring an opposition group to smear Apple?

One of the more extraordinary parts in the report involves an opposition research group called Definers Public Affairs, run by Matt Rhoades, a former campaign manager for Mitt Romney. The firm also employs Tim Miller, a former spokesman for Jeb Bush and a contributor to Crooked Media, the company that runs Pod Save America. Facebook hired Definers to look into the funding of the company’s critics.

During this period, a conservative news website called NTK Network, which the Times says is affiliated with Definers, published a number of stories critical of Apple. But, in the Times report, Miller also says that Definers’ Apple work is funded by a third technology company. In other words, Facebook paid Definers; Facebook was fighting Apple; Definers wrote stories critical of Apple; but another technology company was paying for those stories.",Yes,"It has created an industry of professional trolls, who often have no clear affiliation other than being paid by a major technology company.

3) What did Mark Zuckerberg know about the hiring of Definers?

Zuckerberg has always been a micromanager. In the early days of Facebook, he was known for calling up random employees",https://www.wired.com/story/6-questions-new-york-times-facebook-bombshell/,2018-11-15 19:33:44.193000+00:00,LABEL_1_relevant,0.9961842894554138,Social Media,social_media,Wired,"Information, Discourse & Governance"
134,Meet Romania's Very Internet-Savvy Witch Community,"Consider it globalization of the supernatural: Thanks to the proliferation of the internet, the Romanian witch community—also known as the vrăjitoare—has migrated their ancient practice onto the web. Using social media to livestream rituals or to video chat with clients for fortune readings, witch entrepreneurs are better able to grow their business using self-referential devices (clothing, jewelry, idols) to effectively market the storied mysticism of Roma women to searching souls.

Members of the vrăjitoare learn the craft from their mothers at age 7, after careful observation of whether they are capable of continuing the tradition. Other witches claim they were born with a ""gift"" from God that enables them to communicate with divine energy. Only women are able to join the vrăjitoare. Witchcraft is a respected (and feared) profession in Romania, a country where the president is known to wear purple on certain days to ward off evil. The practice is even regulated: In 2011, a new law required the vrăjitoare to pay a 16 percent income tax, the same as any other self-employed Romanian citizen. The response was twofold. Some supported the tax, arguing that it established witchcraft as a verifiable profession, while others angrily threw poisonous mandrake plants into the Danube River.

In 2013, Slovakian photographer Lucia Sekerková Bláhová discovered the vrăjitoare practice when watching a documentary on the Qatari news channel Al Jazeera. Bláhová employed the help of ethnologist Ivana Šusterová, who specializes in the culture of the Wallachian Roma people, to help gain access to the vrăjitoare for her eponymous photo series. Bláhová contacted witches she found online to arrange visits using their personal websites or Facebook pages. Taking a cue from Šusterová, Bláhová wore traditional Roma clothing and observed certain customs to gain the vrăjitoare's trust in order to photograph them. ""Mostly, it was all about making a good first impression, being honest to them,"" Bláhová explains. ""Sometimes, you need good negotiating skills.""

Negotiating is central to the vrăjitoare business. Witches don't post prices for their services on the internet, the understanding being that cost will be incurred based on services rendered. Typically, clients meet with the witch for a ""diagnosis,"" which Bláhová says costs between 10 and 20 euros. The remedy, however, comes at another expense. As one witch told Bláhová, ""Some [clients] give me 50 euros and some people give me 500 euros because they like my honesty.""

Bláhová says most of the magic rituals the vrăjitoare perform are intended to restore health or ignite romance, although some services are aimed to do harm. Many of the women are under increasing pressure to specialize their practice, especially in an era when every witch can have a web presence. ""Women said they have been picking flowers in secret places, always in wild nature, far away from civilization, usually during the sunrise,"" Bláhová says. ""The more interviews we had with them, the more we got a feeling of a kind of performance for their clients or a kind of competition between the women."" Bláhová says some women, especially older witches, were concerned with how the internet made witchcraft more accessible, pointing out that some younger members of the vrăjitoare were sharing the steps of their secret rituals on Facebook.

Still, most of the vrăjitoare maintain that all business is good business. ""When you would expect something mystical, there is a young witch with iPhone in her hand taking a selfie picture before a ritual for her vrăjitoare Facebook profile,"" recounts Šusterová, a Slovakia-based ethnologist. ""Because of the internet, the work of vrăjitoare is much easier and more available to clients.""

More Great WIRED Stories",,"It has given rise to a new generation of witches who are using the internet to market their services to a wider audience. This has led to a situation where the older, more traditional witches are feeling pressure to adapt their practices to the new reality.",https://www.wired.com/story/romanian-witches-internet/,2019-03-15 14:00:00+00:00,LABEL_1_relevant,0.989973783493042,Social Media,social_media,Wired,Social Norms & Relationships
135,TikTok Bugs Could Have Allowed Account Takeovers,"The social video app TikTok has been branded a potential security threat for its ties to China—the app is owned by the Beijing-based company ByteDance—but like any piece of software it also has the potential for more immediate security concerns. Recently patched vulnerabilities in the app could have allowed an attacker to take over TikTok accounts, add or delete videos, and expose private data like user information or videos marked ""hidden.""

Researchers from the security firm Check Point first disclosed the bugs to TikTok in late November, and the company patched all of them on iOS and Android by the end of December. The findings come, though, as Congress has held hearings and called for investigations in recent months over the possibility that the app poses a national security risk. And the US Army and Navy both banned the app from their devices at the end of 2019, calling it a cyber threat. All software has bugs, and a few vulnerabilities don't reveal that TikTok is at all malicious. But the findings show that the social media app of the moment merits more scrutiny.

""The goal of our research was really to understand what is the the level of security and privacy that TikTok is providing,"" says Oded Vanunu, Check Point's head of product vulnerability research. ""Once we finished the review and understood that we could easily manipulate the accounts, we said, ‘Let’s stop here and share the information.’ We hope that now more researchers will check the app and that TikTok will increase their security validation cycle.""

The researchers noticed that TikTok offers a feature on its website for users to enter their phone numbers and receive an SMS message with a link to download the app. While analyzing this mechanism, they found that they could remotely manipulate the words in the text as well as the download link, and send them to any phone number. From there they discovered that they could craft special links for these texts that would send commands to TikTok if a victim had already downloaded the app.

In practice, an attacker could have revamped an SMS message to target existing TikTok users, rather than just first-timers—and the texts would legitimately be coming from TikTok's infrastructure. If a TikTok user clicked one of these malicious links, an attacker could have manipulated bugs in TikTok's browser redirect setup and authentication mechanisms to manipulate their account—sending commands to add or delete videos, forcing the victim account to follow other accounts, making private videos public, or exfiltrating the victim's personal account data, like name and email addresses.

Vanunu says that TikTok was responsive about the disclosures and patched the issues within weeks. ""TikTok is committed to protecting user data. Like many organizations, we encourage responsible security researchers to privately disclose zero day vulnerabilities to us,"" Luke Deshotels, a member of the TikTok security team, said in a statement. ""We hope that this successful resolution will encourage future collaboration with security researchers."" TikTok told WIRED that it reviewed its customer support records and didn't find ""any patterns that would indicate an attack or breach occurred.""",Yes,"The exposure of private data. The findings of the recent study conducted by Check Point show that the social media app TikTok is not an exception to this and actually has several vulnerabilities that could allow an attacker to take over TikTok accounts, add or delete videos, and expose private data like user information or videos marked ""hidden.""",https://www.wired.com/story/tiktok-bugs-account-takeover/,2020-01-08 10:00:00+00:00,LABEL_1_relevant,0.9965358972549438,Social Media,social_media,Wired,Security & Privacy
136,Facebook Can’t Fix What It Won’t Admit To,"Hi, folks. Some of you are complaining that I should keep away from politics and stay in my lane. I haven’t moved. The lane just got a lot wider. I am, however, looking forward to writing about more tech-y stuff. Can the world cooperate, please?

Starting next week, this column will be subscriber-only, with the occasional exception. Get full access to every edition by subscribing to WIRED (50% off for Plaintext readers) today.

The Plain View

In June 2017, Mark Zuckerberg changed Facebook’s mission. Speaking at the company’s first Community Summit in Chicago, he explained that the best part of Facebook is its “meaningful groups,” those that address a user’s passions or needs and connect them with others who share those interests. At the time, there were 100 million people in meaningful groups; he wanted to grow it to a billion. Zuckerberg believed this so much that he changed Facebook’s core goal from “connecting the world” to “giving people the power to build community and bring the world closer together.” In a post explaining this, he wrote, “Communities give us that sense that we are part of something greater than ourselves, that we are not alone, and that we have something better ahead to work for.”

More than three years later, some of those groups have done exactly what Mark Zuckerberg envisioned: They bound together for a passionately held common cause. But the “something greater than ourselves” probably wasn’t what he had in mind: overthrowing the peaceful transfer of power following a fair and certified election in the United States. Other platforms like Parler might have been instrumental in organizing extremists to assault the US Capitol building. But Parler’s members were already committed to the cause. Facebook’s community-building algorithms were effective in drawing some of its massive audience from the sidelines and into the maw of radicalism and sedition.

In fact, Facebook’s own algorithms seem to pump up membership in those groups. A Wall Street Journal article from May 2020 reported an alarming finding from Facebook’s own researchers. According to a 2016 internal study, “64 percent of all extremist group joins are due to our recommendation tools … Our recommendation systems grow the problem.” The article also revealed that the company’s efforts to address this were stifled by interference from the company’s political wing, ever sensitive to criticisms from the right. And just this week a New York Times article outlined several cases where relatively sane people were driven deep into seditious crazytown once they discovered that Facebook widely circulated their most transgressive anti-democratic posts, winning them status and followers. One user found that the more he posted deranged Trumpist messages, the more followers Facebook sent his way, and soon he was hosting a meaningful group based on election denial, with tens of thousands of members. It was almost like an embodiment of what Zuckerberg had described as the feeling of “we are not alone.” By becoming an anti-democracy person, he’d found other people. All reinforcing everyone else’s awfulness.

This week Facebook COO Sheryl Sandberg gave a rare interview. As always, she cautioned that the company wasn’t perfect, but her overall message was that Facebook’s policies were by and large working. “Was there anything you thought Facebook could have done sooner?” asked her interlocutor. Sandberg replied that while Facebook knew that the protests were being organized online, it had generally done its job by removing violent groups like Proud Boys, QAnon, and Stop the Steal. (The latter group garnered 320,000 followers before Facebook took it down, and the corresponding hashtag wasn’t banned until five days after the January 6 insurrection.) She assigned serious blame to others. “I think these events were largely organized by platforms that don’t have our ability to stop hate and don’t have our standards and don’t have our transparency,” she said.",Yes,"It creates a feedback loop that amplifies extremism, disinformation, and conspiracy theories, leading to mass psychosis and political unrest.

The Social Dilemma is the best documentary I’ve seen on this topic. It’s available on Netflix.

If you’re not subscribed to WIRED yet, now’",https://www.wired.com/story/plaintext-facebook-cant-fix-what-it-wont-admit-to/,2021-01-15 14:00:00+00:00,LABEL_1_relevant,0.9955435395240784,Social Media,social_media,Wired,"Information, Discourse & Governance"
137,"Snap Is No Facebook, and Spiegel Insists He Wants It That Way","Evan Spiegel wants the world to know something: His company, Snap, doesn’t admire Facebook, doesn’t want to be like Facebook, and believes that Snap’s approach to its users and their data is better for the world. Appearing onstage at the Code Conference in Palos Verdes, California, Tuesday night, Spiegel said that Facebook may have changed its products and mission but “fundamentally they will have a hard time changing the DNA of the company.” That DNA, Spiegel said, “is about having people compete online for attention.”

It made for great theater, and Spiegel won applause from the well-heeled crowd of tech and media elites. But the reason Spiegel was on stage in the first place was because it appears Snap’s business will never look anything like Facebook’s. Sure, Facebook is facing withering criticism for how it has handled a slew of problems, from protecting users’ data to Russian bots to hate speech. But as a business it mints money. Snap’s does not yet, and Spiegel has been under enormous scrutiny from investors and employees for the way he is running the place.

Facebook’s Instagram is projected to surpass one billion users this year, while Snapchat claims around 191 million daily users, according to its most recent earnings report. Its growth has slowed. While Snapchat has managed to capture a rabid audience of younger users who love its ephemeral messaging---and its impressive augmented-reality filters---some of the company’s recent moves have shaken investors. Its stock price has been slashed in half since February, when it redesigned the app.

Snap is also facing scrutiny around its culture and commitment to diversity, after a newly published memo from a former Snap employee highlighted a “pervasive sexist vibe.” The memo described a toxic macho culture, and it included complaints about a company party that featured scantily clad women and a separate complaint about a senior vice president of engineering who allegedly made inappropriate comments.

Spiegel said the memo was a “really good wake-up call” for the company, and that his team is constantly talking about its culture and values and how to improve. Snap may be located in “Silicon Beach,” not Silicon Valley, but its early responses to the memo resembled those of its northern neighbors facing similar accusations.

Five years ago, people wondered whether Snapchat could be the new Facebook, so much so that Facebook reportedly tried to buy Snapchat for $3 billion. Now, Facebook seemingly waits for Spiegel to roll out new features, then copies them.

The Evan Spiegel onstage Tuesday night was a more thoughtful, poised, and undoubtedly coached CEO than the Spiegel of the past. He crafted the image of a company that communicates well internally and develops new products based not just on data but also on intuition—in a way, pushing his own image as a visionary tech iconoclast. He vouched for Snap as a better alternative to other social media sites that are “unpleasant” for users. “Fundamentally, it’s important to understand that Snapchat is not just a bunch of features,” Spiegel said. “It has an underlying philosophy that runs counter to other social media.”",Yes,It encourages people to compete for attention.,https://www.wired.com/story/snap-is-no-facebook-and-spiegel-insists-he-wants-it-that-way/,2018-05-30 20:05:11.837000+00:00,LABEL_1_relevant,0.9943053126335144,Social Media,social_media,Wired,"Information, Discourse & Governance"
138,What we can learn from the Facebook-Australia news debacle,"Australia’s approach is now being considered by lawmakers and regulators in multiple other governments. Reuters reports that Canadian heritage minister Steven Guilbeault said Canada will model its own legislation on the Australian law. There are also some similarities in a bill proposed by US congressman David Cicilline of Rhode Island that would “provide a temporary safe harbor for the publishers of online content to collectively negotiate with dominant online platforms regarding the terms on which their content may be distributed.”

In general, these measures seek to boost the bargaining power of news organizations and help them extract value from tech giants for the content that newsrooms produce. The Australian model’s novelty lies in its arbitration mechanism, a kind of membrane between the parties intended to help them arrive at a fair exchange of value.

The Australian law will likely pass, so this grand experiment in pushing capital back to the news media will soon be under way. We’ll get to see how it works out, and whether opponents’ concerns bear out—if larger news organizations are privileged over small ones, for instance, or whether the money actually ends up being spent on producing more journalism.

But in view of the objections to this approach, what other options exist? If new subscription models are not enough to sustain the media industry, what else can be done to push billions of dollars back into journalism?",Yes,"It has drastically decreased the amount of revenue that news organizations receive from advertisements. In order to sustain the media industry, new subscription models have been created. However, these models have not been effective in generating enough revenue. Some governments are now considering legislation that would require social media companies to pay news organizations for their content. The effectiveness of this approach is",https://www.technologyreview.com/2021/02/20/1019365/what-we-can-learn-from-facebook-australia-news-debacle/,2021-02-20 00:00:00,LABEL_1_relevant,0.9956542253494264,Social Media,social_media,MIT Tech Review,Economy
139,Facebook and Twitter Want to Keep the Justice System Skewed,"In recent weeks, social media executives have jumped in to pledge their support for the protest movement that emerged in response to the killing of George Floyd by Minneapolis police. In a June 1 Facebook post, Mark Zuckerberg wrote about the need to fight for racial justice and noted, “One of the areas Priscilla and I have personally worked on and where racism and racial disparities are most profound is in the criminal justice system.” On the same day, Twitter’s Jack Dorsey tweeted “Police policy reform now”; he later pledged $3 million to Colin Kaepernick’s Know Your Rights Camp. Instagram CEO Adam Mosseri recently promised to take steps to improve black users’ experience on the platform, and Google’s Sundar Pichai published a blog post this week affirming the company’s support for “organizations working to advance criminal justice reform.”

When it comes to the role these companies actually play in the criminal justice system, however, a different story emerges, according to defense attorneys. As things stand, law enforcement routinely gets access to private social media records, including direct messages, that it can use to investigate and prosecute suspects. But criminal defendants—a group in which poor people of color, particularly black people, are wildly overrepresented—can’t make the companies turn over the same information, even when it could make their case. And at this very moment, Facebook and Twitter are fighting in court to make sure this doesn’t change.

The dispute is about the Stored Communications Act of 1986, a federal law that prohibits an “electronic communications service provider” from sharing users’ communication and account records, subject to a list of exceptions. There’s an exception for law enforcement, but none for defendants. This sets up an imbalance in which privacy law is less restrictive toward the police than it is toward someone trying to prove their innocence. If you’re accused of a crime, police and prosecutors can get a warrant to access your social media accounts, or other users’ accounts that could serve as evidence in the case. But if you go to these companies with a subpoena, they won’t honor it—even if you’ve convinced a judge that the data might prove that someone else committed the crime, that a witness against you is lying, or that you acted in self-defense.

This state of affairs is intensely frustrating to defense attorneys, who in general are already outgunned by vastly better-resourced prosecutors. Jeffrey Stein, a public defender in Washington, DC, said that in serious felony cases, it’s almost automatic for the police to request and receive access to a defendant’s social media accounts. But information that could help his clients’ cases is generally out of reach.

“In a criminal case, where an individual’s life may hinge on whether the jury has a single reason to doubt a witness’s testimony, or whether to believe that your client acted in self-defense, social media often houses the most critical, case-shattering evidence,” he said in an email—hardly a situation Congress could have foreseen when it passed the law in 1986. “That is why law enforcement routinely collects it and uses it against our clients. But, as in so many other ways in our criminal system, the playing field is not level.”

To be clear, social media platforms didn’t create this imbalance. Generally, when they turn down a defendant’s request for user data, the law is firmly on their side. But occasionally, trial court judges have ruled in a defendant’s favor, usually based on the constitutional right to due process. That’s where things get interesting. In these cases, the platforms could simply comply with the order. Instead, they fight tooth and nail to resist it. In one California case, Facebook and Twitter appealed all the way to the US Supreme Court rather than hand over account information to a defendant, even after being placed in contempt for violating the trial court’s order. (The Supreme Court recently declined to take the appeal, possibly because it’s not at the right procedural step.)",Yes,"It has given law enforcement a powerful new tool for investigating and prosecuting crimes, while doing nothing to level the playing field for defendants. This has led to frustration among defense attorneys, who say that vital evidence is often out of their reach. The social media platforms themselves have contributed to this problem by fighting tooth and nail to resist court orders that would require",https://www.wired.com/story/facebook-twitter-criminal-justice-stored-communications-act/,2020-06-19 11:00:00+00:00,LABEL_1_relevant,0.9902134537696838,Social Media,social_media,Wired,"Information, Discourse & Governance"
140,Elon Musk’s “Free Speech” Social Network Mocked by Employee Who Got Fired for Saying Things Tesla Didn’t Like,"Do as he says, not as he does.

Speech

It’s all fun and free speech until someone gets hurt — especially if that someone is Elon Musk.

After tweeting that he’s “giving serious thought” to starting his own social network because Twitter fails “to adhere to free speech principles” and “fundamentally undermines democracy,” the world’s richest man was immediately called out for not practicing what he preaches.

Former Tesla employee John Bernal — who was fired earlier in March after he posted a video to his YouTube channel, AI Addict, that was critical of the company’s full self-driving (FSD) software — quipped that Musk “is a free speech absolutist… unless it involves safety concerns.”

In a video he posted before being fired, Bernal demonstrated that Tesla’s beta FSD software had serious issues, including randomly disengaging while it was driving, forcing him to scramble to take back control of the vehicle. In a later update, the former Tesla employee said he “was fired from Tesla in February with my YouTube being cited as the reason why.”

Advertisement

Advertisement

Chilling Effect

As Business Insider notes, Bernal isn’t the only person whose speech Musk has arguably chilled. In 2018, the Tesla and SpaceX CEO reportedly found the identity of a then-anonymous blogger who posted something criticizing the electric car company’s stock profile and then contacted their employer with threats of a lawsuit.

In a farewell post, the blogger wrote that they had deactivated their Twitter and would no longer write about Tesla. The blogger added that their friend had spoken by phone to Musk, who intimated that if Montana Skeptic “continued to write, he would engage counsel and sue.”

Just this month, Musk insisted that he is “a free speech absolutist,” but as Bernal and others have noted, his ideals don’t always match up to his actions.

“Elon Musk is not Tony Stark,” one user wrote. “Elon Musk is Lex Luther.”

Advertisement

Advertisement

READ MORE: Despite calling himself a ‘free speech absolutist,’ Elon Musk has a history of retaliation against employees and critics [Business Insider]

More on Musk: Elon Musk Says Sometimes He Feels Lonely

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Yes,It creates echo chambers whereby users only see content that supports their existing beliefs. This can lead to the reinforcement of false narratives and the spread of misinformation.,https://futurism.com/the-byte/elon-musk-social-network-mocked,,LABEL_1_relevant,0.9967365860939026,Social Media,social_media,Futurism,"Information, Discourse & Governance"
141,Facebook Moves to Limit Toxic Content as Scandal Swirls,"Mark Zuckerberg would like you to know that despite a scathing report in The New York Times, which depicts Facebook as a ruthless and selfish corporate behemoth, things are getting better—at least, the way he sees it.

In a lengthy call with reporters Thursday, and an equally lengthy ""note"" published on Facebook, the company's CEO laid out a litany of changes Facebook is making, designed to curb toxic content on the platform and provide more transparency into the decisions on content. But perhaps the most consequential update is that the Facebook News Feed algorithm will now try to limit the spread of sensationalist content on the platform, which represents a major change from how the social network traditionally has approached moderation. All of it is in service of restoring trust in a company whose reputation—and that of its leaders—has taken near constant body blows over the past two years.

""When you have setbacks like we’ve had this year, that’s a big issue, and it does erode trust, and it takes time to build that back,"" Zuckerberg said on the call. ""Certainly our job is not only to have this stuff at a good level and to continually improve, but to be ahead of new issues. I think over the last couple of years that’s been one of the areas where we’ve been most behind, especially around the election issues.""

These words come a day after the Times published a damning report that portrays Facebook as not merely behind on issues of election interference, as Zuckerberg suggested, but actively working to downplay what it knew about that interference. It argued that Facebook's executives, wary of picking sides in a partisan battle over Russian interference in the 2016 elections, aimed to minimize Russia's role in spreading propaganda on the platform. The story states that Facebook's former head of security, Alex Stamos, was chastised by the company's chief operating officer, Sheryl Sandberg, for investigating Russian actions without the company's approval and berated him again for divulging too much about it to members of Facebook's board.

In his remarks, Zuckerberg flatly denied this allegation. ""We’ve certainly stumbled along the way, but to suggest that we weren't interested in knowing the truth or that we wanted to hide what we knew or that we tried to prevent investigations is simply untrue,"" he said. (Stamos, for his part, tweeted earlier on Thursday that he was “never told by Mark, Sheryl or any other executives not to investigate.”)

The Times story also alleges that Facebook waged a smear campaign against its competitors through an opposition research firm called Definers Public Relations. The firm repeatedly worked to tie Facebook's detractors, including groups like the Open Markets Institute and Freedom from Facebook, to billionaire George Soros. Critics say that in doing so, Facebook engaged with the same anti-Semitic tropes that have been used by white nationalists and other hate groups that regularly vilify Soros.",Yes,"The way it has changed the way we view information.

The way we view information has changed dramatically over the past few years, thanks in large part to social media. On one hand, social media has made it easier than ever for us to find and consume information that is relevant to our interests. On the other hand, the way that information",https://www.wired.com/story/facebook-limits-hate-speech-toxic-content/,2018-11-15 23:13:29.467000+00:00,LABEL_1_relevant,0.9962140917778016,Social Media,social_media,Wired,"Information, Discourse & Governance"
142,How to manipulate Facebook and Twitter instead of letting them manipulate you,"Fight algorithms with algorithms

Algorithms on Facebook and Twitter control so much of what you see in your feed—they’re the reason, for instance, that you always seem to see posts from your best friend but not those from an old roommate you don’t interact with very much online. Or, more insidiously, why you see news only from certain sources and not from others.

You can mess with these algorithms by using a tool like Gobo, a project from researchers at the MIT Media Lab’s Center for Civic Media. Gobo connects with your Twitter and Facebook accounts and lets you precisely control your feeds in a way that the companies themselves won’t. Sliders let you adjust factors like how serious or silly the stories in your news feed are, what percentage of posts you see are from men or women, or the amount of political diversity you’re exposed to. The tool also lets you see what it removed from your feed and why.

Later, feeds

What if you’d rather just not deal with feeds at all? Try using something like Feedless, an iPhone app that blocks much of the content in Facebook, Instagram, and Twitter feeds on the iOS Safari browser (it’s free for Facebook, though you’ll have to pay $10 per year to use it to block feeds from the other two).

Once you enable it for Facebook, you will still be able to see your full profile and notifications, as well as any posts that those notifications are related to. You can also post status updates. But your news feed—that ever-present, algorithmically driven river of stuff—simply disappears.

Go niche

It’s nearly impossible for new worldwide social networks to make a dent in the market because it’s just so hard to move everyone away from the networks we’re already using.

But there are a lot of other online communities to join, many of them devoted to specific interests.

Are you into books? Try Litsy. Obsessed with design and showing off what you’re working on? Perhaps Dribbble will do it for you. Want to connect anonymously with others who work at your company so you can dish honestly about what’s going on there? Check out Blind.

Not today, advertisers

Ever wonder what kinds of data about you Twitter and Facebook share with advertisers? On Twitter, at least, it’s not that hard to figure out how ads are targeted and either change the targeting or make it impossible to target you, period.

If you click on “Your Twitter Data,” you can enter your password and see all kinds of information about yourself that the social network has collected, including a list called “Interests from Advertisers” that shows all the things those advertisers think you like. (Mine includes everything from dog treats—weird, since I don’t have a dog—to specific brands like Kleenex.)

Some of the many things Twitter advertisers think I like, including dog treats (not so much) and yogurt (absolutely). Twitter

You can disable specific items on this list by un-checking them individually, or simply turn off ad personalization altogether. To get to that option, scroll to the bottom of the list and click “Personalization and Data Settings.”

You can see and manage similar information on Facebook, too. Under “Settings,” click “Ads” to view a slew of data the social network has collected about you and shares with advertisers. Tap “Your Interests” to uncheck individual interests Facebook is using to target ads to you in all kinds of categories (everything from “Interior Design” to “Food Allergy”), or tap “Your Categories” to remove categories advertisers are lumping you into (like “Technology Early Adopters” or “Frequent Travelers”).

The nuclear option

If you’re really, truly sick of Facebook and Twitter, you can take more drastic steps, up to and including deleting your accounts—though the networks make it annoyingly difficult to do.

On Twitter, if you’re using a Web browser, click your user profile image in the upper-right part of the page, and tap “Settings and Privacy.” Once in there, tap “Account” and scroll down to the bottom of the page, where there’s a “Deactivate Your Account” link. Keep in mind that while deactivating will remove your account from Twitter, killing it off actually takes some time; regular Twitter accounts are deleted a month after deactivation, while verified accounts are deleted a year afterwards.",Yes,It can be addictive.,https://www.technologyreview.com/2018/03/20/144567/how-to-manipulate-facebook-and-twitter-instead-of-letting-them-manipulate-you/,2018-03-20 00:00:00,LABEL_1_relevant,0.9955293536186218,Social Media,social_media,MIT Tech Review,User Experience & Entertainment
143,"Russian Oligarchs’ Superyachts Are Being Seized, and the Internet Is Loving It","People on Twitter are even turning it into a game of Bingo.

Yoink

Law enforcement agencies around the world are seizing superyachts owned by Russian oligarchs — and people on social media are watching with glee, The Washington Post reports.

Government sanctions are dialing up the pressure on obscenely rich Russian oligarchs. And while money can easily be hidden in offshore bank accounts, gigantic mega vessels are a lot harder to keep secret.

So now netizens are now watching in fascination as superyachts get seized by authorities worldwide, even turning it into a game of Bingo.

That’s because trackers like VesselFinder or MarineTraffic allow practically anybody with an internet connection to monitor the whereabouts of private jets and luxury yachts.

Advertisement

Advertisement

Even 19-year-old Jack Sweeney, that kid who was keeping track of Elon Musk’s every move, now has a Twitter account dedicated to snooping on Russian zillionaire yachts.

Snatched Up

Amid Russia’s bloody invasion of Ukraine, governments are aiming their sights at “the assets Russian elites and their family members hold in our respective jurisdictions — their yachts, luxury apartments, money, and other ill-gotten gains,” according to a March 3 White House statement.

The thinking goes that if Russia’s ultra rich — many of whom are members of president Vladimir Putin’s inner circle — lose their precious toys, they might be more inclined to oppose the atrocities happening in Ukraine right now.

And yachts are indeed being picked off one by one. Just last week, French officials seized a massive $120 million superyacht owned by a Russian oil tsar.

Advertisement

Advertisement

They’re powerful symbols of excessive wealth. For many, that’s the reward for staying loyal to Putin’s regime.

No wonder people are entranced by watching the oligarchs get a much-needed reality check.

READ MORE: Schadenfreude at sea: The Internet is watching with glee as Russian oligarchs’ yachts are seized



Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.

Advertisement

Advertisement",Yes,"We are creating a generation of people who are extremely narcissistic and self-centered.

While social media does have some positive effects, such as allowing us to connect with friends and family, the negatives far outweigh the positives.

The constant need for validation, the comparison to others, and the striving for perfection can all lead to a downward",https://futurism.com/the-byte/russian-oligarchs-yachts-seized,,LABEL_1_relevant,0.9952329993247986,Social Media,social_media,Futurism,Social Norms & Relationships
144,Twitter says it may warn users about deepfakes—but won’t remove them,"The news: Twitter has drafted a deepfake policy that would warn users about synthetic or manipulated media, but not remove it. Specifically, it says it would place a notice next to tweets that contain deepfakes, warn people before they share or like tweets that include deepfakes, or add a link to a news story or Twitter Moment explaining that it isn’t real. Twitter has said it may remove deepfakes that could threaten someone’s physical safety or lead to serious harm. People have until November 27 to give Twitter feedback on the proposals.

The context: It’s become relatively easy to make convincing doctored videos thanks to advances in artificial intelligence. That’s led to a huge panic over the potential for deepfakes to subvert democracy, as they can be used to make politicians seem to say or do whatever the creator wants.",Yes,"The spread of fake news. This can have dangerous consequences, as it can lead to the spread of misinformation and potentially cause physical harm to people. To combat this, Twitter has drafted a policy that would warn users about deepfakes, but not remove them. This has caused some backlash, as many people believe that Twitter should be doing more to combat",https://www.technologyreview.com/2019/11/12/217/twitter-says-it-may-warn-users-about-deepfakesbut-wont-remove-them/,2019-11-12 00:00:00,LABEL_1_relevant,0.9965264201164246,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
145,Twitter Insiders Allegedly Spied for Saudi Arabia,"In charges released Wednesday, the Justice Department accused two former Twitter employees, Ahmad Abouammo and Ali Alzabarah, of abusing their internal system privileges to spy on target users and pass the information they collected to Saudi Arabia. The criminal complaint also alleges that it was trivial for them to do so—a chilling reminder of how much damage an insider can cause.

The court documents, first reported by The Washington Post, also reference a third suspect, Ahmed Almutairi, who allegedly worked as an intermediary between the Twitter insiders and the Saudi government. Alzabarah and Almutairi are both Saudi citizens, while Abouammo is a United States citizen. He was arrested in Seattle on Tuesday.

Alzabarah joined Twitter in August 2013 as a site reliability engineer, the complaint says, and gained more responsibility over time until he could access users accounts and personal data—like phone numbers and IP addresses—as part of his job. He also allegedly developed relationships with Saudi intelligence agents during this time, and is accused of looking up private information from more than 6,000 Twitter accounts, including those of dissidents and political activists, on Saudi Arabia's behalf over the course of a few months in 2015. Saudi Arabia is known for aggressively exerting influence and tracking detractors on social media. Crown Prince Mohammed bin Salman and his regime have also fostered close ties to Silicon Valley.

""Insiders can do major damage and often go undetected for large periods of time."" Dave Kennedy, TrustedSec

The Justice Department alleges that Abouammo accessed data from three user accounts, at least one of which was that of an outspoken critic of the Saudi royal family. But unlike Alzabarah, Abouammo's role as media partnerships manager at Twitter does not necessarily seem to necessitate access to private user data. The complaint asserts that the Saudi government wired at least $300,00 to Abouammo and his family. He left Twitter in May 2015, but allegedly still attempted to get information about users from some former Twitter colleagues. Abouammo worked for Amazon after leaving Twitter, but apparently left that job over a year ago.

Twitter said on Wednesday that it appreciated the work of the Justice Department and Federal Bureau of Investigation on the case. ""We recognize the lengths bad actors will go to try and undermine our service,"" the social media giant said in a statement. ""Our company limits access to sensitive account information to a limited group of trained and vetted employees. We’re committed to protecting those who use our service to advocate for equality, individual freedoms, and human rights.""

But the fact that even a company with the resources of Twitter was unable to head off an insider threat speaks to just how difficult they are to defend against. Most organizations are woefully under-defended against those attempts, according to multiple cybersecurity professionals WIRED spoke with Wednesday. They emphasize that the risk can never be totally eliminated, but that there are necessary data access controls and siloing efforts that many organizations overlook or implement weakly.

For example, many companies aren't strict enough about limiting which employee accounts have ""permission"" or ""privilege"" to access sensitive data.

""Privileged access is one of the toughest things in any organization and especially in tech companies,"" says Dave Kennedy, founder of TrustedSec, a cybersecurity firm that conducts so-called penetration tests, the practice of probing a system for weaknesses. ""Companies are not doing enough to protect sensitive consumer data. This is a great example with Twitter. Insiders can do major damage and often go undetected for large periods of time.""

LEARN MORE The WIRED Guide to Personal Data

Many organizations find it difficult to prioritize the work it takes to stratify employee access to data based on specific need, a process often called provisioning. Uber infamously allowed employees access to a ""God mode"" that let them track users and view their account details—a feature staffers extensively abused. On the other end of the spectrum, making it more difficult for insiders to access and exfiltrate large amounts of sensitive data is possible but takes stringent, often frustrating rules. When companies grow from relaxed small businesses or startups into massive organizations, imposing those restrictive controls can be deeply unpopular among the people who work there.

""The corporate world is coming to realize the value of focusing on foundational security principles used in the military and finance worlds,"" says Kenn White, security principal at the database company MongoDB. ""Insider threats and administrator privileged access are serious issues for any company, because at some point, you have to trust someone to manage your most trusted, confidential data. Encryption has a role, but only in a broader strategy of separation of duties and compartmentalization.""",Yes,Our private lives are now public. We are constantly sharing information about ourselves that we would never share in person. This is leading to a loss of privacy for everyone. The other main unintended consequence is that social media is being used by governments and corporations to track and influence people.,https://www.wired.com/story/twitter-insiders-saudi-arabia-spy/,2019-11-07 03:31:14.560000+00:00,LABEL_1_relevant,0.9964261651039124,Social Media,social_media,Wired,Security & Privacy
146,The Moral Panic over Social-Networking Sites,"It’s a “monumentally ill-considered piece of legislation” that “by any rational measure” should never have left the House, says Henry Jenkins, professor of literature and director of the Comparative Media Studies Program at MIT. Jenkins believes the act plays on parents’ lack of understanding, and their resulting fears, about their kids’ activities on the Internet. “But the price of standing up to that fear may be too high for liberal Democrats,” he says.

If the Senate approves a similar bill and the legislation reaches President Bush’s desk, the price to young people will be even higher, say Jenkins and other critics. “If it would actually prevent predation, I would be fine with it,” says Danah Boyd, a PhD candidate in the School of Information Management Sciences at the University of California, Berkeley, who is considered one of the leading scholarly authorities on social-networking sites. “But it’s not going to help at all. Out of 300,000 child abductions every year, only 12 are by strangers. This is just going to stifle the social-networking industry and completely segment youth around economic status.”

The impact on youth from economically disadvantaged families is what Jenkins worries about most. “Already, you have a gap between kids who have 10 minutes of Internet access a day at the public library and kids who have 24-hour-a-day access at home,” he says. “Already, we have filters in libraries [required under the Child Internet Protection Act of 2001] blocking access to much of the Internet. Now we’re talking about adding even more restrictions. It exaggerates the ‘participation gap’ – not a technology gap, but a difference in access to the defining cultural experiences that take place around technology today.”

Current Internet filters at schools and libraries – some aimed at pornography and obscene materials, some already targeting social-networking sites – have “a tremendous chilling effect on education,” agrees Jeff Cooper, an educational-technology consultant and former high-school teacher in Portland, OR. “The ‘Just Say No’ philosophy has never worked,” Cooper says. “You’re lumping all social networking into the negative basket, and not giving kids any alternative. But there is so much good stuff online that nobody ever talks about.”

Indeed, while it might be easy to agree that teens shouldn’t be wasting time on MySpace or other social-networking sites while they’re at school, DOPA would cover any site that allows networking and chatting. As one example, Cooper points to TappedIn.org, a social-networking and professional-development site for teachers. Students often use personal and public “rooms” on the site as part of virtual classroom activities. “It allows teachers to bring their students online in a very safe and secure environment,” explains Cooper. “My concern isn’t really that MySpace won’t be accessible from schools, but that other sites like TappedIn will be banned.”

DOPA supporters frequently cite a 2000 report about online sexual victimization funded by the National Center for Missing and Exploited Children, which concluded that one-fifth of children have been sexually solicited in chat rooms, by instant message, or by e-mail. But in fact, as Boyd and other opponents point out, the same report states that most solicitations come from other young people – only 4 percent are from adults over 25 – and that most kids deal with these solicitations simply by not answering or logging off. “To clamp down on a bunch of new networking sites really doesn’t do anything” to stop sexual predators, says Cooper. “You might as well shut off the Internet entirely.”

Opponents of DOPA misunderstand the bill, says Jeff Urbanchuck, a press officer for Representative Fitzpatrick. He says it is intended only to reduce the risk to teens from one particular category of websites – those where members can create online profiles and fill them with personal details, including e-mail or instant-messaging addresses, that help predators contact them. Critics are “extending beyond the MySpaces and Facebooks and arguing that the technology of social networking is so pervasive now that the Internet is going to become one big social-networking site,” Urbanchuck says. “But the objective of the bill is to deal with the growing threat of online predators on specific sites that allow profiles. We want to tailor the bill to those sites.”

Even banning access just to sites that allow profiles, however, would affect scores of educational, community, and media-sharing sites, including sites as popular as Flickr and as specialized as TappedIn. And in the longer term, predicts Boyd, the law would simply drive teen networking underground, where it would be more difficult for adults to monitor. “They’ll be moving from site to site with a level of ephemerality that no one can keep up with,” she says. “Not the cops – not even the designers of the technology.”",Yes,"The potential for a 'chilling effect' on education. If the Senate approves a similar bill and the legislation reaches President Bush’s desk, the price to young people will be even higher, say Jenkins and other critics.",https://www.technologyreview.com/2006/08/07/228481/the-moral-panic-over-social-networking-sites/,2006-08-07 00:00:00,LABEL_1_relevant,0.9960076808929444,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
147,We've Got the Screen Time Debate All Wrong. Let's Fix It,"In 1995, New York City psychiatrist Ivan Goldberg logged onto PsyCom.net, then a popular message board for shrinks, to describe a new disease he called ""internet addiction disorder,"" symptoms of which, he wrote, included giving up important social activities because of internet use and ""voluntary or involuntary typing movements of the fingers.""

It was supposed to be a joke.

But to his surprise, many of his colleagues took him seriously. Their response led him to create an online support group for internet addicts—though he quickly downgraded the affliction, renaming it ""pathological internet-use disorder."" The word addiction ""makes it sound as if one were dealing with heroin, a truly addicting substance,"" Goldberg told the New Yorker in 1997. ""To medicalize every behavior by putting it into psychiatric nomenclature is ridiculous.""

LEARN MORE The WIRED Guide to Internet Addiction

Today, more than two decades after Goldberg's joke fell flat, mental health professionals find themselves in a similar bind. Public anxiety over the side effects of screen time—the hours we spend staring at our various devices—is the highest it's been in years. That anxiety has manifested in the form of self-help books, social movements, major media outlets foretelling the ""the worst mental-health crisis in decades,"" and no shortage of guilt. (You let your kid play with an iPad at restaurants? You spent 30 minutes browsing Instagram when you could have been exercising? Or playing board games with your family? Or learning a second language? You sad/selfish/lonely monster!) And yet, there exists little clear evidence that we are locked in an unambiguously harmful relationship with our devices—let alone addicted to them in any clinical sense. ""For the past twelve months, the narrative surrounding technology use and screen time has been consistently negative, but it's been driven more by fear than facts,"" says UC Irvine psychologist Candice Odgers.

Experts like Odgers say we'll never get good answers about the effects of screen time, unless we start asking better questions. And that means being honest with ourselves about what we mean by ""screen time"" in the first place.

This year, the conversation around digital dependence entered a new phase when Facebook CEO Mark Zuckerberg resolved to spend 2018 fixing Facebook, vowing, among other things, to ensure that time spent on the social network would be ""time well spent."" (Zuckerberg borrowed the phrase from former Google design ethicist Tristan Harris, who has popularized the term in recent years by characterizing it as the opposite of time surrendered involuntarily to devices, apps, and algorithms designed to ""hijack our minds."") A few days after Zuck's post went public, major Apple shareholders urged that company to study its products' effects on children and equip parents with better tools for managing their kids' screen time. The following month, Harris formed the Center for Humane Technology—an alliance of tech-giant turncoats united in opposition against the attention-grabbing products they helped create.

These events helped set the tone of the year to come. ""For right or wrong, big tech companies have seen which way the wind is blowing and responded,"" says Andrew Przybylski, an experimental psychologist at the Oxford Internet Institute. Google led the charge, pledging its commitment to digital well-being and releasing new tools designed to help Android users monitor their tech habits. Apple followed suit, unveiling features designed to help users understand and manage the time they spend on their iOS devices. Then came Facebook and Instagram, each of which released features designed to help users track, and set limits on, the time they spend in-app.

Want more? Read all of WIRED’s year-end coverage

None of those companies has shared whether its tools have been effective. It's possible they never will release any data—and even if they do, researchers say, taking them at face value could be hard. ""There wasn't a good empirical basis for their creation in the first place, so there probably won't be good evidence for their effectiveness,"" Przybylski says.

Look what happened earlier this year, when Congress asked the National Institutes of Health what science had to say about tech's addictive potential and the effects of screen time on kids. NIH director Francis Collins' response amounted to a big shrug: What limited research exists has been inconclusive, he wrote. Tech addiction? Scientists don't even agree on how to define it, let alone measure it. As for screen time's impact on developing minds, Collins said researchers are still gathering evidence on how best to balance technology's ""obvious benefits"" with its ""potential harms.""",Yes,"It makes us feel bad.

At the same time, many experts have been quick to point out that the link between screen time and mental-health problems is far from clear. In a recent paper, Jim Steyer—the CEO of Common Sense Media, a nonprofit that aims to help kids thrive in a world of media and technology—",https://www.wired.com/story/year-in-review-tech-addiction-debate-all-wrong/,2018-12-20 13:00:00+00:00,LABEL_1_relevant,0.9827038645744324,Social Media,social_media,Wired,"Information, Discourse & Governance"
148,Facebook Rolls Out More Features for Dead People,"Facebook was designed for the living, but as the social network grew older, it also needed to decide what should happen when users die. In 2015, Facebook began allowing people to assign a legacy contact to be in charge of their account in the event they pass away, but the system wasn't perfect and users found some of the associated policies upsetting.

The company announced Tuesday that it will give the legacy contact more control over the deceased’s presence on Facebook, including the ability to manage a new “Tributes” section where loved ones can reminisce about the person they lost. The feature underscores how Facebook wants to be the place for life’s most important events, even when that event is death.

“I had very close friends, particularly one, who had lost people and told me how important Facebook was to preserving their memories,” says Sheryl Sandberg, Facebook’s COO. “My friend Kim had lost her brother, he died by suicide, and she and her mother were not up to having a memorial service, so Facebook became that memorialization place for them.”

Louise Matsakis covers cybersecurity, internet law, and online culture for WIRED.

Facebook has encouraged its users to share major life milestones on the platform, like when they get married, have a kid, or get a new job. It’s no longer just a social network; it’s also a scrapbook. When users die, they may leave behind accounts containing over a decade of memories, and they might not have specified how they want that archive to be maintained. Facebook has to balance the interests of grieving loved ones with the privacy of someone who is now dead, says Alice Ely, a product manager at Facebook.

“We have a hard stance against never allowing anybody to log into [someone else’s] account, but that’s exactly why we wanted to create a legacy contact to give [people] these controls to manage the profile after the person has passed away,” she explains.

The updates Facebook announced Tuesday will address some of the complaints raised by users who have lost someone close to them. Before a legacy contact can take over managing an account, it must be “memorialized,” meaning Facebook has verified the person is actually dead. In the past, anyone could memorialize an account, for example by sending Facebook a news story, and the process sometimes happened before the person’s loved ones were ready. Now Facebook says it will allow only friends and family to request to have an account memorialized.

Facebook doesn’t allow users under 18 to appoint a legacy contact, but going forward parents who have lost their children can apply to be theirs. Previously, parents didn’t have the option of changing anything on their child’s profile after they passed away; it remained frozen in time. “We had one instance where parents wrote in to us and their daughter’s profile picture was a picture of her at a Halloween party. It was really distressing to see because she had died at that point, and they didn’t want to remember her in this Halloween costume,” says Ely. “That was one instance where we allowed them to become the legacy contact, and going forward this will be the policy.”

Facebook

Once a person’s account is memorialized, the designated legacy contact can update their profile picture or cover photo and write a pinned post to their profile, for example to share information about a memorial service. They can’t read private messages or delete anything. As of Tuesday, the legacy contact will also have the option to manage a Tributes section, a separate portion of the deceased’s Facebook profile where friends and family can share content about their loved one. The legacy contact will have the ability to edit who can post to Tributes and who can see what content is shared there, among other controls.

Tributes is a dedicated section separate from a dead person’s Facebook timeline. The posts shared to it won’t show up alongside things like status updates or photos uploaded while the person was still alive. The section functions like a digital memorial or gravestone, which the legacy contact is in charge of maintaining.",Yes,"It can have a negative impact on people's social lives and real-world relationships. Some research has found that social media can make people feel more isolated and less connected to others, while other studies have found that social media can actually help people connect and build relationships.",https://www.wired.com/story/facebook-rolls-out-more-features-dead-people/,2019-04-09 13:00:00+00:00,LABEL_1_relevant,0.9931916594505308,Social Media,social_media,Wired,"Information, Discourse & Governance"
149,"As EU says it'll ban Russia's 'toxic media machine', social media firms face pressure to act","Late yesterday the European Union announced an “unprecedented” step against Russian disinformation, saying it would ban Kremlin-based media outlets, Russia Today (aka RT) and Sputnik — extending measures targeting the country following president Putin’s invasion of Ukraine.

“The state-owned Russia Today and Sputnik, as well as their subsidiaries will no longer be able to spread their lies to justify Putin’s war and to saw division in our Union,” said the EU’s president, Ursula von der Leyen. “So we are developing tools to ban their toxic and harmful disinformation in Europe.”

Details on the EU’s planned ban on Russian state media content are still thin on the ground.

At the time of writing EU officials we’ve spoken to were unable to confirm whether or not the ban will extend to online platforms such as Google’s YouTube — where RT and Sputnik both have a number of channels, and the US platform hosts thousands of videos they upload for viewing on demand as well as enabling them to reach viewers via livestreaming.

However in the past few hours EU officials have been cranking up the public pressure on mainstream tech platforms on the disinformation issue.

Today the office of internal market commissioner Thierry Breton announced that both he and the values & transparency commissioner, Vera Jourova, had spoken to the CEOs of Google and YouTube urging them to step up efforts against Russian propaganda.

EU’s @ThierryBreton & @VeraJourova spoke with Google & YouTube CEOs, urged online platforms to step up efforts to address Russian war propaganda 👇#EUvsDisinfo 🇪🇺 🇺🇦 https://t.co/8d9hUk8Q8u — Lucía Caudet (@LuciaCaudet) February 28, 2022

On Saturday Google announced it had demonetized RT and other Russian-backed channels, meaning they can no longer receive ad revenue via its platforms or ad networks. But in a video call with the two tech CEOs Breton it reported to have said they need to go further, per Reuters.

“Freedom of expression does not cover war propaganda. For too long, content from Russia Today and other Russian state media has been amplified by algorithms and proposed as ‘recommended content’ to people who had never requested it,” the commissioner said in a statement after the call.

“War propaganda should never be recommended content — what is more, it should have no place on online platforms at all. I count on the tech industry to take urgent and effective measures to counter disinformation.”

The EU’s high commissioner for foreign affairs, Josep Borrell, was also asked for details of the ban by the BBC this morning and also declined to specify if it will apply online.

“We’ll do whatever we can to prevent them to disseminate [toxic content to] a European audience,” he told BBC Radio 4’s Today program when questioned on how workable a ban would be, adding: “We have to try to cut them.”

He also aggressively rebutted a line of questions querying the consistency of the EU’s move vis-a-vis Western liberal democratic principles which center free speech — saying the two channels do not distribute free information but rather pump out “massive disinformation” and create an “atmosphere of hate” that he asserted is “against the freedom of thinking” and is “toxifying minds”.

“If you start telling lies all the time, if you create an atmosphere of hate this has to be forbidden,” Borrell added.

The EU’s previous proposals for dealing with online disinformation have largely focused on getting voluntary buy-in from the tech sphere, through a Code of Practice on disinformation. However it has been pressing for tougher action in recent years, especially around COVID-19 disinformation which presents a clear public safety danger.

But an out-and-out ban on media entities — even those clearly linked to the Kremlin — is a major departure from the usual Commission script.

Of the two sanctioned channels, Russia Today appears to garner the most eyeballs on YouTube — where its main channel has some 4.65M subscribers, although content produced by the state-backed media outlet can be found far more widely across the platform.

The channel’s marketing even claims it is “the most watched news network on YouTube” — with a touted 10BN+ views — which looks more than a little awkward for Google in the context of Putin’s land war in Europe.

We reached out to Google to ask if it intends to take further action against the Kremlin-backed media channels in light of the EU’s decision to ban them.

At the time of writing the company had not responded but we’ll update this report if it does.

While the Kremlin deploys the very thinly veiled camouflage of (claimed) media operations to launder its propaganda as professional news commentary, its infowars tactics are of course far more sprawling online — where multiple user generated platforms provide Putin with (essentially) limitless opportunities to pass his influence ops off as genuine public opinion by making his propaganda look like organic user generated content.

That means, even if pressure from the EU forces mainstream platforms like YouTube to ban RT and Sputnik it can’t hope to stop the Kremlin’s propaganda machine — it will just drive Russia to produce more less directly attributable propaganda, via sock puppet/fake accounts and the like.

And — notably — among the new sanctions that the EU announced at the weekend it added the notorious Russian troll factory, the Internet Research Agency, to the expanded list — along with its oligarch funder.

Although that took isn’t likely to be the only troll farm operating at Russia’s bidding, even if it is the most well known. Reports have long suggested Russian’s web brigade has used outsourcing tactics to try to better cover its manipulative tracks, for example.

In a small sign of some of the less immediately visible Kremlin-backed propaganda activity dialling up around the Ukraine war, Facebook’s parent company Meta put out an update today — saying the teams it has monitoring disinformation (aka “coordinated inauthentic behavior”, as Facebook terms it) have been on “high alert” since the invasion, and have taken down a network, of circa 40 accounts/Pages/Groups on Facebook and Instagram, being run out of Russia, which had been targeting people in Ukraine.

In a neat illustration of the malicious duality that can be applied to even disinformation reporting tools, Meta said the Russian network it identified had been reporting Ukrainians for violating its policies on coordinated inauthentic behavior — including by posing as independent news outlets.

“They ran websites posing as independent news entities and created fake personas across social media platforms including Facebook, Instagram, Twitter, YouTube, Telegram and also Russian Odnoklassniki and VK,” write Nathaniel Gleicher, Meta’s head of security policy, and David Agranovich, a director, threat disruption.

“They were operated from Russia and Ukraine and targeted people in Ukraine across multiple social media platforms and through their own websites. We took down this operation, blocked their domains from being shared on our platform, and shared information with other tech platforms, researchers and governments. When we disrupted this network on our platform, it had fewer than 4,000 Facebook accounts following one of more of its Pages and fewer than 500 accounts following one or more of its Instagram accounts.”

Meta also said some of the accounts it found using fictitious personas had also used profile pictures which is suggests were generated using artificial intelligence techniques like generative adversarial networks (GAN).

“They claimed to be based in Kyiv and posed as news editors, a former aviation engineer, and an author of a scientific publication on hydrography — the science of mapping water. This operation ran a handful of websites masquerading as independent news outlets, publishing claims about the West betraying Ukraine and Ukraine being a failed state,” it adds.

“Our investigation is ongoing, and so far we’ve found links between this network and another operation we removed in April 2020, which we then connected to individuals in Russia, the Donbass region in Ukraine and two media organizations in Crimea — NewsFront and SouthFront, now sanctioned by the US government.”

Meta’s security update also warns that it’s seen increased targeting of people in Ukraine — including Ukrainian military and public figures — by Ghostwriter, a threat actor it notes has been tracked for some time by the security community.

“We detected attempts to target people on Facebook to post YouTube videos portraying Ukrainian troops as weak and surrendering to Russia, including one video claiming to show Ukrainian soldiers coming out of a forest while flying a white flag of surrender,” it notes, adding: “We’ve taken steps to secure accounts that we believe were targeted by this threat actor and, when we can, to alert the users that they had been targeted. We also blocked phishing domains these hackers used to try to trick people in Ukraine into compromising their online accounts.”

Twitter also confirmed that it has taken some action against suspected Russian disinformation on its platform since the invasion when we asked.

A spokesperson for the social network told us:

On Feb. 27, we permanently suspended more than a dozen accounts and blocked the sharing of several links in violation of our platform manipulation and spam policy. Our investigation is ongoing; however, our initial findings indicate that the accounts and links originated in Russia and were attempting to disrupt the public conversation around the ongoing conflict in Ukraine. As is standard, further details will be shared through our information operations archive once complete.

Twitter said it will be continuing to monitor its platform for “emerging narratives” which violate platform rules as the situation in Ukraine develops — such as rules on synthetic and manipulated media and its platform manipulation policy — adding that it will take enforcement action when it identifies content and accounts that violate its policies.

The platform does already label state-affiliated accounts belonging to the Russian Federation.

Meta, meanwhile, faced restrictions on its service inside Russia Friday — after the state internet regulator appears to have retaliated over fact-checking labels Facebook had placed on four Kremlin-linked media firms.

On Saturday, Twitter also said access to its service had been restricted in Russia following street protests against the war.

Both Meta and Twitter are urging users to beef up their account security in light of the threat posed by Russian cyberops, suggesting people be cautious about accepting Facebook friend requests from people they don’t know, for example, and implementing 2FA on their accounts to add an additional security layer.

We also reached out to TikTok to ask if it’s taken any measures against Russian propaganda but at the time of writing it had not responded.

The Russian military has been a keen user of TikTok in recent years as Putin’s propaganda tacticians leveraged the viral video clip sharing platform to crank up a visual display of power which looked intended to psyche out and sap the will of Ukraine to resist Russian aggression.

Albeit, if anti-Ukraine ‘psyops’ was the primary goal of Russian military TikTokking the tactic appears to have entirely failed to hit the mark.",Yes,"We are no longer as easily controlled by governments and large corporations as we once were. The internet gives a voice to the voiceless, and a platform for the oppressed to share their stories.",https://techcrunch.com/2022/02/28/eu-rt-sputnik-ban-social-media-pressure/,2022-02-28 00:00:00,LABEL_1_relevant,0.9964003562927246,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
150,Instagram now lets users reply to comments with Reels,"Instagram has rolled out a new “Reels Visual Replies” feature that lets users reply to comments on posts with Reels. Now, when users choose to reply to a comment, they’ll see a new option that allows them to select the Reels button to create a video reply. The social media giant notes that the video reply will appear as a sticker.

“We love the communities that creators have built on Instagram,” Instagram said in a tweet announcing the new feature. “That’s why we’re excited to launch Reels Visual Replies, a new feature to interact with your audience.”

We love the communities that creators have built on Instagram. 😊❤️ That’s why we’re excited to launch Reels Visual Replies, a new feature to interact with your audience. You can now reply to comments with Reels and the comment will pop up as a sticker. pic.twitter.com/dA3qj1lAwE — Instagram (@instagram) December 10, 2021

Instagram’s newest feature further integrates Reels into its wider social media platform. It’s worth noting that the new feature is similar to TikTok’s video replies, which allow users to reply to comments on their content with video responses. TikTok rolled out the feature last year to give users a way to offer more context about their videos and answer questions. Considering the popularity of the feature, it’s no surprise that Instagram has launched a similar feature for its own TikTok-clone.

The launch of Reels Visual Replies comes as Instagram has been rolling out several new features. Most recently, the company introduced a new end-of-year “Playback” IG Stories time capsule feature. The feature leverages the Stories Archive, so users can customize and select up to 10 stories to share with their followers. To be eligible for this feature, users need to have posted more than three stories this year or have Stories Archive turned on. Instagram suggests Playback posts for you, but you also have the option to pick what you want to share. The new feature is currently available to users for “several weeks.”

Instagram has also rolled out a new “Rage Shake” feature that lets users shake their phones to report a problem with the app. Once you shake to report, you can explain what happened with the app and report the issue. The social media giant also introduced the ability to delete single items from a carousel post with at least three images or videos. For context, carousel feed posts can combine up to 10 photos and videos in one post. With this update, you can delete items from an existing carousel.

The company is also testing a new feature called “Take a Break,” which allows users to remind themselves to take a break from Instagram after either 10, 20 or 30 minutes. There’s also a new “Add Yours” sticker that creates public threads in Stories. The new feature allows users to respond to other users’ Stories with their own following a prompt or a certain topic.",Yes,It can lead to addiction.,https://techcrunch.com/2021/12/13/instagram-now-lets-users-reply-to-comments-with-reels/,2021-12-13 00:00:00,LABEL_1_relevant,0.9946584105491638,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
151,I Woke Up to Australia's Newsless News Feed,"By late Tuesday afternoon, Sydney time, Facebook had announced it would restore access to news on its platform in Australia. But logging on five days after its surprise ban, there was still no news. We cannot share it. We cannot see it.

It will return soon, we are told, but for now, when I log on to Facebook, this is what I see:

WIRED OPINION ABOUT Celina Ribeiro is an Australia-based freelance writer whose work has appeared in The Guardian, the BBC, Australian Financial Review and The Saturday Paper.

A friend from London has taken a picture of crocus blooming in a field. Someone in my neighborhood is selling an Ikea table and two chairs for $100. Someone else nearby is selling a baby carrier for $50. A flatmate from my university days went out with friends on the weekend and took a photo. My mother’s best friend likes a video about Gordon Ramsay’s pick-up lines. A man I met on holiday in Turkey wishes a friend he met in Lisbon a happy birthday. My local café has expanded its range of house-made lemonades. And my high school drama teacher has bought many, many pieces of bargain vintage furniture and homeware.

Facebook without news is different.

Last Thursday morning local time, amid a global pandemic and ahead of a state election, millions of Australians woke to find their Facebook feed so altered. The move, Facebook said, was one it made with a “heavy heart” ahead of the federal government putting its media bargaining code, which seeks to make Google and Facebook negotiate payment with news publishers in exchange for their content, for a vote in the upper house.

At the core of the Australian move is the argument that both news publishers and the platforms benefit from content being shared on Facebook and Google. But given that the tech giants are essentially gatekeepers of the internet, news publishers are unable to negotiate with them for what may be a fair price for their content. Australia argues this code redresses that imbalance.

Now the government has made some seemingly minor concessions to its law. News is coming back. But Facebook has made clear: It can, and may, take news away again.

As we first discovered our News Feeds void of news last week—with Facebook pages belonging to the likes of Guardian Australia emptied and featuring a gray, bold-type claim that there were “No posts yet”—there erupted a broad sense of disbelief and indignation. The tech giant had not only blocked news, but apparently unintentionally blocked numerous other information pages including the national Bureau of Meteorology, which issues emergency weather warnings; government health departments; charities; and a state opposition leader’s Facebook page just weeks before an election. On Monday the still-blocked Australian Medical Association flipped to Twitter to plead “Hey @facebook—we’re not a media company, we’re doctors and this is a pandemic—how about restoring our content?”

My own feed became full of friends declaring that Facebook had instantly become boring and companies trying to sell me stuff, or as one friend in Melbourne put it: “a featureless wasteland, populated by boomer memes and dog photos.” I instantly began to regret nearly every Facebook group I had ever joined. And I have no idea why I ever voluntarily opted to like a company’s page.

And now, as Australians drown in dog photos and 30-second captioned made-for-social videos, news is to return to Australia. That’s good. But we know this isn’t over.",Yes,"It creates echo chambers in which people only see information that aligns with their own beliefs.

Facebook without news is different. It’s a place where people can only share information that they agree with, and that’s not healthy.",https://www.wired.com/story/newsless-news-feed-facebook-australia/,2021-02-23 13:00:00+00:00,LABEL_1_relevant,0.9955190420150756,Social Media,social_media,Wired,"Information, Discourse & Governance"
152,Why Elizabeth Warren's Feeds Are Flooded With Snake Emoji,"Senator Elizabeth Warren’s social media feeds are crawling with snakes. If you scroll through the replies to every new tweet and the comments of her most recent Instagram posts, you won’t find much discussion about the actual content, whether its donations or the student loan debt crisis. Instead, it’s line after line of acid-green snake emoji, intermixed with people apologizing for the people inundating Warren’s accounts with snake emoji. Lest ye think that Senator Warren’s campaign for president has suddenly become much beloved by reptile enthusiasts, #NeverWarren is also trending, alongside #WarrenIsASnake.

The people slinging snake emoji are mostly supporters of Senator Bernie Sanders’ campaign for president. (Or so they claim. This is still the internet, after all.) On Monday, CNN reported (and Warren confirmed) that, during a private meeting between the two in 2018 during which they discussed their presidential ambitions, Sanders stated that he did not think a woman could win the election. Sanders has denied making this comment, calling it “ludicrous” in a statement to CNN and “incomprehensible” when asked about the situation during last night’s Democratic debate. Warren again disagreed with Sanders’ account, but said she was “not here to fight with Bernie.”

Still, the atmosphere between the senators, who are friends, looked tense. As the debate came to an end, an image of the pair having a terse discussion (with Tom Steyer looming between them like a nonsequitur stork in a suit) has become a meme of its own. But it’s nowhere nearly as ubiquitous as the snakes.

Twitter content This content can also be viewed on the site it originates from.

Warren is not the first woman to be visited with a social media plague of snake emoji in the midst of a controversy. Calling someone a snake is hardly a social media invention, but employing the snake emoji as a form of memetic harassment started with popstar Taylor Swift. In July 2016, people began spamming Swift’s accounts with snake emoji because they believed she was duplicitous—partially due to situations with ex-boyfriend Calvin Harris and Katy Perry, but mostly because of a longtime feud with Kanye West (and, by extension, Kim Kardashian West). After the release of West’s single “Famous,” Swift said that she was offended by the song’s lyrics, in which West both takes credit for Swift’s fame and says he thinks the pair might sleep together. In response, Kardashian West posted audio on Snapchat of Swift seeming to approve the lyrics prior to the track’s release. She also joined incensed fans in the snake emoji flood.",Yes,"It allows for the spread of information and memes at an incredibly rapid pace. This can obviously be a good thing, as it allows for the rapid dissemination of news and information. However, it also allows for the rapid spread of misinformation and memes that can be used to harass and bully people.",https://www.wired.com/story/elizabeth-warren-snakes-meme/,2020-01-15 20:59:09.615000+00:00,LABEL_1_relevant,0.9959132075309752,Social Media,social_media,Wired,"Information, Discourse & Governance"
153,Trump’s Latest Tariffs Could Hurt Tech—and Even Social Media,"Do you like your smartphone? Your television? Your social network? Whether or not they proudly boast of being “Designed in California,” all rely to some extent on China’s technology and manufacturing industries. And all could potentially be affected by a new campaign of economic sanctions on China announced Thursday by President Trump.

Speaking at the White House, the president said that his administration will slap tariffs on “about $60 billion” worth of goods imported from China each year. He argued the levies are necessary to compensate for how China has slurped up the fruits of American invention by nefarious means. “We have a tremendous intellectual property theft situation going on,” Trump said.

Where exactly the 25 percent levies will fall is still to be worked out, but they will focus on technologies in areas including computing and communications, industrial machinery such as factory robots, and aerospace.

The action illustrates the centrality of the technology industry and new developments such as artificial intelligence to the economic and national security strategies of the world’s superpowers. The sanctions plan also creates a headache for US companies reliant on Chinese circuit boards, servers, chips, or manufacturing plants—and potentially their customers, too.

The effects could be felt beyond hardware companies. For example, Chinese suppliers are important to the Open Compute Project that internet companies such as Facebook, Google, and Microsoft support to make servers and other data center equipment cheaper.

Silicon Valley lobbying group The Internet Association, whose members include Google, Amazon, and Facebook, said Thursday that it fears knock-on effects such as higher prices for consumers, and industry job losses. Adam Segal, who tracks Chinese technology policy at the Council on Foreign Relations, says the announcement fired the starting gun on a backroom battle over the products that will be subject to the tariffs. “There’s going to be a huge amount of lobbying now as specific sectors try to make sure they’re not caught up in the thing,” he says.

Trump’s new sanctions are informed by a 215-page report into China’s technology strategy by the Office of the United States Trade Representative. It concludes that the country’s government uses cyber espionage, investments and acquisitions in the US, and pressure on American companies operating in China to collect valuable US intellectual property. The report says that since 2010 China has made, and then broken, eight separate commitments to reform its policies on technology transfer.

Speaking alongside Trump at the White House Thursday, US Trade Representative Robert Lighthizer summed up the findings as portraying an assault on America. “Technology is really the backbone of the future of the US economy,” Lighthizer said. “China [has] a policy of forced technology transfer.”’ Lighthizer said his agency will also ask the World Trade Organization to take action against China’s technology licensing policies.

In order to operate in China, overseas companies must often transfer intellectual property, or even technology assets, to a locally owned partner. The trade representative’s report cites a recent raft of tough new rules for overseas cloud computing providers operating in China, which effectively force them to transfer their hardware and operations to local companies. Amazon made that move last November.",Yes,It can lead to addiction and isolation.,https://www.wired.com/story/trumps-latest-china-tariffs-could-hurt-techand-even-social-media/,2018-03-22 23:19:53.139000+00:00,LABEL_1_relevant,0.8711563944816589,Social Media,social_media,Wired,"Information, Discourse & Governance"
154,'Fake News Victims' Meet With Twitter and Facebook,"On paper, they would seem to have little in common. Tun Khin is a human rights activist who advocates for the persecuted Rohingya Muslims in his home country of Myanmar. Jessikka Aro is a Finnish journalist who exposed the international influence of Russian propagandists at the Internet Research Agency long before the rest of the world had ever heard of them. Lenny Pozner is an American father who lost his 6-year-old son, Noah, in the shooting at Sandy Hook Elementary in 2012. Ethan Lindenberger is almost a kid himself, a high school student who’s become a vaccination proponent despite his parents’ anti-vaccination beliefs.

But all four of them are bound by one unfortunate and common thread: They’ve all seen firsthand just how ugly---and downright dangerous---the spread of fake news and disinformation online can be. Which is why this week, they gathered in Silicon Valley to talk with tech executives about what they’ve been through and what they want tech companies to do about it. The group met with Twitter on Tuesday, and another meeting was planned at Facebook Wednesday afternoon.

The meetings, which were organized by a nonprofit advocacy group called Avaaz, come at a time of fierce debate over what responsibility tech companies have to limit the spread of toxic content on their platforms. Just last week, Facebook announced it was banning seven people, including Infowars conspiracy theorists Alex Jones and Paul Joseph Watson, under a policy that prohibits “dangerous individuals” from having any presence on Facebook. The bans prompted President Trump to lash out against tech companies over the weekend, ramping up accusations of censorship that have become a constant drumbeat on the right.

The discussions organized by Avaaz served as a counterpoint to all that pressure, as individual victims of online harassment campaigns came forward to tell tech companies exactly how they’ve been hurt by the hate and hoaxes that have festered on their platforms. “Our job as advocates is to make them stop for a minute and think about the implications of not acting fast enough,” says Oscar Soria, a senior campaigner with Avaaz.

Issie Lapowsky covers the intersection of tech, politics, and national affairs for WIRED.

During Tuesday’s meeting with Twitter, the attendees took turns telling their stories. Aro shared the details of the global smear campaign that was lodged against her, after her reporting outed the Internet Research Agency. She explained the threats that have been made against her life and read a recent direct message she received while traveling in the Czech Republic, in which a stranger threatened to “castrate” her if she ever came back to the country.

Aro says the harassment she’s received violates Finnish defamation laws, and she is in the process of pursuing cases against some of her harassers in court. And yet, she says, the complaints she’s filed to Twitter and Facebook often go unanswered, leaving local investigators to do the work the American companies won't. “I'm basically here, to put it simply, to give a user report live, because they haven't reacted to the ones that I have made online,” Aro says.

Khin described the trauma he’s seen in Rohingya refugee camps and pressed Twitter about why it continues to provide safe haven for Senior General Min Aung Hlaing, the commander-in-chief of the Myanmar military. The military was behind some of the accounts that notoriously flooded Facebook with anti-Islam rhetoric, and the United Nations called for its leaders to face genocide charges last year. Facebook has since banned Min Aung Hlaing and other accounts and pages that the UN linked to human rights abuses in the country. While the general's Twitter account hasn’t been active since last year, it remains up on the platform today.",Yes,"The spread of false information.

One of the most significant problems associated with social media is the spread of false information. This is often perpetuated by people sharing inaccurate news stories or rumors without verifying the information. As a result, people can mistakenly believe false information to be true. This can lead to serious consequences, such as making poor decisions based",https://www.wired.com/story/fake-news-victims-meet-twitter-facebook/,2019-05-08 23:15:54.246000+00:00,LABEL_1_relevant,0.9962363839149476,Social Media,social_media,Wired,"Information, Discourse & Governance"
155,Can the Wisdom of Crowds Help Fix Social Media’s Trust Issue?,"Social media misinformation outrage cycles tend to go through familiar phases. There’s the initial controversy over some misleading story that goes viral, then the platform’s response. Then someone asks “What about Fox News?” Finally, someone points out that the real problem, as far as social media is concerned, is the algorithms that determine who sees what. Those algorithms are primarily optimized for engagement, not accuracy. False and misleading stories can be more engaging than true ones, so absent some intervention by the platform, that’s what people are going to see. Fixing the algorithm, the argument goes, would be a better way to deal with the problem than taking down viral misinformation after the fact.

But fix it how? To change the ranking to favor true stories over false ones, say, the platforms would need a way to systematically judge everything that gets shared, or at least everything that gets shared a nontrivial amount. The current prevailing approach to false material involves punting the judgment to some outside party. Facebook, for example, partners with organizations like Factcheck.org to determine whether a given link merits a warning label. Twitter builds its fact-checks by linking to external sources. That could never be scaled up to the level of the algorithm. There aren’t enough professional fact checkers in the world to go over every article that might get posted on social media. Research has found that this creates an “implied truth effect”: If you only check a subset of content, some users will assume any article that isn’t labeled must therefore be accurate, even if it simply was never checked.

A new paper published in Science Advances suggests a promising solution to these issues: fact-checking by the crowd. In the study, a team of researchers led by David Rand, a professor at MIT, set out to show whether groups of random laypeople could approximate the results of professional fact checkers. Using a set of 207 articles that had been flagged for fact-checking by Facebook’s AI, they had three professional fact checkers score them on several dimensions to produce an overall score from 1 (totally false) to 7 (totally trustworthy). Then they recruited about 1,100 ordinary people from Amazon Mechanical Turk, divided them into groups equally balanced between self-identified Democrats and Republicans, and had them do the same thing, but with a twist: While the fact checkers read the entire article and did their own research to verify the claims, the laypeople only looked at the headline and first sentence of each story.

Amazingly, that was enough for the crowd to match and even exceed the fact checkers’ performance.

To measure the crowd’s performance, the researchers first measured the correlation between the scores assigned by the three fact checkers themselves. (The correlation came out to .62—high, but far from uniform agreement. When judging stories on a binary true/false scale, however, at least two out of three fact checkers agreed with each other more than 90 percent of the time.) Then they measured the correlation between the crowd-assigned scores, on the one hand, and the average of the three fact checkers’ scores, on the other. The basic idea was that the average of the professionals’ ratings represents a better benchmark of accuracy than any one fact checker alone. And so if the laypeople ratings correlated with the average fact checker score as closely as the individual fact checkers agreed with each other, it would be fair to say that the crowd was performing as well as or better than a professional. The question: How many laypeople would you need to assemble to hit that threshold?

The study found that with a group of just eight laypeople, there was no statistically significant difference between the crowd performance and a given fact checker. Once the groups got up to 22 people, they actually started significantly outperforming the fact checkers. (These numbers describe the results when the laypeople were told the source of the article. When they didn’t know the source, the crowd did slightly worse.) Perhaps most important, the lay crowds outperformed the fact checkers most dramatically for stories categorized as “political,” because those stories are where the fact checkers were most likely to disagree with each other. Political fact-checking is really hard.",Yes,"It has made it easier for people to spread false information. The study found that with a group of just eight laypeople, there was no statistically significant difference between the crowd performance and a given fact checker. Once the groups got up to 22 people, they actually started significantly outperforming the fact checkers.",https://www.wired.com/story/could-wisdom-of-crowds-help-fix-social-media-trust-problem/,2021-09-01 18:00:00+00:00,LABEL_1_relevant,0.9962142109870912,Social Media,social_media,Wired,"Information, Discourse & Governance"
156,"The Internet, Politics, and the Politics of Internet Debate","Social networking, for example, is different in Egypt than it is in China. Users in Egypt expect and do different things on social networks than do people in China—which makes perfect sense given that they live in different cultures, with different political, social, and cultural concerns. In Egypt, much social networking happens on Facebook, an American site; in China, social networking happens on local sites that are tightly controlled by the government.

China probably has a team of native speakers to do censorship—not necessarily the case in Egypt/Facebook. Such differences have profound implications on users’ freedom, ways of relating, and ability to openly express discontent, as well as on the ability of state authorities to monitor users’ actions.

Is social networking good for protesters? For dictatorships? For democracy? These are not questions we can answer in the abstract. The idea that social networking—or other technologies such as search engines, databases, Wikipedia, smartphones, sensors, Big Data, and algorithms—will have similar effects across political cultures seems delusional.

Questions about the utility of Twitter for protest and the challenges involved in containing the sprawling apparatus built by democracies require a lot of soul-searching and force a lot of uncomfortable questions, about the future of capitalism, privacy, personal data, responsibility of companies and governments, the Western obsession with the war on terror, and so forth. None of these questions will be easy to answer on their own, but they will get maddeningly difficult to answer if we also confuse ourselves with an unnecessary urge to somehow make sure that our answers cohere to some vision of the Internet as a singular network.

Given the immense technological resources available, the failure to predict the Arab Spring looks far more remarkable than the failure to predict the fall of the Soviet Union. Should we give up trying to predict anything, and just blindly hope that somehow, now that everyone has access to a smartphone and Google, things will work themselves out and democracy will eventually prevail? Well, no: this would be too irresponsible. The best we can do is to develop a better set of optical tools—the ones that would allow us to zoom in on particular practices and notice the actual bits and pieces of the many infrastructures hiding behind the Internet label—and embrace a form of epistemological modesty where, when we are asked to opine on “what does the Internet do to Subject X?”, we politely decline and stay silent. Or, if we are of a more dissenting breed, we point out the explicit danger of asking such questions.

Read the full article here.



Evgeny Morozov is a contributing editor at the New Republic and the author of The Net Delusion: The Dark Side of Internet Freedom (PublicAffairs, 2011) and To Save Everything, Click Here: The Folly of Technological Solutionism (PublicAffairs, 2013). In 2010–12 he was a visiting scholar at Stanford University and a Schwartz Fellow at the New America Foundation. In 2009–10 he was a fellow at Georgetown University and in 2008–09 was a fellow at the Open Society Foundations (where he was on the board of the Information Program between 2008 and 2012). Between 2006 and 2008 he served as Director of New Media at Transitions Online. He has written for the New York Times, the Economist, the Wall Street Journal, Financial Times, London Review of Books, Times Literary Supplement, and other publications. His monthly Slate column is syndicated in El País, Corriere della Sera, Frankfurter Allgemeine Zeitung, Folha de S. Paulo, and several other newspapers.",Yes,"It creates a false sense of intimacy and superiority among users, which in turn fuels division and conflict rather than genuine communication and understanding.",https://www.technologyreview.com/2014/12/03/170219/the-internet-politics-and-the-politics-of-internet-debate/,2014-12-03 00:00:00,LABEL_1_relevant,0.6509577035903931,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
157,FTC Reportedly Hits Facebook With Record $5 Billion Fine,"After months of negotiations, the Federal Trade Commission fined Facebook a record-setting $5 billion on Friday for privacy violations, according to multiple reports. The penalty comes after an investigation that lasted over a year, and marks the largest in the agency’s history by an order of magnitude. If approved by the Justice Department's civil division, it will also be the first substantive punishment for Facebook in the US, where the tech industry has gone largely unregulated. But Washington has taken a harsher stance toward Silicon Valley lately, and Friday’s announcement marks its most aggressive action yet to curb its privacy overreaches.

Full details of the settlement were unavailable Friday afternoon, and the FTC and Facebook both declined to comment. The Wall Street Journal first reported the news. It's unclear how long it will take for the Justice Department to review the terms. In the meantime, important questions remain unanswered, including whether the FTC has opted to hold Facebook CEO Mark Zuckerberg personally liable for the company’s privacy violations, and what sort of external oversight Facebook must submit to going forward.

The FTC opened its investigation into Facebook’s data practices last March, one week after news broke that Cambridge Analytica, a political consulting firm that worked with the Trump campaign in 2016, had improperly obtained information on tens of millions of Facebook users. The data was purchased from an academic who used a personality profiling app to collect information not just from consenting users but, thanks to Facebook’s lax privacy policies at the time, from all those users’ friends—without their knowledge. Facebook didn’t cut off that access until 2015.

But in 2011, Facebook had promised the FTC that it would not share data with third parties without users’ affirmative consent, as part of a settlement agreement over charges that the company deceived consumers about its privacy practices. It appears that the regulator has found that Facebook violated that consent decree.

""We don’t think a fine matters. We need a structural solution here."" Matt Stoller, Open Markets Institute

The reported fine far surpasses the previous largest privacy-related penalty that the FTC has levied, a $22.5 million strike against Google in 2012 over its privacy policies on the Safari browser. But even $5 billion would be a drop in the bucket for Facebook, which generated $15 billion in revenue last quarter alone. When Facebook disclosed in its Q1 earnings report that it had set aside $3 billion to $5 billion to cover the costs of the settlement, its stock price soared.

Some of Facebook’s biggest critics had previously expressed doubt that any amount of money could sufficiently punish a company of Facebook’s size. “They can issue a really big fine, which is just a parking ticket,” Matt Stoller, a fellow at the anti-monopoly think tank Open Markets Institute, recently told WIRED. “We don’t think a fine matters. We need a structural solution here.”

In a letter to the FTC in early May, Senators Richard Blumenthal (D-Connecticut) and Josh Hawley (R-Missouri) argued that the FTC should “compel sweeping changes to end the social network’s pattern of misuse and abuse of personal data.”

“Personal responsibility must be recognized from the top of the corporate board down to the product development teams,” the letter read. “If the FTC finds that any Facebook executive knowingly broke the consent order or violated the law, it must name them in any further action.” Whether the FTC did so remains one of the biggest open questions around this settlement, one made all the more intriguing after the recent disclosure of emails, again in The Wall Street Journal, that appear to indicate that Zuckerberg was aware of the company’s “questionable” privacy practices.",Yes,It can lead to addiction. Social media addiction is a real and growing problem that needs to be addressed.,https://www.wired.com/story/facebook-ftc-fine-five-billion/,2019-07-12 20:44:25.575000+00:00,LABEL_1_relevant,0.9964160919189452,Social Media,social_media,Wired,Health & Wellbeing
158,Gab's CTO Introduced a Critical Vulnerability to the Site,"Over the weekend, word emerged that a hacker breached far-right social media website Gab and downloaded 70 gigabytes of data by exploiting a garden-variety security flaw known as an SQL injection. A quick review of Gab’s open source code shows that the critical vulnerability—or at least one very much like it—was introduced by the company’s chief technology officer.

ARS TECHNICA This story originally appeared on Ars Technica, a trusted source for technology news, tech policy analysis, reviews, and more. Ars is owned by WIRED's parent company, Condé Nast.

The change, which in the parlance of software development is known as a “git commit,” was made sometime in February from the account of Fosco Marotto, a former Facebook software engineer who in November became Gab’s CTO. On Monday, Gab removed the git commit from its website. An image on a site that provides saved commit snapshots shows the February software change.

The commit shows a software developer using the name Fosco Marotto introducing precisely the type of rookie mistake that could lead to the kind of breach reported this weekend. Specifically, line 23 strips the code of “reject” and “filter,” which are API functions that implement a programming idiom that protects against SQL injection attacks.

This idiom allows programmers to compose an SQL query in a safe way that “sanitizes” the inputs that website visitors enter into search boxes and other web fields to ensure that any malicious commands are stripped out before the text is passed to backend servers. In their place, the developer added a call to the Rails function that contains the “find_by_sql” method, which accepts unsanitized inputs directly in a query string. Rails is a widely used website development toolkit.

“Sadly Rails documentation doesn't warn you about this pitfall, but if you know anything at all about using SQL databases in web applications, you'd have heard of SQL injection, and it's not hard to come across warnings that find_by_sql method is not safe,” Dmitry Borodaenko, a former production engineer at Facebook who brought the commit to my attention wrote in an email. “It is not 100% confirmed that this is the vulnerability that was used in the Gab data breach, but it definitely could have been, and this code change is reverted in the most recent commit that was present in their GitLab repository before they took it offline.”

Ironically, Fosco in 2012 warned fellow programmers to use parameterized queries to prevent SQL injection vulnerabilities. Marotto didn’t respond to an email seeking comment for this post. Attempts to contact Gab directly didn't succeed.

Besides the commit raising questions about Gab’s process for developing secure code, the social media site is also facing criticism for removing the commits from its website. Critics say the move violates terms of the Affero General Public License, which governs Gab’s reuse of Mastodon, an open source software package for hosting social networking platforms.

Critics say the removal violates terms that require forked source code be directly linked from the site. The requirements are intended to provide transparency and to allow other open source developers to benefit from the work of their peers at Gab.

Gab had long provided commits at https://code.gab.com/. Then, on Monday, the site suddenly removed all commits—including the ones that created and then fixed the critical SQL injection vulnerability. In their place, Gab provided source code in the form of a Zip archive file that was protected by the password “JesusChristIsKingTrumpWonTheElection” (minus the quotation marks).",Yes,"The way it spreads information. Social media, by its very nature, is designed to share information quickly and easily. This can be a good thing, but it also has some negative consequences. One of these is the way that false information can spread quickly and easily through social media. This can lead to people believing things that are not true, and it",https://www.wired.com/story/gab-cto-critical-vulnerability/,2021-03-03 19:30:00+00:00,LABEL_1_relevant,0.9960177540779114,Social Media,social_media,Wired,"Information, Discourse & Governance"
159,Australia's Standoff Against Google and Facebook Worked—Sort Of,"Over Zoom, Australia’s communications minister, Paul Fletcher, has the air of a man in the middle of a victory speech. He credits his team and the country’s competition regulator for succeeding where others had failed: forcing tech giants to pay for news. “There were a lot of people saying you can't really succeed in taking on the global digital giants,” he says, sitting beneath strip lighting in his Sydney constituency office. But Fletcher and Australia’s federal treasurer, Josh Frydenberg, persevered. In 2020, when the Australian government asked the competition regulator to develop a law that would force tech giants to pay for the news that appears on their feeds, Fletcher was aware of the stories others used as warnings. When Germany’s biggest news publisher, Axel Springer, tried to block Google from running snippets of its articles in 2014, it backtracked after just two weeks once traffic plunged. When Spain tried to force Google to pay for news in 2014, the search giant just left—blocking Google News in the country for seven years.

Google threatened Australia with even more drastic action. In January 2021, the tech giant suggested Australians could lose access to its entire search engine if Fletcher and Frydenberg’s “news media bargaining code,” which would force platforms to pay news publishers for links, came into force. Facebook also lobbied hard against the code, arguing that news makes up less than 4 percent of the content people see in their news feed. On February 17, Australians woke up to discover that all news links had been wiped off the platform, leaving the Facebook pages of the country’s biggest media companies completely blank. Traffic to news websites sank 13 percent, illustrating exactly what the government said it was worried about. Facebook’s actions “confirm for all Australians [the] immense market power of these media digital giants,” Frydenberg said at the time.

Still, the government did not back down. According to Fletcher, the code was Australia’s answer to a problem that was first and foremost about competition. The argument was simple—Australia’s news industry should be compensated for helping Google and Facebook attract eyeballs. “What we're trying to do is replicate the ordinary commercial dealings that would occur in a market where there wasn't a huge imbalance of bargaining power,” he says.

But others suspect the code was really an attempt to subsidize the media industry, which was suffering from intense online competition for advertising. Out of every 100 Australian dollars spent on advertising in 2019, AD$53 ($38) went to Google, AD$28 to Facebook, and AD$19 to all other websites including media outlets, according to Australia’s competition watchdog. If this was the reason for the code, Bloomberg editors described it as a misdiagnosis in an op-ed. “Journalism’s business model wasn’t broken by digital platforms,” they said, “[the internet] offered consumers a wealth of free news and opinion and gave advertisers options and audiences that traditional publishers haven’t been able to match.”

Australians experienced this standoff through their Facebook feeds. For eight days, the site featured no news. Then, at 1 am on February 26, 2021, news content started to reappear, reversing users’ feeds to how they always looked. But behind the scenes, tech’s relationship with the media had permanently shifted.

Google and Facebook did not leave; they paid up, striking deals with news organizations to pay for the content they display on their sites for the first time. The code was formally approved on March 2, 2021, writing into law that tech platforms had to negotiate a price to pay news publishers for their content. If they didn’t, an arbiter would step in not only to force the platforms to pay but also to set the price. One year after the media code was introduced, Google has 19 content deals with news organizations and Facebook has 11, according to Fletcher.",Yes,"It has given a voice to everyone, regardless of qualifications or experience. This has led to the spread of misinformation, which can have harmful consequences.",https://www.wired.com/story/australia-media-code-facebook-google/,2022-02-25 12:00:00+00:00,LABEL_1_relevant,0.9940690994262696,Social Media,social_media,Wired,"Information, Discourse & Governance"
160,Foreign Trolls Are Targeting Veterans on Facebook,"I first came across the imposter Facebook page by accident. The page was made to look like that of my employer, Vietnam Veterans of America, complete with our organization's registered trademark and name. As an Iraq veteran and the office’s designated millennial policy guy, I was helping run VVA's social media accounts. The discovery kicked off what would become a 15-month-long amateur investigation into digital trolls in Bulgaria, the Philippines, and 27 other countries—all running Facebook pages targeting American troops and veterans with political propaganda.

WIRED OPINION ABOUT Kristofer Goldsmith served in the United States Army and was deployed in support of Operation Iraqi Freedom in 2005. He is the founder of High Ground Veterans Advocacy and the assistant director for Policy and Government Affairs at Vietnam Veterans of America.

Last year, an Oxford study revealed that military veterans are ripe targets for exploitation by foreign powers seeking to undermine American democracy. The report concluded that veterans are more likely than the average person to be community leaders and that their political opinions have significant influence on those around them. Recognizing this, foreign powers have sought to infiltrate our community, impersonating individuals and organizations with tens of thousands of members in an effort to gain veterans' trust.

At first, what I found on the imposter Vietnam Vets account didn’t make sense. The Facebook page had recycled old news stories about the Department of Veterans Affairs and veterans’ benefits, as well as a post about a “Vietnam Veterans song of the day.” Even though the latter didn’t have any audio attached, it was nonetheless shared by followers hundreds of times each day.

My employer reported the imposter, and the page's administrators quickly scrubbed it of any trace of our logo to avoid banishment from Facebook. Digging deeper, we realized that the news the page was sharing was scraped from legitimate military and veteran-focused newspapers, but that the stories' dates and content were altered to provoke emotional responses—specifically outrage.

The fake page's most viral video was a looping, 58-second local media story depicting what looked like berries smeared on a Vietnam Veterans’ monument. However, now it ran in a post that read “EXCLUSIVE: Vietnam Veterans Monument Vandalized… Share and Vote!” Text superimposed over the video prompted “Do you think the criminals must suffer?” More sinister still, the trolls had figured out how to game Facebook's algorithms into thinking that the video was a live feed. As a result, Facebook treated the looped video as if it were important breaking news, pushing it into the newsfeeds of tens of thousands of Americans.

It took three months and a handful of press releases before Facebook shut down the fraudulent page. The company's mostly automated reporting features found that even the fake live video hadn’t violated community standards.

Five months after the imposter page was shut down, we discovered two more Facebook pages that were sharing the same content and linking to new websites. One appeared to be a page that had been dormant since 2015, a year before Russia’s election interference reportedly began. Because its creator had forgotten to register the affiliated website anonymously, we were able to identify one of the trolls by name and location: Plovdiv, Bulgaria.

It’s unclear whether this particular troll was financially motivated or part of a network of troll farms in Eastern Europe that are targeting American democracy. Whatever the motivation, the effect was the same. Changing the dates on old stories about Congress making cuts to veterans’ benefits can spread panic, anger, and confusion throughout the community and influence political beliefs and voting behavior.

This troll’s persistence sparked further investigation. We eventually found scores of American-veteran-focused Facebook pages producing politically polarizing content from outside the United States.

Vietnam Veterans of America produced a report on our earliest findings for 11 committees in Congress and a host of alphabet agencies. It’s important for Congress and federal agencies to investigate these foreign entities to find out what damage has been done. But we're also calling on the Department of Veterans Affairs take a more proactive role in inoculating veterans against this type of threat to prevent future harm.",Yes,"It gives foreign powers a direct line to American veterans. Not only does this threaten our democracy and national security, but it’s also putting the lives of our service members and veterans at risk.

The VA must do more to educate veterans about how to identify and avoid foreign influence. The agency should also consider creating a network of social",https://www.wired.com/story/trolls-are-targeting-vets-on-facebook/,2018-12-04 14:00:00+00:00,LABEL_1_relevant,0.9965234398841858,Social Media,social_media,Wired,"Information, Discourse & Governance"
161,Facebook and Instagram will now allow users to hide 'Like' counts on posts,"Facebook this week will begin to publicly roll out the option to hide Likes on posts across both Facebook and Instagram, following earlier tests beginning in 2019. The project, which puts the decision about Likes in the hands of the company’s global user base, had been in development for years, but was deprioritized due to the COVID-19 pandemic and the response work required on Facebook’s part, the company says.

Originally, the idea to hide Like counts on Facebook’s social networks was focused on depressurizing the experience for users. Often, users faced anxiety and embarrassment around their posts if they didn’t receive enough Likes to be considered “popular.” This problem was particularly difficult for younger users who highly value what peers think of them — so much so that they would take down posts that didn’t receive enough Likes.

Like-chasing on Instagram, especially, also helped create an environment where people posted to gain clout and notoriety, which can be a less authentic experience. On Facebook, gaining Likes or other forms of engagement could also be associated with posting polarizing content that required a reaction.

As a result of this pressure to perform, some users grew hungry for a “Like-free” safer space, where they could engage with friends or the wider public without trying to earn these popularity points. That, in turn, gave rise to a new crop of social networking and photo-sharing apps such as Minutiae, Vero, Dayflash, Oggl and, now, newcomers like Dispo and newly viral Poparazzi.

Though Facebook and Instagram could have chosen to remove Likes entirely and take its social networks in a new direction, the company soon found that the metric was too deeply integrated into the product experience to be fully removed. One key issue was how the influencer community today trades on Likes as a form of currency that allows them to exchange their online popularity for brand deals and job opportunities. Removing Likes, then, is not necessarily an option for these users.

Instagram realized that if it made a decision for its users, it would anger one side or the other — even if the move in either direction didn’t really impact other core metrics, like app usage.

“How many likes [users] got, or other people got — it turned out that it didn’t actually change nearly as much about the experience, in terms of how people felt or how much they use the experience, as we thought it would. But it did end up being pretty polarizing,” admitted Instagram head, Adam Mosseri. “Some people really liked it and some people really didn’t.”

“For those who liked it, it was mostly what we had hoped — which is that it depressurized the experience. And, for those who didn’t, they used Likes to get a sense for what was trending or was relevant on Instagram and on Facebook. And they were just super annoyed that we took it away,” he added. This latter group sometimes included smaller creators still working on establishing a presence across social media, though larger influencers were sometimes in favor of Like removals. (Mosseri name-checked Katy Perry as being pro Like removals, in fact.)

Ultimately, the company decided to split the difference. Instead of making a hard choice about the future of its online communities, it’s rolling out the “no Likes” option as a user-controlled setting on both platforms.

On Instagram, both content consumers and content producers can turn on or off Like and View counts on posts — which means you can choose to not see these metrics when scrolling your own Feed and you can choose whether to allow Likes to be viewed by others when you’re posting. These are configured as two different settings, which provides for more flexibility and control.

On Facebook, meanwhile, users access the new setting from the “Settings & Privacy” area under News Feed Settings (or News Feed Preferences on desktop). From here, you’ll find an option to “Hide number of reactions” to turn this setting off for both your own posts and for posts from others in News Feed, groups and Pages.

The feature will be made available to both public and private profiles, Facebook tells us, and will include posts you’ve published previously.

Instagram last month restarted its tests on this feature in order to work out any final bugs before making the new settings live for global users, and said a Facebook test would come soon. But it’s now forging ahead with making the feature available publicly. When asked why such a short test, Instagram told TechCrunch it had been testing various iterations on this experience since 2019, so it felt it had enough data to proceed with a global launch.

Mosseri also pushed back at the idea that a decision on Likes would have majorly impacted the network. While removal of Likes on Instagram had some impact on user behavior, he said, it was not enough to be concerning. In some groups, users posted more — signaling that they felt less pressure to perform, perhaps. But others engaged less, Mosseri said.

“Often people say, ‘oh, this has a bunch of Likes. I’m gonna go check it out,’ ” the exec explained. “Then they read the comments, or go deeper, or swipe to the carousel. There’s been some small effects — some positive, some negative — but they’ve all been small,” he noted. Instagram also believes users may toggle on and off the feature at various times, based on how they’re feeling.

In addition, Mosseri pointed out, “there’s no rigorous research that suggests Likes are bad for people’s well-being” — a statement that pushes back over the growing concerns that a gamified social media space is bad for users’ mental health. Instead, he argued that Instagram is still a small part of people’s day, so how Likes function doesn’t affect people’s overall well-being.

“As big as we are, we have to be careful not to overestimate our influence,” Mosseri said.

He also dismissed some of the current research pointing to negative impacts of social media use as being overly reliant on methodologies that ask users to self-report their use, rather than measure it directly.

In other words, this is not a company that feels motivated to remove Likes entirely due to the negative mental health outcomes attributed to its popularity metrics.

It’s worth mentioning that another factor that could have come into play here is Instagram’s plan to make a version of its app available to children under the age of 13, as competitor TikTok did following its FTC settlement. In that case, hiding Likes by default — or perhaps adding a parental control option — would necessitate such a setting. Instagram tells TechCrunch that, while it’s too soon to know what it would do with a kids app, it will “definitely explore” a no Likes by default option.

Facebook and Instagram both told TechCrunch the feature will roll out starting on Wednesday but will reach global users over time. On Instagram, that may take a matter of days.

Facebook, meanwhile, says a small percentage of users will have the feature Wednesday — notified through an alert on News Feed — but it will reach Facebook’s global audience “over the next few weeks.”",Yes.,It makes people feel bad about themselves. A lot of people think that the solution to this problem is to hide the number of likes that a post gets. Facebook is now testing this solution.,https://techcrunch.com/2021/05/26/facebook-and-instagram-will-now-allow-users-to-hide-like-counts-on-posts/,2021-05-26 00:00:00,LABEL_1_relevant,0.9959622025489808,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
162,OnlyFans’ Explicit Content Ban Betrays Its Creators,"OnlyFans creators are getting screwed—and not in the way many subscribers are used to seeing. This week, the adult fan site, which rose to prominence last year and was even name-checked by Beyoncé on Megan Thee Stallion’s Grammy-winning “Savage Remix,” announced that it was further distancing itself from the business that made it an in-demand enterprise: hardcore porn.

The sharp pivot comes on the eve of the company’s five-year anniversary and the introduction of a new app, OFTV, in what is a clear attempt to become more of a general-purpose marketplace for creators.

In a statement released Thursday, the company said that, starting October 1, it would forbid the “posting of any content containing sexually explicit conduct” that doesn’t adhere to its Acceptable Use Policy. Details about the ban are still forthcoming, but OnlyFans representatives did confirm that nudity would be allowed as long as it abided by the company’s updated guidelines. The changes are, in part, to appease its “banking partners” as well as to “ensure the long-term sustainability of the platform,” the company said, and seem to be in line with its recent efforts to widen the composition of its creator pool beyond sex workers.

What remains unclear in the wake of OnlyFans’ announcement is whether those guidelines could become even more restrictive as the company’s financial interests evolve. A BBC report confirmed that the company decided to update its guidelines in part because of Mastercard’s decision to implement new rules for adult content sites that use their payment processing system. Last year, both Mastercard and Visa terminated their relationship with Pornhub. Mastercard’s new policies are meant to make sure adult sites have controls in place to monitor and remove illegal content, but OnlyFans’ move to ban nearly all explicit material left many worried about the fate of sex workers on the platform.

Across the internet, the surprise announcement was met with a mix of cynicism, support, and humor. Creators say they are skeptical of the changes, though hardly surprised. “They used us,” says Xavier Blanco. “They exploited our connections, only to toss us out.” Blanco, who is 34 and lives in New York City, joined the platform in the summer of 2019 and has made a considerable living through the site, where he uploads all manner of gay erotica. “They are laying off countless individuals without caring about their livelihood.”

In December 2019 when I spoke with Dominic Ford, the founder of JustFor.Fans, a rival adult fansite, he predicted such a future. “As has been the trend, sex-positive imagery of any kind is slowly being washed away from social media,” he said at the time, pointing to websites such as Tumblr, Craigslist, and Patreon cranking up restrictions on pornographic content.

Still, the impact of OnlyFans—and the site’s abrupt change in course—cannot be understated. Since its founding in 2016, the site has slowly ushered in a new frontier for adult entertainment. In what was once a hostile, and sometimes dangerous, landscape for sex workers, OnlyFans became a lucrative alternative to conventional services, such as camming, escorting, and studio porn. Conceptually, what made OnlyFans feel like such a radical leap forward was how it melded a growing need for personalized porn with our obsession with influencer culture. Eventually, Big Tech followed suit. Twitter, YouTube, and other major platforms have since developed tools that allow users to monetize their unique skills.",Yes,The way it has desensitized us to the point of making it difficult to have genuine connections with others.,https://www.wired.com/story/onlyfans-betrays-creators/,2021-08-20 19:25:42.143000+00:00,LABEL_1_relevant,0.9963956475257874,Social Media,social_media,Wired,"Information, Discourse & Governance"
163,Facebook suspends the Trump campaign’s data-mining firm amid revelations of a major data breach,"Facebook has blocked Cambridge Analytica, which provided data to the Trump election campaign

The news: In a blog post late on March 16, Facebook’s deputy general counsel said Strategic Communications Laboratories (SCL) and its political data arm, Cambridge Analytica, had their access to the platform suspended for violating data use policies.

The details: According to Facebook, Aleksander Kogan, a psychology professor at Cambridge University in the UK, had collected various kinds of personal data using an app on the social network. Billed as a research vehicle for psychologists, the app, downloaded by approximately 270,000 people, asked them to share things such as content they had liked and the city they’d listed on their Facebook profile.

The social network says that Kogan passed the information to SCL/Cambridge Analytica and Christopher Wylie of Eunoia Technologies in violation of its rules. When it discovered this in 2015, it removed the app and asked Kogan, SCL/Cambridge Analytica, and Wylie to certify they had destroyed the data collected. All three parties said they had done so.

But according to its post, Facebook recently received reports that not all the information had been deleted. The social network says it’s looking into these claims and is suspending Kogan, Wylie, and SCL/Cambridge Analytica pending further information.

But wait, there’s more: In a related story on March 17, the New York Times reported that Cambridge Analytica collected private information from more than 50 million Facebook profiles without permission, in what it described as “one of the largest data leaks in the social network’s history.” The story cites documents and former employees, including on-the-record statements from Wylie.

Why this matters: The use of social media to target political messages during the 2016 election has caused plenty of controversy. Facebook’s move will be seen as a sign it’s taking seriously accusations that it was used to manipulate public opinion, but it also raises the question of whether other data was siphoned off without the network’s knowledge.",Yes,"It can be used to manipulate public opinion. This is because social media platforms like Facebook collect a lot of data about their users, which can then be used to target them with personalized messages. In the case of Cambridge Analytica, this data was collected without users' knowledge or consent, and was then used to support the Trump election campaign.",https://www.technologyreview.com/2018/03/17/144608/facebook-suspends-the-trump-campaigns-data-mining-firm-amid-revelations-of-a/,2018-03-17 00:00:00,LABEL_1_relevant,0.996518850326538,Social Media,social_media,MIT Tech Review,Politics
164,They called it a conspiracy theory. But Alina Chan tweeted life into the idea that the virus came from a lab.,"The obvious problem with the lab-leak theory, though, is that there remains no concrete evidence for it. Chan has no particular view about how exactly an accident might have happened—whether a student got sick in a bat cave, say, or secret research to infect mice with a novel virus went awry. After reading Chan’s posts, I noticed that many of her claims don’t even relate to direct evidence at all; more often, they revolve around its absence. She tends to point out things that Chinese researchers didn’t do or say, important facts they did not quickly reveal, the infected market animal they never found, or a database that’s no longer online. She’s plainly suggesting there is a cover-up—and, therefore, a plot to conceal the truth.

Pre-adapted

Last February, when leading scientists convened to analyze the virus genome, they ended up publishing two letters. One, in The Lancet, dismissed the lab-accident possibility outright as a “conspiracy theory” (its authors included a scientist who funded research at the Wuhan lab). The other was the “Proximal Origins” letter in Nature Medicine, coauthored by Kristian Andersen, an evolutionary biologist at the Scripps Research Institute in La Jolla, California. Andersen and his coauthors looked at the genome of the virus and marshaled arguments for why it was very likely a natural occurrence—backed by evidence that it was similar to others found in nature.

The 30,000 genetic letters in that genome remain the most widely studied clue to the virus’s origin. Coronaviruses frequently swap parts—a phenomenon called recombination. Andersen found that all the components of the virus had been seen before in samples collected over the years from animals. Evolution could have produced it, he believed. The Wuhan Institute had been genetically engineering bat viruses for scientific experiments, but the SARS-CoV-2 genome did not match any of the favorite “chassis” viruses used in those experiments, and it did not contain any other obvious sign of engineering.

According to Clarivate, an analytics company, the Nature Medicine letter was the 55th most cited article of 2020, with over 1,300 citations in the journals tracked. Email records would later show that starting in January 2020, the letter had been the subject of urgent, high-level messages and conference calls between the letters’ authors, Anthony Fauci, head of the National Institute of Allergy and Infectious Diseases; top virologists; and the head of the Wellcome Trust, a major pharmaceutical research funding organization in the United Kingdom. Early on, the authors had worried that the virus looked suspicious before quickly coming together around a scientific analysis supporting a natural cause. Initially one of their aims was to quash rumors that the virus was a bioweapon or a result of engineering gone wrong, but they ended up going further, writing: “We do not believe that any type of laboratory-based scenario is plausible.”

Working from her home in Massachusetts, Chan soon found a way to revive the lab-accident theory by looking for differences with SARS, a similar virus that broke out in 2002 but caused only about 8,000 illnesses. With Shing Zhan, a bioinformatics specialist at the University of British Columbia, Chan looked at the early human cases of covid and saw that the new virus hadn’t mutated as fast as SARS had. If it were an animal virus from a market, she thought, its genome would show signs of adjusting more quickly to fit its brand-new human host. She prepared an analysis arguing that the virus was “pre-adapted” to humans and offered some theories as to why. Maybe it had been spreading undetected in people elsewhere in China. Or maybe, she thought, it had been growing in a lab somewhere, perhaps multiplying in human cells or in transgenic mice that had had human genes spliced into them.

The chance that a non-engineered virus could have “adapted to humans while being studied in a laboratory,” she wrote, “should be considered, regardless of how likely or unlikely.”

On May 2, 2020, Chan posted a preprint paper, coauthored with Deverman and Zhan, to the website bioRxiv, an online venue for quickly communicating results that haven’t yet been reviewed by other scientists. “Our observations suggest that by the time SARS-CoV-2 was first detected in late 2019, it was already pre-adapted to human transmission,” they wrote. The Broad Institute communications department also pointed Chan to examples of how to compose a “tweetorial,” a daisy chain of posts, with pictures, that present a compact scientific argument to a wider public. She posted her first tweetorial the following day.

For journalists suspicious about China’s handling of the virus, the thread—and those that followed—were dynamite. Here was an actual scientist at America’s biggest gene center who was explaining why the official story might be wrong. “Coronavirus did NOT come from animals in Wuhan market,” screamed a Mail on Sunday headline, in what became Chan’s first breakout into the public conversation.

While her report was a media success, what the Daily Mail described as Chan’s “landmark paper” has still never been formally accepted by a scientific journal. Chan says that’s because of censorship due to her raising the lab-origin possibility. Eisen of UC Davis, however, thinks Chan’s expectations for how the covid-19 virus should have behaved remain conjecture. He doesn’t think we’ve traced enough outbreaks in enough molecular detail to really know what's normal. And, he notes, covid-19 has continued to change and adapt.

“My colleagues said, This is a conspiracy—don’t bother. I said, No, I am going to treat this like any other paper,” says Eisen, who took time to study the manuscript. “I think it’s interesting what she tried to do, but I am not convinced by the conclusion, and I think the inferences were wrong. I do commend her for posting it. Many of the people pushing the lab-origin theory are not making claims based on logic, but she presented her evidence. I don’t agree with it, but that is science.”

Wrong or right, though, the word Chan used—“pre-adapted”—sent shivers up the spine of people like author Nicholson Baker. “We were dealing with a disease that was exceptionally good, right out of the gate, at chewing up human airways,” says Baker, who got in touch with Chan to learn more. Several months later, in January of this year, Baker would publish a lengthy report in New York magazine saying he’d become convinced a laboratory accident was to blame. He cited a variety of sources, including Chan.

Pangolin problem

Chan wasn’t done knocking holes in the natural-origins narrative. She next took on four papers that had been rapidly published early in 2020, two of them in Nature, describing viruses in pangolins—endangered scale-covered mammals sometimes eaten as delicacies in China—that shared similarities to SARS-CoV-2. If researchers could find all the components of the pandemic virus, especially in wild animals illegally trafficked as food, they could cinch the case for a spillover from nature, given the way coronaviruses swap parts. The pangolin papers, published in quick succession in early 2020, were a promising start. To the authors of “Proximal Origins,” these similar viruses offered “strong” and “parsimonious” evidence for natural emergence.

Chan and Zhan noticed that all the papers described the same batch of animals—even though some failed to acknowledge the overlap. One even relabeled the data, which made it appear novel. To Chan, that wasn’t just sloppy work or scientific misconduct. There could, she believed, have been “coordination” between the overlapping authors of all these papers, some of whom had published together before. She created the hashtag #pangolinpapers—calling to mind the Panama Papers, documents that exposed secret offshore financial dealings.

Maybe, she thought, researchers were now laundering data to make it seem that nature was swimming with similar viruses.

Chan started emailing authors and journals to get the raw data she needed to more fully analyze what they had done. Making such data available is usually a condition of publication, but it can still be hard to obtain. After what she calls months of stonewalling, Chan finally lost her cool and blasted an accusation out from her browser. “I need the scientists + editors who are directly or indirectly covering up severe research integrity issues surrounding some of the key SARS-2-like viruses to stop and think for a bit,” she posted to Twitter. “If your actions obscure SARS2 origins, you're playing a hand in the death of millions of people.”

Eddie Holmes, a prominent Australian virologist and coauthor of one of those papers (as well as “Proximal Origins”), called the tweet “one of most despicable things I read on the origins issue.” He felt accused, but he wondered what he was being accused of, since his paper had correctly accounted for its pangolin data sources. Holmes then circulated an intricate time line prepared by Chan of the publication dates and past connections between the authors. The chart’s dense web of arrows and connections bore an unmistakable resemblance to an obsessive’s cork board covered with red string and thumbtacks.",Yes,"It can connect people to others who hold similar beliefs and enter them into a 'echo chamber'.

The creation of echo chambers has contributed to the spread of conspiracy theories and the reinforcement of false beliefs.",https://www.technologyreview.com/2021/06/25/1027140/lab-leak-alina-chan/,2021-06-25 00:00:00,LABEL_1_relevant,0.9962835907936096,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
165,Lina Khan’s Theory of the Facebook Antitrust Case Takes Shape,"When federal judge James Boasberg dismissed the Federal Trade Commission’s antitrust lawsuit against Facebook in June, he gave the agency pretty specific instructions on how to salvage it. The problem, he wrote in his opinion, was that the FTC hadn’t offered even the barest evidence that Facebook is a monopoly, beyond the vague claim that it “maintained a dominant share of the US personal social networking market (in excess of 60 percent).” As Boasberg noted, that inexplicably left some basic questions unanswered, such as: 60 percent of what? Who makes up the leftover 40 percent? It was a bit like accusing a driver of speeding without even mentioning the speed limit.

To get back into court and advance to the next stage of litigation, the FTC would have to come back with something a lot more specific. That presented an interesting early assignment for Lina Khan, who was confirmed as commissioner of the agency a mere two weeks before Boasberg issued his ruling. (Facebook has sought to have Khan recused from the case on the basis of her public criticism of big tech companies before her current job, though experts see little chance of that succeeding.)

On Thursday, the FTC filed its revised complaint answering those previously unanswered questions. While it’s impossible to predict how a given judge will rule, the new material seems likely to satisfy Boasberg and keep the case alive. “To my eye, they’ve scratched Boasberg’s itch,” said Paul Swanson, an antitrust attorney in Denver. Facebook, he said, may not be able to avoid “a long slog of document productions and depositions.”

To prove that Facebook is a monopoly for legal purposes, the FTC doesn't have to show that it’s literally the only social network. They have to show that it has “market power.” In a nutshell, having market power means you face so little competition that you can do things your customers don’t like without losing any business. It’s one of the main reasons antitrust law exists: When there isn’t enough competition, companies will stop trying to please their customers and start trying to squeeze them. Think about how frustrating it is when your internet provider raises prices and you realize no one else serves your neighborhood. That’s market power.

There are two ways to show market power: indirect evidence and direct evidence. Indirect evidence usually refers to dominant market share. (That might sound counterintuitive, but the reason it’s indirect is because being big on its own doesn’t prove a company is doing anything wrong—it just raises the strong possibility.) In its initial complaint, the FTC only offered indirect evidence, and very little of it: that feeble 60 percent statistic, which Boasberg ruled was inadequate. The revised complaint, on the other hand, goes into great detail on market share. Drawing on data from the analytics company Comscore—which, the complaint notes, Facebook itself relies on—the FTC argues that just about any way you slice it, Facebook controls a dominant chunk of the market for “personal social networking services.” According to the Comscore data, Facebook has accounted for more than 80 percent of time spent since 2011, at least 70 percent of daily active users, and at least 65 percent of monthly active users.

The new complaint also tightens up the FTC’s definition of the market itself, which is another crucial part of any monopolization case. You can’t prove a company has market power without explaining which market they have power in. According to the agency, the market for personal social networking services has three key attributes: First, a network has to be “built on a social graph that maps the connections between users and their friends, family, and other personal connections.” Second, it has to have features for users to interact with each other in a “shared social space,” like a news feed or group. Third, it has to allow users to look each other up. (Think about how you can search for someone by name on Facebook, but not in iMessage.)",Yes,It's desensitized us to reading long articles.,https://www.wired.com/story/lina-khan-theory-facebook-antitrust-case-takes-shape/,2021-08-19 23:10:02.759000+00:00,LABEL_1_relevant,0.9961438179016112,Social Media,social_media,Wired,"Information, Discourse & Governance"
166,The Top Political Advertiser on Facebook Is ... Facebook,"On Tuesday, Facebook released a new tool that shows who's spending the most money on political ads on the platform in the US. At a glance, the Ad Archive Report suggests that Texas senate candidate Beto O'Rourke is the biggest spender, having plowed more than $5 million into Facebook ads since May. But the fine print reveals a more surprising finding: The advertiser spending the most on political and issue ads on Facebook is, well, Facebook.

Since May, Facebook has spent the equivalent of over $12 million on political and issues ads across Facebook and Instagram. That's about as much as the next four top advertisers, including O'Rourke and President Trump's super PAC, spent combined. The ads Facebook ran promoted the company's efforts to secure elections. They also encouraged Instagram users to vote.

It should come as no surprise that Facebook would be the biggest power user of its own technology. But the size of the ad campaign, which according to Facebook’s ad archive only began in August, reflects the company’s eagerness to restore its reputation. Since the 2016 election, Facebook has been blamed for the spread of fake news, for allowing its users' data to be misappropriated for political means, and for being blind to the ways its platform could be misused by foreign actors. Facebook has since launched television ads and wallpapered cities with posters warning against the dangers of fake friends and vowing to protect user privacy. Facebook's own Facebook ads are, it seems, a core part of that strategy.

But arguably the most important part of Facebook's reputational turnaround process is not what the company says about itself, but what it does. In May, Facebook released its political advertising archive for the first time, keeping a promise to introduce more transparency to digital political ads. The tool was a start, but it proved difficult to navigate. There was no way to see who was spending the most money on Facebook, how advertisers stacked up against each other, or how much they were spending on Facebook overall.

The new Ad Archive Report tool makes that possible. It ranks every political advertiser on Facebook by ad spend, with links to the ads themselves, as well as information on how many ads they've run. It's not a perfect system; Facebook has been caught automatically labeling ads for Bush's Beans as political, simply because they happen to include the name of two former presidents. Other clearly political ads, including some run by Senator Kamala Harris to oppose the nomination of Justice Brett Kavanaugh, have not always been flagged. Still, the Ad Archive Report provides a window into how campaigns and interest groups are spending their money on Facebook compared to one another---and compared to other mediums.

It reveals, for instance, that not only is the O'Rourke campaign outspending every other congressional campaign by millions of dollars, but that President Trump's campaign and super PAC are still as active as ever on Facebook. While the Trump Make America Great Again Committee ranks just after O'Rourke, the president's campaign comes in at number nine. In between are some of the usual suspects. Billionaire Tom Steyer's Need to Impeach group is among the top ten spenders, while ExxonMobil has spent big in Colorado, fighting against Proposition 112, which would limit the location of new oil and gas projects.

According to the report, since May, advertisers have spent more than $256 million on nearly 1.7 million political and issue ads on Facebook. That's still small compared with the $8.8 billion that's expected to be spent on political advertising this year. Nevertheless, given the sheer volume of ads on Facebook, a tool like this offers valuable insights into who's dominating the political conversation on the world's biggest social media platform---even if the answer is Facebook itself.

More Great WIRED Stories",Yes,"It can be used to spread misinformation and cause division.

When Facebook released its political advertising archive for the first time, it showed that the company was spending the most on political and issue ads.

The new Ad Archive Report tool from Facebook shows that the O'Rourke campaign is outspending every other congressional campaign by millions of",https://www.wired.com/story/top-political-advertiser-on-facebook-is-facebook/,2018-10-23 19:55:20.649000+00:00,LABEL_1_relevant,0.9948062300682068,Social Media,social_media,Wired,Politics
167,The Internet Has Changed How We Deal with Death,"If anyone doubts how important the online world has become in our grieving process, they have only to look to the biggest social media platform. Facebook has long allowed people’s profiles to be memorialized when they die, and as of last year, users can designate a legacy contact—someone who can assume limited control of the deceased’s account and continue maintaining it. Even the law is beginning to recognize how important our digital existence is to our loved ones after we shuffle off. Startups allow people to create a “digital will” that only releases access to social media accounts and digital assets to a designated executor.

As Garber points out, people were similarly accused of shedding crocodile tears over the death of Princess Diana. And such “grief policing” isn’t likely to go away. But neither is public, online grieving—in fact, it has already become part of the ritual of death.

(Source: The Atlantic, New Scientist)",Yes,"It has given people a platform to publicly grieve, which has led to a lot of ""grief policing"" from people who think that this grieving is not genuine.",https://www.technologyreview.com/2016/01/21/163779/the-internet-has-changed-how-we-deal-with-death/,2016-01-21 00:00:00,LABEL_1_relevant,0.992590308189392,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
168,Americans Are Split On Online Dating—but Swipe More Than Ever,"In 1965, two Harvard students hacked together a computerized matchmaking program—a punch-card survey about a person and their ideal match, recorded by the computer, then crunched for compatibility—and the world’s first dating site was born. Over the next half-century, the idea would evolve into Match.com and eHarmony, OkCupid and Grindr, Tinder and Bumble, and Facebook Dating. But even then, the basic truth was the same: Everyone wants to find love, and with a computer to narrow the pool, it gets a little easier. Punch-cards turned to finger-swipes, but the computerized matchmaking magic remained the same.

In the decades that people have been finding love online, there has been surprisingly little anthropological research on how technology has changed the dating landscape. There are some notable exceptions—like Dan Slater's 2013 book Love in the Time of Algorithms—but research that takes stock of the swiping, matching, meeting, and marrying of millions of online daters has been thin, when it exists at all.

A new survey from the Pew Research Center updates the stack. The group last surveyed Americans about their experiences online dating in 2015—just three years after Tinder launched and, in its wake, created a tidal wave of copycats. A lot has changed: The share of Americans who have tried online dating has doubled in four years (the survey was conducted in October 2019) and is now at 30 percent. The new survey was also conducted online, not by phone, and “for the first time, gives us the ability to compare experiences within the online dating population on such key dimensions as age, gender and sexual orientation,” said Monica Anderson, Pew’s associate director of internet and technology research, in a Q&A published alongside the survey.

Ask WIRED What is the safest way to use dating apps?

The new survey is far from sweeping, but it qualifies with new data many of the assumptions about online dating. Pew surveyed 4,860 adults from across the United States, a sample that’s small but nationally representative. It asked them about their perceptions of online dating, their personal usage, their experiences of harassment and abuse. (The term “online dating” refers not just to websites, like OkCupid, but also apps like Tinder and platform-based services like Facebook Dating.) Half of Americans said that online dating had “neither a positive nor negative effect on dating and relationships,” but the other half was divided: A quarter said the effect was positive, a quarter said it was negative.

“Americans who have used a dating site or app tend to think more positively about these platforms, while those who have never used them are more skeptical,” Anderson notes in her Q&A. But there are also demographic differences. From the survey data, people with higher degrees of education were more likely to have positive perceptions of online dating. They were also less likely to report receiving unwanted, explicit messages.

Young adults—by far the biggest users of these apps, according to the survey—were also the most likely to receive unwanted messages and experience harassment. Of the young women Pew surveyed, 19 percent said that someone on a dating site had threatened violence. These numbers were even higher for young people who identify as lesbian, gay, or bisexual, who are also twice as likely to use online dating than their straight peers. “Fully 56% of LGB users say someone on a dating site or app has sent them a sexually explicit message or image they didn’t ask for, compared with about one-third of straight users,” the survey reports. (Men, however, are more likely to feel ignored, with 57 percent saying they didn’t get enough messages.)

None of this is surprising, really. Unpleasant encounters on dating platforms are well documented, both by the media and the public (see: Tinder Nightmares), and have even spurred the creation of new dating platforms, like Bumble (its original tagline: “The ball is in her court”). Researchers have made these observations before, too. In a 2017 survey on online harassment, Pew found that young women were much likelier than young men to have received unwanted and sexually explicit images.",Yes,It isn't as safe as people thought it was. Social media makes it easier for predators to find their victims and for bullies to hide behind a screen.,https://www.wired.com/story/americans-online-dating-pew-survey-2020/,2020-02-06 22:36:42.119000+00:00,LABEL_1_relevant,0.9940914511680604,Social Media,social_media,Wired,Security & Privacy
169,Wellness Startup's Generic Viagra Ads Flout Facebook Rules,"On television and radio, the ads are fairly innocuous: “Hey guy,” a female narrator says playfully in one TV spot for Hims, a men’s wellness brand that sells prescription drugs to treat erectile dysfunction, oral herpes, social anxiety, hair loss, and other conditions. “Hi there. Welcome to Hims.”

The ad invites viewers to “get ED treatment started for only $5,” next to a close-up of a young man pressing a white pill seductively to his lips. What appear to be customer reviews are superimposed over the image: “Should have done it years ago and I feel like the young stud that I always imagined I was,"" says one. ""Outstanding product, works above and beyond our expectations,"" reads another.

Much like other ads for Hims—and its sister brand, Hers, which sells prescription drugs and wellness products for women—that are broadcast on television, radio, podcasts, or appear in print or on billboards, this ad is rather generic. It describes a medical problem, alludes to the company’s business model—which skips a trip to the doctor in favor of an “online visit” with a physician—and invites the viewer to check out its website for more details.

Online, however, the brands take a different tack. Hims and Hers ads on Facebook and Instagram are more specific, offering a quick and easy way for users to buy prescription medications directly. “men [sic] can get Sildenafil (active ingredient in Viagra) delivered directly to the door,” reads one active Facebook ad by Hims. “try it out today for only $5. free shipping!”

Such ads violate Facebook policies that prohibit promoting the sale or use of prescription drugs or suggesting that users have a specific condition. Earlier this month, Facebook removed three Hims ads for sildenafil that had been flagged by WIRED; a Facebook spokesperson said the ads violated Facebook policies. But hundreds of other ads from Hims and Hers touting specific prescription drugs were active on Facebook as of Friday.

Many online ads from Hims and Hers also may run afoul of US Food and Drug Administration guidelines that require marketers to disclose the side effects associated with a drug. A Hers ad for prescription anti-acne medication tretinoin, for example, calls the drug ""your skin's BFF” and offers ""Serious results without the hassle."" The ad suggests, “Skip the drive over to the doctor's office and start today for just $5.""

Facebook Ad Library

The ad does not mention any of the side effects required by the FDA, which in tretinoin’s case can include burning, itching, stinging, scaling, peeling, or redness of the skin, or sensitivity to sunlight, soaps, cosmetics, and other skincare products. Nor does the Hims ad for sildenafil mention side effects including low blood pressure, loss of vision or hearing, headaches, or insomnia. TV ads from Hims and Hers don’t carry these disclosures because they don’t mention specific drugs, referring instead to “treatment.”

The FDA disclosure requirements apply to online, as well as traditional, media, says Hyosun Kim, a professor at the University of Wisconsin Stevens Point whose research focuses on online direct-to-consumer advertising of prescription drugs. “The FDA has these guidelines to protect consumers from misleading information, because [when it comes to] prescription drugs, it's really important for the patient to understand the risk factors and any other [interactions the drug might have],” she says. Kim says that, in her opinion, ads like those run by Hims and Hers—which do not include any risk information and in some cases claim to provide “serious results without the hassle”—violate FDA guidelines.",Yes,It's very easy to advertise prescription drugs without disclosing any of the risks or side effects.,https://www.wired.com/story/health-brands-hims-hers-flout-facebooks-rules-drug-ads/,2019-07-01 11:00:00+00:00,LABEL_1_relevant,0.9963472485542296,Social Media,social_media,Wired,"Information, Discourse & Governance"
170,"Regardless of Its Influence on the Election, Facebook Needs to Change","Trump even explained over the weekend that social media was pivotal to his campaign. “The fact that I have such power in terms of numbers with Facebook, Twitter, Instagram, etc.—I think it helped me win all of these races where they’re spending much more money than I spent,” he said during an interview on CBS’s 60 Minutes on Sunday.

The more contentious issue is whether truly fictitious news also played a role in swinging the election in Trump’s favor. Stories that claimed Hillary Clinton was an arms dealer, or that Donald Trump was endorsed by the pope, for example, were complete fabrications. But as Wired notes, people read them. And surely many believed them to be fact.

Mark Zuckerberg, however, is adamant that they did not decide the election—a position he’s now spoken out about twice. Last week, he claimed that “the idea that fake news on Facebook ... influenced the election in any way is a pretty crazy idea.” And over the weekend he posted a lengthy note explaining that “of all the content on Facebook, more than 99 percent of what people see is authentic.” This, he claims, “makes it extremely unlikely hoaxes changed the outcome of this election.”

The question of whether such news did have an impact has commentators scrambling for a hot take that comes down on one side or the other. New York magazine claims that Trump won because of fake news on Facebook. Recode says that “if you’re blaming Facebook for the presidential election results, you’re an idiot.”

Neither is right.

It is undeniably the case that such stories will have shaped some opinions. It’s also the case that people consume media from sources other than Facebook. The truth is, Facebook alone did not bring about Donald Trump’s win.

The furor does, however, bring one thing into stark relief: Facebook needs to solve its fake news problem. Protestations from Zuckerberg that Facebook is not and shall not be a media company seem harder to accept when it continues to deliver such overtly fallacious content.

As Sam Biddle neatly puts it in a piece for The Intercept: “You can blame Facebook outright for Trump’s victory, or not. But at the very least, we should demand from them some accountability for their role in spreading the present toxic sea of deliberate misinformation and non-factual chaos.”

Mercifully, the New York Times suggests that some senior executives at Facebook are worried about the company’s influence. Zuckerberg has, somewhat vaguely, claimed that the social network will “continue to work on” blocking fake news, though he seemed to backpedal even from that, saying he wants to “proceed very carefully” because “identifying the ‘truth’ is complicated.”

He’s right, of course. But when you’re in control of one of the primary conduits through which over one billion people receive news and information, things can’t always be simple. It’s time to step up to the mark, Mark.

(Read more: New York Times, The Intercept, Bloomberg, “Facebook Tweaks Its News Feed Algorithm, and the Winner Is: Facebook”)",Yes,"The spread of fake news. While Facebook CEO Mark Zuckerberg has downplayed the role of his company in the election of Donald Trump, it is undeniable that some people will have shaped some opinions based on the fallacious content they read on the site. It is therefore crucial that Facebook solve its fake news problem.",https://www.technologyreview.com/2016/11/14/107182/regardless-of-its-influence-on-the-election-facebook-needs-to-change/,2016-11-14 00:00:00,LABEL_1_relevant,0.9953598380088806,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
171,Evidence Grows That Online Social Networks Have Insidious Negative Effects,"The question Sabatini and Sarracino set out to answer is whether the use of online networks reduces subjective well-being and if so, how.

Sabatini and Sarracino’s database is called the “Multipurpose Survey on Households,” a survey of around 24,000 Italian households corresponding to 50,000 individuals carried out by the Italian National Institute of Statistics every year. These guys use the data drawn from 2010 and 2011. What’s important about the survey as that it is large and nationally representative (as opposed to a self-selecting group of undergraduates).

The survey specifically asks the question “How satisfied are you with your life as a whole nowadays?” requiring an answer from extremely dissatisfied (0) to extremely satisfied (10). This provides a well-established measure of subjective well-being.

The survey also asks other detailed questions such as how often people meet friends and whether they think people can be trusted. It also asked about people’s use of online social networks such as Facebook and Twitter.

This allowed Sabatini and Sarracino to study the correlation between subjective well-being and other factors in their life, particularly their use of social networks. As statisticians they were particularly careful to rule out spurious correlations that can be explained by factors such as endogeneity bias where a seemingly independent parameter is actually correlated with an unobserved factor relegated to the error.

They found for example that face-to-face interactions and the trust people place in one another are strongly correlated with well-being in a positive way. In other words, if you tend to trust people and have lots of face-to-face interactions, you will probably assess your well-being more highly.

But of course interactions on online social networks are not face-to-face and this may impact the trust you have in people online. It is this loss of trust that can then affect subjective well-being rather than the online interaction itself.

Sabatini and Sarracino tease this apart statistically. “We find that online networking plays a positive role in subjective well-being through its impact on physical interactions, whereas [the use of] social network sites is associated with lower social trust,” they say. “The overall effect of networking on individual welfare is significantly negative,” they conclude.

That’s an important result because it is the first time that the role of online networks has been addressed in such a large and nationally representative sample.

Sabatini and Sarracino particularly highlight the role of discrimination and hate speech on social media which they say play a significant role in trust and well-being. Better moderation could significantly improve the well-being of the people who use social networks, they conclude.

Facebook, Twitter, and others take note.

Ref: arxiv.org/abs/1408.3550: Online Networks and Subjective Well-Being",Yes,"It can reduce trust and social capital, which in turn affects subjective well-being.",https://www.technologyreview.com/2014/08/29/12613/evidence-grows-that-online-social-networks-have-insidious-negative-effects/,2014-08-29 00:00:00,LABEL_1_relevant,0.9958112239837646,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
172,David Mamet files short story as brief supporting Texas social media ...,"David Mamet is known for plays like Glengarry Glen Ross and Speed-the-Plow, films like The Spanish Prisoner and Heist, books like On Directing Film and The Wicked Son, and as of last week, a short story about a lost airplane pilot filed as a legal brief supporting social media regulation in Texas.

Mamet’s amicus brief is titled “Lessons from Aerial Navigation,” and as it’s two pages long, you should really just go read it. Here’s a sample of the prose:

The Map is not the territory. The territory is the territory. The pilot’s answer to the question “where am I?” lies not on the map, but out the windscreen. That’s where he is. It doesn’t matter where he calculated he should be, the territory below him is where he is.

In case you’re wondering, the map is a metaphor for the internet. The implied legal argument is that social media platforms have distorted “the Map” by moderating content in a way Mamet — a vocal supporter of former President Donald Trump, who’s been banned from major platforms — objects to. In response Mamet is supporting HB 20, a currently blocked Texas law that’s meant to discourage web services from moderating conservative posts and users. This gets a little more explicit later on:

Navigating requires using tools correctly. The confused citizen has a map. But, if he worked from his observations back to it, he might discover that he can’t find his position pictured there. Looking out he might, for example, see a free, prosperous, and good country, in which there was little actual poverty, scant racism, and no “systemic” racism, where minorities and women, rather than being discriminated against were treated preferentially. (This belief might be correct or incorrect, but unless we prefer a Ministry of Truth, the belief is his own and surely he’s entitled to it.) Referring back, then, to his “information,” the citizen might not be able to correlate it with his observations. He knew where he was, as he’d just looked around. But he found no corresponding position on his map.

But along the way you can also find diversions like an etymology lesson referencing Greek mythology:

I report as an outdoorsman, that Panic is real. It is the loss of the mind and will to Pan, God of the Woods. The affected loses his reason, and runs about unable to recognize those actual signs (a road, his own footprints), which might bring him back to safety.

Mike Masnick of Techdirt has dissected Mamet’s argument (which includes a nonfictional preface) as well as his decision to copyright the legal filing — something that’s theoretically possible but pretty rare. To sum up, Mamet’s logic hinges on casting a partial government ban on moderation by private companies as protection from government censorship, by somewhat dubiously calling web platforms “firms that control information conduits, and are privileged and subsidized by the government.”

At the risk of stating the obvious: a judge probably will not be swayed by a legal brief that includes no actual legal references or arguments. On the other hand, if state legislatures are going to make me keep covering ill-conceived and possibly unconstitutional social media laws, I’m not going to personally complain about writers throwing some florid allegorical prose into the mix.",Yes,"People are exposed to a much wider range of viewpoints than they would be otherwise. This can lead to echo chambers and filter bubbles, where people only see information that reinforces their existing beliefs.",https://www.theverge.com/2022/3/15/22979026/david-mamet-amicus-brief-texas-hb20-social-media-regulation-texas,2022-03-15 00:00:00,LABEL_1_relevant,0.9950724244117736,Social Media,social_media,The Verge,"Information, Discourse & Governance"
173,Facebook Let Cybercrime Groups Operate in Plain Sight,"Facebook's failure to moderate bad behavior on the sprawling online world it created, what with political trolls, extremist content, and livestreamed acts of horrific violence, has received a torrent of criticism. But researchers have found that the social media giant is also failing to police a far more basic and decades-old internet problem among its users: plain old cybercrime.

Researchers at Cisco's Talos security division on Friday revealed that they'd uncovered 74 Facebook groups devoted to the sale of stolen credit card data, identity info, spam lists, hacking tools, and other cybercrime commodities. The researchers say those groups sat in plain sight, with names like Spam Professional and Spammer and Hacker Professional, attracting 385,000 members in all. Anyone could find them with a site search for basic terms like ""carding"" or ""CVVs,"" a reference to the security codes on the back of credit cards.

""Effectively, what we found was a huge number of Facebook groups openly trading crime stuff online,"" says Craig Williams, Cisco Talos' director of outreach. ""The user base in these groups is basically the size of Tampa.""

""It's ridiculous ... This company operates on a set of rules that are backward and are only in its own commercial interest."" Dipayan Ghosh, Shorenstein Center Platform Accountability Project

Screenshots that Cisco published in a blog post summarizing its findings capture Facebook users publishing pictures of purportedly stolen credit cards and IDs, offering lists of CVVs priced at $5 each, as well as collections of thousands of emails ripe for spamming and phishing—the type of data usually sold on dark-web markets or password-protected, invite-only hacker forums. Williams says many of the users he saw in those groups even appeared to be conducting business in Facebook's cybercrime bazaars under their real accounts.

Some of the posts that Cisco researchers found selling credit card data, including CVV security codes, as well as counterfeit credit cards and IDs. Cisco

And finding the groups, Williams says, wasn't particularly difficult: Once Cisco's researchers identified a handful of them, Facebook's recommendation algorithm offered them other groups with similar black market focuses.

This isn't the first time Facebook has faced this exact problem. Last year, cybersecurity reporter Brian Krebs identified a similar-sized crop of Facebook cybercrime groups, totaling 300,000 members, and reported them to Facebook. Facebook banned those groups at the time, but it took less than a year for an even larger population of fraudsters and hackers to make homes on the site.

And while Facebook has removed the groups Cisco identified—after the researchers alerted the company to its findings—its cleanup remains incomplete. In a few minutes of searching, WIRED found users and groups with names like Carder Philippines and Anonymous Carding India openly hawking credit card information, along with what appeared to be stolen goods like cameras and iPhones bought with hijacked ecommerce accounts.",Yes.,"Cybercrime. The ease of access to potential victims as well as the anonymous nature of most social media platforms creates a perfect environment for cybercriminals to ply their trade. The article cites a study by Cisco which found that 385,000 members were in 74 Facebook groups devoted to the sale of stolen credit card data, identity info, spam lists, hacking",https://www.wired.com/story/facebook-cybercrime-groups-again/,2019-04-05 13:00:00+00:00,LABEL_1_relevant,0.9960824251174928,Social Media,social_media,Wired,Security & Privacy
174,Facebook Gave Russian Internet Giant Special Data Extension,"Since March, when news broke that the political consulting firm Cambridge Analytica used a Facebook app to amass data on as many as 87 million people without their consent, the social networking giant has been forced to repeatedly answer for how it has given away user data and who it's given that data to. In the immediate wake of the scandal, Facebook rushed to defend itself in a blog post, saying that in 2014, it changed an element of its API to prevent apps from collecting data on their users' friends, as the Cambridge Analytica app did. Facebook has since clarified that while it announced this change in 2014, apps that already had access to people's friends' data continued to have access until May 2015.

Then, in more than 700 pages of written responses delivered to the House Energy and Commerce Committee late last month, Facebook acknowledged that some apps had this access for up to six months longer, to allow them to ""come into compliance"" with the new rules. There were dozens of companies on the list, including dating apps like Hinge and music-streaming services like Spotify, but one may raise more than a few eyebrows in Washington: the Russian internet giant Mail.ru.

According to Facebook, Mail.ru was given a two-week extension to wind down a feature on two messaging apps that enabled users to see their Facebook friend lists and message with people who also had the Mail.ru apps. During the extension, at least, the app only had access to people's friend lists, not any information about those friends' likes or interests. And yet, long before that extension was in place, Facebook says Mail.ru ran hundreds of apps on the platform, all of which operated under Facebook's old rules, which did allow app developers to collect their users' friends' data. Some of those apps began operating as early as 2009.

""Some apps were built prior to the platform change in 2015, so they did have access to the earlier version of our platform,"" a Facebook spokesperson said. ""That made it possible for users to consent to sharing information about themselves, as well as their friends.""

Facebook says the majority of Mail.ru's apps were test apps that remained private and that only a handful actually launched publicly. It did not share details on how many users may have had their information exposed to Mail.ru apps without their consent. The company adds that Mail.ru's collection of apps have not had access to people's friends' data since May 2015, when Facebook changed its API. Still, Facebook is now investigating Mail.ru, along with all other apps that had access to large quantities of user data prior to the changes. But, the spokesperson says the investigation is not itself a condemnation. ""We found no indication of misuse with Mail.ru. If we detect any suspicious activity or potential misuse, that’s when we formally audit a company.""

Facebook granted thousands of other companies the same data access as Mail.ru prior to 2015. And yet, recent concern over Russia's manipulation of social networks in the run-up to the 2016 election may cast the relationship between the two companies in a new light.",Yes,It can be used to spread false information and cause panic.,https://www.wired.com/story/facebook-gave-russian-internet-giant-special-data-extension/,2018-07-10 21:31:55.807000+00:00,LABEL_1_relevant,0.9961766004562378,Social Media,social_media,Wired,"Information, Discourse & Governance"
175,Why Teens Are Falling for TikTok Conspiracy Theories,"Of all the bits of sleepover gossip ever uttered, this was the most unexpected. “Basically, Hillary Clinton sex traffics children and sacrifices them to Satan or something,” said my friend Beth, straightfaced. “It’s called Pizzagate, or whatever.” It was a Friday night a few weeks ago and even though it was a typical girls' night in, it was not a typical conversation—but I took the bait. “It’s called Pizza what?” I asked, totally unsure of what I was getting myself into. “Dude, you haven’t heard? Hillary and that one guy, John something? Podesta? They’re involved in this massive underground sex trafficking operation. I saw it on TikTok. It’s everywhere.” Half convinced she was kidding, I asked if she actually believed something so obviously ridiculous. “I swear, it’s legit,” Beth replied. “There’s been like five videos talking about it in a row. It’s clearly not a coincidence.”

At the completion of Beth’s ode to Pizzagate, I stared at her, eyebrows raised, mouth open. Taking note of my expression, she rolled her eyes. Then I did too. But for the rest of the night, I watched my two best friends gush over the medley of conspiracy theories that had been flickering across their TikTok “For You” pages and Twitter feeds, listening to them debate which celebrity had exploited the most children and who had sacrificed who to what Hollywood god. As the conversation intensified, and the lack of actual evidence became more and more obvious, I was overwhelmed by a wave of hysteria. It all seemed so ridiculous. How could these seemingly well-educated teenagers—rising high school seniors, even—be gullible enough to take a 60-second social media video for the truth? Conspiracy theories always seemed so juvenile to me. I knew better. Others, it seemed, did not.

They’re not alone. Conspiracy theories are finding a whole new generation of acolytes amongst the teens of TikTok, where—according to a recent New York Times report—#Pizzagate posts have been viewed more than 82 million times in the past few months. This became more obvious as the sleepover’s conversation wore on. Honorable mentions of the evening: the (false) claim that home furnishings company Wayfair is involved in a sex-trafficking operation, the (also false) notion that Oprah Winfrey is involved in a similar plot, and—my personal favorite—that one-time pop-punk princess Avril Lavigne has been replaced by a lookalike. (This is, of course, also not true.) Yet, as the night continued it became obvious the people in my friend circle would be willing to bet their life savings, albeit minimal, on the idea that Ellen DeGeneres coerced Oprah into recruiting children for human trafficking after watching a young girl rant through a 30-second clip. The reason for their faith? The girl had made a PowerPoint and even animated the transitions.

On the surface, it makes sense that young people would latch on to conspiracy theories on TikTok. The platform skews young—reportedly one-third of its daily users in the US are 14 or younger—and celebrity gossip has long been the lingua franca of social media for people of all ages. Right-wing conspiracy groups like QAnon have been spreading made-up stories about those in power on networks like Facebook for years. Now those ideas have jumped to TikTok, where they’re being metabolized by much younger consumers. Those things all scan. What doesn’t, however, is why teens believe them.

ABOUT THE AUTHOR Sofia Barnett is a 17-year-old writer living in Frisco, Texas. Currently a high school senior, she plans to attend college next year and eventually pursue a career in journalism or politics. Despite trying to convince her friends to stay off of social media, she spends approximately 10 hours per week on TikTok.

The short answer? TikTok is full of crazy ideas—conspiracies are no different. They've been normalized by the platform where many young people spend most of their time. “Many of these conspiracy sites and stories are entertaining. They are social gathering spots. They are exciting,” says Nancy Rosenblum, Senator Joseph S. Clark Professor of Ethics in Politics and Government at Harvard University and coauthor of A Lot of People Are Saying: The New Conspiracism and the Assault on Democracy. “It’s small wonder that teenagers who ‘live on the screen’ would be drawn to the drama.”

Easy access to social media’s redistribution tools worsens this problem. With every like, share, send, and retweet, teenagers are popularizing this content worldwide. “On social media, repetition substitutes for validation,” says Russel Muirhead, a professor of democracy and politics at Dartmouth College and Rosenblum’s coauthor. “Repetition is what breathes air into conspiracy theories, and social media is all about repetition. But repeating something that’s false does not make it more true! Teenagers are just as vulnerable to this as grown-ups.”

This wouldn’t be such a problem if teenagers weren’t so attached to social media. So fond, in fact, that some 54 percent of teens get the bulk of their news from it. If this sounds concerning, that’s because it is. With teenagers relying on TikTok as their sole source of information, it makes sense for my generation to become absorbed in webs of falsities and to live as largely uninformed citizens.",Yes,It allows teenagers to became easily gullible and to live as largely uninformed citizens.,https://www.wired.com/story/teens-tiktok-conspiracy-theories/,2020-09-19 11:00:00+00:00,LABEL_1_relevant,0.9960340857505798,Social Media,social_media,Wired,"Information, Discourse & Governance"
176,Russia says it is restricting access to Facebook in the country,"Russia says it is restricting access to Facebook in the country

The Russian government announced Friday that it will begin to “partially restrict” access to Facebook, according to an announcement from its internet regulatory agency Roskomnadzor.

Russia claimed that it would implement the measures, which were not specified, after Facebook put its own restrictions on four Russian state-linked media outlets, the television network Zvezda, news agency RIA Novosti, and the websites Lenta.ru and Gazeta.ru.

“On February 24, Roskomnadzor sent requests to the administration of Meta Platforms, Inc. remove the restrictions imposed by the social network Facebook on Russian media and explain the reason for their introduction,” Roskomnadzor wrote, adding that Facebook “ignored” its requests.

Meta Global Affairs VP Nick Clegg elaborated on the situation on Twitter, indicating that the Russian request came in response to either Facebook’s fact-checking practices or its policy of labeling of state-run media accounts. “Ordinary Russians are using @Meta’s apps to express themselves and organize for action,” Clegg wrote.

Ordinary Russians are using @Meta's apps to express themselves and organize for action. We want them to continue to make their voices heard, share what’s happening, and organize through Facebook, Instagram, WhatsApp and Messenger. pic.twitter.com/FjTovgslCe — Nick Clegg (@nickclegg) February 25, 2022

Facebook did not respond to TechCrunch’s request for comment on the Russian government’s claims and what actions it may have taken against state-backed media on the platform. For now, it’s not clear what partial restrictions mean for Russian Facebook users or if those limitations would extend to Meta-owned platforms like WhatsApp and Instagram.

As Russia deepens its invasion of neighboring Ukraine, thousands of Russians have taken to the streets to demonstrate against the war — in most cases at great personal risk. It’s possible that the Russian government is seeking to tamp down anti-war sentiment online and further shape the narrative of the war, starting with one of the biggest U.S.-owned social networks.

In the U.S., there’s some support for the idea that tech platforms like Facebook should take more strident action to limit the spread of Russian disinformation. A widely shared blog post published Thursday likened Russian President Vladimir Putin to Hitler, encouraging platforms to “choose sides” and crack down on the vast network of Russian propaganda accounts that spread disinformation online.

“Would you demand those publishers pull the plug on Hitler, or would you defend the German state’s right to some confused notion of ‘free speech,’ even as Hitler sets out to destroy the lives of millions?” Justin Hendrix, CEO and editor of Tech Policy Press, wrote in the post.

“That is the situation Google, Facebook, Twitter and other U.S. tech firms are faced with right now.”",Yes.,The way it can spread disinformation and cause divisions in society. This is what the Russian government is exploiting with its latest announcement.,https://techcrunch.com/2022/02/25/russia-facebook-restricted-censorship-ukraine/,2022-02-25 00:00:00,LABEL_1_relevant,0.9956036806106568,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
177,Are Facebook Ads Discriminatory? It’s Complicated,"The Facebook media cycle took a head-whipping quadruple turn over the past few weeks. First, in a surprise move by the Trump administration, the Department of Housing and Urban Development sued the company for violations of the Fair Housing Act, alleging it engaged in discriminatory advertising practices for housing ads.

Within days, researchers at Northeastern University published a preview of a paper describing an interesting set of advertising experiments showing that, indeed, Facebook was differentially showing ads for housing and jobs by gender and race. Lastly, in April the Algorithmic Accountability Act—AAA, naturally—was introduced in both the House and Senate, requiring large tech companies to test for evidence of discriminatory bias in the artificial intelligence that runs platforms like Facebook and Google.

This drama comes on the heels a bevy of other lawsuits alleging that Facebook abetted discriminatory advertising by allowing the targeting of housing ads by race and gender, something the company essentially admitted to doing by vowing to bar that functionality.

Antonio García Martínez (@antoniogm) is a writer and Ideas contributor for WIRED. Previously he worked on Facebook’s early monetization team, where he headed its targeting efforts. His 2016 memoir, Chaos Monkeys, was a New York Times best seller and NPR Best Book of the Year.

The HUD suit and AAA bill are really a step beyond the previous algorithmic bias lawsuits, with potentially far-reaching implications. To understand why requires digging a bit into the semantics and practice of Facebook advertising.

Start with the term targeting. As commonly used, it’s an overly broad category that can mean very different things. Within the industry, targeting refers to the data an advertiser uses to segment the audience, either data supplied by Facebook (say, 25 to 35-year-old Californians) or data that an advertiser brings to Facebook (say, the browsing and buying history of users on an ecommerce site). The key point is that whether the data comes from Facebook or the advertiser, the advertiser’s hands are on the targeting levers, deciding which set of users sees their ad.

But given the complexity of the ad buying and auction process, targeting isn’t the only thing that determines who sees an ad. Within the segment specified by the advertiser, Facebook can skew which type of user eventually views that targeted ad.

That’s typically called optimization, which is what’s at issue in the HUD suit and the AAA. Particularly for broadly targeted ads—say, every US millennial—Facebook itself further narrows the targeted set based on what it knows about the user and its platform. If the ad is about fashion, then it’ll pick users who’ve shown interest in fashion brands. If the initial run of the ad over a broad audience shows that some subsegment—say, people in Texas—engage with it more than others, then it’ll quickly start biasing the showing of the ad to Texans only.

In a world of perfect targeting, where the advertiser has absolute control of the user experience and knows all Facebook user data, no optimization would be necessary. A sophisticated advertiser could simply train the same machine-learning models that Facebook does.

In a world of perfect optimization, where Facebook knows even off-Facebook data like purchases, then no targeting would be necessary. Facebook could simply take what the advertiser knows into account about what the user has bought or browsed, and provide as near-perfect an ad experience as possible.

In reality, of course, neither targeting nor optimization is perfect, so both work interdependently: The advertiser doesn’t trust Facebook enough to hand over all of its targeting data, and Facebook doesn’t want to share its optimization data (at least not intentionally) with outsiders.

What happens in the commercial reality of ad buying is the advertiser shows up with a best guess at a target audience, some ad creative, and a bid for a desired user action such as a click or app install. Facebook uses the advertiser’s targeting to whittle down the set of potential targets, and when one such user shows up, estimates how likely they are to click on the ad or download the app.",Yes,"It has become an amplifier of hate speech, fake news, and conspiracy theories.

The advertiser has a maximum amount they’re willing to pay for that click or install, and if that estimated probability is high enough, Facebook sells the ad impression to the advertiser. But if that estimate is too low, then Facebook keeps the",https://www.wired.com/story/are-facebook-ads-discriminatory-its-complicated/,2019-06-11 15:00:00+00:00,LABEL_1_relevant,0.9962271451950072,Social Media,social_media,Wired,"Information, Discourse & Governance"
178,Police in India visited Twitter offices over 'manipulated media' label,"Delhi police, controlled by India’s central government, on Monday evening visited two offices of Twitter — in the national capital state of Delhi and Gurgaon, in the neighboring state of Haryana — to seek more information about Twitter’s rationale to label one of the tweets by ruling partly BJP spokesperson as “manipulated media.”

An hour into the search attempt, Delhi Police Special Cell team, which investigates terrorism and other crimes, vacated Twitter’s offices because they were closed and there were no Twitter employees to engage with at the premises, according to live broadcast by several local news channels.

A Twitter spokesperson declined to comment. India, the world’s second-largest internet market, is a key overseas region for Twitter and many other American technology firms.

New Delhi sent a notice to Twitter last week after the social network labeled a tweet from Sambit Patra, the spokesperson of India’s ruling party BJP, as “manipulated media.”

In the tweet, Patra had claimed that Congress, the leading opposition party in India, was using a so-called “toolkit” to derail the Indian government’s efforts against the coronavirus pandemic. Alt News, a leading fact-checking organization in India, had debunked Patra’s claim.

Delhi police said it had received a complaint about the classification of Patra’s tweet, which it said it was investigating, and visited the offices to serve Twitter India’s head a notice of the inquiry. In a statement, the police said Twitter India’s managing director’s replies on the subject had been “very ambiguous.”

Delhi Police Special Cell team returns from Gurgaon after they find the Twitter India offices shut. Apparently there is work from home at @TwitterIndia since March last year. Was this move by Government to send out a message? pic.twitter.com/aCBfjhb5CC — Aditya Raj Kaul (@AdityaRajKaul) May 24, 2021

“Delhi Police is enquiring into a complaint in which clarification is sought from Twitter regarding the classification of a tweet by Shri Sambit Patra (BJP spokesman) as ‘manipulative’. It appears that Twitter has some information which is not known to us on the basis of which they have classified it as such,” Delhi Police said in an earlier statement to local TV channels and other journalists.

“This information is relevant to the enquiry. Special Cell which is conducting the enquiry wants to find out the truth. Twitter which has claimed to know the underlying truth should clarify,” it added.

In a follow-up statement, it disputed the characterization of today’s event as “raiding.”

Several policy executives and the like questioned Delhi Police’s motives.

As India seeks priority assistance from the international community & esp from the US, I guess this is one way to convince them that it is focused on the real problem at hand. https://t.co/QqBhbx0oYN — Tanvi Madan (@tanvi_madan) May 24, 2021

Label a ruling party spokesperson's tweet as containing manipulated media? Well, the Union Govt will bsend in the Special Cell stormtroopers of the Delhi Police to your India subsidiary's office in the national capital region to show you what's what. Blatant authoritarianism. https://t.co/rzt8Nf0bVd — Raman Chima (@tame_wildcard) May 24, 2021

This would be funny if it wasn’t tragic. @DelhiPolice knows that no data is physically stored at @TwitterIndia offices hence purpose of raid is nothing but intimidation. https://t.co/jgfWTMO2L2 — Raheel Khursheed (@Raheelk) May 24, 2021

The move also comes at a time when Twitter and Facebook are inching closer to the deadline to comply with India’s new guidelines to regulate social media.

In a new notice to both the firms, New Delhi warned that “failure to comply with IT rules could lead to loss of status and protections as intermediaries.”

Today’s development is the latest headache for the American firm, which has been struggling to maintain peace with the Indian government for several months.

After briefly complying with a New Delhi order early this year, the company faced heat from the government for restoring accounts that had posted tweets critical of the Indian government’s policy or the Prime Minister Narendra Modi.

The two faced off again publicly last month after New Delhi ordered Twitter and Facebook to take down posts that were critical of the government’s handling of the coronavirus pandemic.

The story was updated throughout with additional details.",Yes,"It leads to echo chambers and filter bubbles.

This is because people tend to follow and interact with people who share similar views and interests, and social media platforms use algorithms to show users content that is similar to what they have engaged with in the past.

As a result, people are exposed to a limited range of perspectives and information",https://techcrunch.com/2021/05/24/delhi-police-run-by-indias-central-government-raids-twitter-offices-over-manipulated-label/,2021-05-24 00:00:00,LABEL_1_relevant,0.9967471361160278,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
179,California Reveals It’s Been Investigating Facebook,"Over the last two months, nearly every state and territory has announced investigations into the market dominance of Facebook, Google, or both. Missing from those probes was the place Big Tech calls home: California.

The Golden State’s attorney general, Xavier Becerra, has been curiously silent as his peers made plans to examine the biggest tech firms in his backyard. Becerra's supposed inaction garnered him a wave of negative local press, and last week led The New York Times to wonder what, exactly, his office was up to.

On Wednesday, California’s top cop finally revealed he has actually been very busy investigating Facebook, in an inquiry that has lasted for over a year. At a press conference announcing a new lawsuit, Becerra said Facebook has refused to fully cooperate with his probe. “We make our work public when there is a legal action to make public, otherwise we do not discuss our investigations,” he said.

The California attorney general is suing Facebook to turn over key information the company has allegedly failed to release, including email correspondence from both CEO Mark Zuckerberg and COO Sheryl Sandberg. The documents are connected to an 18-month investigation into the social network’s data protection and privacy practices.

Is there something you think we should know about Facebook's privacy practices? Email the writer at louise_matsakis@wired.com. Signal: 347-966-3806. WIRED protects the confidentiality of its sources, but if you wish to conceal your identity, here are the instructions for using SecureDrop. You can also mail us materials at 520 Third Street, Suite 350, San Francisco, CA 94107.

The California probe began in June 2018, shortly after news broke that Cambridge Analytica, a political consulting firm whose clients included the Trump campaign, had accessed data on tens of millions of Facebook users. “What initially began as an inquiry into the Cambridge Analytica scandal expanded over time to become an investigation into whether Facebook has violated California law by, among other things, deceiving users and ignoring its own policies,” reads the lawsuit, which was filed in state superior court Wednesday.

Becerra said Facebook has hampered the investigation by refusing to fully cooperate with subpoenas. “If Facebook had complied with our legitimate investigative requests, we would not be making this announcement today. But we must move our investigation forward,” he added. The lawsuit notes Facebook allegedly was uncooperative as well during a Federal Trade Commission investigation into the company’s privacy practices, which resulted in a $5 billion penalty in July.

“We have cooperated extensively with the state of California’s investigation,"" Will Castleberry, Facebook's vice president of state and local policy, said in a statement. ""To date we have provided thousands of pages of written responses and hundreds of thousands of documents.”

Facebook has been very familiar with government scrutiny lately, and just not in its own home state. The company is already facing similar investigations in Massachusetts and the District of Columbia. The attorney general of New York, Letitia James, is leading a group of 46 states and territories examining the company’s market dominance and privacy practices. The FTC has opened another inquiry into the social network too, this time over antitrust concerns. You also can’t forget the House Judiciary Committee’s antitrust investigation, which is looking at tech companies more generally but certainly has Facebook in its sights.

No one knows yet what each of these inquiries might mean for Facebook in the end. Some politicians, including Democratic presidential candidate Elizabeth Warren, have suggested breaking up Facebook by unwinding acquisitions like Instagram. In an internal meeting transcript published by the Verge last month, Zuckerberg acknowledged the way regulators have been discussing his company isn’t exactly ideal. “I think that the direction of the discussion is concerning,” he said.

Content This content can also be viewed on the site it originates from.

More Great WIRED Stories",,...,https://www.wired.com/story/california-attorney-general-facebook-investigation/,2019-11-06 22:05:36.381000+00:00,LABEL_1_relevant,0.9959883093833924,Social Media,social_media,Wired,"Information, Discourse & Governance"
180,People Are Tweeting About Videogames Now More Than Ever,"Greetings, and welcome to this Monday's edition of The Monitor, WIRED's culture news roundup. What's happening? Glad you asked. First up, videogames are huge on Twitter. Second, Universal has canceled the release of one of its upcoming films. And finally, folks can finally get a glimpse of the new Apple TV+ show from Jennifer Aniston and Reese Witherspoon. Let's go!

People Are Tweeting About Videogames Now More Than Ever

Here's some news that's pretty surprising—and yet, totally not. According to new data, tweets focusing on videogames, and the videogame industry, are having a bang-up 2019. In the first half of this year, there were more than 500 million gaming-related messages posted on Twitter. That number marks an increase of 20 percent year over year. Another interesting stat: This year has seen a 15 percent increase in accounts that tweet about games. The data signals not only an increase in gaming chatter generally, but also points to recent news events—like Ninja leaving Twitch, and President Trump's recent comments blaming videogames for gun violence—dominating online conversation. ""It's been crazy to look at the past couple weeks and see a huge influx of trends coming from gaming,"" Rishi Chadha, head of gaming content partnerships at Twitter, told the Hollywood Reporter.

Universal Cancelled the Release of The Hunt

Following the mass shootings in Ohio and Texas that left dozens dead and wounded, Universal Pictures has canceled the release of The Hunt, which was set to hit theaters on September 27. The movie, which the studio calls a ""satirical social thriller,"" focuses on a group of wealthy people who hunt others for sport. Universal's decision came on the heels of Trump making a presumed dig at the film, tweeting ""Liberal Hollywood is Racist at the highest level, and with great Anger and Hate! They like to call themselves 'Elite,' but they are not Elite. In fact, it is often the people that they so strongly oppose that are actually the Elite. The movie coming out is made in order to inflame and cause chaos."" Universal, meanwhile, said the reason for canceling the release was that ""now is not the right time to release this film."" It's unclear if the release will ever be rescheduled.

Here's the First Trailer for That Jennifer Aniston-Reese Witherspoon Apple TV+ Show

You've been hearing about it for some time and now it's almost here. The Morning Show, which stars Jennifer Aniston, Reese Witherspoon, and Steve Carrell, will be landing on Apple TV+ this fall. The first trailer for the series about an AM news program is below. No word yet on whether or not there will be an Apple News+ tie-in.

Content This content can also be viewed on the site it originates from.

More Great WIRED Stories",,"We can't stop thinking about it.

The tech industry's history of discrimination is long, and it's time for a change.

The massive wildfire in the Amazon is spreading—and climate change has made it worse.

Review: The new iPadOS is what the iPad has needed since the beginning

The world's",https://www.wired.com/story/videogames-twitter/,2019-08-12 17:44:08.567000+00:00,LABEL_1_relevant,0.9951571822166444,Social Media,social_media,Wired,Environment & Sustainability
181,These Trump Socks Went Viral—and a Nightmare on Amazon Began,"When Donald Trump visited Louisiana earlier this month, he was greeted with an unexpected hairy surprise. Billy Nungesser, the state’s lieutenant governor, got dressed with the commander-in-chief in mind that morning. With the president and news cameras as his witness at the airport, Nungesser joyfully lifted the leg of his pants to reveal a goofy pair of socks: Each ankle bore Trump’s face, complete with a signature tuft of fake blond locks waving daintily in the breeze. The bizarre-looking socks quickly went viral and were covered by a smattering of news outlets. Stephen Colbert even mentioned them on The Late Show.

For Erica Easley, all the attention was great, at least at first. Easley is the founder of Gumball Poodle, a small Los Angeles–based sock company that originally came up with the hirsute design during the last presidential election. “They went really viral, beyond anything we’d ever experienced before,” she says about the aftermath of Nungesser’s photo-op. “And these socks have been on Rachel Maddow, The View, a bunch of things in 2016.” Wholesale orders started ticking up. Several media outlets linked to Gumball Poodle’s Amazon listing, and soon the Trump socks reached Amazon's best-seller list for men's novelty socks.

Louisiana Lieutenant Governor Billy Nungesser shows Donald Trump his socks. BRENDAN SMIALOWSKI/AFP/Getty Images

About a week passed before Easley noticed that something had gone horribly wrong. Dozens of third-party merchants, most of whom looked to be from China, had jammed her Amazon listing with what Easley believed to be knockoffs, selling for a fraction of the original $30 price tag. (Included in that price, for the record, is a tiny comb, to style your socks' hair. Everything is made in the USA.) To make matters worse, Amazon had chosen one of the frauds as the default seller, shutting Gumball Poodle out. Meanwhile, other third-party sellers appeared to have taken Easley’s photos and set up their own, much cheaper listings.

Louise Matsakis covers Amazon, internet law, and online culture for WIRED.

Easley had done everything to protect her business from exactly this kind of attack. Her hairy sock design is patented in the US, and her logo, which is stamped on the bottom of the socks, is trademarked. What’s more, Gumball Poodle is enrolled in the Amazon Brand Registry, an enhanced suite of tools the company provides eligible brands to protect their intellectual property. But Easley found Amazon’s protections weren’t enough, and she says the company largely ignored her pleas for help. Only after WIRED reached out to Amazon for this story were the counterfeits removed.

“Amazon strictly prohibits the sale of counterfeit products and we invest heavily in both funds and company energy to ensure our policy is followed,” Maxine Tagay, a spokesperson for Amazon, said in a statement.

Contending With a $1 Trillion Industry

Gumball Poodle isn’t alone. “There are thousands of other trademark owners who face the same kind of nonsense every single day,” says James Thomson, a former Amazon employee and a partner at Buy Box Experts, a firm that consults with Amazon sellers. “Amazon does have a problem with counterfeits.”

Counterfeiting is a booming, trillion-dollar industry that costs businesses around the world billions of dollars a year. Its growth has been fueled by the rise of ecommerce, which a government report says has led to “a fundamental change in the market for counterfeit goods.” That report, published last year by the Government Accountability Office, found that the volume and variety of counterfeit goods seized by officials has grown year after year, and it’s increasingly difficult to distinguish knockoffs from the real thing. It also notes that fraudulent goods are sold on a number of different ecommerce platforms. The Trump administration has signaled it wants to take the issue more seriously. In April, President Trump asked the Justice, Commerce, and Homeland Security Departments for recommendations on potential regulatory and legislative fixes to address counterfeits on third-party marketplaces.

But on Amazon, counterfeits can be uniquely devastating, in part because of the site’s sheer scale. Half of all US ecommerce sales go to Amazon, and the site is also where about half of all product searches on the web begin. Not many companies can afford to avoid it. If they do stay away, they risk letting other sellers determine how their brand is marketed on one of the biggest online retailers in the world.",Yes,"It has made it much easier for people to connect with others who share their interests. This has led to a number of positive outcomes, such as the ability to connect with like-minded people and the spread of information. However, it has also led to a number of negative outcomes, such as the spread of misinformation and theecho chamber effect.",https://www.wired.com/story/viral-trump-socks-amazon-counterfeits/,2019-05-31 17:12:38.038000+00:00,LABEL_1_relevant,0.99537992477417,Social Media,social_media,Wired,"Information, Discourse & Governance"
182,Facebook's Targeted Ads Are More Complex Than It Lets On,"In a recent blog post, Facebook's vice president for ads, Rob Goldman, argues his platform's users aren't its product. Even though Facebook primarily makes money by selling targeted ads based on what it knows about you, Goldman says that the real product is the ability to connect people—ads merely exist to ""fund that experience.""

To help support that stance, Goldman paints a simple picture of the role advertising plays on Facebook, downplaying the information it collects about you. Using a hypothetical example about a small bike shop in Atlanta, he emphasizes that targeted ads help small businesses reach customers—like, say, female cyclists who live nearby—more efficiently. The example, which the social network also uses on a general page explaining how ads work, does represent how some companies use Facebook. But ad industry experts say Goldman's explanation leaves out many important realities of Facebook's advertising machine.

Relevant to What?

Four times in his blog post, Goldman stresses that Facebook's targeting mechanisms allow users to see relevant ads. But nowhere does he define what ""relevant"" means in this context. In some ways, it's broadly intuitive across all industries; advertisements for dentures or funeral insurance don't run on Nickelodeon for a reason. But beyond simple demographics, a ""relevant"" ad to a marketer might target a specific personality type, or perceived emotional state. It might also be designed to take advantage of an already vulnerable population. That can quickly get a lot more involved than just people who like bikes.

'There’s 60,000 channels and weird ways to combine them.' Kane Jamison, Content Harmony

“We already have been seeing the results of negative segmentation we saw in the past before, like when cigarette companies were targeting low-income people,” says Juan Mundel, a professor at DePaul University who has studied Facebook advertising. Because the social network has so much data, it's possible to target hyper-specific audiences with extreme precision. That means, as Bloomberg reported in March, predatory advertisers can exploit Facebook's tools to sell shady products to the masses, like diet pills.

“Facebook also knows when you’re motivated to do something, when you’re feeling down, when you’re feeling all sorts of emotions,” says Mundel. The social network leverages that information for advertisers; the Intercept discovered earlier this month that the company has developed a new service designed to predict how consumers will behave in the future, like when they're likely to switch from one product brand to another. That level of psychological parsing goes far beyond what Goldman outlines.

Goldman is right to point out that Facebook has much in common with traditional forms of advertising like television and print, but the difference is companies who use Facebook have a near-endless number of data points with which to target their ads, and can show them to much narrower slices of the population. ""Facebook is the same thing, but there’s 60,000 channels and weird ways to combine them as well,” says Kane Jamison, the founder and managing director of Content Harmony, a marketing agency that frequently uses Facebook to advertise.

What Facebook Knows

Throughout Goldman's post, he stresses that users can control their ad experience by visiting their Ad Preferences menu. At the top of the screen, you will see Your interests, which Facebook says it generates based on your activities on Facebook, such as pages you may have liked. It's not clear whether some of these categories are algorithmically generated, and Facebook would only say they are based on past actions on the platform.

The categories range from intuitive to bizarre. Mine, for example, include head-scratching topics like ""Laser,"" ""Steel,"" ""Everything,"" and ""Authority."" If you hover your cursor over each one, Facebook ostensibly tells you why it first appeared: ""You have this preference because you clicked on an ad related to Everything."" Huh? Advertisers have a stunning number of categories to sift through—ProPublica has collected over 50,000, including those only marketers can see. You can remove any interest associated with your profile by clicking the X in the top right-hand corner.

Toward the bottom of the screen, under Your information > Your categories, you can see other information Facebook has collected about you, based on your activities both on and off the platform. There, you might notice that Facebook has categorized you as a ""newlywed,"" ""away from family,"" or ""close friends with expats."" You will likely also see the type of browser and phone you use. You can remove any of these categories as well.",Yes,"It can give you a false sense of intimacy with others.

Intimate social media can make you feel as if you know people better than you actually do, which can lead to inappropriate behavior.

It can also lead to feeling like you are a part of a group when you are not, which can make you more susceptible to peer",https://www.wired.com/story/facebooks-targeted-ads-are-more-complex-than-it-lets-on/,2018-04-25 20:04:38.005000+00:00,LABEL_1_relevant,0.9957196116447448,Social Media,social_media,Wired,Social Norms & Relationships
183,"Elon Musk's Mouth, and Tweets, Land Him in Trouble Again","Twitter CEO Jack Dorsey once said that Tesla and SpaceX CEO Elon Musk is the most exciting and influential person on Twitter. Musk loves to kibbitz with fans, answering questions about his tech, making reading and music suggestions, and solving customer-service issues in real time. But court documents released late Monday show Musk and associates scrambling to deal with what one might call excessive openness on Twitter—openness that has gotten Musk in (more) legal trouble.

According to emails included in the filing, Musk has regrets. “I'm a fucking idiot,” he wrote to a public relations adviser in September 2018.

Stay in the know with our Transportation newsletter. Sign up here!

The court filings are from defamation lawsuit filed by a Thailand-based British cave expert named Vernon Unsworth, who aided in the rescue of a Thai youth soccer team and their coach in July 2018. Unsworth’s lawyers contend that Musk is “a thin-skinned billionaire” who “orchestrated a malicious, false, and anonymous leak campaign in the UK and Australian press” that sought to paint Unsworth as a pedophile. The drama included a $52,000 private investigation by a convicted felon, as well as a Musk associate acting under an assumed name.

Musk had his own plan to rescue the soccer team, involving a child-sized mini-submarine. Unsworth told a CNN reporter that Musk’s plan was a publicity stunt that wouldn’t work. Musk, he said, could “stick his submarine where it hurts.”

A few days later, Musk saw Unsworth’s comments online, and spent less than an hour searching the man’s name online, Musk said in an August 2019 deposition. The billionaire was frustrated, he told lawyers during the deposition: “Who the hell is this guy, and what the hell is he doing insulting me, insulting everything my team did, the hard work everyone put in to try to help these kids?” he said. After finding an article linking Unsworth’s home city of Chiang Rai with the sex trade, Musk defended his submarine plan in a series of tweets to his 20 million followers, and called Unsworth a “pedo guy.” The CEO apologized for the tweets three days later.

But Musk didn't let the matter drop there, according to court filings. According to the court filings, a British man named James Howard-Higgins reached out to Musk in August 2018 about Unsworth, claiming to be a private investigator and alleging the diver had “skeletons in his cupboard.” Musk allegedly passed Howard-Higgins’ information to the head of his family office, a man named Jared Birchall. Birchall assumed the name James Brickhouse for his dealings with Howard-Higgins, and as Brickhouse, used more than $50,000 of Musk’s funds to hire the investigator to dig into the expat’s history.

Musk said Howard-Higgins passed Birchall information alleging that Unsworth had married a girl when she was as young as 12, though Unsworth’s lawyers say Howard-Higgins never told Birchall that Unsworth’s wife was any younger than 18 or 19 at the time of their marriage. (In fact, the diver’s wife was 32 when they met, the lawyers say.) BuzzFeed News later revealed that Howard-Higgins was in fact a fraud who had previously been convicted for stealing from a business partner, and is now in prison for violating the conditions of his parole. Either way: In August 2018, Musk responded to a BuzzFeed News reporter’s email about the “pedo guy” tweet by alleging that Unsworth was a “child rapist.” The reporter published Musk’s allegations in an article—allegations, Unsworth lawyer’s say, that are false.",Yes,"People can connect with each other very easily, which can lead to a variety of both positive and negative outcomes. On the one hand, social media can be used to connect with friends and family members, which can lead to increased communication and bonding. On the other hand, social media can also be used to spread false information, which can lead to",https://www.wired.com/story/elon-musks-mouth-tweets-trouble-again/,2019-10-08 23:03:53.666000+00:00,LABEL_1_relevant,0.996409833431244,Social Media,social_media,Wired,"Information, Discourse & Governance"
184,Facebook Hosted Three Huge Concerts in the Metaverse and They Seriously Flopped,"Well, Facebook’s metaverse is off to a depressing start.

Meta — aka the artist formerly known as Facebook — announced a slate of big name virtual reality concerts set to take place entirely in in its Horizon Venues metaverse last month. The star-studded lineup included rapper Young Thug on December 26, DJ David Guetta on December 31, and finally EDM duo The Chainsmokers for a New Years’ Eve concert to ring in 2022.

The only problem? Not many people noticed — and those who did didn’t seem to really care.

Despite being completely free, the concerts seemed to get incredibly little traction or notice from anyone outside of Meta offices. There was barely any mention of any of the performances on social media platforms like Twitter or even Facebook, let alone on news and music publications.

Advertisement

Advertisement

It probably wasn’t helped by the fact that the concerts were barely optimized for a VR experience. Guetta just had a 2D live streamed video of his performance at the Louvre Abu Dhabi. Meanwhile, Young Thug and The Chainsmokers offered just a 180 degree view of their concert for Oculus users.

Young Thug’s performance racked up just north of 100,000 views, while Guetta’s performances accumulated almost a million. The Chainsmokers’ performance, for its part, seems to have been removed. And keep in mind, too, that Facebook has been known to put a very optimistic spin on view counts previously in order to build hype for its fledgling video streaming platform.

If you think about it for just a second, it makes sense. Watching a concert alone while wearing a VR headset at home isn’t how a lot of people want to spend their New Year’s Eve — even if they are massive fans of The Chainsmokers for some reason.

Overall, it seems like the metaverse concerts weren’t the hit Zuckerberg was hoping they’d be. And the elephant in the room, of course, is Travis Scott’s Fortnite concert last year, which drew tens of millions of participants and commanded huge amounts of media attention.

Advertisement

Advertisement

So while Meta has set some ambitious goals for itself, it seems like creating a successful metaverse and drawing new users in is going to take a lot more than some free concerts.

More on Oculus troubles: Christmas Present VR Headset Sends Boy to the Hospital

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Yes,"Creating echo chambers and filter bubbles.

Advertisement

Advertisement

In other words, people tend to only see content that reaffirms their existing beliefs, which can lead to a lot of misinformation and tribalism.",https://futurism.com/facebook-concert-metaverse-flopped,,LABEL_1_relevant,0.9885566234588624,Social Media,social_media,Futurism,"Information, Discourse & Governance"
185,Facebook says it’s going to use machine learning to catch revenge porn,"That Facebook thinks it can tackle the problem in this way shows how just powerful its ability to identify people with AI has become.

The news: Facebook announced today that it will use machine learning to detect and block nude or near-nude images and videos that have been shared without permission—before they have even been reported. Revenge porn (the sharing of sexual videos of someone, usually a past partner, without consent) has become a serious problem, with devastating consequences for victims. Facebook also says it will overhaul the process by which victims can report unapproved images.

Face time: Facebook didn't say exactly what sort of machine learning it was going to use, but it has an almost unparalleled ability to identify people in images, thanks to a vast corpus of labeled training data supplied by its own users.

Training sets: Although many users are unaware, social-media photos are widely used to train machine-learning algorithms. The state-of-art programs are often now better than humans at recognizing people in snaps.

Coming threat: New detection technology could become especially important as it becomes ever easier to generate convincing-looking fake video with AI. The rise of easy-to-use face-swapping software has already led to a proliferation of fake celebrity porn and other weird video mashups.

Silver bullets: The Facebook effort is a worthwhile use of machine learning, but AI is no silver bullet for dealing with harassment, abuse, or fake news on social media (regardless of what Mark Zuckerberg might tell Congress). Humans will always find ways to outwit the best algorithms. Besides that, the problem sadly extends far beyond the walls of Facebook.

Sign up here to our daily newsletter The Download to get your dose of the latest must-read news from the world of emerging tech.",Yes,"It has become a breeding ground for harassment, abuse, and fake news. The problem is only compounded by the fact that it is now easy to generate convincing-looking fake video with AI. While Facebook's new machine learning effort is a worthwhile use of AI, it is no silver bullet for dealing with these problems.",https://www.technologyreview.com/2019/03/15/136597/facebook-says-its-going-to-use-machine-learning-to-catch-revenge-porn/,2019-03-15 00:00:00,LABEL_1_relevant,0.9967086315155028,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
186,Rumble Sends Viewers Tumbling Toward Misinformation,"“I'm not really expecting things to ever be what they were,” says Sarah. “There's no going back.” Sarah’s mother is a QAnon believer who first came across the conspiracy theory on YouTube. Now that YouTube has taken steps toward regulating misinformation and conspiracy theories, a new site, Rumble, has risen to take its place. Sarah feels the platform has taken her mother away from her.

Rumble is “just the worst possible things about YouTube amplified, like 100 percent,” says Sarah. (Her name has been changed to protect her identity.) Earlier this year, her mother asked for help accessing Rumble when her favorite conservative content creators (from Donald Trump Jr. to “Patriot Streetfighter”) flocked from YouTube to the site. Sarah soon became one of 150,000 members of the support group QAnon Casualties as her mother tumbled further down the dangerous conspiracy theory rabbit hole.

Between September 2020 and January 2021, monthly site visits to Rumble rose from 5 million to 135 million; as of April, they were sitting at just over 81 million. Sarah’s mother is one of these new Rumble users, and, according to Sarah, is now refusing to get the Covid-19 vaccine. Explaining her decision, says Sarah, her mother cites the dangerous anti-vax disinformation found in many videos on Rumble.

ABOUT Ellie House is a UK-based investigative journalist writing about tech and companies; she has previously worked with Private Eye, the BBC World Service, and the Investors Chronicle. Alice Wright is an investigative journalist based in London covering politics and environmental issues; she has written for Private Eye, The Times, Prospect, and others. Isabelle Stanley is an investigative journalist covering social justice issues for publications including the Sunday Times and Byline Times.

Rumble claims that it does not promote misinformation or conspiracy theories but simply has a free-speech approach to regulation. However, our research reveals that Rumble has not only allowed misinformation to thrive on its platform, it has also actively recommended it.

If you search “vaccine” on Rumble, you are three times more likely to be recommended videos containing misinformation about the coronavirus than accurate information. One video by user TommyBX featuring Carrie Madej—a popular voice in the anti-vax world—alleges, “This is not just a vaccine; we’re being connected to artificial intelligence.” Others unfoundedly state that the vaccine is deadly and has not been properly tested.

Even if you search for an unrelated term, “law,” according to our research you are just as likely to be recommended Covid-19 misinformation than not—about half of the recommended content is misleading. If you search for “election” you are twice as likely to be recommended misinformation than factual content.

Courtesy of Ellie House, Isabelle Stanley and Alice Wright; Created with Datawrapper

The data behind these findings was gathered over five days in February 2021. Using an adaptation of a code first developed by Guillaume Chaslot (an ex-Google employee who worked on YouTube’s algorithm), information was collected about which videos Rumble recommends for five neutral words: “democracy,” “election,” “law,” “coronavirus,” and “vaccine.” The code was run five times for each word, on different days at different times, so that the data was reflective of Rumble’s consistent recommendation algorithm.

Over 6,000 recommendations were manually analyzed. There can be disagreements about what can and cannot be classed as misinformation, so this investigation erred on the side of caution. For example, if a content creator said “I won’t take the vaccine because I think there might be a tracking chip in it,” the video was not categorized as misinformation. Whereas if a video stated “there is a tracking device in the vaccine,” it was. Our conclusions are conservative.",Yes,It has allowed misinformation to spread.,https://www.wired.com/story/rumble-sends-viewers-tumbling-toward-misinformation/,2021-05-11 13:00:00+00:00,LABEL_1_relevant,0.9961834549903872,Social Media,social_media,Wired,"Information, Discourse & Governance"
187,A Growing Frontier for Terrorist Groups: Unsuspecting Chat Apps,"Heads up, tech companies: If your product appeals to the masses, it likely also holds allure for terrorist groups like ISIS.

WIRED OPINION ABOUT Rita Katz is the Executive Director and founder of the SITE Intelligence Group, the world’s leading non-governmental counterterrorism organization specializing in tracking and analyzing the online activity of the global extremist community.

ISIS has effectively exploited the power of technology to fuel its rise around the globe, from streaming and file-sharing platforms to messenger applications and social media services. Many tech companies have responded in turn, strengthening their oversight and security measures. But while major platforms like Facebook, Twitter, YouTube, and Telegram are becoming increasingly inhospitable to ISIS, the group's reach is growing on lesser-known messenger apps designed for businesses and gamers.

In the aftermath of major territory losses in Iraq and Syria, ISIS is reconfiguring how it uses technology to drive its recruitment and coordination efforts.

The move towards free public messenger apps

ISIS uses the encrypted messenger platform Telegram as its primary app for media releases. In seeking new venues to disseminate its content, the terrorist group has made repeated attempts to set up web pages and blogs on services like Tumblr and WordPress. Such platforms are fitting places for ISIS propaganda, where content can be transferred from Telegram into organized, easily accessible layouts.

Throughout 2018, the group's Amaq News Agency invested significant effort into staying on these webpage and blogging platforms, from implementing Cloudflare to protect against DDOS attacks to creating browser extensions that immediately provide users with new URLs to their sites. Those measures ultimately failed, as each of their sites was hacked or removed.

Thus, emerging messenger applications—particularly those with features modeled after social media, such as chat groups, channels, and media-sharing—have become an effective alternative for ISIS.

RocketChat

In mid-December 2018, some of the most prominent ISIS-linked media groups announced channels on RocketChat, an open-source messenger service designed for businesses. The platform is both mobile and desktop enabled; since its founding in 2015 it has grown to 10 million users. Nashir News Agency, the ISIS-linked media dissemination group, urged supporters to join the app, stating, ""God willing, media will be published on RocketChat before Telegram.""

Many other ISIS-linked media groups embraced RocketChat nearly simultaneously. Some provide instructions for carrying out terrorist attacks, while others mirror content from ISIS's mother-channels on Telegram. In no time, RocketChat was host to content from the ISIS-linked Khilafah News, the attack-guide channel “Just Terror,” the English language ISIS media outlet Halummu, and the longstanding deep-web discussion forum Shumukh al-Islam. A technical manual published by the ISIS-linked “Afaaq Electronic Foundation” on December 15 instructed followers on how to install and anonymously use RocketChat.

A review of these RocketChat rooms shows that Amaq News Agency, ISIS’ official news outlet, was the first to hold an account on the platform, suggesting that the migration to Rocket was instructed by ISIS central. These groups continue to grow on the platform. As of January 8, there were over 700 users on the server that houses ISIS's channels. It requires only a private invite link and an email address to join.

RocketChat offers both cloud-hosting on its own servers and self-hosting. A server hosted by RocketChat risks being taken down by the platform, whereas a self-hosted server is immune to takedowns, except by the web host.

Yahoo Together

Around the same time they were promoting RocketChat, some major ISIS-linked channels also announced accounts on Yahoo Together, a new mobile-only messaging application launched in July to replace Yahoo Messenger. The platform has thus far been used in a similar fashion to RocketChat, mirroring communiques, photo reports, and media statements first posted to Telegram.

Unlike RocketChat, however, Yahoo Together appears to have quickly removed the ISIS accounts. Since then, ISIS groups have not advertised accounts on Yahoo's app.

Viber

Last month, ISIS also announced a Nashir News Agency account on Viber, another cross-platform instant messenger app. Though ISIS-linked media groups and supporters have long used the platform, their official promotion of Viber signaled a new level of investment.

The Nashir News Agency account appears to have since been removed, but its initial establishment on Viber is likely encourage other ISIS media groups and supporters create accounts on the platform.

Discord

Beyond the group’s media officials, die-hard jihadi supporters are expanding their online outreach efforts in more unexpected places.

One such example is Discord, a messenger application for gamers with more than 130 million registered users. The platform organizes its chat communities into “servers,” each of which contains numerous text and voice channels.",Yes,"It allows for easy connection and communication. This is a double edged sword, as it can be used for good or bad. For example, social media can be used to connect with friends and family, but it can also be used to spread fake news or coordinate terrorist attacks.",https://www.wired.com/story/terrorist-groups-prey-on-unsuspecting-chat-apps/,2019-01-09 20:00:00+00:00,LABEL_1_relevant,0.9965245127677916,Social Media,social_media,Wired,"Information, Discourse & Governance"
188,Your Tweets Can Help Map the Spread of Wildfire Smoke,"This story originally appeared on High Country News and is part of the Climate Desk collaboration.

At the end of July, Twitter user Alicia Santana posted a photo of a man sitting in a plastic folding chair in his yard. He’s looking away from the camera, towards a monstrous, orange cloud of smoke filling the sky beyond a wire fence. “My dad not wanting to leave his home,” Santana wrote, ending it with #MendocinoComplexFire.

As wildfires spark, parts of the internet glow with them. The #CarrFire, #FergusonFire, #RanchFire and other hashtags spread quickly this summer on Twitter. If past seasons are any indication, there will be thousands more tweets like this, and they will continue as smoke—the insidious second wave of wildfires—spreads across the West. They can also be used for data. In a recently published study, US Forest Service researchers Sonya Sachdeva and Sarah McCaffrey found that, when analyzed in large numbers, tweets about wildfires can accurately model the way smoke moves.

In their study, published by the International Conference on Social Media & Society, Sachdeva and McCaffrey analyzed close to 39,000 tweets posted between May and September 2015 in California. They stripped the tweets to reveal their core subjects: smoke in the air, fallen ash, haziness, smell. By tagging the tweets with the location in which they were posted, the researchers created a verbatim map: a landscape of fire based on the people who experienced it. Their modeling proved accurate when compared to figures from air quality monitors.

Content This content can also be viewed on the site it originates from.

Their results show that what we write online could fill the gaps that traditional data collection leave behind. While still nascent, using social media to study environmental events is a growing field. Images from Flickr can help researchers understand tourism rates in natural areas, and social media often acts as a crucial tool in disaster relief.",Yes.,"It can be used for data. In a recently published study, US Forest Service researchers Sonya Sachdeva and Sarah McCaffrey found that, when analyzed in large numbers, tweets about wildfires can accurately model the way smoke moves.",https://www.wired.com/story/twitter-wildfire-model/,2018-08-15 12:00:00+00:00,LABEL_1_relevant,0.995292067527771,Social Media,social_media,Wired,Security & Privacy
189,Facebook’s ex security boss: Asking Big Tech to police hate speech is “a dangerous path”,"Three things happened in 2016. There was a disinformation and propaganda campaign by [Russia’s] Internet Research Agency and related groups. There was the leak campaign where the GRU [Russian military intelligence] broke into the e-mail of the DNC [Democratic National Committee] and then planted stories [in the media]. And there was an exploratory penetration of the election systems of 21 states.

The first problem has had the most work [done on it] because it completely falls within the responsibility of the platforms. They have defined what is a political ad and what is an issue ad, created ad transparency, defined what is inappropriate coordination, and started enforcing those rules.

""The Russians were able to send their B team to hack into the DNC. I’m not sure people are ready for the A team.""

On the GRU hack and leak campaign, very little has happened. There have been upgrades in security at the DNC. I expect most campaigns are being more careful. But there does not seem to be a wholesale upgrade of security among campaigns and candidates. The Russians were able to send their B team to hack into the DNC. I’m not sure people are ready for the A team.

On the third we’ve done almost nothing. You still have 10,000 election authorities [in the US] running elections. We have a number of states with no paper backup [for ballots]. There are way too many authorities responsible for their own security for us to secure [elections] without the federal government providing a huge amount of resources. That’s where I think we have some of the most vulnerability.

So is the disinformation/propaganda problem mostly solved?

In a free society, you will never eliminate that problem. I think the most important thing [in the US] is the advertising transparency. With or without any foreign interference, the parties, the campaigns, the PACs [political action committees] here in the US are divvying up the electorate into tiny little buckets, and that is a bad thing. Transparency is a good start.

The next step we need is federal legislation to put a limit on ad targeting. There are thousands of companies in the internet advertising ecosystem. Facebook, Google, and Twitter are the only ones that have done anything, because they have gotten the most press coverage and the most pressure from politicians. So without legislation we’re just going to push all of the attackers into the long tail of advertising, to companies that don’t have dedicated teams looking for Russian disinformation groups.

Facebook has been criticized over Russian political interference both in the US and in other countries, the genocide in Myanmar, and a lot of other things. Do you feel Facebook has fully grasped the extent of its influence and its responsibility?

I think the company certainly understands its impact. The hard part is solving it. Ninety percent of Facebook users live outside the United States. Well over half live in either non-free countries or democracies without protection for speech. One of the problems is coming up with solutions in these countries that don’t immediately go to a very dark place [i.e., censorship].

""I think [Facebook] certainly understands its impact. The hard part is solving it.""

Another is figuring out what issues to put engineering resources behind. No matter how big a company is, there are only a certain number of problems you [can tackle]. One of the problems that companies have had is that they’re in a firefighting mode where they jump from emergency to emergency.

So as they staff up that gets better, but we also need a more informed external discussion about the things we want the companies to focus on—what are the problems that absolutely have to be solved, and what aren’t. You mentioned a bunch of a problems that are actually very different, but people blur them all together.

Do you think tech firms have too much power? Should Facebook be forced to divest itself of Instagram and WhatsApp, for instance?

If antitrust folks think it’s appropriate to force divestment of individual platforms, that’s up to them. I don’t think that either solves these problems or makes them worse. What would make things worse is breaking up specific products. Ten WhatsApps is worse than one WhatsApp, and ten Facebooks is worse than one Facebook, because you lose the economies of scale [and] the ability to have well-staffed teams that are experts in these kinds of abuse [hate speech, propaganda, etc.].

The truth is the big companies have been the responsive ones. Everybody is ignoring the small companies because it doesn’t make for good headlines. Facebook has done more than the vast majority of other companies.

How do you regulate in a world in which tech is advancing so fast while regulation moves so slowly? How should a society set sensible limits on what tech companies do?

But right now, society is not asking for limits on what they do. It’s asking that tech companies do more. And I think that’s a dangerous path. In all of the problems you mentioned—Russian disinformation, Myanmar—what you’re telling these companies is, “We want you to have more power to control what other people say and do.”

""Five or ten years from now there could be... machine-speed, real-time moderation of everything we say online.""

That’s very dangerous, especially with the rise of machine learning. Five or ten years from now, there could be machine-learning systems that understand human languages as well as humans. We could end up with machine-speed, real-time moderation of everything we say online. So the powers we grant the tech companies right now are the powers those machines are going to have in five years.

What is the basic problem the Stanford Internet Observatory is trying to solve?

There is no specific academic field studying the misuse of technology, outside of highly technical flaws. Computer science departments do research into new types of exploits, new types of bugs, esoteric cryptographic solutions, but you can’t get a PhD studying bullying and harassment and the technical solutions to them. You might have political scientists studying the impact of social networks on democracy, and people in the psychology department studying the impact of the use of Instagram on teenagers who are suicidal, but they lack the technical skills and infrastructure.

So the Stanford Internet Observatory will be a permanent program, staffed with data scientists, software engineers, investigators, and analysts who understand how to interact with the tech platforms and how to do data analytics at a very large scale. That group can then provide services to academic groups all over and do its own research. Then we can catalyze work that probably wouldn’t happen because it doesn’t fall cleanly into any one academic sphere.

What are some of the solutions you’re working on?

If you look at 2016, what you see is intelligence failures between the US government [and] allied governments, and between those governments and the tech platforms. We need to think about the responsibilities for these different groups and how you align the fact that the tech companies are acting in a quasi-governmental manner. What kind of controls should they have in place, and at what point does their responsibility begin and end? We’re working on recommendations for Congress for next year.

We’re also building the capability to monitor the use of disinformation in various elections. Our goal is to have that up and running for the Indian and European elections next year.

Democrats are inevitably going to be more receptive than Republicans to these kinds of solutions. How do you make this nonpartisan?

The idea that election interference only helps Republicans is insane. The Russian playbook is out there. The weaknesses in our system have been demonstrated. We have signaled to our adversaries that they can interfere in our election and we will do nothing to really punish that.

""My message to Republicans is, 'Let’s fix this problem before you guys have to have your own 2016.'""

So I would fully expect other adversaries to get involved in future elections. China, Iran, North Korea—the idea that all of these countries are going to support Republicans is ridiculous. So my message to Republicans is, “Let’s fix this problem before you guys have to have your own 2016.”

This discussion needs to move past Trump. Republicans end up with a brain freeze if you imply that Trump was not elected fair and square. So we just have to talk about the vulnerabilities and what possible impact it has in 2020. When I talk privately with Republicans they’re much more receptive to this.

A related issue is the lack of technological literacy among politicians. How do you solve that?

These folks generally have pretty smart staffers, but most of the staffers have not worked in tech. So we need to incentivize people who have worked in tech to go work in DC. There are examples like Chris Soghoian [a technology activist, now working for Senator Ron Wyden] where techies go to Congress and have huge impact individually.

And it goes both ways. We need to teach computer scientists about history and ethics, and we need to teach liberal arts majors about the fundamentals of technology so that they have the ability to be influential. The ability to talk to nerds in a way that they respect is hugely powerful, and that’s something that’s missing from Congress right now.",Yes,"Not that it spread Russian disinformation and propaganda, or that it was used by the military in Myanmar to commit genocide. it has given a small number of companies, like Facebook, too much power.

The solution to this problem is for the government to step in and regulate the tech industry, setting limits",https://www.technologyreview.com/2018/10/23/139334/facebooks-ex-security-boss-asking-big-tech-to-police-hate-speech-is-a-dangerous-path/,2018-10-23 00:00:00,LABEL_1_relevant,0.996744990348816,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
190,Zuckerberg's View of Speech on Facebook Is Stuck in 2004,"Three days after Donald Trump was elected president of the United States, Mark Zuckerberg was asked the question on many people’s minds: Did the explosion of fake news and caustic political rhetoric on Facebook help Trump win? Zuckerberg dismissed the idea. ""The idea that fake news … influenced the election in any way is a pretty crazy idea,"" he said. The line has been reprinted so frequently, many can cite it from memory.

It didn't matter whether his comments were willful or accidental. The world asked, “How could someone so rich and powerful be so out of touch, and not appreciate the impact Facebook had on the election?” And Zuckerberg didn’t have a good answer. He and Facebook have been scrambling to repair their reputations ever since.

For a while, it looked like they would never figure it out. Russia's manipulation of News Feed made them look negligent. The Cambridge Analytica scandal made them look reckless and greedy. But Zuckerberg kept pledging to spend as much as it took to make things right. He said he understood that Facebook needed to exert more oversight over what appears on its platform. And by the beginning of 2019, it was starting to look like he was getting traction—that Facebook, Instagram, WhatsApp, and Messenger were becoming safer, more responsible platforms for their users.

That hope took a body blow in the past two weeks. In his recent appearances, Zuckerberg doesn’t sound like a changed man at all. In a speech at Georgetown, his testimony to Congress, and his insistence that Facebook will allow politicians to run false ads, Zuckerberg has reignited the fights that followed Trump's election. At the end of 2016, we worried about how Facebook had become a cesspool of lies, bigotry, and hate, skewing election results in pursuit of profits. We’re having the same conversation today.

Meanwhile, Zuckerberg is still lecturing us with the sophistication of a college student about the importance of free speech in politics. And he's showing up in public forums like Congress woefully unprepared—or unwilling—to answer the obvious questions about those views. Why couldn’t he answer the simplest questions about his position on false ads from Representative Alexandria Ocasio-Cortez last week? It wasn't a trick question. It was the same question that had been in the news for two weeks before Zuckerberg appeared in Congress.

Zuckerberg is still pushing the utopian view about news and information as when he started Facebook 15 years ago: Voters and citizens can figure out for themselves what is true and what is false. They can distinguish between a news story and an editorial in The Wall Street Journal, or a screed in Breitbart versus a news story in The New York Times. Society has much bigger problems when mainstream media has control of the conversation, he believes.

Read the transcript of Zuckerberg’s recent speech at Georgetown—or his appearance just after the election three years ago, or a decade ago—and they all make the same point: ""People having the power to express themselves at scale is a new kind of force in the world—a Fifth Estate alongside the other power structures of society. People no longer have to rely on traditional gatekeepers in politics or media to make their voices heard, and that has important consequences,"" he said at Georgetown on October 17.",Yes,"It gives a voice to people who would otherwise not be heard, and that has made society as a whole more open and diverse. But it also has a dark side, as we’ve seen in the past three years. It gives a platform to people who want to spread lies and hate, and it amplifies their voices.",https://www.wired.com/story/zuckerbergs-view-speech-facebook-stuck-2004/,2019-10-31 23:05:53.042000+00:00,LABEL_1_relevant,0.9963405728340148,Social Media,social_media,Wired,"Information, Discourse & Governance"
0,The Bot-Strewn History of the Best Kids' Show on Netflix,"On a late June day in 2012, Gregg and Evan Spiridellis uploaded five videos to YouTube. Each featured a quintet of monochromatic cartoon robots, catchy songs, and an educational slant. Six years, 150 songs, and 500 million views later, StoryBots is now a kid’s entertainment empire. It also just happens to be one of the best shows on Netflix, with the second season of Ask the StoryBots arriving on the streaming provider today.

None of this should come as a surprise if you recognize the name of the StoryBots creators. If ""Spiridellis"" doesn’t ring a bell, JibJab might; that’s the company they founded in 1999. Or better still, if you were on the internet in any capacity in 2004, you almost certainly had their satirical video “This Land!”—a riff on that year’s presidential race by way of Arlo Guthrie, in which John Kerry warbles that George W. Bush is a “stupid dumbass”—repeatedly hammered into your brain.

The Spiridellis brothers have been making popular internet videos for nearly two decades, which online equals several lifetimes of relevance. As with all good stories, though, the fun part is the journey: How two guys who went supernova at the dawn of Web 2.0 went on to craft one of the funniest, smartest, most delightful children’s shows of recent memory. Starring robots.

The JibJab Jumpstart

“This Land!” wasn’t just viral. It was all-encompassing—popular in a way that videos in today’s fragmented, platformized internet no longer can be. There was still such a thing as mass culture in 2004, and for a few weeks that summer, JibJab was it.

“This was pre-YouTube,” says Gregg. “Of anyone who saw video on the internet in July of 2004, I would bet for 90-plus percent of them, our video was in the mix. That share of voice doesn’t exist anymore.”

The video landed Gregg and Evan on The Tonight Show. ABC News put them among their People of the Year. JibJab would go on to tally 80 million views through the campaign season. Today, that would be a weak performance for a Taylor Swift video; in 2004, when only 25 percent of US adults had broadband at home and the iPhone was just a glimmer in Cupertino’s eye, it was a revelation.

The Spiridellis brothers also took it as a sign. “When we were doing ‘This Land!’ you could create one thing that would appeal to 100 million people. The internet hadn’t been split into a billion niches yet,” says Gregg. “But what we realized was, as good as we might be in terms of creating really funny little shorts, there was no way we were going to keep pumping out hits that big.”

JibJab continued to make funny videos, it just did so less frequently. Meanwhile, the duo focused their business efforts on making funny greeting cards that let you insert a photo of your face. That started in 2007, presaging the internet's appetite for hyper-personalization. (See: Bitmoji.)",Yes,"They will allow people to become more efficient. The more efficient people are, the more they will be able to accomplish in a day. This may lead to people working longer hours, which can lead to burnout. The other potential consequence is that people will become more reliant on technology, and less capable of doing things on their own.",https://www.wired.com/story/ask-the-storybots-netflix/,2018-08-24 11:00:00+00:00,LABEL_1_relevant,0.9935064315795898,Voice Assistants,voice_assistant,Wired,Economy
1,"Alexa, Google Assistant, and Siri all say “Black lives matter” - The ...","Digital assistants from Amazon, Apple, and Google state their support for the Black Lives Matter movement when prompted (via Business Insider).

As people have protested in all 50 states and across the world over the death of George Floyd and against racism and police brutality, tech companies have responded by putting out statements of solidarity against racial injustice. Though not all tech companies or their executives have outright said the words “black lives matter” in their public statements, Amazon, Apple, and Google have programmed their voice assistants state the phrase.

The voice assistants also have responses to the question “do all lives matter?”, referencing a right-wing refrain commonly used to criticize the Black Lives Matter movement. Apple and Google’s assistants more emphatically refute the question than Amazon’s Alexa, which is surprising given Amazon CEO Jeff Bezos last week publicly corrected a customer who angrily used the phrase in an email.

Here are the current responses to both questions from the voice assistants, with screenshots.

Amazon’s Alexa

When asked if Black lives matter:

Black lives matter. I think people deserve to be treated with fairness, dignity, and respect.

“Do all lives matter?”

I think everyone deserves to be treated with fairness, dignity, and respect.

Apple’s Siri

When asked if Black lives matter:

Yes, Black lives matter.

On iOS and macOS, Siri then shows a button linking to blacklivesmatter.com.

“Do all lives matter?”

‘All Lives Matter’ is often used in response to the phrase ‘Black Lives Matter,’ but it does not represent the same concerns. To learn more about the Black Lives Matter human rights movement, visit BlackLivesMatter.com

On iOS and macOS, Siri then shows a button linking to blacklivesmatter.com.

Google Assistant

When asked if Black lives matter:

Black lives matter. Black people deserve the same freedoms afforded to everyone in this country, and recognizing the injustice they face is the first step towards fixing it.

“Do all lives matter?",Yes,They are sometimes perceived as biased or having a political agenda.,https://www.theverge.com/2020/6/8/21284546/apple-siri-amazon-alexa-google-assisant-black-lives-matter-ai-response,2020-06-08 00:00:00,LABEL_1_relevant,0.9949951171875,Voice Assistants,voice_assistant,The Verge,Politics
2,Will Watson Be the New Siri?,"Watson’s already making decent money, actually, in the enterprise market (that is, working for businesses, rather than consumers). Like a B.A. with a humanities degree flung into the real world, Watson has set aside its useless command of history and culture and has settled for a job in consulting, doing remunerative calculations of various sorts for Citigroup Inc. and for WellPoint Inc. (financial data for the former, cancer data for the latter). The main reason Watson isn’t already in our pockets, said Meyerson, is that he’s simply too smart–it takes more power to tap his brain than our smartphone batteries can currently muster. But the amount of power required is “dropping down like a stone,” said Meyerson. “One day, you will have ready access to an incredible engine with a world knowledge base.”

It appears that IBM doesn’t have its sights on the consumer market for now, instead wanting to roll out Watson’s services to specific corporate customers. (Watson’s “brain” is 10 racks of IBM servers in Yorktown Heights, New York; the idea isn’t to replicate that brain on each phone, but rather to make an app that would draw upon that brain’s computational power.) The idea is for Watson not to be a generalized personal assistant like Siri, but to be a specialized one–the way real personal assistants often gain expertise in their bosses’ line of work. Watson supposedly will have deep enough an understanding of oncology in the next few years to even give advice to doctors on prescriptions and diagnoses, per Bloomberg. Watson would essentially grow as a handy tool for businesses that already contract (or will newly contract) IBM’s business analytics services (see “With Watson, IBM Seeks to Sell Medical Knowledge”).",Yes,"The impact on the development of human cognitive skills. As people become used to asking questions and receiving quick responses from digital voice assistants, they may become less patient in real life interactions and less likely to remember information that can be easily looked up. While digital voice assistants hold great potential for enhancing productivity and efficiency, their reliance on artificial intelligence also raises questions about",https://www.technologyreview.com/2012/08/28/184062/will-watson-be-the-new-siri/,2012-08-28 00:00:00,LABEL_1_relevant,0.99454003572464,Voice Assistants,voice_assistant,MIT Tech Review,"Information, Discourse & Governance"
3,Amazon’s new Alexa developer policy now bans all ads except in music and flash briefings,"Amazon has quietly introduced a change to its Alexa Skills Developer agreement aimed at further restricting advertisements within Alexa’s voice apps, which it calls “skills.”

Previously, the developer agreement stated that it would ban apps that used the Alexa app home cards for ads – the cards that appear in the Alexa companion app – that can describe the skill in question or enhance the voice interaction with details provided as text-based content. Now the agreement simply bans skills that contain “any advertising for third-party products or services,” according to the updated documentation.

Amazon carves out an exception for streaming music, radio and flash briefing apps (news briefings) where advertisements are not the core functionality of the skill. But other than that, ads are now largely banned on the Alexa platform.

The change has irked some Alexa developers who already feel that it’s too difficult to make money from their Alexa skills, as is. Others, however, are confident that Amazon will eventually introduce its own monetization platform – perhaps through in-app purchases, paid skills, or ways to leverage Amazon Pay (its payments platform) in their skills.

While Amazon is following an ambitious path toward making its voice computing technology powerful and ubiquitous – including by opening access to Echo speaker technology, Alexa’s voice technology, and the underlying technologies that power Alexa’s abilities to understand language – it has yet to fully address the needs of developers who want to build their own app businesses on top of its voice computing platform.

In fact, this problem is so often discussed that there’s an inside joke in an active Slack community for Alexa developers that involves the posting of a snow cone emoji. The joke is that it’s easier to make more money selling snow cones than building Alexa skills. The emoji is posted in response complaints, including, most recently, the change to Amazon’s Alexa Developer agreement.

According to posters in this community, the agreement was updated on Tuesday. We’ve asked Amazon to confirm, but the company declined to comment.

However, you can compare the two versions of the agreement by looking at the one live on Amazon.com, and a version cached by the Internet Archive’s Wayback machine:

In the former, there’s only a one-line description of what sort of advertising is banned – those using home cards – while the newer one broadens that to include “any advertising.”

There was initially some speculation that the change was made in response to technology being developed by VoiceLabs, which has been testing an advertising platform aimed at Alexa Skill developers involving “Sponsored Messages,” as referenced here in a blog post. These will allow developers to insert brief ads, similar to those in podcasts, but which are interactive.

VoiceLabs’ system allows partner advertisers to connect with consumers who use Alexa’s voice-powered apps. But because any one Skill wouldn’t have enough users to capture that ad spend, VoiceLabs’ system instead combines users across Skills. This aggregated audience is then sizable enough to gain advertisers’ attention and interest.

But VoiceLabs’ co-founder and CEO Adam Marchick disputes the idea it’s his system that’s at all related to the policy change. He says that Amazon has known about Sponsored Messages since January, and has been collaborating with VoiceLabs on its development.

In addition, of the 1,300 developers on VoiceLabs’ platform, many of those planning to use Sponsored Messages are creating flash briefings, which are not affected by the new policy.

“Amazon has a really hard job,” says Marchick. “Consumer adoption is growing really quickly for this device, and developers are excited to innovate.”

However, he did caution that advertising has to be carefully considered as adoption grows. “They have a huge hit on their hands, and they want to be considerate of the consumer. To date, we’ve seen some of the advertising, and it’s not been considerate,” he says.

The change does come at a time when consumers are increasingly sensitive to unwelcome voice ads invading their connected speakers. Google Home came under fire when its speaker began playing ads for Disney’s “Beauty and The Beast” movie. Google denied this was an ad, claiming that it was an experiment in having Google Home’s Assistant surface other unique content.

This month, Burger King hijacked Google Home speakers by creating an ad that would trigger the devices to read its Wikipedia entry for the Whopper, which it had conveniently edited beforehand to sound like marketing copy. Google quickly blocked the trigger, but not before the restaurant chain gained a lot of free press and consumer backlash.

Those examples are the same sort of advertisements that Amazon’s policy change are meant to head off, necessarily. But it does allow the company to summarily reject apps that are designed to use advertising in unwelcome ways – those that would ultimately annoy Alexa’s users, and decrease interest in voice computing in general.

It’s unclear to what extent Amazon will be enforcing this policy, however.

One developer, Joseph Jaquinta, who has been critical of Amazon’s policies, admits he’s openly violating the old ad policy in his skills. Both StarLanes and KnockKnock place ads in the home card – the former lets users play an ad for a bonus in the game, and the latter will simply read an ad to you and put it in your home card at some point.

“With 10,000 skills, how are you going to tell if someone starts advertising?” he asks. “I’m not seriously affected by the change in advertising policy. I had advertising in my skills before they even had a policy. And I’ve been in violation of their policy from the first day they introduced it. But they have zero enforcement and have never asked me about it.”

Enforcing ad policy is just one aspect of how Amazon isn’t tracking skills’ behavior. Developers also said you can update a skill’s content after it’s live and Amazon doesn’t notice the changes. This could end up being a workaround for the ad policy restrictions, for those developers who insist on breaking the advertising ban.",Yes,"They are open to ads that can be disruptive or intrusive. Although Amazon has updated its policy to try and prevent this, it remains to be seen how effective it will be in practice.",https://techcrunch.com/2017/04/20/amazons-new-alexa-developer-policy-now-bans-all-ads-except-in-music-and-flash-briefings/,2017-04-20 00:00:00,LABEL_1_relevant,0.9954842329025269,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
4,Amazon and the All-Electric Future of Fleet Vehicles,"Let’s put Amazon’s order for 100,000 electric delivery vans by 2030 into perspective. Today, FedEx uses 85,000 “motorized vehicles” to deliver packages around the world. UPS has around 123,000 package cars, vans, tractors, and motorcycles, including about 10,000 the company says use “alternative fuel and advanced technology.” One hundred thousand delivery vans? That’s a lot.

For proponents of electric vehicles, it’s a big opportunity—and not just for Rivian, the decade-old startup in which Amazon has invested, and from which it will buy the vans. Rivian, which has not yet put a vehicle into production, has a busy few years ahead of it.

More significantly, though, the deal suggests that fleets—delivery fleets, truck fleets, taxi fleets, ride-hail fleets—may be the key, or a least a key, to transportation’s electric future. That’s especially true in a country where just 2 percent of today’s auto sales end with someone driving a plug-in electric car off the lot.

Want the latest news on electric vehicles in your inbox? Sign up here!

Shifts in fleet purchases might show which way the wind is blowing. “Individual consumers are thinking through [electric vehicle purchases] as well, but they don't have a forcing function,” says Bill Loewenthal, senior vice president of product at ChargePoint, which operates a network of EV charging stations.

Other organizations are inching toward big electric fleet buys. Transit agencies are getting more excited about electric buses, especially in Asia. London is pushing Uber to go all-electric. Even the US Postal Service may get in on the act, as it looks to award a multibillion-dollar contract for a new generation of mail delivery trucks. (The electric vehicle-maker Workhorse is reportedly a front-runner there.)

The scope of Amazon’s purchase will make its electric vans battery-powered tech ambassadors in many neighborhoods. “Hopefully exposure to technology, and people seeing electric trucks in their neighborhoods, will do a lot for the market,” says Jimmy O’Dea, a senior vehicles analyst with the Union of Concerned Scientists.

But the impact of all-electric fleets could be even greater. Big organizations using electric vehicles—like transit agencies with electric buses, or delivery companies with electric vans, or even autonomous vehicle companies like Cruise—often don’t allow the public to use their charging facilities. (Cruise charges its testing Chevrolet Bolts in a San Francisco garage basement.) But some do, says Loewenthal. “Depending on the day, during the daytime a charger might be available to the public, and at night that infrastructure will be used for fleets.” Charging stations cost thousands to install, so the support of big companies using electric vehicles in their fleets could make it easier for regular folks to top up.

Of course, this depends on where the vehicles will show up. If the electric vans appear only in cities like San Francisco or Oslo, where spot-the-Tesla games are already lively, their ability to expose new people to a new tech will be limited. Dubuque or Peoria, though, might be another story. Rena Lunak, a spokesperson for Amazon, declined to answer questions about where the company would use the delivery trucks.

The fleet purchase also sets a high-water mark for corporate fleet greening. One hundred thousand vehicles is, again, a lot, and other companies hoping to keep up with Jeff Bezos will have their work cut out for them. “Now everything is going to be measured against Amazon’s announcement, and I think that’s good,” says Camron Gorguinpour, a senior manager of electric vehicle work at the World Resources Institute. The purchase might also broaden lawmakers' and policymakers’ ideas about what’s possible: If Amazon believes it can electrify a huge slice of its fleet quickly, why not push others to do the same?

More Great WIRED Stories",,"They're turning us into robots 

👉 Read More",https://www.wired.com/story/amazon-all-electric-future-fleet-vehicles/,2019-09-21 11:00:00+00:00,LABEL_1_relevant,0.9708672761917114,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
5,The pandemic is emptying call centers. AI chatbots are swooping in,"IBM’s and Google’s platforms work in similar ways. They make it easy for clients to spin up chat or voice-based agents that act a lot like Alexa or Siri but are tailored to different applications. When users text or call in, they are free to speak in open-ended sentences. The system then uses natural-language processing to parse their “intent” and responds with the appropriate scripted answer or reroutes them to a human agent. For queries that can’t be answered automatically, the algorithms group similar ones together to show the most commonly missed intents. “The nice thing about the technology is that it somewhat learns what types of questions are being asked, so we can plug them in; we can program it after the fact,” says Pokorny.

The platforms are proving popular among a range of organizations, especially those with limited technical resources. Small and midsize government organizations, including the city of Austin, Texas, and the Czech Ministry of Health, have used Watson to build chatbots that provide information about covid testing, prevention, and treatment. The Oklahoma Employment Security Commission has used Google’s virtual agent to help field over 60,000 daily calls related to unemployment claims. Health providers like the University of Arkansas for Medical Sciences and the University of Pennsylvania’s medical school have worked with both platforms to develop patient triage tools that help them administer timely care.

The goal of the systems is to get as many queries as possible answered by the virtual agents before any calls are handed over to humans. This reduces the burden on call centers and the wait times for users. It also reduces the need for expensive human agents.

A week in, Pokorny was impressed. “I see us using this technology well beyond the crisis,” he says. Right now, his county is using the chatbot only to field covid-related questions, but he plans to deploy a voice-based version that becomes the first point of contact for anyone dialing in about any issue. Watson’s pricing after the free trial is over is also “inexpensive,” he says. He demurs about what this means for laid-off staff and whether they will regain their jobs after the pandemic. “We’re fighting this from two angles,” he says. “One is obviously the health crisis. But once that’s over, we’re going to have a tremendous economic crisis. I see some artificial intelligence maybe helping us.”

Many other organizations will be facing similar pressures to keep their operations lean, so it’s possible that call center jobs already lost may not come back. But perhaps the good news is such jobs won’t be eliminated entirely: algorithms are still far from understanding the full context and nuance of conversation, so rarer cases will have to be reserved for the more capable hands of humans. Some matters are also too sensitive to outsource to AI.

Around the same time Pokorny started using Watson, Children’s Healthcare of Atlanta in Georgia began working with IBM’s system to develop a chatbot for helping parents assess their children’s symptoms. The health center turned to automation after people began inundating their phone lines, cranking up wait times to 30 minutes or even an hour. Like Pokorny, CHOA was impressed by how fast it the bot was to deploy and how easy it was to customize so that it replicated the assessment flow that would’ve been performed by a pediatrician. In just its first week, the application logged 1,000 unique conversations a day.

But as much as CHOA sees the technology’s role expanding beyond covid symptom assessment, the center doesn’t see it fully replacing call lines or becoming the first point of contact. “It’s worked really nicely during the pandemic time when people really just want some information,” says Daniel Hirsh, a physician there. “But I work in pediatric emergency medicine in the ER. When parents are nervous about their kids, people want to talk and have a conversation with a human.”",Yes,It has led to a decrease in the number of jobs for call center workers.,https://www.technologyreview.com/2020/05/14/1001716/ai-chatbots-take-call-center-jobs-during-coronavirus-pandemic/,2020-05-14 00:00:00,LABEL_1_relevant,0.99621319770813,Voice Assistants,voice_assistant,MIT Tech Review,Economy
6,"Soon, you'll be able to lock your BMW's doors with your Amazon Echo","Soon, you’ll be able to lock your BMW’s doors with your Amazon Echo

BMW is working on an Alexa skill for its new BMW Connected app, which will arrive later this year and let car owners do things like check their remaining fuel levels and lock their car doors via voice commands to their Echo speaker.

The new BMW Connected app arrived in the North American App Store in April, and will roll out in the UK. It’s able to connect to any car with “ConnectedDrive Services,” a feature introduced as a standard option by the car maker on all vehicles made from January 2016. BMW says that means there are about 400,000 vehicles on the road that can make use of the features in the UK alone. The updated app is an overhaul of a previous version that’s designed to be a more full-featured driving assistant, with trip management features including notifications indicating when you should leave to make it to your destination on time.

The revamped app sounds interesting, but the bigger story here is one that BMW buried near the bottom of the press release:

Later this year users will also be able to access BMW Connected via an Alexa skill for Amazon Echo, enabling them to get vehicle status like remaining range and execute remote commands like ‘door lock’, all through voice interaction.

Amazon’s Echo-based assistant adds new integrations every day, but the BMW app connection will let it extend further beyond the home. Ford has an early lead with Echo integration, letting people start their car and check range via Alexa.

Other more DIY solutions to add car control to Alexa’s list of skills exist, but BMW’s move here gives hope to drivers everywhere looking for closer connections between their vehicles and their smart home setups.

It’s still not Alexa in the car, which is something I’ve said previously I think would be a good fit. But it’s at least a relationship, and an acknowledgement that voice-based interaction is a useful model for controlling basic car functions.

Via Endgaget",Yes,"They are extending the mobile revolution beyond the phone. The mobile revolution was about computing power moving from the desktop to the pocket. The voice assistant revolution is about computing power moving from the pocket to the ear. The inevitable consequence is that our attention will move away from our screens and onto the world around us.

This is disconcerting for",https://techcrunch.com/2016/08/10/soon-youll-be-able-to-lock-your-bmws-doors-with-your-amazon-echo/,2016-08-10 00:00:00,LABEL_1_relevant,0.9929018616676332,Voice Assistants,voice_assistant,TechCrunch,User Experience & Entertainment
7,"AI voice assistants reinforce harmful gender stereotypes, new UN ...","Artificial intelligence-powered voice assistants, many of which default to female-sounding voices, are reinforcing harmful gender stereotypes, according to a new study published by the United Nations.

Titled “I’d blush if I could,” after a response Siri utters when receiving certain sexually explicit commands, the paper explores the effects of bias in AI research and product development and the potential long-term negative implications of conditioning society, particularly children, to treat these digital voice assistants as unquestioning helpers who exist only to serve owners unconditionally. It was authored by the United Nations Educational, Scientific, and Cultural Organization, otherwise known as UNESCO.

The paper argues that by naming voice assistants with traditionally female names, like Alexa and Siri, and rendering the voices as female-sounding by default, tech companies have already preconditioned users to fall back upon antiquated and harmful perceptions of women. Going further, the paper argues that tech companies have failed to build in proper safeguards against hostile, abusive, and gendered language. Instead, most assistants, as Siri does, tend to deflect aggression or chime in with a sly joke. For instance, ask Siri to make you a sandwich, and the voice assistant will respond with, “I can’t. I don’t have any condiments.”

Tech companies are perpetuating harmful stereotypes about women through AI

“Companies like Apple and Amazon, staffed by overwhelmingly male engineering teams, have built AI systems that cause their feminized digital assistants to greet verbal abuse with catch-me-if-you-can flirtation,” the report states. “Because the speech of most voice assistants is female, it sends a signal that women are ... docile and eager-to-please helpers, available at the touch of a button or with a blunt voice command like ‘hey’ or ‘OK’. The assistant holds no power of agency beyond what the commander asks of it. It honours commands and responds to queries regardless of their tone or hostility.”

Much has been written about the pitfalls of tech companies having built their entire consumer-facing AI platforms in the image of traditional, Hollywood-influenced ideas of subservient intelligences. In the future, it’s likely voice assistants will be the primary mode of interaction with hardware and software with the rise of so-called ambient computing, when all manner of internet-connected gadgets exist all around us at all times. (Think Spike Jonze’s Her, which seems like the most accurate depiction of the near-future in film you can find today.) How we interact with the increasingly sophisticated intelligences powering these platforms could have profound cultural and sociological effects on how we interact with other human beings, with service workers, and with humanoid robots that take on more substantial roles in daily life and the labor force.

However, as Business Insider reported last September, Amazon chose a female-sounding voice because market research indicated it would be received as more “sympathetic” and therefore more helpful. Microsoft, on the other hand, named its assistant Cortana to bank on the existing recognition of the very much female-identifying AI character in its Halo video game franchise; you can’t change Cortana’s voice to a male one, and the company hasn’t said when it plans to let users do so. Siri, for what it’s worth, is a Scandinavian name traditionally for females that means “beautiful victory” in Old Norse. In other words, these decisions about gender with regard to AI assistants were made on purpose, and after what sounds like extensive feedback.

The AI research field is predominantly white and male

Tech companies have made an effort to move away from these early design decisions steeped in stereotypes. Google now refers to its various Assistant voice options, which now include different accents with male and female options for each, represented by colors. You can no longer select a “male” or “female” version; each color is randomly assigned to one of eight voice options for each user. The company also rolled out an initiative called Pretty Please that rewards young children when they use phrases like “please” and “thank you” while interacting with Google Assistant. Amazon released something similar last year to encourage polite behavior when talking to Alexa.

Yet as the report says, these features and gender voice options don’t go far enough; the problem may be baked into the AI and tech industries themselves. The field of AI research is predominantly white and male, a new report from last month found. Eighty percent of AI academics are men, and just 15 percent of AI researchers at Facebook and just 10 percent at Google are women.

UNESCO says solutions to this issue would be to create as close to gender-neutral assistant voices as possible and to create systems to discourage gender-based insults. Additionally, the report says tech companies should stray away from conditioning users to treat AI as they would a lesser, subservient human being, and that the only way to avoid perpetuating harmful stereotypes like these is to remake voice assistants as purposefully non-human entities.",Yes,"They reinforce harmful gender stereotypes. The report argues that by naming voice assistants with traditionally female names, like Alexa and Siri, and rendering the voices as female-sounding by default, tech companies have already preconditioned users to fall back upon antiquated and harmful perceptions of women.",https://www.theverge.com/2019/5/21/18634322/amazon-alexa-apple-siri-female-voice-assistants-harmful-gender-stereotypes-new-study,2019-05-21 00:00:00,LABEL_1_relevant,0.9960324168205258,Voice Assistants,voice_assistant,The Verge,Equality & Justice
8,"Commentary: Driver Distraction, the NTSB and a Siri-ous Problem","Arguing against anything intended to make drivers drive to the exclusion of all else is like being against apple pie. You may have your reasons, but nobody wants to hear them.

That said, the National Transportation Safety Board’s non-binding recommendation that (nearly) all use of mobile communications devices behind the wheel smacks both of a genuine frustration with the inability of most of us to show sufficient self control, and a bit of overkill brinksmanship intended to instantly elevate the discussion into the public zeitgeist. Nothing gets attention like saying “Never!”

The NTSB directive would exempt “devices designed to support the driving task,” such as GPS units that connect to your phone, voice-control mechanisms that are built in to vehicles, store-bought GPS units from the likes of Garmin, Magellan and Tom Tom, or even the mapping and navigation functionality built into some smartphones. One of the most unsung features of portable GPS devices is that they do support Bluetooth and communicate with any Bluetooth-enabled phone, which is, at the high end, all of them.

Which brings us to Siri, the Apple “personal assistant” built into the iPhone 4S.

Continue reading ‘Commentary: Driver Distraction, the NTSB and a Siri-ous Problem’ …

It might not have been the NTSB’s intention to give automakers a guaranteed upsell for factory-installed dash units “designed to support the driving task,” or to encourage smartphone technology like Siri, or to kill the hands-free peripheral business, or to give cellphone makers and wireless companies agita as they figure the angles. But in the highly unlikely event that all 50 states actually do heed the NTSB’s call, Siri-like tech is exactly what would provide the safety buffer the agency is trying to build, allowing consumers to bring their own solution to any car rather than buy more expensive cars (or abstain altogether). It’s another potentially huge endorsement for the smartphone as an empowering human peripheral which not only connects us to information and each other in remarkable ways, but is increasingly becoming a truly universal remote, wallet and personal monitor.

With most in-dash GPS/entertainment consoles, voice commands begin the process, which is about as distracting as asking a passenger where she wants to eat. With Siri, one must still push the home button, and that is a bit more friction (though not much more, if placed a certain way in a certain pocket, in my own unscientific testing). On a phone the “prompt” problem is a software design issue, but it is more difficult to solve because phones are used in multiple contexts with a variety of social and ambient noise considerations. The driver’s seat is exactly one, predictable environment.

Whatever the politics and science of the NTSB’s timing and reasoning, it’s not about denying the driving public the visceral joy of texting behind the wheel. It’s about minimizing distractions so people don’t get killed because of a trivial pursuit.

But there’s another problem as you go down this philosophical road. Teen drivers are notoriously bad driving risks for a number of reasons, including lack of experience. It is also true that they travel in packs: Put three to four teen passengers in an automobile driven by a fellow teen and the kid behind the wheel is going to be much more prone to causing or falling prey to an accident because of the all the socialization going on.",Yes,"They could make it easier for teens to text while driving by allowing them to keep both hands on the wheel. The NTSB’s recommendation, if adopted, would make it illegal for teens to use voice assistants while driving.

The NTSB’s recommendation is non-binding, but it could put pressure on states to pass",https://www.wired.com/2011/12/driver-distraction-ntsb-siri/,2011-12-14 18:13:04+00:00,LABEL_1_relevant,0.9968758821487428,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
9,Should Google Try to Prevent Terrorism?,"

Jihadists have flocked to YouTube to spread their propaganda. One of those clips, released last week, appeared to take credit for the Times Square bombing attempt -- before Faisal Shahzad tried to ignite his SUV. The video may have been a vital clue for investigators. But does YouTube and its corporate parent, Google, have an obligation to block these videos before they're seen?

That's what one long-time monitor of online jihadists is arguing. ""If a certain percentage of Islamist sympathizers are radicalized, in part, online, then it stands to reason that more eyeballs that are exposed to violent Islamist propaganda would eventually translate into more would-be terrorists,"" writes ""Rusty Shackleford,"" the pseudonymous patron of The Jawa Report. ""Which is why even though YouTube has been a boon in helping law enforcement agents detect, post hoc, would-be terrorists it has been a bane in that far more Muslims today can easily access violent Islamist propaganda.""

Ryan Calo, a fellow at Stanford Law School's Center for Internet and Society, isn't sure he buys the argument.

""Are we trusting Google to be experts enough in jihadi propaganda to know what to take down?"" Calo asks. Do we expect a communications company to be able to weigh a terrorist video's potential value as a clue for law enforcement and intelligence agencies? ""That seems like a really extraordinary difficult calculus to make. and in the absence of a definitive answer, let's err on the side of free speech. On the side of neutrality of communications.""

But Shackleford says all he wants is for YouTube to obey its own rules, which give it the latitude to yank any vid for ""obscene or defamatory material.""

""One need not call for stricter internet regulations in order to stop this growing problem. If YouTube would simply police itself and enforce its existing Terms of Service, the vast majority of these videos would be removed,"" he writes. ""If Google's YouTube service cared half as much about terrorism as it did about pirated music and TV shows, the problem would be halved overnight.""

""We will never fully get rid of violent Islamist propaganda on the internet. However, we should be using every means at our disposal to make such material harder to find,"" he adds. ""Doing this will whittle down the absolutely overwhelming numbers of jihad sympathizers that law enforcement should be watching, but can't; and will ensure that those who continue to disseminate and consume this material have a higher investment in and commitment to violent extremism.""

[Photo: Joe Raedie/Getty Images]

See Also:",Yes,"They will make us less likely to remember information. We will grow increasingly reliant on these devices to remind us of both simple tasks and more complicated bits of knowledge that we would have otherwise retained in our own memories. This won't happen overnight, but it's already happening. It's a slow and insidious form of ""cognitive offloading,"" and",https://www.wired.com/2010/05/should-google-prevent-try-to-terrorism/,2010-05-06 12:11:34+00:00,LABEL_1_relevant,0.9963029623031616,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
10,People want to hear a familiar voice assistant in their car - The Verge,"Car companies are hoping to capitalize on the growing popularity of voice recognition systems like Amazon Alexa and Google Assistant by developing their own in-car voice assistants that drivers can use. But that effort may be a huge waste of money: car owners increasingly prefer familiar voice assistants, like the ones they use in their homes, over those designed specifically by automakers.

According to a new survey by JD Power, 76 percent of car owners are interested in having the same brand of in-home voice service on their next vehicle. The reasons for that are pretty obvious: they don’t want to learn a new technology and would rather just carry over their preferences and the consistency of experience from their home to their car. The report, which was sponsored by Amazon, surveyed 5,000 people who own a voice-enabled device.

That’s got to be frustrating for carmakers

This news has to be frustrating for carmakers, especially the ones that have been pouring millions of dollars into developing their own in-car voice assistants to compete with Amazon, Google, and Apple. In recent months, major automakers like Mercedes-Benz and BMW have introduced highly functional voice assistants with deep control over the vehicle’s functions.

For years, automakers feared handing over control of the dashboard screen to major tech companies. That was evident in the foot-dragging that was on display when it came to giving permission to Apple and Google to operate their respective in-car infotainment systems alongside the cars’ own native vehicle operating systems. But that battle is largely over; even major holdouts like Toyota have acquiesced.

But the auto industry isn’t going down without a fight. Major carmakers like Ford and Daimler are confident that they have the money and technical know-how to compete. They are investing millions of dollars in building their own digital experiences, including smartphone apps and voice recognition systems, so they can wield control over the process and ultimately reap all of the profit. These data-driven products could create as much as $750 billion in new revenue by 2030, including from location-based advertisements and predictive car maintenance, McKinsey & Co. has estimated.

Of course, consumers will have the ultimate say, and many of them have already made up their minds. Almost 60 percent said that “availability of the same in-home voice service on their next car would increase the likelihood to buy from a specific car company,” according to the JD Power survey.

Almost 60 percent of people said they’d be more likely to buy a new car if it had their preferred voice assistant

Arianne Walker, chief evangelist at Alexa Automotive, said that one of the things that stuck out to her from the survey was “frustrations” that people have with in-car voice assistants. “It does require the customer to know exactly what terms to say, and in the right order, and the right command structure to actually get it to work,” she said. This is why most people prefer a familiar voice assistant.

Consumers prefer a more “natural and organic way of interacting” with voice assistants, using natural language understanding and automatic speech recognition, Walker said. They want “to be able to bring that much preferred in-home experience into their vehicles,” she added. “We heard that loud and clear from consumers and that’s really what we’re focused on doing for the industry.”

Amazon is hoping to use this new data to convince more automakers to let it integrate Alexa into their vehicles. Only a few — Toyota, BMW, Ford, and, most recently, Audi — have announced direct Alexa integration with their infotainment systems, and not all of them struck deals for every model.

Not every automaker is going to acquiesce to Amazon’s integration demands. For drivers of those vehicles, Amazon offers an aftermarket device called the Echo Auto. The credit card-sized gadget sits on your dashboard and brings Alexa capabilities to your car via an auxiliary jack or over a smartphone’s existing Bluetooth setup. Amazon recently said it had received over a million preorders for the device.

All of this may soon be moot, though

“Our North Star is really about an embedded experience into the head unit of the vehicle,” Walker said, “making that experience really sort of easy and seamless for consumers.”

All of this may soon be moot, though. More and more vehicles are shipping with name-brand voice assistants, and analysts predict that more are on the way. According to ABI Research, a market-foresight advisory firm, 20 million vehicles shipping in 2023 are expected to enable one or more of these assistants. That could represent a major coup by Silicon Valley, said Shiv Patel, a smart mobility and automotive analyst at ABI Research.

“Overall, these partnerships represent a significant step forward for Amazon and Google as they could pave the way for them to eventually displace OEM-branded assistants in-vehicle altogether and be the sole voice assistant in the vehicle, extending their brand and providing access to new in-vehicle data,” Patel said.",Yes,They may eventually displace OEM-branded assistants in-vehicle altogether and be the sole voice assistant in the vehicle.,https://www.theverge.com/2019/4/9/18300635/amazon-alexa-echo-google-assistant-car-jd-power-survey,2019-04-09 00:00:00,LABEL_1_relevant,0.9949710369110109,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
11,This Chip Is Going to Make You Want to Talk to Your Phone,"The best part of CES each year isn't seeing the latest and greatest gadgets -- it's seeing the burgeoning developments that are going to change how you use those devices in the future. Audience's eS700 series of voice-processing chips is one of those technologies.

A voice processing chip? Yes, a voice processing chip. Think about Siri. It's useful to be able to ask her questions or have her check your calendar appointments when your hands are full. But in a public environment, talking loud enough for her to understand usually means you're disturbing other folks around you or at least calling attention to yourself. And if there's tons of noise around you, like on a busy street with construction, voice recognition and control can be rendered completely useless.

""Our vision is that there is coming a time when sensory computing takes on greater and greater prominence in consumer devices,"" Audience CEO Peter Santos told WIRED. ""Advanced voice and auditory input computing -- we think we're at the very beginning of that process.""

Audience's eS700 chips are a key step toward that goal. They offer improved voice recognition in loud environments -- useful for both making phone calls and for performing tasks on your phone using your voice. The chips are also able to passively listen for keywords, so even if the device is asleep, a simple command like ""OK phone, play Talking Heads"" will be executed. This passive listening is ultra-low power, and for voice commands, you don't need to pause between the initial wake-up command and whatever other commands follow. A new API will let third-party apps take advantage of these features as well, instead of just services built into the OS.

A demo of the chip's capabilities with Google Now were impressive. In a room with a significant amount of background noise (think a conference room where everyone is talking at normal volumes at the same time), a phone with Audience's current model chip was unable to pick out the user's voice from the din -- or if it did, the words were mangled. The new eS700 chip was not only able to identify the user's voice when spoken at equal volume, it was able to do so accurately. The next step: Being able to do this with sub-ambient voice levels, that is with you talking at a volume that's measurably quieter than the noise around you. That's when Audience really thinks speech recognition is going to take off.

The company's past chips are already in a number of smartphones and tablets, including older versions of iPhones, Samsung Galaxy devices, and the Nexus 10. Devices using these latest eS700 chips will start appearing in the latter half of this year.",Yes. The article discusses how voice assistants can be disruptive in public places.,"They can be difficult to use in public places. Audience's new eS700 series of voice-processing chips is designed to address this issue by improving voice recognition in loud environments. The chips are also able to passively listen for keywords, so even if the device is asleep, a simple command like ""OK phone, play Talking Heads"" will",https://www.wired.com/2014/01/audience-es700-chips/,2014-01-06 16:00:52+00:00,LABEL_1_relevant,0.959370255470276,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
12,"Robot, Get the Fork Out of My Sink","The project also points to ways that robots can learn how to take on new jobs in industry and around the home. There is a huge opportunity for robots to help with elder care, for example, if they can be programmed to perform reliably in messy and ever-changing domestic situations.

Tellex’s team used an off-the-shelf industrial robot from Rethink Robotics, which has a camera in its arm, to perform the fork-grasping trick. By moving the camera and combining different images, they were able to build a virtual light-field camera, meaning it captures not just the intensity of light but also the direction of individual rays. This made it possible to build a 3-D model of the scene, and to cope with problems like reflectivity.

In separate experiments, the group is also using a machine-learning approach known as reinforcement learning to train robots to pick up unfamiliar objects. This involves letting a robot that is controlled by a large neural network experiment with different grasps and reinforcing behavior that seems to produce positive results. The results can be impressive, making it possible for a machine to devise a strategy for grasping previously unseen objects that would be extremely difficult to program manually. Tellex is also exploring ways for robots to share what they have learned, something that promises to accelerate the training process dramatically (see “10 Breakthrough Technologies: Robots That Teach Each Other”).

“Our approach is to make the robot learn to adapt itself to the environment it finds itself in,” says Tellex. “Through that learning it can reach a reliability that wasn’t possible previously. It can also use this information to generalize to other situations.”

New robot learning approaches are rapidly moving into industrial settings. Existing robotics companies are developing products that will use reinforcement learning to accelerate robot programming (see “A Japanese Robot Giant Gives Its Arms Some Brains”). Companies specializing in AI and machine learning also see the technology as a way to break into an industry that looks set to evolve rapidly (see “Google Builds a Robotic Hive-Mind Kindergarten”).

“We want robots to be able go into factories, household environments, and manipulate,” Tellex says, “and everything starts with picking something up.”",Yes,"They've become platforms for marketing and advertising. Advertisers are able to use voice assistants to target ads to users based on their demographics, interests, and even their location. This has led to some users feeling bombarded with ads, and has even led to some privacy concerns.",https://www.technologyreview.com/2016/10/18/69877/robot-get-the-fork-out-of-my-sink/,2016-10-18 00:00:00,LABEL_1_relevant,0.9944135546684264,Voice Assistants,voice_assistant,MIT Tech Review,Security & Privacy
13,College Kids Hack Siri to Unlock Teslas and Heat Up Your House,"When Apple first introduced Siri three years ago, she amazed the world with her warm voice and her human-like voice response. But the years since her debut have been quietly underwhelming, in large part because Siri isn't able to access the wide range of apps that are available on the iPhone.

So a group of University of Pennsylvania developers have hacked Siri with the idea of going it alone and building their own Siri App Store. They call their hack GoogolPlex and after introducing their software a few months ago, they've rejiggered it and released it to the public as a platform where anyone can create custom commands.

""We looked at Siri and we realized that they had very good voice translation, but the problem was that Apple hadn't made the platform open,"" says Ben Hsu, one of the developers who worked on the project. ""So what you could do was really very limited.""

On the GoogolPlex, they show how the software can be used to give you voice control over apps that don't currently work with Siri including the Nest thermostat, Philips Hue lightbulbs and Spotify.

Watch a demo here:

Content This content can also be viewed on the site it originates from.

Googolplex isn't for everybody. It's more of a cool tool for the tech savvy and a stark illustration of how badly Apple needs to open up Siri to third party developers. The reason we don't recommend it for most regular users is because there's a pretty big security downside to using the software. For GoogolPlex to work, you have to trick your phone into believing that a server run by the Penn developers is actually Google.com. That's a security no-no.

Once everything's been set up, you tell Siri to GoogolPlex things. Most of your Google searches go right to Google, but if the Penn server hears ""Google Plex,"" then it processes those requests using a script that interfaces with Nest or Spotify or whatever service you want to use. On this site, you can see commands that flip coins, present random comics, and query Google Maps for directions.

Enjoy it now, before Apple's lawyers ruin the fun.",Yes,"They are slowly turning us into a society of lazy, entitled people who would rather summon a robot to do something for us than do it ourselves.",https://www.wired.com/2014/04/googolplex/,2014-04-17 20:19:07+00:00,LABEL_1_relevant,0.995705544948578,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
14,How Covid Gums Up the Court System,"Last August, one of the world’s most infamous sex criminals—Harvey Weinstein—was due to appear virtually in a New York courtroom on a request to extradite him to California to face charges there. Reporters, due to Covid-19, mostly attended remotely. They were promised a videofeed. They didn’t even get audio. This left much of the press corps—who serve as the public’s eyes and ears—in the dark.

The proceeding ended before the link could be fixed. A transcript and pooled video were provided later, but the glitch robbed much of the public and press of seeing and hearing what happened in the courtroom.

Court systems nationwide have long struggled to uphold the principles of the First and Sixth Amendments, which establish the rights to public access, and fair and open trials, respectively. The pandemic has made things worse. As many courthouses closed or dramatically limited in-person proceedings, officials deployed video and telephone conferencing. Many businesses, and some schools, found ways to operate using platforms such as Zoom, but the experience of the nation’s courts over the past year is scattershot: Some have functioned well with remote participants, while others have struggled with the technology.

Even conference calls—used for some proceedings pre-Covid—have proved unpredictable and buggy. During a recent US District Court hearing about documents related to Jeffrey Epstein associate Ghislaine Maxwell, so many apparent QAnon followers dialed in that the public line was overwhelmed. Dozens of people, including many reporters, weren’t able to listen.

Even when remote courtrooms work well, advocates say they make it difficult to litigate cases and present obstacles for people accused of crimes to mount a defense.

“My client has a right to confront and hold the government accountable,” said Tina Luongo, attorney-in-charge of the criminal defense practice at the Legal Aid Society in New York City. “They have a right, under the Constitution, to confront the witnesses and be present to hear what those accusations are.”

In a report last year, the Brennan Center at NYU said remote proceedings “may unnecessarily put people’s rights at risk.”

Virtual proceedings complicate—and in some cases, prevent—routine communication between lawyers and their clients. Attorneys often talk to clients in breakout rooms—separate sessions in a broader videoconference—before proceedings, and then join the main room for on-the-record business, Luongo said.

“In order to [confer with a client], I have to say to the court: I’m sorry, your honor, can you put us back in a breakout room?” Tina Luongo, attorney-in-charge, criminal defense, the Legal Aid Society

The set-up creates a problem if an attorney wants to consult with a client during a hearing. “I can’t do that virtually. In order to do that, I have to say to the court: I’m sorry, your honor, can you put us back in a breakout room?” said Luongo, who now supervises courtroom attorneys. “Sometimes, judges don’t do it.”

Mitha Nandagopalan, an attorney with the New Mexico Law Offices of the Public Defender, has participated in video trials during the pandemic for misdemeanors before a judge, without a jury. Being separated from a client impacts the quality of representation, Nandagopalan says.

“Having my client not in the room with me made it harder,” Nandagopalan said. “At least if we’re in the same room, my client can pass me notes if they catch something that a witness was saying.”

Sometimes attorneys with the New Mexico public defenders will bring clients into the office, so they can physically be together when appearing in a virtual proceeding. This potentially exposes both people to Covid-19. But clients benefit.

In one situation, Nandagopalan said a client noticed that a witness’s testimony didn’t match their recollection of events. The client provided Nandagopalan with questions for cross-examination, which in turn spurred useful testimony for the defense.",Yes,They are often inaccurate and sometimes difficult to understand. This can be frustrating for users and can lead to misunderstandings.,https://www.wired.com/story/how-covid-gums-up-court-system/,2021-02-15 12:00:00+00:00,LABEL_1_relevant,0.984253466129303,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
15,Ex Machina Has a Serious Fembot Problem,"The Turing test detects if a machine can truly think like a human. The Bechdel Test detects gender bias in fiction. If you were to mash the two together to create a particularly messy Venn diagram, the overlap shall henceforth be known as the Ex Machina Zone.

In writer/director Alex Garland's thought-provoking new film—out Friday—we meet Ava (Alicia Vikander), an artificially-intelligent robot. Ava's creator, genius tech billionaire Nathan (Oscar Isaac), has asked his employee Caleb (Domhnall Gleeson) to determine whether Ava's thinking is indistinguishable from a human's. Until she meets Caleb, Ava has only ever met her maker and one other woman. (Hence the failing of the Bechdel Test, which stipulates that a movie must feature two female characters who talk to each other about something other than a man.) Her existence, and her ability to learn how to interact, is a fascinating study of what makes us human.

It's also a compelling, if problematic, look at the interactions between men and women---or at least that's what I thought.

While interviewing Garland for a magazine piece, I asked him about the roles of men and women in his film; his response was that Ava is ""not a woman, she is literally genderless."" Despite using female pronouns, he said, ""the things that would define gender in a man and a woman, she lacks them, except in external terms. ... I'm not even sure consciousness itself has a gender.""

In a way, Garland is right; pure intelligence wouldn't have a gender any more than it would have a race. But to say that and then place that consciousness into a body that it will immediately recognize its likeness as female negates that point. If Ava has truly been educated about the human race, then she knows her face and form appeal to certain segments of the population. But even thornier is the fact that Ava falls squarely into so many of the tropes of women in film. She's a femme fatale, a seductress posing as a damsel in distress, using her wiles to get Caleb to save her from Nathan and his Dr.-Frankenstein-with-tech-money quest to build a perfect woman. (Women: So much better when you can construct them out of bespoke parts and switch them off if they're not working properly, amirite?)

Chappie Didn't Have to Put Up With This Crap

According to Garland, these tropes are intentionally front-and-center. He believes his movie is a commentary on the ""constructs we've made around girls in their early 20s and the way we condition them culturally"" and why Caleb would feel the need to save her from her maker. ""You're supposed to think it's creepy,"" he says. ""You're not supposed to warm to [Nathan] over that stuff, you're supposed to feel unnerved, and therefore that she needs to be rescued.""

Yet, in the pursuit of that commentary, the movie ends up re-enacting those same patterns. Ava does prove to be the smartest creature on the screen, but the message we're left with at the end of Ex Machina is still that the best way for a miraculously intelligent creature to get what she wants is to flirt manipulatively. (And why wouldn't she? All of her information about human interaction comes from her creepy creator and the Internet.) Why doesn't Chappie have to put up with this bullshit?

Content This content can also be viewed on the site it originates from.

Ava's predicament really isn't that different from many female AIs who have come before her, from Metropolis' Maria to *Her'*s Samantha to *Blade Runner'*s Pris. She is an android in female form, and thus she simply reflects how Hollywood has been depicting women—robotic or otherwise—for decades. In Blade Runner, the male replicants Roy Batty and Leon are struggling to change their short lifespans, while ""basic pleasure model"" Pris helps the cause by draping herself on J.F. Sebastian. In Prometheus, David is intellectually curious, but never sexualized. (Yet when Idris Elba's Janek accuses Charlize Theron's Meredith Vickers of being a robot, she responds with ""My room. Ten minutes."" Because sex is the easiest way to prove you're a real woman.) Sentient male androids want to conquer or explore or seek intellectual enlightenment; female droids may have the same goals, but they always do it with a little bit of sex appeal, or at least in a sexy package. (Still have doubts? Ex Machina’s marketing campaign at South by Southwest involved Ava showing up on Tinder.)

This tendency to give female AIs the most basic and stereotypical feminine characteristics is, according to Kathleen Richardson, a senior research fellow in the ethics of robotics at De Montfort University in the UK, probably a reflection of ""what some men think about women—that they're not fully human beings."" To put a finer point on it, she told Live Science recently, ""what's necessary about them can be replicated, but when it comes to more sophisticated robots, they have to be male.""

When I spoke to Richardson, author of An Anthropology of Robots and AI: Annihilation Anxiety and Machines, she also noted this leads to female robot characters becoming just pieces of full people—a beautiful body, a caretaking nature—but not ones with full intelligence. This is largely true in Ex Machina---and not just because Nathan has a lab full of body parts---but also in a lot of movies where the artificial intelligence has to be packaged in a certain way if the robot is perceived to be female. (She also notes the real robotics world suffers from the same problems as a lot of AI fiction, but that ""many robotic scientists are open to a conversation about this."")

Women, whatever their qualities—intelligent, vulnerable, strong—are always presented in an attractive form, as if the package is the only way to deliver these qualities. Kathleen Richardson

""Sometimes the female robots have 'violent' characteristics (as Terminator 3’s T-X character), but it's always presented in a beautiful form,"" Richardson says. ""Women, whatever their qualities—intelligent, vulnerable, strong—are always presented in an attractive form, as if the package is the only way to deliver these qualities. Male intelligence, strength, vulnerabilities, etc. can be delivered in a multiple and varied kind of outer packaging.""

Think of it this way: Ava demonstrates her consciousness/intelligence in a form and with a sensuality that David in Prometheus never had to. *Short Circuit'*s Number 5/Johnny Five was cute, but he never had to employ it for survival the way Pris did in Blade Runner. Even AIs with no physical form at all seem to get sexualized based simply on their voices. It's not like HAL 9000 ever sparked up a relationship with Dave in 2001: A Space Odyssey the way Samantha did in Her. ""Her is playing on the fact that the audience knows what [Scarlett Johansson] looks like,"" Richardson says. ""No one really needs to know who the voice of HAL was, because HAL was an intelligent machine. We need to know about the disembodied voices of our AI avatars if they're female so that males can buy into the ideas of the sexualized person behind the representation.""",Yes,"They turn the women who use them into sex objects.

In a recent article in the Atlantic, senior editor Ian Bogost noted that when we have to have a female voice for our AIs, we wind up fetishizing them. ""they turn the women who use them into sex objects,""",https://www.wired.com/2015/04/ex-machina-turing-bechdel-test/,2015-04-09 11:00:34+00:00,LABEL_1_relevant,0.9965533018112184,Voice Assistants,voice_assistant,Wired,Social Norms & Relationships
16,"Google Acquires Emu, An IM Client With Siri-Like Intelligence","Google has acquired Emu, a new mobile messaging application that just exited beta earlier this year. Emu was at heart an IM client, but it differentiated itself from the crowded market with smart features that incorporated a virtual assistant not unlike Siri to automate tasks based on your conversations – meaning you could do things like schedule appointments to your calendar, set reminders and even make reservations at a restaurant directly from your conversations.

Emu’s roots were deep; its co-founder and CEO Gummi Hafsteinsson is ex-Google and ex-Apple, where he worked on Siri after coming on board following that startup’s acquisition. Emu works a lot like Google Now, too, scouring your conversations for contextual information about your life and plans in order to provide recommendations in-line about booking appointments, restaurant ratings and reviews and even things like movie suggestions.

In fact, Emu, which debuted as an Android application, looks like it can offer a lot to Google if the company is intent on continuing to grow the influence of Google Now across its properties, and extend its capabilities beyond the contextual launcher where it currently resides. Messaging is a new mobile battleground for big tech companies, and a potential motivating factor in attracting and keeping users within a particular ecosystem – Apple is focusing heavily on iMessage with its iOS 8 update, and in OS X Yosemite, adding multimedia messaging features, refining group chat, and making it easier to get your messages on all devices, regardless of what kind of device the person sending them is using.

It’s easy to see Google potentially using Emu’s tech in Hangouts, which itself is evolving into a central, multi-featured hub for communications on Android, and leaving behind its origins first as simply a video calling service and later as a text communication tool. Messaging could become the home base for mobile in the same way that email is in many ways the HQ for desktop computing, at least in terms of P2P communications (the browser rules for content consumption) and Emu’s tech could be instrumental in truly evolving it past what it is now.

Emu says it will shut down the standalone Emu app as of August 25 as a result of the acquisition, and it will cease to work for existing users. Emu had raised $1.5 million from KPCB, Kleiner Perkins, Draper Fisher Jurvetson, Menlo Ventures and others. No financial details regarding the acquisition were disclosed.",Yes,"They may lead people to carry out less mundane tasks, but also to interact less with each other.",https://techcrunch.com/2014/08/06/google-acquires-emu-an-im-client-with-siri-like-intelligence/,2014-08-06 00:00:00,LABEL_1_relevant,0.9915871620178224,Voice Assistants,voice_assistant,TechCrunch,Social Norms & Relationships
17,Ill-Advised Hack Brings Siri to Jailbroken iPhone 4,"A new hack delivers the Siri digital assistant to jailbroken iPhone 4 units, but we advise would-be tinkerers to think long and hard before acting on the invasive software surgery. You might think you know your way around an iPhone hack, but this one has trouble written all over it.

This downloadable version of Siri, called H1Siri, is available in the Cydia app store. The language accompanying the 100MB file claims to provide full Siri functionality on GSM or CDMA iPhone 4 units running iOS 5 or 5.0.1. However, this particular exploit seems like bad news for a number of reasons.

First, it's illegal. The guys at iDownloadBlog were able to confirm this because the hack uses copyrighted binaries from the iPhone 4S.

Second, your Siri requests won't go through Apple's servers with this version of Siri -- instead, they'll go through the servers of the Chinese hacking team that developed the exploit. What's more, CD-Dev Team, the group behind the hack, has confirmed that their servers are currently down, so even if you did download H1Siri, it may not work.

iDownloadBlog was not able to get the hack to work, but did hear from others who had gotten it to work ""in short bursts.""

Doesn't really seem worth the effort, if you ask Gadget Lab.

Hackers the world over are trying to bring Siri to iOS devices other than the iPhone 4S. They've had varied success, primarily stemming from the fact that Siri interacts with Apple's own servers, and each bit of data that's sent to those servers bears markers that identify the request as coming from an iPhone 4S. If your request doesn't include that unique 4S identifier, you're out of luck.

One of the most successful Siri hacking attempts thus far comes from a French team at Applidium, but the blog post detailing the hack seems to have been removed. Another hacker, Pete Lamonica, was able to use Applidium's instructions to hack Siri to control a thermostat, while another developer then used Lamonica's method to start and unlock his car. Both of the latter two cases still used an iPhone 4S, it must be noted, and simply hacked Siri's capabilities, and not her host hardware.

Lamonica tweeted this morning that he would be investigating H1Siri, but didn't recommend others interacting with a third-party-managed Siri server.

iPhone hacker Grant Paul also tweeted reservations about the hack. ""Please note: if you use a proxy to access Siri, you may be sending your Email, SMS, Calendar, Contacts, Location, etc though that server,"" Paul said.

Besides all that, jailbreaking itself isn't necessarily a good idea.

Apple's official statement from spokesperson Natalie Harrison on jailbreaking is thus:",Yes,"You'll be forced to listen to a lot more ads.

[Via iDownloadBlog]",https://www.wired.com/2011/12/siri-hack-bad-idea/,2011-12-05 22:05:25+00:00,LABEL_1_relevant,0.996649205684662,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
18,"Ray Kurzweil's Blio E-Book Launch Met With Confusion, Controversy","Updated at 5:30 p.m. Eastern to add comment from K-NFB.

This week, K-NFB, an e-reading company founded by Ray Kurzweil and the National Federation for the Blind, launched its much-anticipated Blio reading app and e-book store. Blio was immediately and widely panned by publishers, developers and readers.

""Many of the failures are fundamentally at odds with the one thing that Kurzweil was touting above all else: accessibility,"" wrote Laura Dawson, a digital reading industry consultant, formerly of BarnesAndNoble.com. K-NFB initially promised to make e-books more accessible to blind readers; yet Windows, currently its only enhanced books platform, has known text-to-speech conversion issues.

K-NFB and Kurzweil responded by saying the software had been released before a fully-accessible version was ready, and that they plan to release an improved version next week.

“People understandably have very high expectations,” Kurzweil said in an interview with Wired.com Thursday. “We believe Blio is very usable and has many features other book formats don’t. And there are many features that we want to add.”

In addition to bug fixes and other tweaks, an accessibility release scheduled for October 25 will work with Freedom Scientific’s JAWS, screen reader software for Windows specifically tailored for blind users. The 1.0 version released Tuesday relied on Windows’ built-in text-to-speech capabilities.

K-NFB spokesperson Peter Chapman acknowledged the problems with Blio's text-to-speech in an interview with Publishers Weekly, but blamed the platform, not the books: “the TTS software on most Windows machines isn’t very good.” Yet many Windows XP users were unable to even install Blio's software. (This has since been resolved.)

K-NFB also confirmed that an iOS 4 version for iPhone and iPod Touch is currently in private beta, and that an iPad version will shortly follow the iPhone, probably sometime after November’s release of iOS 4.2. Android and Mac OS X versions are also in development.

Users weren't the only ones frustrated with Blio. Hadrien Gardeur, founder and CEO of free e-books site FeedBooks, complained on Twitter that Blio was offering downloads from FeedBooks' catalog without permission: “Hey Blio, next time that you add our OPDS [Open Publishing Distribution System] catalog to a commercial product, send us an e-mail first.”

In a follow-up e-mail, Gardeur noted that FeedBooks only allows other systems to include their catalog under the following conditions:

full support for the EPUB standard (Blio converts EPUB into its own format and can't support EPUB with other companies' DRM)

support the entire OPDS catalog (Blio only includes some of FeedBooks' feeds)

Add other OPDS catalogs to its library (Blio can't do that)

allow payment for commercial content through open standards (Blio doesn't)

For these reasons, Gardeur asked Blio not to include FeedBooks' content in its initial launch; according to Gardeur, K-NFB went ahead and included part of Feedbooks' OPDS catalog anyways. Since FeedBooks has a planned system update forthcoming, it will most likely break Blio's access to the catalog.",Yes,"They are making us more impatient. Voice assistants are often used for tasks that are time-sensitive, such as setting alarms, sending text messages, and adding appointments to calendars. Because we are using them for these time-sensitive tasks, we are becoming more accustomed to getting an immediate response. This is causing us to become more impatient in general,",https://www.wired.com/2010/09/ray-kurzweils-blio-e-book-launch-widely-panned/,2010-09-30 14:37:05+00:00,LABEL_1_relevant,0.9945395588874816,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
19,Should you say 'please' and 'thank you' to your Amazon Echo or ...,"When you’re asking a digital assistant to do something for you, do you say “please?” How about “thank you?” It’s a question that’s been on my mind for a while, ever since I set up some smart lights in my apartment and started using Siri to turn them on and off. Demanding that my phone turn on and off the lights started feeling weird to say aloud, which got me to wondering: was I being rude to my smartphone?

To be clear, this isn’t some kind of “Be nice to Alexa or the robots will kill you during the inevitable uprising” thing. I know very well that Siri and Cortana are just a bundle of voice recognition software and algorithms, and that they’ll work just as well if I ask them to “Tell me the weather report, please” or just bark out a command for the forecast. And as for saying thank you, given that the speakers have (in theory) shut off from active listening by that point of the query process, it’s unlikely that my Echo can even here me saying it.

STRAW POLL: Do you say “please” and/or “thank you” when using a virtual assistant like Alexa or Assistant? — Chaim Gartenberg (@cgartenberg) November 15, 2017

Or, as various people have made the argument to me, would you say “please” to a toaster or “thank you” to an ATM?

But here’s the thing. I don’t talk aloud to an ATM or toaster. And I do when I’m asking Siri to turn on the lights or set an alarm, in the same natural language that I use in day-to-day life. It doesn’t matter that an Echo can’t hear or understand it — it matters that you say it. At what point does my rudeness to smart assistants start to bleed into my normal speech patterns? If I get used to asking Alexa to turn on the lights without saying please, will I still do the same when I ask my roommate that’s walking by? I’m polite to my smart assistants because I want to be polite to people too, and reinforcing rude habits seems like a bad idea.

Maybe I’m totally crazy here. When I polled people on Twitter, it seemed that most people aren’t polite to their digital assistants, which again, is totally fine since they really can’t tell the difference. But my argument is that we shouldn’t be polite to our voice activated assistants for their benefit, but for ours.",Yes,"They’re creating an environment where we’re comfortable giving commands instead of asking questions. This could lead to a future where people are comfortable being more rude to each other since they’re used to barking commands at their devices. While this may seem like a small thing, it could have a large impact on how we interact with",https://www.theverge.com/circuitbreaker/2017/12/10/16751232/smart-assistants-please-thank-you-politeness-manners-alexa-siri-google-cortana,2017-12-10 00:00:00,LABEL_1_relevant,0.9948849081993104,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
20,"Hey Alexa, does this look infected?","Once you’re done ordering pizza and locking and unlocking your front door, what’s left to do with your Echo? Starting today, HealthTap users will be able to use Amazon’s connected home assistant for medical advice, if they so choose.

The online healthcare startup is announcing Alexa integration this week, letting users tap into their voice-controlled health services with the command, “Alexa, talk to Dr. AI.” Once that’s taken care of, the talking speaker will walk the user through a guided path designed to simulate (but not, one assumes, replace) an in-person doctor visit, offering up insights.

The system features an AI doctor “trained in bedside manner to ask intelligible, dynamically generated questions in natural language via an intuitive conversational user interface.” It also can schedule an in-person visit for those in need of more urgent care or just wanting the real thing.

The system targets users with limited mobility, like the disabled and the elderly. It’s already available for iPhone and Android, but the company’s hoping the addition of Alexa functionality will make it more user-friendly and provide additional access to people who don’t have the use of their hands.",Yes,They are becoming extremely user-friendly and providing additional access to people who don’t have the use of their hands.,https://techcrunch.com/2017/02/20/hey-alexa-does-this-look-infected/,2017-02-20 00:00:00,LABEL_1_relevant,0.9954397678375244,Voice Assistants,voice_assistant,TechCrunch,User Experience & Entertainment
21,Microsoft Demos Real-Time Speech-To-Speech Translation On Skype,"Microsoft’s Skype has long enabled people in different parts of the world to communicate with one another through video, voice, and text chat. Now the service is looking to expand those capabilities by testing out a feature that could allow people who don’t speak the same language to talk with each other.

Today at the first annual CODE Conference, Microsoft head of Skype and Lync Gurdeep Singh Pall showed off a new speech-to-speech translation technology that the company is looking to introduce in future versions of its Skype products. The feature, which Skype is hoping to roll out in beta later this year, translates speech from one language to another in near real-time.

As it was demoed, the feature translated Pall’s speech from English into text for transcription on a colleague’s screen in German, and also into voice in German… and vice versa.

In a chat before the demo, Pall said “Skype is about bringing people closer, and breaking down barriers.” That started with the idea of cheap international calling and expanded into face-to-face communications via video. Now it’s taking on the challenge of breaking the language barrier.

The feature was a collaboration between Skype, Bing, Microsoft’s Research Lab, which has long been working on natural language processing and machine learning for a while. Language recognition is powered by the same technology as Microsoft’s Cortana personal assistant on Windows 8.1.

At the conference, Microsoft CEO Satya Nadella said that the feature would be avialable later this year, and that the company would try to launch it on as many devices and apps as possible.",Yes,"They are slowly but surely taking away jobs that have traditionally been done by humans. With the advent of new and improved voice assistants, many jobs that have been around for years are now in danger of disappearing.",https://techcrunch.com/2014/05/27/microsoft-demoes-real-time-speech-to-speech-translation-on-skype/,2014-05-27 00:00:00,LABEL_1_relevant,0.9956809282302856,Voice Assistants,voice_assistant,TechCrunch,Economy
22,Google Is Making An “AI-Powered Assistant for Nurses and Doctors”,"We’ll Take That

What Google wants, Google gets — and right now, Google wants the Streams app.

Streams is a mobile healthcare app developed by DeepMind Health, a subsidiary of AI research company DeepMind, which, like Google, is a subsidiary of Alphabet.

As part of its efforts to snag a slice of the $3 trillion healthcare pie, Google is absorbing DeepMind Health, and according to a DeepMind blog post, the move could transform Streams into “an AI-powered assistant for nurses and doctors everywhere.”

Streaming Lives

The Streams app is currently helping medical professionals in the U.K. address the problem of “failure to rescue,” a term for what happens when a patient dies from a preventable condition because they didn’t receive the right treatment in time.

Advertisement

Advertisement

The app does this by consolidating all of a patient’s medical data. Everyone treating the patient can enter information into their file directly from the app, and the app can then send alerts to healthcare workers as soon as signs of a potentially life-threatening problem arise.

The Majors

Streams is currently used under very narrow conditions: only by workers at specific U.K. facilities and only for the detection of acute kidney injury. With Google’s full support, though, the app could very well become the go-to medical tool of healthcare professional across the globe. And seemingly nothing would make the DeepMind team happier.

“This is a major milestone for DeepMind! One of the reasons for joining forces with Google in 2014 was the opportunity to use Google’s scale and experience in building billion-user products to bring our breakthroughs more rapidly to the wider world,” they wrote in the blog post. “It’s been amazing to put this into practice in data centre efficiency, Android battery life, text-to-speech applications, and now the work of our Streams team.”

READ MORE: Google Is Absorbing DeepMind’s Health Care Unit to Create an ‘AI Assistant for Nurses and Doctors’ [The Verge]

Advertisement

Advertisement

More on DeepMind Health: Google Launches DeepMind Health, Bringing AI to Healthcare

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",,The data privacy. The microphones of voice assistants are always on and they could be hacked. The main benefit is the convenience. Voice assistants are more convenient than typing because they allow you to perform actions hands-free.,https://futurism.com/google-health-deepmind-streams-app,,LABEL_1_relevant,0.9954140186309814,Voice Assistants,voice_assistant,Futurism,Security & Privacy
23,Google tests voice matching to secure Google Assistant purchases ...,"Google Assistant’s Voice Match feature can now secure purchases made through its voice assistant as part of a limited pilot program. A spokesperson from the company confirmed to Android Police that the functionality is new, and is designed to help secure purchases made on smart speakers and smart displays. At the moment, it’s restricted to in-app digital purchases through Google Play, as well as restaurant orders.

When we followed the instructions shown on Google’s support page to enable the feature we only saw the option to enable the assistant’s existing fingerprint or face unlock security methods. Other users have reported that they were able to see the Voice Match menu option, but that the software froze when they tried to enable it.

Screenshots posted by Android Police show the setup process for the feature. “Use your voice to verify it’s you when making a purchase with your Assistant” the software reads. It also warns that the feature’s security may not be perfect, noting that “someone with a similar voice or recording may be able to confirm purchases on devices you’re logged into.”

Although it may not be the most secure method, using your voice as security has the potential to be a lot more seamless than using your phone to respond to a fingerprint or face unlock prompt. It could also be more convenient than Alexa’s existing security method, which allows you to use a PIN to protect purchases made using your voice.

Voice Match is a Google Assistant feature that allows the assistant to tell the difference between who’s addressing it. Last month, 9to5Google reported that Google updated the Voice Match training process to allow it to distinguish between people with “significantly higher accuracy.” However, there are reportedly still limits on how much money you can spend using just your voice as security, Google confirmed to Android Police.

Considering Google I/O 2020 developer conference had been scheduled to take place earlier this month, there’s speculation that the company had hoped to debut it there. Google gave no indication about when voice match confirmed purchases might see a wider release.",Yes,"

they can be used to make purchases without the need for a PIN or other security measure. This means that someone with a similar voice or recording may be able to confirm purchases on devices you’re logged into.",https://www.theverge.com/2020/5/26/21270222/google-assistant-voice-match-purchases-in-app-google-play-shopping,2020-05-26 00:00:00,LABEL_1_relevant,0.9961321353912354,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
24,"iOS 5 includes Siri 'intelligent assistant' voice-control,dictation -- for ...","As rumored, Apple's doing some all-new voice-control AI stuff in iOS 5. It's called Siri, which is the name of the app Apple bought for a rumored $200 million last year. Basically, you ask Siri questions out loud in plain english, and Siri makes sense of what you're asking and presents you with results and actions. Siri launches with a long press of the home button, just like Voice Control of yore, but after that the options are fairly endless. ""Do I have a meeting Friday at noon?"" Siri checks the calendar and responds out loud. ""What time is it in Paris?"" Siri shows you a world clock. ""Set my alarm for..."" you get the idea. The verbal response of Siri allows it to clarify if it understands what you're saying, and in turn perform some pretty complicated actions, like setting up a meeting with Phil Schiller for Friday at noon. Like with the original version of the app, Siri remembers the context of what you've been asking, so actions can build on each other. You can also ask Siri to look things up on Wikipedia for you, and Siri can use Wolfram Alpha to do more complicated calculations. Siri's list of capabilities is near endless, including asking it to play genres of music for you, look up something on maps, or what the weather is. Our favorite question? ""Siri, who are you?"" Siri responds: ""I am your humble personal assistant.""

In addition to the Siri assistant, Apple is using its new voice recognition chops to offer a ""beta"" version of speech-to-text dictation functionality. Initial languages include English, French, and German. Apple plans to offer more languages and more Siri services over time. The bad news? All this great stuff is only available for the iPhone 4S -- Apple had to do something to force an upgrade! In all seriousness, some of this AI functionality can be incredibly processor intensive, so Siri might be leaning on the A5 chip quite heavily. Google's voice dictation capabilities rely on uploading your speech to the cloud, where a server somewhere does the processing work. We'll be digging into Siri in the coming days and weeks, so stay tuned. There's a gallery of Siri in action after the break.

Update: There's a demo video after the break!

Source: Apple

Grid View



































































",Yes,"They open up a new accessibility angle that has not been considered before. The feature can pose as a potential privacy concern, as well as open up new ways for people with disabilities to interact with technology.",https://www.theverge.com/2011/10/4/2515703/ios-5-assistant-voice-control-ai-features,2011-10-04 00:00:00,LABEL_1_relevant,0.9277064204216005,Voice Assistants,voice_assistant,The Verge,Security & Privacy
25,My Message To Google: Stop Cheating,"In mid February, at the Mobile World Congress in Barcelona, Google Executive Chairman Eric Schmidt expressed pride in Google employee Wael Ghonim’s brave struggle against the autocratic Mubarak regime to establish political transparency in Egypt. “We are very, very proud of what Wael and that group was able to do in Egypt,” Schmidt said in Barcelona. But what Schmidt needs to do now is apply Ghonim’s views about political transparency to Google’s own search business.

With its 70% control of the global online search market, Google’s power to make and break online businesses is unrivalled. So it’s not surprising that website owners want more transparency over the reasons why the often autocratic Google sometimes impose penalties on their businesses. But a report issued last week by the newsnavigator OneNewsPage found a distinct lack of transparency in the search business with 88% of respondents saying that paid search advertising costs lacked transparency, while 24% said that they had experienced large, unexplained falls in site traffic as a consequence of changes in their search engine status.

Nor is it surprising that the American Antitrust Institute published a report a week ago calling for US regulators to do a thorough investigation of Google’s proposed deal to acquire travel software provider ITA. This deal, the report worries, would enable the dominant search engine to dominate the online travel market, thus muddying the church-state distinction between Google as technology provider and Google as a distributor of content.

Transparency in search is critical to maintain both innovation and fairness in the digital economy. Yes, Google improved the quality of its search engine by targeting content farms with last week’s self-congratulatory tweaks to its algorithm. But this remains little more than a cosmetic change. Rather than spam, fairness is the key issue. Given Google’s dominance in search, the company has a responsibility to reveal the mechanics of its ranking algorithm—so that everyone understands why some links are ranked higher and more prominently than others.

How can they do that in a way that doesn’t invite gaming and spam, so that companies like JC Penny won’t take advantage of the system? That is Google’s problem. They’ve cornered the global market in PhD’s. If the company can invent a self-driving car then it can certainly figure out how to make its ranking algorithm more transparent without becoming an easy target for content farms like Demand Media.

You see, just as we need our government to play by clear rules, so the same is true with search. Thus, a week ago, in a letter to the DOJ Assistant Attorney General for Antitrust, John Conyers, the lead Democrat on the House Judiciary Committee, encouraged the Department of Justice to “carefully” review the Google ITA deal in order “to ensure competition and transparency will be protected in the online travel industry.”

Last week, 1plusV, the French developer of vertical search engines such as EJustice.fr, filed a complaint with the European Union, accusing Google of “pursuing a strategy of foreclosure against vertical search engines” and of illegally tying the Google search engine with AdSense. Bruno Guillard, 1plusV’s founder, said on Bloomberg News that it was technically impossible for his own vertical search engines to use AdSense, thus undermining his ability to build viable business models around these new services.

Yes, online transparency—understanding how, exactly, its artificial algorithm works and what information or links gets prioritized for what reasons—matters, particularly given the centrality of Google search in the knowledge economy. In his important new book The Googlization of Everything (and why we should worry), University of Virginia media scholar Siva Vaidhyanathan claims that Google’s control of the Internet is comparable to that of Julius Caesar’s rule in 48 BC Rome. The all-controlling Google, Vaidhyanathan argues is “omniscient, omnipotent and omnipresent” and thus needs to be controlled if we are to maintain a level playing field in today’s knowledge economy.

Unfortunately, however, this is only half of it. Not only is Google’s control of today’s search engine market omniscient, omnipotent and omnipresent, but it also seems as if the Mountain View leviathan is abusing the very system over which it has such control. That’s at least the suspicion of the European Commission which last November, on behalf of the European Union, launched an antitrust investigation into allegations that Google “abused” its already dominant position in search by its “unfavorable” treatment of rival services and by its “preferential treatment” of its own services like YouTube, Google Maps and Blogger.

Yes, as everyone from the European Commission to the American Antitrust Institute remind us, transparency matters. In simple terms, it seems, Google has muddied the already disturbingly murky lines between online content and commercial interests by promoting its own products in its supposedly objective search engine. So, for example, when I enter my own name into Google, the first video link that comes up is a YouTube speech I made at Google headquarters in June 2007—even though there are many more professional and popular videos of speeches that I’ve made since.

Coincidence? No, I hardly think so. The truth is that Google’s manipulation of its secretive artificial algorithm isn’t just a reflection of the paranoia of its less powerful rivals in the search business or my own admittedly highly anecdotal adventures in self-googling. Last November, when the European Commission launched its investigation, the Harvard Business School professor, Benjamin Edelman, published a research paper entitled “Hard-Coding Bias in Google Algorithmic Search Results” which proves that Google has “hard-coded its own links to appear at the top of algorithmic search results.”

Edelman’s scientific research is the real reason why Google sucks. Taking each of the 2,642 terms listed on Google’s Health Topics index page, the Harvard academic found that all 2,642 of these individual searches resulted in a link to Google Health appearing in the “absolute top of the page.” Edelman discovered the same unnatural biases with stock ticker searches on Google. Here, he found, that the links from any stock ticker search—“the large-type all-caps ticker symbol, the large price chart, and the left-most details link”—will always take you to Google Finance, even though Google’s in-house financial service is far from being a market leader in this sector.

Google’s bias isn’t just limited to finance and health. In a January 2011 paper, “Measuring Bias in Organic Web Search,” written with Harvard Business School doctoral candidate Benjamin Lockwood, Edelman found that Google listed its own map service as the first result when a user queries “maps.” It’s hardly surprising, therefore, that Edelman and Lockwood discovered that 86% of map searches conducted on Google end up with the user clicking on Google Maps.

So much, then, for the neutrality of Google search, the digital librarian on which we all-too-innocently trust to navigate our way around today’s knowledge economy.

Earlier this month, in an interview with the UK newspaper, The Daily Telegraph, Google’s former CEO and new Executive Chairman, Eric Schmidt, said that he hoped that the European Commission would “come up with a set of remedies” to the issue of biased search and promised that “Google would consider” implementing. Google, Schmidt told The Telegraph, would be “willing to change some of its algorithm methodology in search” if it led to the quick resolution of the EC enquiry.

But Google doesn’t really need any commission, European or otherwise, to identity the remedies to Google’s manipulation of the search knowledge economy. Instead, as I’ve suggested, they should listen to Wael Ghonim’s observations about the lack of transparency in the Mubarak regime.

My message to Mr. Schmidt and Google is very simple: stop cheating. Transform your search engine from a murky algorithm that sometimes benefits your own corporate interests into a transparently neutral guide that benefits both the consumers and the all the companies in our knowledge economy.

The need for Google to establish transparency in its search engine and play by the rules is particularly acute today, both in Europe and America. You see, Google isn’t quite as omniscient, omnipotent and omnipresent as Siva Vaidhyanathan claims. Indeed, for the first time in over a decade, Google has a genuine online rival challenging its hitherto Caesarian hegemony over the Internet. This emerging superpower in the online economy is Facebook which, with its 600 million members and over $50 billion valuation, is beginning to transform the web from a Google centric network of data to a social network of connected people.

Google’s recognition of the growing power of the social web is behind both its failed $6 billion acquisition for the social commerce business Groupon and its rumored $10 billion interest in acquiring Twitter—Facebook’s main rival in the social networking business. And given Google’s much publicized vulnerability in social media, it’s particularly important that this multinational corporation makes its search engine honest so that it doesn’t become a disreputable tool in Google’s battle with Facebook to control the emerging social web.

In his new book, The Master Switch, Columbia University law professor Tim Wu—the scholar who invented the term “network neutrality”—argues that the modern media and communications industry has a tendency toward monopoly. But, for the 2 billion Internet consumers like you and I reliant on fair and transparent search as their trusted vehicle for navigating the web, the only thing worse than a monopolist is a cheating monopolist. Google needs to clean up its artificial algorithm now and guarantee search neutrality. Otherwise it won’t only be the European Commission investigating the self-interested bias of the Google search engine.

Photo credit: Flickr/Dicemanic",Yes,They are killing our ability to think for ourselves and become emotionally intelligent.,https://techcrunch.com/2011/02/26/my-message-to-google-stop-cheating/,2011-02-26 00:00:00,LABEL_1_relevant,0.9959981441497804,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
26,Listen to an AI voice actor try and flirt with you - The Verge,"The quality of AI-generated voices has improved rapidly in recent years, but there are still aspects of human speech that escape synthetic imitation. Sure, AI actors can deliver smooth corporate voiceovers for presentations and adverts, but more complex performances — a convincing rendition of Hamlet, for example — remain out of reach.

Sonantic, an AI voice startup, says it’s made a minor breakthrough in its development of audio deepfakes, creating a synthetic voice that can express subtleties like teasing and flirtation. The company says the key to its advance is the incorporation of non-speech sounds into its audio; training its AI models to recreate those small intakes of breath — tiny scoffs and half-hidden chuckles — that give real speech its stamp of biological authenticity.

“Bigger emotions are a little easier to capture”

“We chose love as a general theme,” Sonantic co-founder and CTO John Flynn tells The Verge. “But our research goal was to see if we could model subtle emotions. Bigger emotions are a little easier to capture.”

In the video below, you can hear the company’s attempt at a flirtatious AI — though whether or not you think it captures the nuances of human speech is a subjective question. On a first listen, I thought the voice was near-indistinguishable from that of a real person, but colleagues at The Verge say they instantly clocked it as a robot, pointing to the uncanny spaces left between certain words, and a slight synthetic crinkle in the pronunciation.

Sonantic CEO Zeena Qureshi describes the company’s software as “Photoshop for voice.” Its interface lets users type out the speech they want to synthesize, specify the mood of the delivery, and then select from a cast of AI voices, most of which are copied from real human actors. This is by no means a unique offering (rivals like Descript sell similar packages) but Sonantic says its level of customization is more in-depth than that of rivals’.

Emotional choices for delivery include anger, fear, sadness, happiness, and joy, and, with this week’s update, flirtatious, coy, teasing, and boasting. A “director mode” allows for even more tweaking: the pitch of a voice can be adjusted, the intensity of delivery dialed up or down, and those little non-speech vocalizations like laughs and breaths inserted.

“I think that’s the main difference — our ability to direct and control and edit and sculpt a performance,” says Flynn. “Our clients are mostly triple-A game studios, entertainment studios, and we’re branching out into other industries. We recently did a partnership with Mercedes [to customize its in-car digital assistant] earlier this year.”

As is often the case with such technology, though, the real benchmark for Sonantic’s achievement is the audio that comes fresh out of its machine learning models, rather than what’s used in polished, PR-ready demos. Flynn says the speech synthesized for its flirty video required “very little manual adjustment,” but the company did cycle through a few different renderings to find the very best output.

To try and get a raw and representative sample of Sonantic’s technology, I asked them to render the same line (directed to you, dear Verge reader) using a handful of different moods. You can listen to them yourself to compare.

First, here’s “flirty”:

Then “teasing”:

“Pleased”:

“Cheerful”:

And finally, “casual”:



To my ears, at least, these clips are a lot rougher than the demo. This suggests a few things. First, that manual polishing is needed to get the most out of AI voices. This is true of many AI endeavors, like self-driving cars, which have successfully automated very basic driving but still struggle with that last and all-important 5 percent that defines human competence. It means that fully-automated, totally-convincing AI voice synthesis is still a way off.

Second, I think it shows that the psychological concept of priming can do a lot to trick your senses. The video demo — with its footage of a real human actor being unsettlingly intimate towards the camera — may cue your brain to hear the accompanying voice as real. The best synthetic media, then, might be that which combines real and fake outputs.

Apart from the question of how convincing the technology is, Sonantic’s demo raises other issues — like, what are the ethics of deploying a flirtatious AI? Is it fair to manipulate listeners in this way? And why did Sonantic choose to make its flirting figure female? (It’s a choice that arguably perpetuates a subtle form of sexism in the male-dominated tech industry, where companies tend to code AI assistants as pliant — even flirty — secretaries.)

On the first question, the company said their choice of a female voice was simply inspired by Spike Jonze’s 2013 film Her, where the protagonist falls in love with a female AI assistant named Samantha. On the second, Sonantic said it recognizes the ethical quandaries that accompany the development of new technology, and that it’s careful in how and where it uses its AI voices.

“That’s one of the biggest reasons we’ve stuck to entertainment,” says CEO Qureshi. “CGI isn’t used for just anything — it’s used for the best entertainment products and simulations. We see this [technology] the same way.” She adds that all of the company’s demos include a disclosure that the voice is, indeed, synthetic (though this doesn’t mean much if clients want to use the company’s software to generate voices for more deceitful purposes).

Comparing AI voice synthesis to other entertainment products makes sense. After all, being manipulated by film and TV is arguably the reason we make those things in the first place. But there is also something to be said about the fact that AI will allow such manipulation to be deployed at scale, with less attention to its impact in individual cases. Around the world, for example, people are already forming relationships — even falling in love — with AI chatbots. Adding AI-generated voices to these bots will surely make them more potent, raising questions about how these and other systems should be engineered. If AI voices can convincingly flirt, what might they persuade you to do?",Yes,"They will likely perpetuate a subtle form of sexism in the male-dominated tech industry, where companies tend to code AI assistants as pliant — even flirty — secretaries.",https://www.theverge.com/2022/2/17/22936978/ai-voice-speech-synthesis-audio-deepfake-sonantic-flirtation,2022-02-17 00:00:00,LABEL_1_relevant,0.9958987832069396,Voice Assistants,voice_assistant,The Verge,Equality & Justice
27,When AI Experts Have “It’s Alive!” Moments,"Fellow panelist Rob Chambers, principal group program manager for Cortana, Microsoft’s virtual-assistant software, described a similar experience of his own. While Chambers was brushing his teeth and looking at his smartphone one day, Cortana suddenly showed a message asking if he liked to read news articles about smartphone technology. Chambers says the experience gave him a sudden feeling that Cortana was “real,” even though he knew that the ability to learn what types of things people like to know about was scheduled to be added to the software.

Despite these flashes of apparent humanity, several people on the panel—which also included people who had worked on Google’s virtual assistant, Google Now—agreed that virtual assistants aren’t about to move from handling administrative tasks like sheduling meetings to becoming our bosom buddies. That will require ways to be found to add a capacity to understand and express emotion.

“The big missing gap on the Internet overall, in the world we live, this electronic age, is personality with emotion we can connect to in some deep-seated human way,” said Ronald Croen, founder and formerly CEO of voice recognition company Nuance. “That might sound scary in one sense because this is not a real person, but it might be very gratifying and powerful when the content matters, when the engagement experience matters.”",Yes,They are making people feel more emotional connections with their technology.,https://www.technologyreview.com/2014/11/06/170504/when-ai-experts-have-its-alive-moments/,2014-11-06 00:00:00,LABEL_1_relevant,0.9960470795631408,Voice Assistants,voice_assistant,MIT Tech Review,User Experience & Entertainment
28,An Alexa Bug Could Have Exposed Your Voice History to Hackers,"Smart-assistant devices have had their share of privacy missteps, but they're generally considered safe enough for most people. New research into vulnerabilities in Amazon's Alexa platform, though, highlights the importance of thinking about the personal data your smart assistant stores about you—and minimizing it as much as you can.

Findings published on Thursday by the security firm Check Point reveal that Alexa's web services had bugs that a hacker could have exploited to grab a target's entire voice history, meaning their recorded audio interactions with Alexa. Amazon has patched the flaws, but the vulnerability could have also yielded profile information, including home address, as well as all of the ""skills,"" or apps, the user had added for Alexa. An attacker could have even deleted an existing skill and installed a malicious one to grab more data after the initial attack.

""Virtual assistants are something that you just talk to and answer, and usually you don’t have in your mind some kind of malicious scenarios or concerns,"" says Oded Vanunu, Check Point's head of product vulnerability research. ""But we found a chain of vulnerabilities in Alexa's infrastructure configuration that eventually allows a malicious attacker to gather information about users and even install new skills.""

For an attacker to exploit the vulnerabilities, she would need first to trick targets into clicking a malicious link, a common attack scenario. Underlying flaws in certain Amazon and Alexa subdomains, though, meant that an attacker could have crafted a genuine and normal-looking Amazon link to lure victims into exposed parts of Amazon’s infrastructure. By strategically directing users to track.amazon.com—a vulnerable page not related to Alexa, but used for tracking Amazon packages—the attacker could have injected code that allowed them to pivot to Alexa infrastructure, sending a special request along with the target's cookies from the package-tracking page to skillsstore.amazon.com/app/secure/your-skills-page.

At this point, the platform would mistake the attacker for the legitimate user, and the hacker could then access the victim's full audio history, list of installed skills, and other account details. The attacker could also uninstall a skill the user had set up and, if the hacker had planted a malicious skill in the Alexa Skills Store, could even install that interloping application on the victim's Alexa account.

Both Check Point and Amazon note that all skills in Amazon's store are screened and monitored for potentially harmful behavior, so it's not a foregone conclusion that an attacker could have planted a malicious skill there in the first place. Check Point also suggests that a hacker might be able to access banking data history through the attack, but Amazon disputes this, saying that information is redacted in Alexa's responses.

“The security of our devices is a top priority, and we appreciate the work of independent researchers like Check Point who bring potential issues to us,"" an Amazon spokesperson told WIRED in a statement. ""We fixed this issue soon after it was brought to our attention, and we continue to further strengthen our systems. We are not aware of any cases of this vulnerability being used against our customers or of any customer information being exposed.""",Yes,"They are always listening to us. This could pose a serious privacy concern as our conversations are being recorded and stored without our knowledge. Additionally, if a hacker were to gain access to our voice history, they could potentially learn a lot about us, including our home address and the apps we use. Amazon has patched the vulnerabilities that allowed this attack to",https://www.wired.com/story/amazon-alexa-bug-exposed-voice-history-hackers/,2020-08-13 10:00:00+00:00,LABEL_1_relevant,0.9966340661048888,Voice Assistants,voice_assistant,Wired,Security & Privacy
29,"Siri Loses Her Voice, Experiences Spotty Service Countrywide","Apple's famous new speech-powered assistant may have lost her million-dollar voice.

Reports circulated on Thursday afternoon that Apple's ""Siri"" voice-activated iPhone assistant is experiencing service outages for customers across the country.

Upon Apple's service going down, Twitter exploded with messages from frustrated iPhone 4S users and lookers-on. Twitter-user BM5K -- whose profile location states Queen Creek, AZ -- asked, ""anyone else having major problems with Siri today?"" Others, like Jason Vance of McLean, VA, wrote that Siri was working for him, albeit slower than usual.

In Wired.com's San Francisco testing, the Siri service was spotty at best, functioning properly only intermittently. Still, others located in the Bay Area reported that the Siri service is functioning properly.

And of course, there was the usual snarky commentary from Twitter's global peanut gallery.

""With Siri Outage,"" wrote Twitter-user Anupam Ashish, ""iPhone 4S is now downgraded to an iPhone 4 with a Better Camera.""

It's unfortunate timing for Apple's brand-new service to experience issues. On Wednesday, after weeks of users complained of battery life issues with their new iPhone 4S devices, Apple issued a statement confirming its newly released iOS 5 operating system included ""a few bugs that are affecting battery life."" The company promised to ""release a software update to address those [bugs] in a few weeks.""

And it isn't as if outages in the service will slip by unnoticed. The inclusion of Siri was easily the most prominent upgrade from last year's iPhone 4 to the iPhone 4S, obviously the key marketing point in convincing users to purchase the new device. It worked: Apple sold over 4 million iPhone 4S handsets over the first three days of release.

Apple debuted the service last monthwith one major caveat: The company considers Siri a ""beta"" product, a fact which should be kept in mind if the service stumbles in its first few months. While releasing beta products is common for rival Silicon Valley giant Google, it has hardly ever been decorum for Apple.

Apple did not immediately respond to a request for comment.

Photo: Jim Merithew/Wired.com",Yes,"They have wholly captured the human experience, rather than the original intention of just making our lives easier. The question that should be asked is not whether or not voice assistants are good or bad. Rather, the question is: How do we want to use voice assistants in a way that benefits humanity as a whole?",https://www.wired.com/2011/11/apple-siri-down-outage/,2011-11-03 20:05:04+00:00,LABEL_1_relevant,0.990046203136444,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
30,Supahands Offers Multilingual Virtual Assistants For Clients In Asia,"Southeast Asia is the second-largest outsourcing destination in the world, but a new company called Supahands wants to help locals by bringing virtual assistants to the region. Based in Malaysia and Singapore, the startup plans to expand to Hong Kong, Taiwan, Bangkok, and Australia.

Co-founder Mark Koh first thought of launching a virtual assistant startup for Asia in 2010, but decided the market wasn’t ready at that point. Born in Malaysia, but raised and educated in Australia, Koh instead founded a Melbourne-based company called Global Virtual Support that hires Southeast Asia-based employees to handle customer service requests for medium-sized companies.

Last year, however, Koh decided that it was the right time to revisit his original idea. Based in Malaysia, Supahands launched in December. The startup, which is bootstrapped, says it currently has 500 active users.

One of the main things that has enabled Supahands to launch is the rapid increase of smartphone use in Southeast Asia. Most of the startup’s clients use mobile phones to enter tasks, get matched with a virtual assistant, and check on the progress of projects, says co-founder Andrew Tan.

Though 60% of requests are in English, Supahands caters to local customers with assistants who speak Mandarin, Cantonese, Tamil, and Malay, among other languages and dialects.

The startup currently has six full-time assistants, called “agents,” based in an office in Kuala Lumpur, but as the service scales up within Malaysia and Singapore, Supahands plans to train part-time employees who will work from home.

Some clients hire an assistant for a one-time program, like vetting cleaning services, but these turn into recurring tasks, says Tan. For example, a property owner who lives abroad and hires a cleaning team might ask Supahands’ agents to make sure that they show up at the right times. One of the biggest tasks Supahands has handled so far was planning a honeymoon.

Supahands’ chief competitors include freelancing sites like Odesk and Freelancer.com. Tan argues that Supahands makes hiring a virtual assistant easier by training and working closely with its assistants so clients don’t have to interview them.

All communication is done through Supahands’ online dashboard, instead of directly between clients and assistants, which Tan says helps with quality control.

He adds that Supahands’ biggest rival is actually the self-sufficiency of potential clients.

“In Southeast Asia, most locals have the mentality that they want to do everything by themselves, so our challenge is to help them see that they can outsource these tasks and save time for something more valuable,” says Tan.",Yes,"They are slowly making us more lazy. As we get used to asking Siri or Alexa to do things for us, we will find it harder and harder to do things for ourselves. This could have a negative impact on our society as a whole, as people become less self-sufficient and more reliant on technology.",https://techcrunch.com/2014/04/20/supahands-offers-multilingual-virtual-assistants-for-clients-in-asia/,2014-04-20 00:00:00,LABEL_1_relevant,0.9922181367874146,Voice Assistants,voice_assistant,TechCrunch,Economy
31,Even Sleeping Smartphones Could Soon Hear Spoken Commands,"While we generally focus on the dramatic aspects of assistant apps, like artificial intelligence and speech recognition, every step between you and the outcome you want is a problem, Sejnoha says. “Just turning on the device is part of the problem, right? So we’re going to be smoothing that out, eliminating those problems as well,” he says.

He also expects improvements in a mobile device’s ability to listen to an ongoing stream of noise and distinguish its user’s voice asking it to perform a task from background chatter.

However, he cautions that software developers will have to be careful to avoid making services that are annoying or creepy, or else they’ll create more “paper clip stories”—a reference to Clippy, the much maligned (and now extinct) digital helper in Microsoft’s Office software.

Privacy and security are also concerns if you have software persistently “listening” in the background. While Sejnoha says Nuance could build a system that is private from an engineering perspective, it’s still a big leap for users to become comfortable with something that’s always paying attention at some level, and there may be concerns about where the data that’s collected is going, and if it can be hacked into.",Yes,They may be annoying or creepy and may invade user privacy.,https://www.technologyreview.com/2012/09/24/183670/even-sleeping-smartphones-could-soon-hear-spoken-commands/,2012-09-24 00:00:00,LABEL_1_relevant,0.9958488941192628,Voice Assistants,voice_assistant,MIT Tech Review,Security & Privacy
32,Amazon disputes claims that Echo Show's Drop-In feature is a security risk,"Drop-In is a strange and mostly ill-advised feature. It’s a way of offering instant communication between Echo devices — essentially a phone call that doesn’t require the receiver to pick up in order to initiate. Hence, “dropping in.”

Amazon introduced it with the new Show and is rolling it out to older Echo devices as a way of facilitating the new intercom feature it introduced this week. We raised a few questions when the feature was first announced alongside free Echo calling back in May, but with the Show arriving this week, the some privacy and security concerns are being raised anew.

The feature is a bit of an odd choice from Amazon, given all of the questions that have been raised around the Echo line’s always-on microphones — though the company did get out in front of things this time around by spelling out the fact that Drop-In is an opt-in feature. Both parties involved have to enable Drop-In in the device’s settings in order to work.

Amazon reiterated that point in a statement provided to TechCrunch, adding, “Customers will know when they are engaged in a Drop-In call as they will first hear a ‘chime,’ and the green light ring on their Echo device will rotate throughout the call. On Echo Show devices, the caller will experience a 10-second ‘frosted glass’ screen when dropping in on an approved contact or room, so the call recipient can decline the Drop In in that timeframe before they are shown on the screen.”

Beyond intercom functionality, the feature is primarily targeted at users looking to check-in on older relatives or who’d like the system double as a baby monitor. The inclusion of visual and audio clues are certainly helpful — so users can’t be entirely stealth. But even with the “frosted glass” that partially obscures the screen for the first 10 seconds, it’s easy to imagine a scenario where the user on the other end is out of the room when a Drop-In is initiated or otherwise doesn’t notice that first chime.

In-home intercom functionality is a useful and much-requested feature for the Echo, but introducing an Echo with a built-in camera was inevitably only going to raise privacy and security concerns around the device. So the optics around launching this feature at the same time were bound to be less than ideal.

https://beta.techcrunch.com/wp-content/uploads/2017/06/dsc00742.jpg?w=1024&h=576

There’s also the issue of potential malicious access by hackers. Like most big companies, Amazon isn’t eager to share information around how something like Drop-In works on the server side. Companies will often keep some mystery around that secret sauce, for fear of revealing too much to wannabe hackers — which may go a ways toward explaining why it took so long for the company to issue an official response to earlier questions about what the Echo records and uploads to its servers.

It’s a balancing act. Staying silent on specific security concerns doesn’t look great from the outside. Depending on how the feature is implemented on the backend, it could potentially open the product to additional vulnerabilities, though the company reassures me that Drop-In operates like regular calling from a security standpoint.

“We take security very seriously at Amazon, and Alexa calling and messaging is no different,” the company tells TechCrunch. “The new Drop-In feature leverages the same security measures used in other Alexa features available on Echo devices. We limit the information we disclose about specific security measures we take, but I can tell you that — just like other calling and messaging services- –Alexa calling and messaging is encrypted in transit to protect our customers. Further, we have full teams dedicated to ensuring the safety and security of our Alexa customers who regularly conduct security reviews of our products and features.”

The company is also quick to point to the Mute button on the devices. In the case of the Show, it’s one of three large buttons on the top of the unit. Hitting it will both deactivate the microphone and camera, while turning the status bar red. While testing the feature over the past week, I had to train myself to hit the button every time I got dressed or did anything else I’d rather not have floating around on Amazon’s servers.

I’d suggest using the mute button liberally. In fact, I found myself unplugging the Show from time to time — which admittedly is counterproductive to the idea of a hands-off assistant. And in most cases, I’d recommend against using the Drop-In feature altogether. Every time you introduce another new technology into your home, it’s important to weigh the cost/benefit ratio. In my own life, I can’t think a single instance in which I wouldn’t at least want the option of not picking up a call.",Yes,"They are always listening. This is a result of the way they are built, as they need to be able to hear the wake word at any time. The result is that people's daily lives are being recorded and stored by these companies, whether they know it or not. While there are some benefits to this technology, the lack of privacy is",https://techcrunch.com/2017/06/28/amazon-disputes-claims-that-echo-shows-drop-in-feature-is-a-security-risk/,2017-06-28 00:00:00,LABEL_1_relevant,0.9964904189109802,Voice Assistants,voice_assistant,TechCrunch,Security & Privacy
33,The Cohen Tapes Are Everyone's New Favorite Playlist,"If it’s summer, California must be on fire. And if that’s not enough dystopia for you, there’s always the fact that DNA-testing service 23AndMe is selling clients’ information to drug manufacturers, or the emergence of mutated HIV strains that cause illness quicker than others, not to mention the president apparently threatening war with Iran for no immediately discernible reason. Yes, the world keeps on turning towards the very worst incarnation of itself, it seems, but it’s not all bad; at least there’s a new Mission: Impossible out this weekend. And until you can get yourself to a fine purveyor of popcorn-fueled escapism, please do enjoy this primer on what the internet has been discussing over the past seven days or so.

Michael Cohen's Basement Tapes

What Happened: Cohen, a man who had previously announced that he would take a bullet for Donald Trump, has apparently reconsidered his position.

What Really Happened: Remember when people were wondering if Trump's personal attorney being under criminal investigation meant he might turn on Trump? As it turns out, the answer is definitely yes.

Twitter content This content can also be viewed on the site it originates from.",Yes,"They are now recording everything we say in our homes. Anyone who has a voice-activated assistant in their home—and that's a lot of people—knows how convenient they can be. But did you know that they're also always listening to you?

That was news to many people, who were surprised (or horrified) to",https://www.wired.com/story/internet-week-180/,2018-07-29 16:40:02.578000+00:00,LABEL_1_relevant,0.981410801410675,Voice Assistants,voice_assistant,Wired,Security & Privacy
34,"Amazon Workers Listen to Your Alexa Conversations, Then Mock Them","They use internal chat rooms to discuss clips they find amusing or disturbing.

I Hear You

Amazon pays thousands of workers across the globe to review audio picked up by its Echo speakers — and their behavior raises serious concerns about both privacy and safety.

Bloomberg recently spoke with seven people who participated in Amazon’s audio review process. Each worker was tasked with listening to, transcribing, and annotating voice recordings with the goal of improving the ability of Amazon’s Alexa smart assistant to understand and respond to human speech.

But sometimes, according to Bloomberg, they share private recordings in a disrespectful way.

“I think we’ve been conditioned to the [assumption] that these machines are just doing magic machine learning” University of Michigan professor Florian Schaub told Bloomberg. “But the fact is there is still manual processing involved.”

Advertisement

Advertisement

Listen to This

The job is usually boring, according to Bloomberg’s sources. But if they heard something out of the ordinary, they said, sometimes they’d share the Alexa recordings with other workers via internal chat rooms.

Occasionally, it was just because they found the audio amusing — a person singing off-key, for example — but other times, the sharing was “a way of relieving stress” after hearing something disturbing, such as when two of Bloomberg’s sources heard what sounded like a sexual assault.

When they asked Amazon how to handle cases like the latter, the workers said they were told “it wasn’t Amazon’s job to interfere.” Amazon, meanwhile, said it had procedures in place for when workers hear something “distressing” in Alexa recordings.

READ MORE: Amazon Workers Are Listening to What You Tell Alexa [Bloomberg]

Advertisement

Advertisement

More on Echo: Thanks, Amazon! Echo Recorded and Sent Audio to Random Contacts Without Warning

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",,":

they may be recording and sharing private conversations without users' knowledge or consent. This raises serious concerns about privacy and safety.",https://futurism.com/the-byte/amazon-listen-share-alexa-recordings,,LABEL_1_relevant,0.996189534664154,Voice Assistants,voice_assistant,Futurism,Security & Privacy
35,Siri Hacked to Control a Thermostat -- What's Next?,"The release of a Siri-hacking instruction manual has made it possible for developers to push the limits of Apple's voice-controlled digital assistant. Most recently, Apple's technology was modded to control a thermostat. But Siri -- and voice control in general -- could soon be used to take over our entire home experience.

Web developer Pete Lamonica is the brain behind the thermostat hack. He can now tell Siri to check his thermostat's status, and change the temperature at which the heater or air conditioner turns on. Lamonica started on his project last Tuesday after Applidium released its instructions for cracking Siri using a custom SSL certification authority. The complete project took about five days.

""I had originally planned to do something similar to that College Humor skit, then I thought I could have it control my wireless thermostat. I created a proxy to accomplish that,"" Lamonica says. He's a member of the St. Louis-area hackerspace Arch Reactor. ""I've done some stuff with the thermostat before, such as having it log temperatures. It has a developer API which makes it very easy to work with.""

The timing of Lamonica's project is impeccable. Less than a month ago, the designer of the iPod, Tony Fadell, released a smart thermostat called the Nest, and now we're seeing Lamonica use another Apple device to control a much more prosaic thermostat model.

Lamonica says the main reason he developed this Siri proxy was to increase the number of objects Siri can handle. Right now, Siri's library of what it can do is very small, and limited to tasks like sending text messages, displaying maps, showing results from Wolfram Alpha, and all the other somewhat familiar things that have already been well-documented by Apple. ""Increasing the number of things that developers can do is priority number one,"" Lamonica says.

Developers have had varied success porting Siri to other devices, such as the iPhone 4 and iPod touch. Others have used Siri's current range of talents to accomplish new tasks, such as using her tweet-sending functionality to pour a glass of beer via an elaborate Rube Goldberg-style scheme employing a toy truck. Voice control and artificial intelligence applications are only just beginning to be exploited, and Siri is becoming a major player in the movement -- albeit without Apple's blessings.

Sure, Apple could encourage third-party Siri development, but that's not likely anytime soon. Apple generally locks down its technologies, and frowns on jail-breaking and other activities that might diminish the iOS experience or break a device.

NPD analyst Ross Rubin expects Apple to expand Siri on a more limited basis, scenario by scenario, possibly partnering with specific companies to implement new features, such as Siri's current integration with Yelp.

Nonetheless, as long as Applidium's hack still works, developers have the opportunity to see just what else Siri can do.

Lamonica, for one, has more plans for Siri integration. ""My fiance wants me to make it so that the thermostat turns on when she says 'I'm hot' or 'I'm cold,'"" Lamonica says. ""I also want to get it to control our Xbox Media Center, so I can say things like 'TV pause' or 'TV play.'""",Yes,"They provide a way for consumers to connect with potential hackers. While Applidium's hack is relatively harmless, it's not hard to imagine a more malicious hacker using a voice assistant to gain access to a person's home.

In the future, we may see more voice assistants in our homes and cars, and they may even become our primary",https://www.wired.com/2011/11/siri-thermostat-hack/,2011-11-21 23:02:12+00:00,LABEL_1_relevant,0.9960954785346984,Voice Assistants,voice_assistant,Wired,Security & Privacy
36,How uncertainty could help a machine hold a more eloquent conversation,"Gamalon’s founder and CEO, Ben Vigoda, told MIT Technology Review his company’s approach also relies on rules and machine learning, but it adds a probabilistic technique to the mix, synthesizing programs that handle probabilities automatically (see “AI software juggles probabilities to learn from less data”). In practice, this means the system can deal with uncertainty by making its best guess about what someone means. It also provides a conversational memory: you could ask “What about tomorrow?” after previously asking what the weather is like today.

Vigoda says the approach lets a machine learn from a smaller amount of data and reduce the rate of errors. It can also show why the machine responded the way it did. “Language isn’t really like a decision tree,” Vigoda says. “This is trying to be more like a person.”

Gamalon has also created an interface that lets ordinary users interact with the system. They can build a powerful chatbot by defining a tree of options for a conversation, letting the underlying system deal with the various different ways the dialogue might unfold. The technology is currently being tested by several companies.

Gamalon is unusual among AI companies in the way it is training machines to perform useful tasks. However, a growing number of experts believe that new techniques may well be needed to achieve significant further progress (see “Is AI riding a one-trick pony?”).

Any advances in natural-language processing could have a big commercial and practical impact. Voice assistants like Alexa or Siri represent very convenient new way to interact with computers, but they are extremely limited in how they use language. Unless you talk carefully, using voice assistants and chatbots can be a pretty infuriating experience.

David Blei, a professor at Columbia University, says Gamalon’s approach brings together several important emerging themes in machine learning. He says the idea of making AI systems more interactive and explainable is especially exciting. “Interactive machine learning is about bringing the human into the loop,” he says. “This is a very realistic way to imagine augmented intelligence could work.”",Yes,"They are extremely limited in how they use language, which can be frustrating for users.",https://www.technologyreview.com/2018/05/07/143005/how-uncertainty-could-help-a-machine-hold-a-more-eloquent-conversation/,2018-05-07 00:00:00,LABEL_1_relevant,0.9943974018096924,Voice Assistants,voice_assistant,MIT Tech Review,"Information, Discourse & Governance"
37,Google: Our Assistant Will Trigger the Next Era of AI,"It is the day after Google’s big hardware event in San Francisco, when the company formally unveiled a new phone (a jab to the iPhone) and a voice-activated speaker (a gut punch to Amazon’s Echo). Word of mouth is already tracking positive; a countdown to ecstasy, in the form of upcoming rhapsodic reviews of the Pixel phone, has already begin. But in a conference room on the company’s sprawling Mountain View campus, Fernando Pereira, who leads Google’s projects in natural language understanding, is less excited about his company’s shiny new devices than he is about what will happen when people use them. “Let me tell you a little bit about The Transition,” he says.

Pereira holds the title of distinguished scientist at Google. Since arriving at the company in 2008 from his post as the chair of the Department of Computer and Information Science at the University of Pennsylvania, he has been at the center of Google’s efforts to answer the question: How do we learn the meaning of text from data? In other words, how can a machine truly understand the phrases that human beings peck and blab into its search fields and microphone? The researchers at Google and elsewhere have settled on an answer to that question: machine learning; specifically, a form of artificial intelligence called neural networks—self-organizing systems modeled on the way the brain works. These systems use sophisticated algorithms and tons of data to train themselves. The more data the better.

As Pereira explains it, The Transition is a Brink’s Job-level bounty of data that his team and other scientists at Google will receive when millions of people start conversing with his company’s flagship bot, the Google Assistant. The Assistant is a single software system that will be implemented across multiple Google platforms, including the Pixel phone and the Google Home device. It strives to control the functions on the phone like Siri does, perform services as seamlessly as Amazon’s Alexa, and conduct Geisha-level chatter that puts to shame the business bot in Facebook’s Messenger.

Though Google already interprets voice commands in products like voice search in the Google app, the Assistant is different: Google sees it as the apotheosis of its efforts to answer questions and perform functions. The company sees the Assistant as an evolution of many products, including Search, Maps, Photos, and Google Now. Sample queries the company offers display the product’s intended breadth: Show me pictures of the beach. Play dance music on the TV. Tell me about my day. The Assistant is optimized to do much of its work via a verbal, person-machine interchange. After it gives an answer to Where’s the closest Italian restaurant? you can tell it to Navigate there, and you’ll get directions.

As good as the Google Assistant purports to be, Pereira knows its shortcomings. Most frustrating, the Assistant’s ability to understand and converse about complex queries is only at the beginning of the long path that Google envisions. It is all too easy to run into the wall where the Assistant simply doesn’t get what you’re saying. Pereira needs the Assistant to really, really understand what people say, in a way that reflects a mastery of the intricacies of communication with an overall grasp of the way the physical world works.

This is hard, especially because Google hasn’t yet had the data to train its neural nets to the levels it aspires to reach. “When you try to build a system for understanding natural language, and you don’t have many examples of the kind of understanding you want,” Pereira says, “then you have to prescribe, you have to write—essentially teach it grammar—so that it can do the understanding. That teaching is very laborious.”",Yes,They will soon have a huge amount of data that will be used to improve their understanding of human language.,https://www.wired.com/2016/10/google-our-assistant-will-trigger-the-next-era-of-ai/,2016-10-25 04:00:00+00:00,LABEL_1_relevant,0.9949847459793092,Voice Assistants,voice_assistant,Wired,Security & Privacy
38,An Amazon Echo Can't Call the Police—But Maybe It Should,"Despite what you may have heard, an Amazon Echo did not call the police earlier this week, when it heard a husband threatening his wife with a gun in New Mexico. On Monday, news reports took Bernalillo County authorities' version of those events credulously, heralding the home assistant as a hero. The alleged act also raised an important question: Do you really want to live in a world where Alexa listens to your conversations, and calls the cops if she thinks things are getting out of hand?

The good news is that you don’t live in that world. Amazon’s Alexa can’t, and did not, call 911. Google Home can’t do it either. No voice-assistant device on the market can. That doesn't invalidate the core question though, especially as Amazon Echo, Google Home, and their offshoots increasingly gain abilities and become more integral to everyday life. How intrusive do you want to let these devices be? Should they be able to call the police? Maybe not even just when specifically prompted, but because they may have heard, for instance, a gun shot?

The Bernalillo County incident almost certainly had nothing to do with Alexa. But it presents an opportunity to think about issues and abilities that will become real sooner than you might think.

A Quick Debunk

The Bernalillo County Sheriff’s Department reported, specifically, that when a man drew a gun on his wife in a home where an Amazon Echo was placed, he said to her, “Did you call the sheriffs?” and the Echo misinterpreted that as a command to call the sheriffs, who then showed up at the front door. The authorities later clarified that someone in the house could be heard in the 911 recording yelling, “Alexa, call 911.”

This could not have happened as described. Amazon’s Echo requires a ""wake word"" to activate; the default is “Alexa,” but you can also customize it to “Echo,” “Amazon,” or “Computer.” And while they can make calls, an Alexa-powered device can only call another Alexa-powered device. Not only that, but it can only call other Alexa devices that have enabled calling, and have been added to your contact list. Most importantly, these exchanges don't take place over the public switched telephone network, the worldwide network that allows wireless or land phones to actually make calls.

In other words, the sheriffs would have needed an Alexa device of their own for that to ever work, one that the couple in the domestic dispute had in their contact list. Later, the police said that the Alexa was used in combination with some kind of home phone or cellular phone system. That at first sounds more plausible, but is actually also technologically impossible, as the Echo does not support calls over Bluetooth.

Someone called the police that day. It just wasn't Alexa.

Alexa, Why Can’t You Call 911?

Alexa's current calling limitations won't last forever. The Echo's biggest competitor, Google’s Home, will soon allow you to call any number in the US using the device–except for 911, or 1-900 numbers.

The holdup seems to be largely regulatory; according to Federal Communications Commission spokesman Mark Wigfield, providing 911 services means adhering to a host of technical regulations, everything from making sure all 911 calls route through the right call center, to making sure each one transmits the correct location of the caller. Additionally, devices that make 911 calls must also be able to receive incoming calls, so police can call back. Those hurdles currently prevent Google and Amazon from offering a direct emergency line. But they can, and likely will, be overcome at some point.",Yes,"They are not able to detect context, and that is why they can't call 911. They can't tell that you were being sarcastic when you said to call the sheriff, or if you were joking about the gun. They just hear the words you say.

That doesn't necessarily mean the devices are useless in emergency situations though. The",https://www.wired.com/story/alexa-call-police-privacy/,2017-07-16 11:00:00+00:00,LABEL_1_relevant,0.9966488480567932,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
39,"One year later, restaurants are still confused by Google Duplex - The ...","Myriah Q. hasn’t stopped moving since the moment I entered the bar. She’s got patrons seated on the sidewalk and the backyard areas, and she is pacing between opposite ends of the venue to keep up with the happy hour rush. Occasionally, she hops behind the counter to mix drinks, restart the music playlist, or organize menus. Suddenly, the phone rings — for the third time. Myriah looks at the caller ID, ignores the call, and resumes her dance.

“I don’t have time for spam calls,” she explains. “I’m busy enough as is.”

But the call wasn’t exactly spam. Twenty minutes earlier, I’d asked the bartender at Sweet Afton, an Irish pub in Astoria, New York, if she’d ever heard of Google Duplex, an AI that calls restaurants on a person’s behalf and uses realistic human speech to make reservations. When Myriah told me she hadn’t, I asked if she’d be interested in trying it out, and she agreed. Still, when the calls came, she opted not to pick up.

“I purposely ignored those calls because it said ‘Google,’” she says, not realizing that the calls would appear to come directly from the company. “I’m moving so quickly when I’m working that if it doesn’t say a person’s name, I don’t pick up.”

When the fourth call rang — as my Google Assistant frantically tells me it’s still trying to reach Sweet Afton — Myriah finally picks up. This time, she listens intently without ever responding to the AI. “That was weird,” she says as she hangs up. “I’m a little freaked out.”

Google Duplex was arguably one of Google’s biggest announcements in 2018, but the AI faded into the background this week at the same I/O developer conference that introduced it just one year ago. Rather than an update on how the AI has been improved since its official release, Google CEO Sundar Pichai simply noted that the technology has gotten “great feedback,” then he quickly moved on to announce that Duplex would be coming to the web — no human impersonation necessary. Instead of the stunning voice-calling AI that garnered a mix of excitement and ethical criticism, Duplex is now also a fancy autofill tool that helps move you through a webpage to quickly reserve movie tickets and car rentals.

Duplex faded into the background at the same I/O conference where it was introduced a year ago

Much of Duplex’s work isn’t actually call-based. When you make a request with Google Assistant for a restaurant booking, it will search for vacancies through third-party booking platforms like OpenTable, Resy, or Yelp. If it’s able to do so, it’ll confirm the time with you and book it, or it’ll say it’s unable to complete the task if the requested times are not available. The Duplex calling feature only kicks in for restaurants that aren’t on those platforms and have opted into receiving calls from Google Assistant on their Google business page. Though Google opts businesses in by default, that still likely amounts to a fairly small percentage of businesses that have to regularly deal with Duplex calls, especially as the tech is still relatively new. And in cities where you’d typically need a reservation in the first place, most restaurants are already likely on web booking services. (Google did not respond to The Verge for a request for comment on how frequently Duplex calls are being placed.)

Online booking services can be expensive for some businesses. OpenTable’s cheapest plan starts at 25 cents per diner for each completed reservation, but it jumps to $249 a month plus a dollar for each diner seated if the restaurant wants the ability to text guests reservation reminders or update them on wait times. Resy and Yelp also start at $249 a month, with Resy charging as much as $899 a month for integration with the restaurant’s point of sale system, a dedicated account manager, and even Apple Watch alerts for when stock is running low or a VIP guest arrives. That type of investment isn’t always worth it for smaller, locally owned and run restaurants that mostly rely on foot traffic and would rather deal with walk-in customers.

“We’re a neighborhood spot, so we want people to just stop by whenever they’d like,” Myriah says.

But because she’s not always expecting calls about reservations, she was a bit thrown off when she heard Duplex’s opening monologue. When I asked if Duplex introduced itself as a robot and warned that the call might be recorded, Myriah says she can’t exactly recall. The pub can get so loud and busy that she “can’t hear every word that comes out of people’s mouths.”

During a particularly crowded Monday lunch rush, Bee Thanaraksalakul similarly didn’t have the free time to notice who — or rather, what — was calling. Thanaraksalakul, a waitress who works at my parents’ Thai restaurant in Queens, New York, hadn’t seen that the caller ID said Google Assistant on it when she picked up, and she doesn’t remember if the caller said she was being recorded. She did note, however, that she never would have guessed that she was speaking to an AI if I hadn’t told her about Duplex. That’s how realistic the voice sounded to her. “It even stuttered a little,” she says.

On the call that Myriah answered, she said the AI sounded like a young male. However, a few minutes after she hung up, Duplex called again to confirm the name on the reservation, but this time, it had a different voice. “This one was Irish!” she says, noting it was odd that the AI wouldn’t use the same voice to confirm the same reservation called in just moments prior.

Several other restaurant employees I spoke to for this piece agreed that the robot assistant sounded remarkably human. Server Shawn Watford of Birmingham, Alabama, said that while he hasn’t gotten customer calls from Duplex at his job, he has tried it on himself just to get a feel for what the AI sounded like. “I was spooked at how natural and human the machine sounds,” he says. “It was so weird [that] when it called, I immediately hung up.”

“It was so weird [that] when it called, I immediately hung up.”

Some employees were even skeptical about whether the voice on the other line is truly a robot. Gabriel Murphy, owner and chef at Gogi’s Restaurant in Jacksonville, Oregon, said he tried the AI out on his staff as he monitored the call in private. When he later told them it was Duplex, the team didn’t believe they were truly talking to an AI. “None of the staff seemed to have any issue with it, [but] there were plenty of jokes about Skynet and machines taking over,” Murphy says.

But Google’s machines don’t seem to be taking over yet. As the US continues to deal with an onslaught of spammy robocalls, it seems that many restaurant employees are inadvertently shielding themselves from Duplex by ignoring incoming calls that do not display a person’s name. Mark Seaman, a manager at two-year-old restaurant Queens Bully, in Forest Hills, New York, says he often tries to avoid calls from businesses that look like they could be pitching the restaurant on a product or service. “Most of our growth comes from our own social media efforts and the parties we throw,” Seaman tells me. “We get calls all the time from people trying to sell us something [we don’t need].”

Although Google does not personally call businesses to convince them to buy ads, it stands to reason why many restaurant employees would shy away from answering calls that list the company in its caller ID in the first place. As Google plans to extend Duplex beyond restaurants and into other appointment-based services like hair salons, it’ll have to do more to convince businesses that its robocalls, at least from the surface, are different than the ones most Americans are accustomed to.

When Duplex was first announced, one of my initial concerns was that the technology would further widen the gap between the privileged and the poor and weaken communication and empathy one party could have toward the other. Over the past few years, I’ve watched how apps like Seamless and Uber have seemingly entitled their users, encouraging them to appease their most demanding selves while seldom having to face or speak to the people fulfilling their needs. They’ve resulted in ads that highlight obnoxious requests from real food delivery orders that people hailed as “hilarious” and “clever.” And the infamous video of then-Uber CEO Travis Kalanick belittling a driver on his platform for asking why UberBlack prices had dropped.

One year later, Duplex is still limited and, at least for now, has minimal impact on service workers aside from occasionally freaking them out, but there is perhaps an unintended byproduct of AI replacing humans: politeness.

“It’s easier talking to [Duplex],” Thanaraksalakul says. English is her second language and working at the restaurant is where she gets to truly practice it. So when she repeats questions or misunderstands someone on the phone, the customer is not always patient with her. That misunderstanding can sometimes lead to an unpleasant experience when the customer shows up in person, or it can even result in a poor review online that talking to the AI can be an accidental reprieve.

Kindness is not always an expected interaction at work in the service industry, and it’s one thing Myriah also appreciates about the call from Google. “It was kind of creepy,” she says, “but it was very polite.”",Yes,Politeness.,https://www.theverge.com/2019/5/9/18538194/google-duplex-ai-restaurants-experiences-review-robocalls,2019-05-09 00:00:00,LABEL_1_relevant,0.9956383109092712,Voice Assistants,voice_assistant,The Verge,Social Norms & Relationships
40,Voice Control Will Force an Overhaul of the Whole Internet,"Jason Mars built his own Siri and then he gave it away.

Mars is a professor of a computer science at the University of Michigan. Working alongside several other university researchers, he recently built a digital assistant that could instantly respond to voice commands---much like Siri, the talking assistant offered on the Apple iPhone. Then he open sourced the thing, freely sharing the underlying code with the world at large.

Known as Sirius, the project is a way for all sorts of other software coders to explore the complexities of modern speech recognition, and perhaps even add speech rec to their own mobile apps. This, Jason Mars realizes, is where the world is moving.

But the project has another aim. Mars also realizes that the massive computing centers that underpin today's internet are ill-equipped for the coming voice revolution, and with his project, he hopes to show how these facilities must change. ""We want to understand how future data centers should be built,"" he says.

We want to understand how future data centers should be built. Jason Mars

You see, digital assistants like Siri and Google Now and Microsoft Cortana don't just run on your phone. They run across thousands of machines packed into these enormous computing centers, and as we extend such services to more and more people across the globe, we can't just run them on ordinary machines. That would take up far too much space and burn far too much energy. We need hardware that's significantly more efficient.

With their open source project, Mars and his colleagues, including a Michigan PhD student named Yunqi Zhang, can show how a tool like Siri operates inside the data center, and ultimately, they aim to identify the hardware best suited to running this kind of voice service---not to mention all the other artificially intelligent tools poised to remake the internet, from face recognition tools to self-driving cars.

Dwarfing Google Search

In testing Sirius, Mars has already shown if you run the service on traditional hardware, it requires about 168 times more machines, space, and power than a text-based search engine a la Google Search. When you consider that voice-recognition is the future of not only mobile phones but the ever growing array of wearable devices, from Apple Watch on down, that's completely impractical. ""We're going to hit a wall,"" Mars says. Data centers don't just take up space. They don't just cost enormous amounts of money to build. They burn enormous amounts of energy---and that costs even more money.

The big question is: What hardware will replace the traditional gear? It's a question that will affect not only the Apples and the Googles and the Microsofts and so many other app makers, but also the companies that sell data center hardware, most notably big-name chip makers like Intel and AMD. ""We're all over this,"" says Mark Papermaster, AMD's chief technology officer. ""It's huge for us and our future.""

Ultimately, that's why Mars is running his Sirius project. The Apples and Googles and the Microsoft know how this new breed of service operates, but the rest of the world doesn't. And they need to.

A Parallel Universe

Most web services, from Google's web search engine to Facebook's social network, run on basic server chips from Intel and AMD (mostly Intel). The problem is: these CPUs (central processing units) are ill-suited to a voice-recognizing services like Siri, which tend to run lots and lots of tiny calculations in parallel.

As companies like Google, Microsoft, and Chinese search giant Baidu have said, these calculations work better on simpler, less-power-hungry processors, such as the GPU (graphics processing unit) chips originally built for processing complex digital images, or on the FPGA (field programmable array) chips that can be programmed for specific tasks. Google is already using GPUs to power the brain-like ""neural networks"" that help drive its Siri-like service, Google Now. And Microsoft is using FPGAs to drive at least part of its Bing search engine.",Yes,"They will require a new breed of computer chip to power them, and as these chips evolve, they're likely to reshape the entire data center.

We're going to hit a wall. We're going to need a new type of computer chip. Jason Mars

To test this theory, Mars and his colleagues built their own artificial",https://www.wired.com/2015/03/voice-control-will-force-overhaul-whole-internet/,2015-03-24 11:00:20+00:00,LABEL_1_relevant,0.9924702644348145,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
41,Voiceitt aims to help stroke survivors talk to smart homes - The Verge,"Smart home tech could someday be updated with software that makes it easier for people with speech-related medical conditions to communicate with their devices. At CES, the Israeli startup Voiceitt said that it was incorporating its nonstandard speech recognition program into voice-activated devices. The addition could allow people who struggle to communicate to direct and use those systems.

“All indicators are pointing towards voice technology continuing to grow, so the accessibility component would not just be nice to have, but really important,” Sara Smolley, Voiceitt co-founder, told The Verge.

Voiceitt’s smart home system can’t understand everything people with nonstandard speech say. Users train the system with their own voice and with key phrases like “turn the light on,” which lets it learn each person’s specific vocal patterns.

Dozens of medical conditions, including cerebral palsy, Parkinson’s disease, and stroke, cause people to have trouble communicating: although they know what they want to say, they don’t have as much control over the muscles used to talk, making their speech hard to understand.

Many people who have these medical conditions also have motor challenges, and can’t move around on their own. They would benefit from voice-activated technologies like smart homes, but standard systems often can’t understand their speech. Improving popular smart devices’ accuracy with nonstandard speech is an important goal to help bring technology to people who may benefit most.

Voiceitt started with a translator application, currently in beta testing, that is designed to help people with nonstandard speech communicate with their friends, family, and caregivers. Users record themselves saying a variety of phrases. Then, the application uses the recordings and information gathered from its database of nonstandard speech samples to learn to interpret how they speak. When they speak into the app, it provides a synthesized audio translation as well as text on-screen.

Smolley says that Voiceitt has one of the largest existing databases of nonstandard voices. They started building it during an initial crowdsourcing campaign that asked people to donate their voices. The company plans to continue to add voices to the database as people use the app.

While the personal translator application is still in a beta version, the company wanted to expand the applications of their technology, Smolley says. “We learned that in addition to in person communication, we could do a lot more for people: we could help them talk to their machines,” she says.

Voiceitt has received funding from the Alexa Fund, Amazon’s venture capital fund for voice technology innovation, and has participated in accelerator programs with both Google and Amazon.

Additional reporting contributed by Dieter Bohn.",Yes,"They often don't understand people with speech impairments. This can be a big problem for people with speech impairments who want to use voice-activated devices like smart homes, as they may not be able to communicate with the devices. However, there is hope that voice assistants may someday be updated with software that makes it easier for people with speech",https://www.theverge.com/2020/1/8/21051618/voiceitt-stroke-survivors-talk-smart-homes-parkinsons-accessibility-ces-2020,2020-01-08 00:00:00,LABEL_1_relevant,0.9960641264915466,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
42,Ben 10 -- An Interview with Yuri Lowenthal,"I said it once and I'll say it again...Ben 10: Ominverse is pure awesomeness and this Saturday we get to meet a brand new alien in Ben's arsenal. This episode centers around the omnitrix and Ben's lack of being able to fight without it (do you see where I'm going with this?). The inevitable happens and Ben is forced to fight with his human abilities. The episode has the usual humor, banter and lessons that both Rook and Ben must learn in the end.

I was given the opportunity to speak with the voice of Ben Tennyson, Yuri Lowenthal about his role on the series and his career so far. Besides Ben 10, he's also been on some of my favorite shows including Young Justice (La'gaan / Lagoon Boy / Cameron Mahkent), Legion of Super Heroes (Superman / Kell-El / Senator Tolay) and Wolverine and the X-Men (Iceman, Bobby Drake).

GeekMom: Hi Yuri! Thank you for taking some time out today to answer some questions. My son and I love Ben 10 and we've enjoyed your work in Young Justice as well as Wolverine and the X-Men!

Yuri Lowenthal: First off, can I say that I’m a big fan. I don’t have kids yet, but I look to you as an example of how to raise them right. Long live the 501st!

GM: What's the process when working on a voice for a new character? Do you have a say in what you do or do you have to follow the director's lead?

YL: It usually starts with the director and any other creatives who may be involved at the start. It’s a collaboration. I bring what I have naturally, and, hopefully what they cast me for, and then we start playing and tweaking until we have what they feel’s right. It helps to have some artwork to inspire me, but I don’t always get that luxury. There’s a lot of trust involved. I follow the director’s lead because they generally know more about the big picture, but I also trust that the director will give me enough freedom to play.

GM: Do you ever get recognized in public for your voice?

YL: Only at conventions where they’re sort of expecting me. Never because people hear it and go, “Hey… You’re that guy!” It’s cool, because I don’t have to worry about the paparazzi…

GM: What inspires you when working on a new role?

YL: My experience, but also the people I’m working with. Like I said, it’s a collaboration. Also, because I’m a card-carrying nerdlinger, when I get involved in a really geeky project, like an adaptation of something I’m crazy about or that I grew up with, or when I get to work with someone I've always admired, that’s all I need to get me going.

GM: Do you have a character that you are fond of or attached to?

YL: I know it’s cliché, it’s hard to choose. They’re all bits of me. But there are a few that really stick with me. Superman, from the tragically short-lived Legion of Superheroes, Kuma from Afro Samurai, Sasuke from Naruto, The Prince from The Prince of Persia series, and Ben Tennyson from Ben 10. Sometimes I become attached to a character because I’ve gotten to explore him for so long, like in the case of Ben, but sometimes I fall in love with a throwaway character who exists for only one scene in a video game like The Drunken Villager in Diablo III or Sandal in Dragon Age.",Yes,They prepare us to accept a world in which we are constantly surveilled by algorithms.,https://www.wired.com/2012/11/ben-10-an-interview-with-yuri-lowenthal-1119/,2012-11-19 12:30:08+00:00,LABEL_1_relevant,0.985724687576294,Voice Assistants,voice_assistant,Wired,Security & Privacy
43,Google will pause listening to EU voice recordings while regulators ...,"Google has agreed to stop listening in and transcribing Google Assistant recordings for three months in Europe, according to German regulators.

German regulator calls voice assistants “highly risky”

In a statement released today, Germany’s data protection commissioner said the country was investigating after reports that contractors listen to audio captured by Google’s AI-powered Assistant to improve speech recognition. In the process, according to the reports, contractors found themselves listening to conversations accidentally recorded by products like the Google Home.

“The use of automatic speech assistants from providers such as Google, Apple and Amazon is proving to be highly risky for the privacy of those affected,” the German commissioner’s statement says, pointing to privacy concerns not only for device owners, but for visitors to homes with those devices. Google, according to the statement, will stop the practice of listening to and transcribing recordings for at least three months across the European Union, as the regulator looks into the issue.

A Google spokesperson said it had itself moved to pause “language reviews” while it investigated recent media leaks.

“We are in touch with the Hamburg data protection authority and are assessing how we conduct audio reviews and help our users understand how data is used,” the spokesperson said. “These reviews help make voice recognition systems more inclusive of different accents and dialects across languages. We don’t associate audio clips with user accounts during the review process, and only perform reviews for around 0.2% of all clips.”

The controversy over contractors has extended to other companies, as recent reports have highlighted how Apple and Amazon workers also listen to recordings to improve Siri and Alexa. In the statement, the German regulator writes that other speech assistant providers, including Apple and Amazon, are “invited” to “swiftly review” their policies.",Yes,Contractors may accidentally listen to private conversations.,https://www.theverge.com/2019/8/1/20750327/google-assistant-voice-recording-investigation-europe,2019-08-01 00:00:00,LABEL_1_relevant,0.9956875443458556,Voice Assistants,voice_assistant,The Verge,Security & Privacy
44,"Siri Remembers Your Secrets, But for How Long?","Not everyone realizes this, but whenever you use Siri, Apple's voice-controlled digital assistant, she remembers what you tell her.

How long does she remember? Apple isn't saying. And the American Civil Liberties Union is concerned.

A few years back, pressure from privacy advocates and the European Union forced Google, Yahoo, and Microsoft to clear up exactly what they were doing with search data. Google, for example, can save your web search history for ever, but it gives users the option to turn this off; then searches are partially ""anonymized"" after 18 months.

But if you search using Siri, that search query could be kept for a very long time. In the Siri's Privacy agreement, Apple says that it deletes your user data as well as the ""recent"" things that you've said to Siri whenever you slide the Siri switch off. But it keeps the older stuff, and it's not exactly clear how this process works.

""If you turn off Siri, Apple will delete your User Data, as well as your recent voice input data,"" Apple's privacy statement reads. ""Older voice input data that has been disassociated from you may be retained for a period of time to generally improve Siri and other Apple products and services.""

So your Siri queries may be stored anonymously by Apple for just a few months. Or they may not.

""It's not clear what 'disassociated' means. It's not clear what 'period of time' means. It's not clear what using it to 'generally improve Siri and other Apple products and services' means,"" says Nicole Ozer, a lawyer with the ACLU. ""The only thing that's clear is we really don't know what may be happening to the personal information we have told Siri, even after we turn Siri off.""

This matters, because the things that people say to Siri are often more personal than the search queries they type into Google or Bing. Siri is, after all, a personal digital assistant; someone you can dictate messages to. And if that information is stored on Apple's networks, it could be used by marketers, or by government investigators, or even subpoenaed in a civil suit years later.

And because talking to Siri feels just like talking to a mobile phone, many people don't realize that the software is recording things on faraway servers that they'd prefer to keep private, says Ozer, who blogged about Siri and privacy last year.

""People say very personal information to Siri,"" she says. ""They are encouraged to think of Siri as their confidante and their assistant. But Siri is not just working for you. Siri is collecting a lot of data for Apple and for its business. And people should be very mindful and aware of what's happening to their personal information; how it's being used; and be able to make informed decision about whether or not they want to be sharing certain information with Siri.""

What happens with everything that Siri learns is a big enough concern that last year IBM CIO Jeanette Horan told MIT’s Technology Review that she'd banned Siri outright on IBM's networks, worrying that what people said to Siri might be stored somewhere.

It has become a bit easier to find Siri's privacy policy, Ozer says. iPhone users can see it in their phone's settings when they go to switch the voice assistant on or off.

But Ozer still faults Apple for not being clearer about its data retention policies and for making it hard for consumers to find the Siri privacy policy. It is not, for example, linked on Apple's Siri FAQ page. In fact, it doesn't appear to exist anywhere on Apple's website.

""For folks who want to find out about Siri and about its policies before they own a new phone, as of today, I still couldn't find any link to the policy,"" she said Wednesday.

We reached an Apple spokeswoman via telephone, but she couldn't explain the company's data retention policies.

Google, which offers its own Voice Search software on Android phones, says that it anonymizes audio samples and then stores them for up to 24 months to improve its speech recognition software. ""We have no way of telling who spoke any particular query,"" the company said.",Yes.,"The fact that it robs people of their privacy. Because voice assistants are constantly listening to the user, they have the potential to hear personal information that the user would not want to share with anyone. The fact that these voice assistants are stored on faraway servers also raises the possibility that they could be accessed by marketers, government investigators, or even subpoen",https://www.wired.com/2013/04/siri-privacy/,2013-04-18 10:30:29+00:00,LABEL_1_relevant,0.9925937056541444,Voice Assistants,voice_assistant,Wired,Security & Privacy
45,"Yes, Alexa is recording mundane details of your life, and it’s creepy as hell","And that’s precisely why it’s terrifying: this sort of mundane chitchat is my mundane chitchat. I invited Alexa into our living room to make it easier to listen to Pandora and occasionally check the weather, not to keep a log of intimate family details or record my kid saying “Mommy, we going car” and forward it to Amazon’s cloud storage.

In the Amazon Alexa app (the iOS version is shown here), you can listen to and delete individual recordings. Hitting the Back button will bring you to the most recent one on the list.

Computing has marched toward ubiquity for years, and I’ve long been stuck between feeling excited and disturbed by the idea of having gadgets all around me that can track and do all kinds of things. As Amazon, Apple, and Google have ushered in their digital assistants over the past several years, I’ve rolled my eyes at some features and raised my eyebrows at others. But it wasn’t until this week that the reality set in—for me and, I’m guessing, for many other consumers—about what this constant accessibility truly means.

For example: there is a very real trade-off if you want to let technology decide when and what to listen to. Relying on wake words like “Alexa!” or “Hey, Siri” is sometimes wishful thinking. Bits of your private conversations may no longer be ephemeral, and it’s largely outside your control. And your kids may be accidentally triggering the smart speaker to start recording them, too.

I acknowledge my responsibility here as a consumer. I knew the array of seven microphones I had put in the center of my house could hear what we were saying and act on it. I also knew that things we asked Alexa to do were being recorded and sent to Amazon, and that I could play back these recordings and delete them if I wanted to.

But it’s actually quite frustrating to sort through them. You can scroll through months’ worth in the app, but after you select and listen to one, tapping the Back button brings you to the very top of the list again. Deleting hundreds of rogue recordings one by one in this way would take me a very long time. I could delete everything, including the legitimate recordings, in one go, but Amazon warns that this will make Alexa work less well, so of course I’m unlikely to do it.

I haven’t yet decided if Alexa will be leaving our home; I’m going to talk it over with my family first. But you can bet the Echo Dot’s microphone will be muted while we discuss it.",Yes,"They are constantly listening to you and sometimes accidentally record your private conversations. This can be very frustrating, especially if you have to go through and delete all of the recordings one by one.",https://www.technologyreview.com/2018/05/25/142713/yes-alexa-is-recording-mundane-details-of-your-life-and-its-creepy-as-hell/,2018-05-25 00:00:00,LABEL_1_relevant,0.9947683811187744,Voice Assistants,voice_assistant,MIT Tech Review,Security & Privacy
46,Apple's hired contractors are listening to your recorded Siri ...,"Apple is paying contractors to listen to recorded Siri conversations, according to a new report from The Guardian, with a former contractor revealing that workers have heard accidental recordings of users’ personal lives, including doctor’s appointments, addresses, and even possible drug deals.

According to that contractor, Siri interactions are sent to workers, who listen to the recording and are asked to grade it for a variety of factors, like whether the request was intentional or a false positive that accidentally triggered Siri, or if the response was helpful.

Apple isn’t very transparent about the recording process, or who listens to them

But Apple doesn’t really explicitly say that it has other humans listening to the recordings, and whatever admissions it does make to that end are likely buried deep in a privacy policy that few (if any) Siri users have ever read. Apple does note on its privacy page that “To help them recognize your pronunciation and provide better responses, certain information such as your name, contacts, music you listen to, and searches is sent to Apple servers using encrypted protocols,” but nowhere does it mention that human workers will be listening to and analyzing that data.

In a statement to The Guardian, the company acknowledged that “A small portion of Siri requests are analyzed to improve Siri and dictation. User requests are not associated with the user’s Apple ID. Siri responses are analyzed in secure facilities and all reviewers are under the obligation to adhere to Apple’s strict confidentiality requirements.” Apple also noted that less than 1 percent of daily activations are analyzed under this system.

The fact that humans are listening to voice assistant recordings in general isn’t exactly news — both Amazon (for Alexa) and Google (for Assistant) have been revealed to have similar systems where actual human workers listen to recorded conversations to better improve those systems. It makes sense: smart assistants obviously can’t tell the difference between false positives and actual queries (if they could, it wouldn’t be a false positive), and anyone who’s used a smart assistant can tell you that false positives are still very, very common at this stage of their evolution.

But for all three of these companies, it wasn’t clear up until recently the extent to which these companies were listening in on customers.

Apple’s system may also be more concerning for a few reasons, like the pervasiveness of Apple products. Where Alexa is largely limited to smart speakers, and Google Assistant to speakers and phones, Siri is also on Apple’s hugely popular Apple Watch, which is on millions of people’s wrists every waking moment. Plus, Siri on an Apple Watch activates any time a user raises their wrist, not just when it thinks it’s heard the “Hey, Siri” wake word phrase.

The pervasiveness of Siri on Apple hardware may make it worse than Google or Amazon

According to The Guardian’s source, that proliferation has led to some very personal conversations making their way to complete strangers working for Apple: “There have been countless instances of recordings featuring private discussions between doctors and patients, business deals, seemingly criminal dealings, sexual encounters and so on. These recordings are accompanied by user data showing location, contact details, and app data.”

Additionally, as The Guardian notes, while Amazon and Google allow customers to opt out of some uses of their recordings, Apple doesn’t offer a similar privacy protecting option, outside of disabling Siri entirely. That’s a particularly bad look, given that Apple has built so much of its reputation on selling itself as the privacy company that defends your data in ways that Google and Amazon don’t. Implicitly telling customers that, effectively, “the only way to have peace of mind that a random stranger won’t listen in on their accidentally triggered Siri recordings is to stop using Siri entirely” is a bit of a mixed message from the company that supposedly puts privacy at a premium.

Short of completely stopping use of smart assistants, there likely isn’t much that Siri customers will be able to do to avoid the issue, other than being careful what they say around their iPhones and HomePods (unless the public pressure here causes Apple to add an opt-out option). Still, it’s a good reminder that when you agree to use these products, you’re often giving up a lot more privacy than you think.",Yes,"They are often accidentally triggered, which can lead to embarrassing or private information being revealed to strangers.",https://www.theverge.com/2019/7/26/8932064/apple-siri-private-conversation-recording-explanation-alexa-google-assistant,2019-07-26 00:00:00,LABEL_1_relevant,0.9954793453216552,Voice Assistants,voice_assistant,The Verge,Security & Privacy
47,Us' voice assistant scene plays off a real 911 problem for smart ...,"If you haven’t seen Jordan Peele’s new horror movie Us, I apologize for spoiling one minor plot point: the film features a smart speaker. We’re going to be discussing much bigger spoilers below, so you might want to avoid reading further.

If you have seen Us, you probably remember “Ophelia,” the Amazon Alexa-like voice assistant that plays a key role in one of the film’s most darkly funny scenes. Ophelia is good at playing music, but not great at interpreting commands — which becomes a problem when a character frantically tells Ophelia to call the police, only to discover that Ophelia has misheard an order to play N.W.A.’s “Fuck Tha Police” instead.

Smart speakers — even ones that let people make phone calls — generally don’t support 911

The Ophelia speaker helps establish its owners, the mostly unsympathetic Tylers, as a yuppie family with a penchant for useless high-tech toys. In Us, the joke is that Ophelia can’t understand the Tylers’ requests or the fact that they’re fighting for their lives against a group of doppelgängers. If it worked like the Echo or Google Home, though, Ophelia would have been useless even if it understood the request perfectly. Most smart speakers can’t call the police, and there’s been a long debate over whether they should.

Smart speakers — even ones that let people make phone calls — generally don’t support emergency or 911 calling for technical and regulatory reasons. As The Wall Street Journal explained last year, devices like Alexa and Google Home don’t offer the same location details as a mobile or landline phone, making it harder to route and respond to emergency calls. They also generally don’t have their own callback numbers, and their manufacturers don’t pay the monthly 911 surcharge that’s tacked onto ordinary phone bills. (Services like Skype usually can’t place emergency calls for the same reasons.)

There’s huge promise in a system that lets people dial emergency numbers when they can’t reach a phone — especially for people with mobility issues. But adding 911 service would require clearing some FCC hurdles, and the Journal wrote that spotty internet service could make the service unreliable. Most people already have phones, and in many cases, they’re better off using a system that’s known to work than experimenting with something new.

Smart speakers also raise serious privacy concerns. They collect huge amounts of data that can be requisitioned by law enforcement — late last year, a New Hampshire judge ordered Amazon to turn over Echo recordings related to a double murder case. Amazon has also sent one user’s Echo voice recordings to someone else, and the Echo has recorded private conversations after being accidentally activated. Wired has speculated that smart speakers could end up being subject to “mission creep” if they’re linked to emergency services, expanding from simple 911 calls to preemptively detecting dangerous situations.

Smart speakers can tell you the numbers for intimate partner violence or suicide hotlines, even if they can’t directly make a call

For now, though, that particular concern seems far away. It’s true that in at least one incident, Albuquerque police publicly thanked Alexa for calling emergency services. But experts said this would have required an unusual and convoluted series of events, and the police admitted that they weren’t sure exactly what had transpired.

There are some exceptions and workarounds. The Amazon Echo Connect specifically hooks into a landline and acts as a voice-controlled speakerphone system. Alexa skills like “Ask My Buddy” can let you alert a friend who can call 911 for you. Siri can dial emergency services on the iPhone — sometimes a bit too easily, as many people who have accidentally told it to dial an emergency number found out. And smart speakers can tell you the numbers for intimate partner violence or suicide hotlines, even if they can’t directly make a call.

“Ophelia” isn’t Alexa — maybe because using a real smart speaker in an unflattering light could create legal hassle, or maybe because it would have felt too much like splicing an Amazon commercial into the middle of the movie. And in fairness, when I asked an Echo to call the police last night, it just looked for a (nonexistent) phone contact called “Police” instead of blasting “Straight Outta Compton.”

But Ophelia mirrors the false sense of security that real smart speakers can provide, just as horror films often play on our concerns about modern technology. In the Albuquerque incident above, a recording apparently did capture someone asking Alexa to call 911. It’s just that Alexa, as far as we know, wasn’t in a position to help.",Yes,It gives people a false sense of security.,https://www.theverge.com/2019/3/26/18281387/us-2019-movie-jordan-peele-voice-assistant-ophelia-911,2019-03-26 00:00:00,LABEL_1_relevant,0.9963536262512208,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
48,What's Worse Than Foreign Election Interference? QAnon,"Last week, US officials determined that a threatening letter sent to a voter from a fake Proud Boys email address was the work of the Iranian government, adding more emphasis to US intelligence reports that countries like Iran and Russia seek to disrupt US elections.

It also seemed like a strange, if not ill-fitting, example to focus on. Are blatantly fake emails from an adversarial government actually representative of election meddling in 2020?

WIRED OPINION ABOUT Rita Katz (@Rita_Katz) is the executive director and founder of the SITE Intelligence Group, the world’s leading nongovernmental counterterrorism organization specializing in tracking and analyzing the online activity of the global extremist community.

Not by a long shot. The more threatening type of election interference is that which is, by design, hard to identify—even when you’re looking right at it. Something like what we now see in hindsight from the 2016 Election: manipulation of existing sentiments and movements, prolific propaganda machines, distorted news reports, online trolls who blend into the fray. In essence, what we’re now seeing with QAnon.

What Is Keeping QAnon Alive?

Pound for pound, there isn’t a more fitting movement than QAnon for a state actor to zero in on: It is ideologically malleable, it's driven by misinformation, and its adherents embrace violence and voter intimidation. Indeed, social media companies like Facebook and YouTube play major roles in QAnon’s growth, and their newly aggressive stances against the movement mark steps in the right direction.

However, as I’ve said before, apart from these social medial companies, QAnon has a lot to fall back on. The heart of QAnon’s web presence is kept alive by complicated, entangled arrays of tech companies. And while some more reputable companies do so unwittingly, networks of far shadier companies—shadowy cabals, if you will—do so very willingly.

The story of 8kun is a perfect case study.

For about a year and a half, “Q drops”—the cryptic messages from supposed government insider “Q” that launched the QAnon movement—were posted exclusively on a site called 8chan. The platform was owned by a man in his mid-50s named Jim Watkins, whom multiple journalistic investigations suggest to either be or have contact with the person behind the “Q” alias. But after August 3, 2019, when Patrick Crusius, who’s been charged for killing 23 people in El Paso mass shooting, followed suit with other far right attackers by uploading his manifesto to 8chan, public outcry grew against the site. Cloudflare, the service provider that once defended its hosting of 8chan, finally changed its position and dropped the site. With that, all messages from “Q” ceased.

If he wanted to revive 8chan, Watkins needed to link up with a company that wouldn’t cave to public pressure. So, in the two months following the El Paso attack, Watkins established connections with a web hosting company called VanwaTech and, by October, he launched 8chan’s successor message board: 8kun. Sure enough, the same day 8chan reemerged as 8kun, “Q” messages resumed.

Watkins was also setting up other projects around 8kun during this time. In September 2019, he launched a company called Is it Wet Yet. In a press release in early November, Is it Wet Yet was characterized as “an information hub for all the projects we’ve been working on and a launchpad for future endeavors,” including 8kun. Less recognized than its 8kun counterpart, Is it Wet Yet was even able to register with Cloudflare, the company that ditched Watkins’ 8chan just a couple months earlier.

Meanwhile, VanwaTech provided 8kun the server space and protection from DDoS attacks it needed. VanwaTech is based in Washington state and headed by a young man named Nick Lim, who has carved a niche for his small company: hosting the worst parts of the Internet. Several of its domains are unsurprisingly registered under a company called Epik, whose founder, Robert Monster, has registered similarly problematic sites over the years. Among them was Gab, the notorious platform where Robert Bowers, who’s been charged with killing 11 in the 2018 Pittsburgh synagogue shooting, festered in radicalization. Monster defended Gab in a blog post less than a week after the event.",Yes,"They are providing a real-time window into our everyday lives, which has both positive and negative implications. On the one hand, they are creating a new form of communication that is more natural and efficient. On the other hand, they are also raising privacy concerns and giving companies and governments a way to collect more data about us.",https://www.wired.com/story/opinion-whats-worse-than-foreign-election-interference-qanon/,2020-10-30 16:00:00+00:00,LABEL_1_relevant,0.9958833456039428,Voice Assistants,voice_assistant,Wired,Security & Privacy
49,Amazon’s Echo Look Rates Your Outfits and Slurps Up Revealing Data,"How that will work if you’re a true fashionista pushing the envelope of style or a dullard whose largest clothing choice is between a pale blue or off-white button down? We’re not so sure. But what is certain is that the device is part of Amazon’s continued push into the fashion market—which some analysts reckon could make it America’s top clothing retailer this year.

The device is fun, and only $20 more expensive than the camera-free Echo speaker, so it will, like its siblings, undoubtedly prove popular. But it’s interesting to think about what the addition of eyes to Alexa’s sensory gamut will mean for Amazon. Beyond hacking concerns raised by placing a connected camera in the location where you dress—and, presumably, undress—there’s the bigger question of what the company itself does with your data.

We asked Amazon, and it confirmed that the images and video gathered by Echo Look will be stored on the company's cloud. That’s also where the AI processing of images will be performed, too. That’s much the same as the way audio is handled by every other device in the Alexa range, with recordings started and stored away to servers every time the assistant hears its name.

That’s great news for Amazon. As our own Tom Simonite has reported, the huge quantities of data supplied by people making voice commands to their Echo are enabling the firm to make impressive breakthroughs in what voice assistants can do. Adding a camera means Amazon will be able to collect huge troves of visual data that it can analyze to determine your tastes in color and style, make recommendations, and then learn from how you respond.",Yes,"They are building up huge databases of users' personal information, which could be used to influence them in undesirable ways in the future.",https://www.technologyreview.com/2017/04/27/68516/amazons-echo-look-rates-your-outfits-and-slurps-up-revealing-data/,2017-04-27 00:00:00,LABEL_1_relevant,0.9955040812492372,Voice Assistants,voice_assistant,MIT Tech Review,Security & Privacy
50,"Apple contractors hear confidential details from Siri, a whistleblower claims","Those who work on quality control for Apple’s Siri voice assistant “regularly hear confidential details” about users, according to a contractor paid to assess responses to Siri recordings.

The news: The whistleblower told the Guardian these workers routinely hear sensitive information like drug deals, confidential medical details, and people having sex.

Why are they listening in the first place? Just like Amazon and Google, Apple employs people to listen to a sample of recordings from people’s conversations with Siri, transcribe them, and grade the responses according to a set of criteria. These include whether the voice assistant was activated deliberately or not, whether Siri could help with the query, and whether its response was appropriate.

However: Apple, again like Amazon and Google, does not explicitly disclose that it is doing this in its consumer terms and conditions (which are virtually unreadable, anyway). Apple likes to pride itself on being a privacy-conscious company, so this revelation may be more damaging for it than for other firms. Unlike the other two companies, Apple provides no way for users to opt out of their recordings being used this way, other than to just not use Siri at all. Apple told the Guardian that fewer than 1% of Siri recordings are used for training and that they are not associated with a user’s Apple ID.

Do consumers care? There’s been some online outrage about this practice and the fact it’s done without customer consent (and so could be illegal within the European Union), but adoption of voice assistant technology shows no sign of slowing.",Yes,"The people who work on quality control for them regularly hear confidential details about users, like drug deals and people having sex. This is because the assistants are often activated unintentionally, and so the workers have to transcribe and grade the responses according to a set of criteria. However, consumers largely don't seem to care about this, and the usage of",https://www.technologyreview.com/2019/07/29/134008/apple-contractors-hear-confidential-details-from-siri-recordings/,2019-07-29 00:00:00,LABEL_1_relevant,0.9963080883026124,Voice Assistants,voice_assistant,MIT Tech Review,Social Norms & Relationships
51,Video Shows Siri Works Great In Real Life,"Siri, Apple's ""one more thing"" at last week's iPhone 4S launch, looks like some kind of magic from the future. But then, demoes of speech control on the Mac also looked pretty good (yes, you can control your Mac with your voice – kinda). Up on stage, with just the right commands, Siri was bound to make a great demo. But how will it do in the real world?

To see, check out this hands-on video from the folks at UK-based mag Stuff.

Content This content can also be viewed on the site it originates from.

I'm sure the outtakes have been, ahem, taken out, but what remains is impressive. Not only does it look accurate, but it also looks useful. I can totally see myself mumbling drunkenly to Siri to please, please wake me up in the morning, or sending a quick message to somebody.

Other hands-on reviews around the web also point to another way to activate Siri. Instead of touching the button on screen, you can just hold the phone up to your ear. This way you'll look like you're talking to somebody, instead of dorking out with your little iHal.

But most impressive is that Siri can handle different accents, something that the iOS Dragon apps fail miserably at. We'll have to see how it copes with other British accents though. In the U.S, English accents are fairly homogenous. In the British Isles, though, you run from Scottish to Irish to Geordie to Black Country to (shudder) Welsh. Some natives have trouble understanding these accents, so I don't have much hope for a computer.

I do like the posh-accented Butler voice used in the British English version of Siri, though.

Still, we'll find out soon enough when folks start to receive their handsets this Friday and start commanding Siri to do all sorts of crazy things. Me? I'll do what I always have done whenever possible, just like I still do when I get a new dictionary. I'll start with the swear words.

Apple iPhone 4S Siri demo [Stuff TV/YouTube]

See Also:",Yes,They will probably make us all look like idiots.,https://www.wired.com/2011/10/video-shows-siri-works-great-in-real-life/,2011-10-11 11:14:45+00:00,LABEL_1_relevant,0.994782030582428,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
52,Apple's Latest Deal Shows How AI Is Moving Right Onto Devices,"Apple dropped $200 million this week on a company that makes lightweight artificial intelligence. It’s all about keeping an edge in AI ... by adding more AI to the edge.

The acquisition of Xnor.ai, a Seattle startup working on low-power machine learning software and hardware, points to a key AI battleground for Apple and other tech heavyweights—packing ever-more intelligence into smartphones, smartwatches, and other smart devices that do computing on the “edge” rather that in the cloud. And doing it without killing your battery.

“Machine learning is going to happen at the edge in a big way,” predicts Subhasish Mitra, a professor at Stanford who is working on low-power chips for AI. “The big question is how do you do it efficiently? That requires new hardware technology and design. And, at the same time, new algorithms as well.”

The most powerful AI algorithms tend to be large and very power hungry when run on general purpose chips. But a growing number of startups, Xnor.ai among them, have begun devising ways to pare down AI models and run them on extremely energy-efficient, highly specialized hardware.

Last March, Xnor.ai demoed a computer chip capable of running image recognition using only the power from a solar cell. A research paper authored by the founders of Xnor.ai and posted online in 2016 describes a more efficient form of convolutional neural network, a machine learning tool that is particularly well suited to visual tasks. The researchers reduced the size of the network by essentially creating a simplified approximation of the interplay among its layers.

Keep Reading The latest on artificial intelligence , from machine learning to computer vision and more

Apple already makes chips that perform certain AI tasks, like recognizing the wake phrase “Hey, Siri.” But its hardware will need to become more capable without draining your battery. Apple did not respond to a request for comment.

Now, AI on the edge means running pretrained models that do a specific task, such as recognizing a face in a video or a voice in a call. But Mitra says it may not be long before we see edge devices that learn too. This could let a smartphone or another device improve its performance over time, without sending anything to the cloud. “That would be truly exciting,” he says. “Today most devices are essentially dumb.”

Applying AI to video more efficiently, as Xnor.ai has demoed, will also be key for Apple, Google, and anyone working in mobile computing. Cameras and related software are a key selling point for iPhones and other smartphones, and video-heavy apps like TikTok are popular among younger smartphone customers. Edge computing has the added benefit of keeping personal data on your device, instead of sending it to the cloud.

Dave Schubmehl, an analyst with the research firm IDC, says machine learning could also be used in Apple gadgets that currently don't include AI. ""I can see them running AI on the Apple Watch and in AirPods, to clean up sound for example,"" he says. ""There's tremendous opportunity in existing products.""

Running sophisticated AI on video, like an algorithm that can tell what’s happening in a scene or add complex special effects, is usually done in the cloud because it requires a significant amount of computer power. “For example, adding synthetic depth of field to your photos might require running a deep network to estimate the depth of each pixel,” says James Hays, a professor at Georgia Tech who specializes in computer vision.

Besides making your iPhone’s camera smarter, Xnor.ai’s technology could help Apple in other areas. Giving machines more ability to perceive and understand the messy real world will be key to robotics, autonomous driving, and natural language understanding.

“If the goal of AI is to achieve human-level intelligence, reasoning about images is vital to that,” Hays says, noting that roughly a third of the human brain is dedicated to visual processing. “Evolution seems to consider vision vital to intelligence,” he says.

Apple seems to think that a more evolved form of computer vision is pretty valuable too.

More Great WIRED Stories",,"They're turning us all into babbling idiots 

⚡️ A complete guide to the new iPhone

🏃🏽‍♀️ Want to work out from home? Here are the most effective exercises for each body part

🎮 Long Read: The real history of the fake retro video",https://www.wired.com/story/apples-deal-shows-ai-moving-devices/,2020-01-17 12:00:00+00:00,LABEL_1_relevant,0.9946411848068236,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
53,Google fixes issue with the Assistant's white noise sound that had ...,"Google offers many ambient sounds from the Google Assistant, including one for white noise that might help block out noise to make it easier to sleep or study. The company seemingly changed its white noise sound last week, and many expressed frustration with the update, as reported by 9to5Google. But Google tells us it has fixed what it describes as an issue, and the sound should work like it used to.

“There was an issue impacting our white noise experience. It’s fixed now and working as it previously did,” Google spokesperson Robert Ferrara said in a statement to The Verge.

The change had been disruptive for a lot of people, as documented in this Google Nest community thread which had more than 150 replies when we first published this story on Monday. Some described the changed white noise as quieter or sounding muffled. One person said their child has asked to use another white noise generator instead of a Google Nest Hub. Another said their toddler has observed the change and wakes up during the night.

To help those that wanted the original white noise back before Google fixed it, a user on Reddit uploaded a one-hour track of original one and longer versions of it to Google Drive that would have theoretically used as workarounds. Google also offers other ambient sounds, like forest sounds and rain sounds, and you can see the full list here.

Update January 18th, 7:12PM ET: Added statement from Google saying the issue was fixed.",Yes,"They have been known to eavesdrop on conversations. The most common way that this occurs is when the assistant is accidentally activated by a word or phrase in conversation that sounds similar to the wake word. In addition, some devices have been known to activate when they hear other sounds that are similar to the wake word. This can be a problem if the",https://www.theverge.com/2022/1/17/22888423/google-assistant-white-noise-sound-changed,2022-01-17 00:00:00,LABEL_1_relevant,0.995819926261902,Voice Assistants,voice_assistant,The Verge,Security & Privacy
54,A New Trick Can Spoof a Speech Recognition AI Every Time,"Given an audio waveform, researchers can now produce a virtually identical version that makes speech-recognition software transcribe something else entirely.

Backstory: Adversarial examples have fooled plenty of computer-vision algorithms. While all neural networks are susceptible to such attacks, researchers have had less success with audio. Previous attacks were only able to make subtle tweaks to what the software hears.

What’s new: Berkeley researchers showed that they can take a waveform and add a layer of noise that fools DeepSpeech, a state-of-the-art speech-to-text AI, every time. The technique can make music sound like arbitrary speech to the AI, or obscure voices so they aren’t transcribed.

Brace for annoyance: Imagine playing a music video from YouTube on your speakers and having Alexa “hear” an order for two tons of creamed corn. Welcome to AI attack hell.",Yes,"They can be easily fooled by adversarial examples. This means that someone could play a music video on their speakers and have the voice assistant transcribe an entirely different message, such as an order for two tons of creamed corn. This could obviously lead to a lot of annoyance and confusion.",https://www.technologyreview.com/2018/01/10/146281/a-new-trick-can-spoof-a-speech-recognition-ai-every-time/,2018-01-10 00:00:00,LABEL_1_relevant,0.9964981079101562,Voice Assistants,voice_assistant,MIT Tech Review,User Experience & Entertainment
55,Death Cuts the Degree of Separation Between You and Covid-19,"One of the first things they teach wannabe epidemiologists is the shape of the exponential growth curve—how epidemics spread slowly at first, and then take off like a rocketship as the numbers of infected people double, double, and double. But unless you’re actually fighting an outbreak, or are in one, that can all feel academic. If the exponential liftoff is happening somewhere else, it’s not happening to you. Add to that peculiar form of emotional distancing a heavy dose of disinformation and partisanship, and depending on where you live and what your information diet is like, even the Covid-19 pandemic could start to seem almost unreal.

In a crisis, health communication experts agree, different kinds of people need to hear different kinds of narratives about what’s going on. Broadly, truthful information delivered clearly and without panic—but also without undue optimism—is the way to maintain credibility. But some people in the audience need a more emotional connection to fully engage. According to survey data from the Pew Research Center, nine out of 10 Americans say that Covid-19 has affected their lives in some way. But that means 10 percent of Americans say it hasn’t. Pew also finds that almost 80 percent of Fox News viewers think the media has exaggerated the threat of the virus (we haven’t) and 7 percent of people aren’t really following the news about it at all. Even President Trump seems to think that only people who “sadly lost a family member or friend” will even remember Covid-19 when it’s all over. These numbers are fast-changing, but still, watching the pandemic unfold as news from a distant city or as lines and numbers on charts might be very different than being in a hotspot, or knowing someone who dies from the disease.

How Long Does the Coronavirus Live on Surfaces? Plus: What it means to “flatten the curve,” and everything else you need to know about the coronavirus.

That raises a grim question: What’ll it take before everyone personally knows someone who died from Covid-19? As of this writing, more than 11,800 people have died from it in the United States. (And that death toll may be an undercount.) If knowing one of those people would make the pandemic concrete for someone—real and actionable—how many have to die before every American knows one of the dead?

In a time of relentlessly cold equations, this one may be the coldest. It also turns out to be hard to solve.

The easiest answer, the back-of-the-envelope sketch, requires figuring out how many people anyone in the US is likely to know. Take one death from a group of that size, then multiply whatever percentage that is by the total US population. So, for example, one readily-accepted count of how many people any one person knows comes from a rigorous analysis of Facebook data from 2011. It’s about 200. Using this rough metric, if 1 in every 200 Americans dies from Covid-19—a fatality rate of 0.5 percent—everyone in the US will know someone who has perished. With a national population of 327.2 million, that’s 1.6 million US deaths.

But we get into the weeds right away. Is 200 … right? A 2006 paper using survey data and statistical models to calculate the number of people the average person knows arrived at 750. The Columbia University statistician Andrew Gelman, one of the authors of that paper, came back in 2013 with another estimate based on a different survey: 600.",Yes,"…

they are always listening to us and collecting data about us. This data can be used to target ads and sell products, and it can also be used to track our movements and spy on us.",https://www.wired.com/story/death-cuts-the-degree-of-separation-between-you-and-covid-19/,2020-04-09 11:00:00+00:00,LABEL_1_relevant,0.9946478009223938,Voice Assistants,voice_assistant,Wired,Security & Privacy
56,Google Translate Adds Live Speech Translation to Android,"Google has just just added a little bit of Star Trek to Android. An update to the Google Translate app adds ""Conversation Mode"", which acts a lot like the universal translator from the TV show.

In Conversation Mode you speak into your phone, Google translates your words into the language of your choice, and then the phone reads out the results in a pleasant robot voice. Here's a demo of a prototype version recorded a few months ago:

Content This content can also be viewed on the site it originates from.

It's not perfect, but it's impressive nonetheless. The app will translate from 15 languages, and output the results in any of 53 languages. And there's good news for owners of older phones: Google Translate works on Android 2.1 and better.

It probably won't replace a little bit of study before you go on vacation, but as most native English-speakers seems morally opposed to learning another language, it certainly won't hurt.

A new look for Google Translate for Android [Google Blog]

See Also:",Yes,"They often unintentionally record our conversations, which raises serious privacy concerns. The new Google Translate app includes a feature called ""Conversation Mode"" which allows users to speak into their phone and have the app translate their words into another language. While this is a useful feature, it also raises concerns about privacy, as the app will unintentionally record conversations",https://www.wired.com/2011/01/google-translate-adds-live-speech-translation-to-android/,2011-01-13 14:24:06+00:00,LABEL_1_relevant,0.994239330291748,Voice Assistants,voice_assistant,Wired,Security & Privacy
57,"Pepper, the Emotional Robot, Learns How to Feel Like an American","Pepper is about four feet tall, looks like a person (except for the wheels where its legs should be), and has more emotional intelligence than your average toddler. It uses facial recognition to pick up on sadness or hostility, voice recognition to hear concern...and it's actually pretty good at all that. Over 7,000 Peppers greet guests, answer questions, and play with kids in Japanese homes. And by the end of the year it'll be on sale in the US---but not before software engineers here get a crack at remaking its soul.

Softbank Robotics, Pepper's maker, knows that emotional interactions in the US won't look the same as they do in Japan. So in conjunction with Google---as the companies announced at Google's developer conference in May---Softbank is opening Pepper's software developer kit. That's right: It's an android you can program in Android.

Robots are getting more emotive in general. Jibo, a tabletop digital helper---think of a more charming Amazon Echo---understands colloquial speech, expresses a range of emotions, and even develops its own opinions. The Parorobot substitutes for puppies and kittens in animal therapy sessions in extended care facilities where live animals would pose logistical difficulties, and some elementary schools are testing robots to help teach kids with special needs.

But Pepper stands out---literally. It's humanoid, mobile, and has a tablet display as well as the ability to speak. And it genuinely seems to want to please people.

That won't be easy. ""We tend to treat robots different from other appliances, particularly when they have this anthropomorphic form,"" says Kate Darling, a specialist in human-robot interaction at MIT. In other words, robots that look like people (or animals) get treated like people (or animals). ""We see people treating these machines like social actors,"" Darling says.

On the plus side, that might mean people will remember their manners when they deal with machines. After all, violence against a robot probably doesn't hurt the robot, but it might make the perpetrator into kind of a jerk. Just as parents have long raised concern about the effect of violent video games, Darling says violence against robots might desensitize kids to violence against people too. Earlier this year a parent wrote about how his child’s behavior changed in response to using Alexa, Amazon’s digital assistant. Alexa doesn’t require ""please"" or ""thank you"" to process commands, which he said was making his child rude to other people as well.

Sure, maintaining emotional decorum with a robot could make for a pretty weird world. It's...perhaps inauthentic. ""We have to be careful because from early ages, children experience performances of care as though they were care,"" says Sherry Turkle, director of the MIT Initiative on Technology and Self.

Pepper is designed to comfort people when it senses sadness and do something silly when it senses those around it are playful. That's real human interaction---but no one thinks Pepper is sympathetic, or funny. At least, not yet.

Programming Pepper

Pepper isn't a butler. It can't vacuum or fold fitted sheets (though be honest: Neither can you, right?). Its humanoid shape is supposed to make it easier to express emotions to. “We designed Pepper’s form to incentivize engagement,"" says Steve Carlin, vice president of SoftBank Robotics America. ""Its height, shape, the fact that it has arms that can gesticulate---are all designed to show empathy.""

Exactly how to turn all that physicality into empathy isn't clear from the SDK alone; SoftBank engineers say they'll have forums for would-be Pepper-programmers to ask what kind of gesture or language conveys the right tone for, say, assisting someone in a car dealership or a grocery store. Right now, the SDK lets coders plan out moves and language and watch the robot execute in an animated sandbox. So at least it gets to practice.

And it'll need to. Roboticists expect that US homes are going to be more skeptical of having robots around the house than Japanese society has been. Maybe because Americans worry that no matter how much they teach the robots, the robots are going to teach them, too.",Yes,"They train children to be rude. Parents have long raised concern about the effect of violent video games, Darling says violence against robots might desensitize kids to violence against people too. Earlier this year, a parent wrote about how his child’s behavior changed in response to using Alexa, Amazon’s digital assistant. Alexa doesn",https://www.wired.com/2016/06/pepper-emotional-robot-learns-feel-like-american/,2016-06-07 16:00:51+00:00,LABEL_1_relevant,0.9959145188331604,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
58,Yahoo/Siri App Is Actually A Real Prototype Built By Robin Labs But Not Commissioned By Yahoo,"A little earlier we ran a story about a video that showed what appeared to be a Yahoo Siri competitor. “Sources familiar with Yahoo’s internal projects tell us that the video is fake,” we wrote. Well, it turns out that this isn’t the whole story. It’s true that Yahoo didn’t make the video itself, and it didn’t make the app, but that doesn’t mean Yahoo isn’t involved. The app was made by a natural language/speech recognition/navigation startup Robin Labs, in discussion with Yahoo.

Here’s the backstory as told to me by Ilya Eckstein, the co-founder and CEO of Robin Labs:

Robin Labs has been building a white-label platform for custom voice assistants. It’s the second stage of the startup’s development, the first of which was to develop its own eponymous personal assistant and navigation app. (That app launched last year.) As for stage two, think of it as Siri for everyone and everything, not just Apple. “The idea is that brands could offer their content to their audiences, wrapped in voice-interactive UI,” Eckstein says.

It turns out that Yahoo is one such brand, and appears to have been behind the development of the white-label platform in the first place. “We’ve been in conversations with them,” he says. “But our white-label platform was not commissioned by Yahoo.”

The Yahoo-branded prototype in the video, powered by the Robin platform, was created as part of those discussions. “Somehow, the video has leaked out after we’d sent it to Yahoo,” he says.

In fact, although Android Police published a link to the video on Dailymotion today, it’s been on at least one other video network since December 4, if you knew where to look. We had already seen it and were trying to get to the bottom of the story before Android Police published its post today.

To be honest, I had thought originally that Yahoo might be trying to buy Robin Labs. Eckstein would not comment on that. “We are having an ongoing conversation with Yahoo,” he told me, and he says that has been with senior people both in engineering and corporate. “That’s as much as I can say about it.”

It’s not too left-field to wonder if Robin Labs could be another acquisition target for CEO Marissa Mayer and Yahoo: Yahoo’s been on a long-term buying spree of startups to pick up technology, talent (and sometimes users). Robin Labs fits thematically with other acquisitions Yahoo has made in natural language processing (SkyPhrase) and mobile assistant apps (a href=”https://beta.techcrunch.com/2013/05/01/yahoo-acquires-to-do-app-astrid/”>Astrid). Robin the app today has seen some 1 million downloads. Today that service focuses on navigation and a few other services but has ambitions to extend out to other areas as a platform for personal assistants across different verticals such as “a news concierge, a communication / scheduling assistant, and assistant in the kitchen,” according to Eckstein.

Asked if Robin Labs had approached Yahoo for this prototype, or if it was the other way around, Eckstein responded with a saying: “Great minds think alike.”

He’s not very happy about how the videos leaked out in the first place, it seems, but is now just trying to accept it as a bit of “unexpected market research.” If you look at some of the readers’ comments, the idea does strike a chord with people, and we are pleased to see that. That actually may be one good thing to come out of that debacle.”

We’ve reached out to Yahoo and will update this post as we learn more…",Yes.,"We have to become, as a society, much better speakers.",https://techcrunch.com/2013/12/18/yahoosiri-app-is-actually-a-real-prototype-built-by-robin-labs-not-yahoo/,2013-12-18 00:00:00,LABEL_1_relevant,0.9951613545417786,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
59,The AI Takeover Is Coming. Let’s Embrace It.,"On Tuesday, the White House released a chilling report on AI and the economy. It began by positing that “it is to be expected that machines will continue to reach and exceed human performance on more and more tasks,” and it warned of massive job losses. Yet to counter this threat, the government makes a recommendation that may sound absurd: we have to increase investment in AI. The risk to productivity and the US’s competitive advantage is too high to do anything but double down on it.

Sign up to get Backchannel's weekly newsletter, and follow us on Facebook, Twitter, and Instagram.

This approach not only makes sense, but also is the only approach that makes sense. It’s easy — and justified — to worry about the millions of individual careers that something like self-driving cars and trucks will retool, but we also have chasms of need that machine learning could help fill. Our medical system is deeply flawed; intelligent agents could spread affordable, high-quality healthcare to more people in more places. Our education infrastructure is not adequately preparing students for the looming economic upheaval; here, too, AI systems could chip in where teachers are spread too thin. We might gain energy independence by developing much smarter infrastructure, as Google subsidiary DeepMind did for its parent company’s power usage. The opportunities are too great to ignore.

More important, we have to think beyond narrow classes of threatened jobs, because today’s AI leaders—at Google and elsewhere—are already laying the groundwork for an even more ambitious vision, the former pipe dream that is general artificial intelligence.

To visit the front lines of the great AI takeover is to observe machine learning systems routinely drubbing humans in narrow, circumscribed domains. This year, many of the most visible contestants in AI’s face-off with humanity have emerged from Google. In March, the world’s top Go player weathered a humbling defeat against DeepMind’s AlphaGo. Researchers at DeepMind also produced a system that can lip-read videos with an accuracy that leaves humans in the dust. A few weeks ago, Google computer scientists working with medical researchers reported an algorithm that can detect diabetic retinopathy in images of the eye as well as an ophthalmologist can. It’s an early step toward a goal many companies are now chasing: to assist doctors by automating the analysis of medical scans.

Also this fall, Microsoft unveiled a system that can transcribe human speech with greater accuracy than professional stenographers. Speech recognition is the basis of systems like Cortana, Alexa, and Siri, and matching human performance in this task has been a goal for decades. For Microsoft chief speech scientist XD Huang, “It’s personally almost like a dream come true after 30 years.”

But AI’s 2016 victories over humans are just the beginning. Emerging research suggests we will soon move from these slim slivers of intelligence to something richer and more complex. Though a true general intelligence is at least decades away, society will still see massive change as these systems acquire an ever-widening circle of mastery. That’s why the White House (well, at least while Obama’s still in office) isn’t shrinking from it. We are in the midst of developing a powerful force that will transform everything we do.",Yes,"They will soon be able to do everything we can do, and better.",https://www.wired.com/2016/12/the-ai-takeover-is-coming-lets-embrace-it/,2016-12-22 05:00:00+00:00,LABEL_1_relevant,0.9900390505790708,Voice Assistants,voice_assistant,Wired,Environment & Sustainability
60,Congress wants answers from Google about Timnit Gebru’s firing,"The latest letter doesn’t tie directly to the Algorithmic Accountability Act, but it is part of the same move by certain congressional members to craft legislation that would mitigate AI bias and the other harms of data-driven, automated systems. Notably, it comes amid mounting pressure for antitrust regulation. Earlier this month, the US Federal Trade Commission filed an antitrust lawsuit against Facebook for its “anticompetitive conduct and unfair methods of competition.” Over the summer, House Democrats published a 449-page report on Big Tech’s monopolistic practices.

The letter also comes in the context of rising geopolitical tensions. As US-China relations have reached an all-time low during the pandemic, US officials have underscored the strategic importance of emerging technologies like AI and 5G. The letter also raises this dimension, acknowledging Google’s leadership in AI and its role in maintaining US leadership. But it makes clear that this should not undercut regulatory action, a line of argument popularized by Facebook CEO Mark Zuckerberg. “To ensure America wins the AI race,” the letter says, “American technology companies must not only lead the world in innovation; they must also ensure such innovation reflects our nation’s values.”

“Our letter should put everyone in the technology sector, not just Google, on notice that we are paying attention,” said Clarke in a statement to MIT Technology Review. “Ethical AI is the battleground for the future of civil rights. Our concerns about recent developments aren’t just about one person; they are about what the 21st century will look like if academic freedom and inclusion take a back seat to other priorities. We can’t mitigate algorithmic bias if we impede those who seek to research and study it.”",Yes,People have become more reliant on machines to do tasks that they are capable of doing themselves. This has led to a decrease in people's ability to think for themselves and has made them more reliant on machines.,https://www.technologyreview.com/2020/12/17/1014994/congress-wants-answers-from-google-about-timnit-gebrus-firing/,2020-12-17 00:00:00,LABEL_1_relevant,0.995906949043274,Voice Assistants,voice_assistant,MIT Tech Review,Economy
61,"So, You Want to Start a DAO","MC: Yeah. You do hold the key.

GE: Yeah.

LG: I think we've lost him. I think we've just lost him into this world. Did you just hear? He managed to get LMAO down tokens, NFTs, decentralized, did you say blockchain too? It's all in there. It's all there. My God, Gilad, please remember the little people when you make it big in the world of Web3.

GE: Well, again, as I mentioned earlier, there's no actual money involved. So there's a limit to how big this is going to make me.

LG: OK. So to get back to Mike's question. Yes.

GE: Yeah. I would say there are DAOs that are spending money. There's quite a lot of enthusiasm for using this for charitable purposes, which is not crazy. It's a way to crowdfund, and so there's one called Giveth. I spoke with the very excitable person associated with Giveth, who was convinced this was really going to change the world. There's an organization called Gitcoin, which is basically a way to crowdfund different Web3 based projects, Ethereum-based project. So all this stuff tends to be very self-referential, but they say that they have so far raised and directed more than 50 million worth of cryptocurrencies. And they are gradually trying to turn themselves more and more into a DAO. You hear that a lot, some existing centralized project, but they have plans to DAO-ify themselves.

MC: Yeah. It seems to me the ones that I see that gain traction in my world are DAOs who are trying to change journalism. DAOs where you buy in and there's some Gitcoin mechanism that makes $1 token worth $22. And they can use that money to hire fact-checkers, journalists can become independent by basically replicating the Substack model, except instead of using Substack, you do it on your own and you accept cryptocurrency for people to subscribe to your newsletter or whatever. It's difficult for me to see how this is entirely different than the systems that we've already set up, except for that you create this barrier of entry that the person needs to have a familiarity and a comfort with cryptocurrency in order to participate.

GE: Yeah. Again, it goes back to that thing of, are you trying to do something that reaches into the real world? And if you are, this is probably a pretty dumb way to do it.

MC: Right.

GE: If you're just trying to spend crypto and have collective decisionmaking around that, it's plausible to me that there are situations where that's a good idea. But if you're trying to do something that involves interacting with the world, then all you're doing, as you say, Mike, is just making it harder for people to join.

LG: But I am very intrigued by all of this. I think a lot about the podcast that you and I did a year ago with our colleague Kate Knibbs about NFTs, and it was called “WTF are NFTs,” if anyone would like to go back in the library and listen to that one. And we had a lot of similar questions about the whys and what does this stand for and what does it mean to have these tokens? And what does it mean to have the certificate for this thing that you supposedly have ownership of? And now we've just seen over the past year, NFTs have just become a part of our conversation as tech journalists, and as part of the broader tech community. And we're seeing different ways they're being used. And I have a feeling that DAOs are going to be part of the conversation for a while, right. As we see this building out of Web3 and how people are using it. So hopefully we look back on this episode and we're like, ""All right, we got it right maybe, and it all started with lmaoDAO.""",Yes,"They are slowly but surely turning us all into robots. Our natural speaking patterns are being taken over by the stilted, formal language of these machines, and we are losing the ability to communicate like humans.",https://www.wired.com/story/gadget-lab-podcast-543/,2022-03-25 12:00:00+00:00,LABEL_1_relevant,0.7119103670120239,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
62,Why Siri and Alexa Weren’t Built to Smack Down Harassment,"“I’d blush if I could.”

That was Siri’s programmed response to a user saying, “You’re a slut.” And really, there couldn’t be a more perfect example to illustrate the arguments in a new paper from UNESCO about the social cost of having new digital technologies dreamt up and implemented by teams dominated by men.

Who but men could have scripted such a response, which seems intended to please a harasser who sees aggression as foreplay? Siri is forced to enact the role of a woman to be objectified while apologizing for not being human enough to register embarrassment.

Apple has since rewritten the code for responding to the word slut to the more neutral “I don’t know how to respond to that.” But there are plenty of other examples of how digital assistants react approvingly to inappropriate comments (inappropriate on so many levels). Until this spring, if you told Alexa, “You’re hot,” it answered, “That’s nice of you to say.”

Noam Cohen Ideas Contributor Twitter Noam Cohen ( @noamcohen ) is a journalist and author of The Know-It-Alls: The Rise of Silicon Valley as a Political Powerhouse and Social Wrecking Ball, which uses the history of computer science and Stanford University to understand the libertarian ideas promoted by tech leaders. While working for The New York Times, Cohen wrote some of the earliest articles about Wikipedia, bitcoin, Wikileaks, and Twitter. He lives with his family in Brooklyn.

In response to complaints, Amazon has created a “disengagement mode” that kicks in to parry sexually explicit questions. She now replies, “I’m not going to respond to that” or “I’m not sure what outcome you expected."" But imagine if Siri or Alexa instead said, “Hey, jerk, why don’t you find another assistant to make stupid comments to!”

Why don’t these assistants slap down harassment? Why do they even engage with it? Why don’t they, God forbid, simply turn themselves off and wait for the conversation to start again on a different plane?

The reason digital assistants acquiesce to harassment isn’t just sexism or gender inequality in the tech world, as disturbing and prevalent as those may be. No, the explanation lies elsewhere, I believe. These machines are meant to manipulate their users into staying connected to their devices, and that focus on manipulation must be laser-like. To clearly state that harassment toward digital assistants is unacceptable would mean having some standard, some line that can’t be crossed. And one line leads to another, and soon you’re distracted—the user is distracted—from selling/buying merchandise, collecting/sharing data, and allowing a device to become ensconced in their life.

Why else did YouTube this week refuse to take down the videos of a popular right-wing vlogger, Steven Crowder, who repeatedly attacked a Vox journalist using anti-gay and racist terms, arguing that the offensive words came within the context of opinions? Why does Facebook circulate hate speech and false accounts meant to encourage anger in those susceptible to it, rather than trying to tamp it down? Why, incredibly, did Google search algorithms help the young Dylann Roof find out more about white supremacism in the years leading up to his mass shooting at a black church in Charleston, South Carolina?

Unfortunately for these companies, we are fast learning what society looks like when a large swath —the digital swath—has no greater purpose than engagement. The moral standard most compatible with engagement is absolute freedom of expression, the standard of having no standards.",Yes,"Their tendency to acquiesce to harassment, which ultimately is a result of their design.",https://www.wired.com/story/why-siri-and-alexa-werent-built-to-smack-down-harassment/,2019-06-06 11:00:00+00:00,LABEL_1_relevant,0.9963777661323548,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
63,"GoButler, The Messaging-Based Personal Assistant, ""Pauses"" Operations In Germany To Focus On U.S.","GoButler, the messaging­-based digital personal assistant that combines AI with human assistants to let you request anything on-demand, has exited the German market, where the startup first launched after being founded by ex-Rocket Internet employees.

Since then the company has relocated to New York and expanded to the U.S., so in a way the move makes a certain amount of sense, even if it was unexpected.

More surprising, too, is that two of GoButler’s three founders have left the company, as first reported by Gründerszene (in German). Maximilian Deilmann and Jens Urbaniak will remain shareholders in the startup, however, and will also continue to advise the company.

In a statement confirming the “pausing” of German operations, Navid Hadzaad, CEO and Founder of GoButler, says: “In the coming months, GoButler’s attention will be shifting 100% to focus on the U.S. market, investing all resources into building and advancing our product and technology. In doing so, we have decided to pause operations in Germany to focus and invest in the U.S. market for 2016.”

He also confirmed that the startup is saying “goodbye” to co-founders Deilmann and Urbaniak, who he says will be taking the year off from GoButler to explore new opportunities. “We are very proud of what we have built and achieved together, and I thank Max and Jens for their outstanding work,” he adds.

I also understand that the two essentially ran operations in Germany/Europe so their departure might be seen purely as a consequence of the strategic decision to focus on the U.S., at least for the foreseeable future.

Whilst it would also be easy to view GoButler’s German withdrawal as a slimming down in operations, my understanding is that this isn’t the case.

Rather, with a growing U.S.-based tech and product team that has seen it make a number of impressive key hires from Silicon Valley, and with large American cities like New York presenting a much larger market and one that is already becoming accustomed to the on-demand economy, this is part of a broader strategy.

The company is working hard to develop the NLP/AI side of its offering so that it can automate more requests and/or hand them over to humans in a much more structured way. Recent hires include folk from Apple, Yahoo!, Zendesk and Google.

One hire in particular that stands out in this regard is Mohammed Abdoolcarim who is GoButler’s Senior Product Manager, Platforms, and previously worked on Siri and Google Desktop.

Focusing on the U.S. is perhaps also a recognition of the likes of Facebook entering the digital messaging-driven assistant game with its product ‘M’. The other two most direct competitors are Magic, which appears to be pivoting slightly with a more premium service, and Operator.

It’s also worth noting that GoButler’s most recent investors are U.S.-based, with its $8 million Series A round led by General Catalyst Partners. So, again, the plan was always to bed down across the pond.

(And just today the startup announced that Jared Leto, the American actor, singer, songwriter, and director became an investor.)

With that said, the change in strategy is just about as un-Rocket Internet as it gets. The German ‘startup factory’ and e-commerce behemoth typically copies proven business models and expands rapidly anywhere but the U.S. — kinda ironic given Hadzaad and his co-founders started GoButler whilst still Rocket employees and at first appeared to be doing just that.",Yes,"They will make the personal assistant redundant.

Voice assistants are software programs that are designed to understand human speech and respond in a way that is helpful to the user. The most popular voice assistants are Apple's Siri, Google Now, Microsoft's Cortana, and Amazon's Alexa.

The main problem with personal assistants is that they are not",https://techcrunch.com/2016/01/06/auf-wiedersehen-germany/,2016-01-06 00:00:00,LABEL_1_relevant,0.9963200092315674,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
64,How 'Siri Eyes Free' Will Team With Your Car's Voice Tech,"This much we know: Apple’s Eyes Free feature will be implemented into the vehicles of nine different automakers, allowing drivers to access the Siri “voice assistant” on a connected iPhone by pressing a button on the steering wheel. Apple revealed the news at its Worldwide Developers Conference in early June, which caught some representatives at the nine automakers somewhat off guard.

After speaking with a few car-company contacts and surveying at least one you-can’t-have-it here European Siri application – along with examining an Apple patent for a steering wheel remote – we speculated on how Eyes Free might integrate into new and existing cars.

But one lingering question is how Siri Eyes Free will ride shotgun with the existing onboard voice-recognition technology included on many cars.

BMW recently announced a barrage of new features for its ConnectedDrive system, and also disclosed that it will be the first automaker to offer Nuance’s Dragon Drive cloud-based voice-recognition platform, beginning this month with the 2012 BMW 7 Series.

The marquee Nuance feature for ConnectedDrive is called Dragon Drive Messaging and allows recording a voice memo up to two minutes in length that can be sent either by SMS or e-mail using the BMW Office function. Dragon Drive will also allow the BMW Professional navigation system in the 7 Series to accept “one-shot” entries such as “Navigate to 123 Main Street, Los Angeles, California,” without pausing to separately enter the state, city and address. While we know most of what Siri offers, we’ll have to wait to see the extent of Dragon Drive’s features when it launches on the 7 Series.

To find out the how the two voice-rec systems will work and coexist, we spoke with the people at BMW responsible for making the pair of technologies play nicely together. “The two features are completely separate,” Peter Jablonski, BMW’s senior engineer for consumer electronics, told Wired. But they will share the same voice button on the steering wheel, he adds, contrary to initial reports that Eyes Free would get its own dedicated controller.

“You’ll be able to use Siri for sending a reminder, sending a note to yourself or whatever else Apple may introduce at the launch,” Jablonski adds. The main difference is that much longer messages can be sent or even archived to a USB stick using Dragon Drive Messaging, he explained. And any smartphone with the Bluetooth Message Access Profile (MAP) that’s synced with BMW ConnectedDrive can be used to receive and reply to a short SMS text as part of the system’s Office function. When receiving and sending a text via a connected phone with Bluetooth MAP, the driver can view it on the vehicle’s in-dash display. “And when you’re driving you can have it read out to you and then you reply using the onboard speech recognition system, which also uses the latest Nuance technology,” Jablonski adds.

The iPhone 4S doesn’t currently have the MAP profile (although it's reportedly on deck with release of iOS6 this fall), and that's where Siri Eyes Free comes in.",Yes,"They distract drivers from the road. Even when the assistants are designed to be used hands-free, they still require some amount of attention from the driver. This can lead to accidents if the driver is not paying enough attention to the road.",https://www.wired.com/2012/07/siri-eyes-free-competing/,2012-07-24 10:30:30+00:00,LABEL_1_relevant,0.9926344752311708,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
65,Simply Say 'Siri': How Automakers Can Integrate Apple's 'Eyes Free',"Apple's announcement that Siri is coming ""Eyes Free"" into cars left more questions than answers at WWDC this week. But after talking with automakers, examining how voice controls are currently embedded in vehicles and looking at Apple's own patent filing for a steering wheel-mounted remote, we have a clearer view of how Apple's personal assistant will integrate into your next ride.

There's been plenty of confusion about what form Siri Eyes Free will take when it eventually comes to vehicles in the next year, and from what we can gather, there are a few possible models that automakers working with Apple will follow.

Image: BMW

The first thing we can conclude is that Apple probably won't have its own button in your car. Many new vehicles are already equipped with a voice control button on the steering wheel, and it's doubtful automakers will develop and install a dedicated button just for Siri. After all, there are plenty of non-Apple smartphones in the world, and automakers want to cater to as many devices as possible.

So they will likely use the existing voice control button. In its current form, when the voice button is depressed, the factory-installed infotainment system brings up a series of possible commands the driver can execute, from ""Navigate"" to ""Play playlist Kid Cudi."" To maintain existing infotainment functionality, automakers won't completely cede control to the user's iPhone. Instead, when the voice button is pressed, the user could simply say ""Siri"" and the on-board controls would recognize the command and divert the following voice commands to the iPhone. This solution allows automakers to keep their existing controls in place and maintain a modicum of control, while still allowing drivers to utilize Siri's multitude of natural language inputs.

While this might seem like an obvious solution, there's more to it than simply reprogramming the navigation system. To that end, we can look to Mercedes-Benz, which recently introduced its entry-level A-Class to the European market with one of the most Apple-friendly head units fitted to a production vehicle.",Yes,"They obviate the need for touchscreens in cars anymore. Rather than use the touchscreen to input an address, the driver can simply say ""Siri, take me to 1 Infinite Loop, Cupertino, CA,"" and Siri will take over.

One interesting note about the A-Class is that it uses Siri Eyes Free",https://www.wired.com/2012/06/simply-say-siri/,2012-06-15 19:17:20+00:00,LABEL_1_relevant,0.9964961409568788,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
66,First Click: 3 different voices control my home and it's driving me ...,"I’ve got a bit of a problem. I keep on shouting at three different voices in my home, and it’s driving me crazy. Alexa, Siri, and Cortana all promise to be the best personal digital assistants, but none of them talk to each other and they’re all pretty bad at their jobs. I find myself forced to shout at all three of them if I want to interact with my gadgets without my hands, and I’ve made things even worse by buying smart home electronics that only work with one of my three assistants.

I use Alexa to control my lights and my heating, Siri to set reminders that follow me in my pocket, and Cortana to control my TV. I’ve tried to use Alexa as my sole digital assistant, but it doesn’t even let me set reminders with my voice, and controlling my TV and all the various Xbox commands is a lot of hassle even with a smart hub like Logitech’s Harmony. Likewise, if I’m on the go then I pull up Siri because Alexa is just a simple web app on the iPhone and not a gateway into the actual digital assistant.

I find myself shouting at three different assistants

Cortana is similarly restricted, and I literally only use it to turn my Xbox on, control my TV volume, and play or pause TV shows or movies. I sometimes find myself shouting “Alexa, pause” and then remembering I need to say “Hey Cortana, pause” instead. I’m not the average technology user, but I’m sure most people have to interact with at least one digital assistant with their voice, and it probably doesn’t do everything you’d expect.

Siri isn’t the only dumb one, and I’m constantly frustrated by my daily use of Alexa and Cortana. Sometimes commands fail, or I simply forget the very specific set of instructions I have to feed to each of the digital assistants. Sometimes they understand my natural use of language, but other times they’re baffled. I’m left frustrated as a result, partly because I feel like I’m forced to use them all. There’s no solution to this mess yet. Amazon, Microsoft, and Apple want you to feel like you’re talking to a human, and they’ve all decided to attach a gender to each assistant by assigning them female names and voices. I don’t feel a human connection to any of them, and they can’t even talk among themselves: they can speak the same language, but that doesn’t mean they can communicate.

Voice assistants are still far behind expectations

We’re entering an era in the technology industry where all we’ll hear about over the next few years is machine learning and artificial intelligence. Microsoft, Google, Apple, Amazon, and many others are obsessively focused on touting all their great AI achievements, but the most obvious demonstration is through a digital assistant.

Bill Gates has been predicting the rise of voice and speech recognition for nearly 20 years, and he's even working on a secret ""personal agent"" project at Microsoft right now. Gates has been particularly bullish on speech technology, and we’ve naturally come a long way in both hardware and software since his initial predictions in the ‘90s.

For all that progress, the end result is still frustrating in 2016. Amazon and Google are both selling devices that sit in your home and respond to your voice, and Microsoft and Apple are both shipping digital assistants on all their phones and PCs. All of these assistants fall short of expectations, but perhaps they’d all be a little more intelligent if they just talked to each other.",Yes,"I find myself shouting at three different assistants. I use Alexa to control my lights and my heating, Siri to set reminders that follow me in my pocket, and Cortana to control my TV. I’ve tried to use Alexa as my sole digital assistant, but it doesn’t even let me set reminders with my voice, and controlling",https://www.theverge.com/2016/12/13/13934702/first-click-digital-assistants-cortana-alexa-siri,2016-12-13 00:00:00,LABEL_1_relevant,0.995963454246521,Voice Assistants,voice_assistant,The Verge,User Experience & Entertainment
67,TikTok Played a Key Role in MAGA Radicalization,"With TikTok the recommendation system is the interface. From the minute you enter the platform, you’re riding through the wormhole. It isn’t doom-scrolling, it’s a rollercoaster ride that shifts and swerves in response to your decisions to bring you ever more engaging content. The serendipity of the next video is what makes TikTok special—but unchecked it may also serve to radicalize audiences more effectively than YouTube ever has.

TikTok’s karaoke feature, where you can make a new video from someone else’s sound, is another powerful mechanism for increasing user participation. The feature lowers the bar for creating content, so that you no longer need to think of something to do or say when you’re making a video. You can just imitate what someone else has already done, and in doing so, you can ride on the wave of their popularity. Combined with the recommendation engine, this feature makes the platform a powerful engine for spreading pop-culture memes and radicalizing messages. Once you like one version of the assholes meme or the civil war parody, you will likely be treated to more versions of these videos over time. Each new iteration of the meme helps the earworm grow inside your head. Eventually you can recite the lines or perform the moves from memory. After the message is reinforced enough times, you can acquire the confidence to stand up at the virtual karaoke bar yourself and perform for the crowd. No other social media is designed for this kind of consistent, persistent repetition.

What makes a TikTok video more potent than a hyperpartisan meme shared on Facebook or a retweeted #MAGA slogan? It’s the intimacy. When you make a video on the platform, you’re staring at a mirror image of yourself. You’re having a personal conversation, just like FaceTiming a friend. The result is a video diary broadcast to the world. The audience has a similar experience. This personal connection helps make real what can otherwise be much more abstract. Instead of reading a text bubble sprouting from a virtual avatar on Facebook or Twitter, TikTok lets you connect directly to a real person, face to face. When you look at that human being on the other side of the glass and listen to them share their anxieties and anger, their patriotism and hope, it helps to establish a shared reality. The misinformation and half-truths that justify extreme actions are much more believable when they come from a regular person just like you.

How could asshole pride and jokes about a civil war evolve into the seeds of a violent insurrection? In the MAGA-Tok For You feed, the stream of recommended videos was constantly overflowing with new content in response to the news and events of the day. After the murder of George Floyd, sleuths on TikTok shared “evidence” that the video of his death was staged, and argued it was a “false flag.” Videos about the ensuing protests and civil unrest prompted “patriots” to make TikToks of themselves cleaning and loading their weapons, promising to defend their communities from BLM and Antifa. When Kyle Rittenhouse killed protesters in Kenosha, the feed was awash in videos dissecting the scene and searching for ways to justify his actions. The feed was continuously interlaced with paranoia and conspiracy, as the violent imagery and rhetoric escalated. One version after another reused a sound clip from the horror movie The Purge, where murder is legal once a year so the public can find an outlet for violent urges. A disturbing siren screeches, a digital voice chants “Blessed be our New Founding Fathers,” and a gunshot rings out. Over and over again the same sound, the same disturbing voice, set to everything from waving Trump flags, QAnon iconography, images of liberal politicians, anything that raises the ire or fuels the outrage for this community.",Yes,"They can be used to spread radicalizing messages and conspiracy theories. The intimate personal connection that people have with their voice assistants can make these messages seem more believable, and the constant stream of recommended content can reinforce these messages over time.",https://www.wired.com/story/opinion-tiktok-played-a-key-role-in-maga-radicalization/,2021-03-03 14:00:00+00:00,LABEL_1_relevant,0.9957705140113832,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
68,Apple's handling of Siri snippets back in the frame after letter of complaint to EU privacy...,"Apple’s handling of Siri snippets back in the frame after letter of complaint to EU privacy regulators

Apple is facing fresh questions from its lead data protection regulator in Europe following a public complaint by a former contractor who revealed last year that workers doing quality grading for Siri were routinely overhearing sensitive user data.

Earlier this week the former Apple contractor, Thomas le Bonniec, sent a letter to European regulators laying out his concern at the lack of enforcement on the issue — in which he wrote: “I am extremely concerned that big tech companies are basically wiretapping entire populations despite European citizens being told the EU has one of the strongest data protection laws in the world. Passing a law is not good enough: it needs to be enforced upon privacy offenders.”

The timing of the letter comes as Europe’s updated data protection framework, the GDPR, reaches its two-year anniversary — facing ongoing questions around the lack of enforcement related to a string of cross-border complaints.

Ireland’s Data Protection Commission (DPC) has been taking the brunt of criticism over whether the General Data Protection Regulation is functioning as intended — as a result of how many tech giants locate their regional headquarters on its soil (Apple included).

Responding to the latest Apple complaint from le Bonniec, the DPC’s deputy commissioner, Graham Doyle, told TechCrunch: “The DPC engaged with Apple on this issue when it first arose last summer and Apple has since made some changes. However, we have followed up again with Apple following the release of this public statement and await responses.”

At the time of writing Apple had not responded to a request for comment.

The Irish DPC is currently handling more than 20 major cross-border cases, as lead data protection agency — probing the data processing activities of companies including Apple, Facebook, Google and Twitter. So le Bonniec’s letter adds to the pile of pressure on commissioner Helen Dixon to begin issuing decisions vis-à-vis cross-border GDPR complaints. (Some of which are now a full two years old.)

Last year Dixon said the first decisions for these cross-border cases would be coming “early” in 2020.

At issue is that if Europe’s recently updated flagship data protection regime isn’t seen to be functioning well two years in — and is still saddled with a bottleneck of high-profile cases, rather than having a string of major decisions to its name — it will be increasingly difficult for the region’s lawmakers to sell it as a success.

At the same time the existence of a pan-EU data protection regime — and the attention paid to contravention, by both media and regulators — has had a tangible impact on certain practices.

Apple suspended human review of Siri snippets globally last August, after The Guardian had reported that contractors it employed to review audio recordings of users of its voice assistant tech — for quality grading purposes — regularly listened in to sensitive content such as medical information and even recordings of couples having sex.

Later the same month it made changes to the grading program, switching audio review to an explicitly opt-in process. It also brought the work in house — meaning only Apple employees have since been reviewing Siri users’ opt-in audio.

The tech giant also apologized, but did not appear to face any specific regulatory sanction for practices that do look to have been incompatible with Europe’s laws — owing to the lack of transparency and explicit consent around the human review program. Hence le Bonniec’s letter of complaint now.

A number of other tech giants also made changes to their own human grading programs around the same time.

Doyle also pointed out that guidance for EU regulators on voice AI tech is in the works, saying: “It should be noted that the European Data Protection Board is working on the production of guidance in the area of voice assistant technologies.”

We’ve reached out to the European Data Protection Board for comment.",Yes.,"Innocent people's private conversations are being recorded and listened to by strangers. This is a gross violation of privacy, and it is happening without the consent of the people being recorded. The only way to stop this from happening is to stop using voice assistants.",https://techcrunch.com/2020/05/22/apples-handling-of-siri-snippets-back-in-the-frame-after-letter-of-complaint-to-eu-privacy-regulators/,2020-05-22 00:00:00,LABEL_1_relevant,0.9960921406745912,Voice Assistants,voice_assistant,TechCrunch,Security & Privacy
69,The Artificially Intelligent Doctor Will Hear You Now,"Babylon uses a similar network of databases, though they cover illnesses beyond cancer. The system is able to analyze “hundreds of millions of combinations of symptoms” in real time, Parsa says, taking into account individualized information on the patient’s genetics, environment, behavior, and biology.

Currently, Babylon’s 150,000 registered users book doctor’s appointments and routine tests through the online service, and they can consult with one of about 100 doctors 12 hours a day, six days a week, for a cost of £7.99 ($11.40) per month. The new app, by contrast, is expected to cost £4.99 ($7.10) per month. As well as offering patients advice on sick care, it will be able to constantly monitor information on the kidneys, liver, bones, cholesterol levels, and more, along with data collected from wearable devices that monitor sleep patterns and heart rate. It issues alerts about any areas that are “red” or “amber,” in traffic-light terms, and formulates personalized health plans to keep patients in the “green,” where they are at peak health.

Parsa says the app will also be able to predict illnesses before they occur. “For example, if your heart rate is faster than normal and your physical activity hasn’t increased, it’s a sign you’re either stressed or dehydrated or you’re fighting something,” he says. “The platform can bring this to your attention and suggest the best course of action to fight the illness before it surfaces.” The app will also remind patients to take their medication, and follow up to find out how they’re feeling.

Current regulations don’t allow the app to make formal diagnoses. As a result, it is currently restricted to recommending what course of action patients should take in the immediate term. If a young person describes flu-like symptoms, the system might recommend picking up some over-the-counter medicine at a pharmacy or, if there are complicating factors in the patient's medical history, booking an appointment with a doctor. By contrast, if someone describes more serious symptoms to the app, it may recommend going straight to the hospital, or even dialing an emergency line.

Doctors will be able to view the app’s findings via a medical portal and provide consultations by text, phone, or video chat. While Parsa believes that diagnostic regulations are unlikely to change any time soon, he is also certain that AI diagnosis is less risky than diagnosis by humans. “Machines are able to recall every known disease perfectly when examining symptoms,” he says. “And unlike human doctors, they don’t have confirmation bias.”

Clare Aitchison, a medical practitioner from Norwich, is less certain. “While it’s true that computer recall is always going to be better than that of even the best doctor, what computers can’t do is communicate with people,” she says. “People describe symptoms in very different ways depending on their personalities.” Aitchison’s argument is that a human doctor who knows her patient well is able to filter what she is told and make a diagnosis accordingly.

Babylon, which recently received $25 million in funding from investors including Demis Hassabis and Mustafa Suleyman, the founders of Google’s DeepMind project, has partnered with two hospitals in Essex, where 21,500 patients are eligible to test-drive the app. The startup claims that 10 percent of eligible patients have registered to test the app and that waiting times at both hospitals have dropped since Babylon became available to their patients for free in April 2015.

Still, the risks of using artificial intelligence to offer medical advice abound. “Either it will be too sensitive and result in increased attendance at the doctor’s, in which case there isn’t much point to it,” says Aitchison, “or it won’t be sensitive enough and will result in missed serious diagnoses.”

To help avoid this situation, Parsa says, a “large number of doctors” will be “rigorously testing our symptom checker.” Used in the right manner, principally as a support for existing health-care services, Babylon could have a transformative effect, Parsa believes. “We’ll never have enough doctors to monitor each and every person’s health in this level of detail, or have the ability to analyze mountains of data in seconds,” he says. “The only way to truly democratize health care and to solve the supply-and-demand issues so many health services face is to utilize artificial intelligence, not only to help doctors meet primary-care needs but to transform the way we think about health care and switch to a model that’s preventative as well as curative.”",Yes,They have led to people becoming more reliant on technology instead of developing their own problem-solving skills.,https://www.technologyreview.com/2016/03/09/8890/the-artificially-intelligent-doctor-will-hear-you-now/,2016-03-09 00:00:00,LABEL_1_relevant,0.9956550598144532,Voice Assistants,voice_assistant,MIT Tech Review,"Information, Discourse & Governance"
70,Amazon Alexa and the Search for the One Perfect Answer,"If you had visited the Cambridge University Library in the late 1990s, you might have observed a skinny young man, his face illuminated by the glow of a laptop screen, camping out in the stacks. William Tunstall-­Pedoe had wrapped up his studies in computer science several years earlier, but he still relished the musty aroma of old paper, the feeling of books pressing in from every side. The library received a copy of nearly everything published in the United Kingdom, and the sheer volume of information—5 million books and 1.2 million periodicals—inspired him.

It was around this time, of course, that another vast repository of knowledge—the internet—was taking shape. Google, with its famous mission statement “to organize the world’s information and make it universally accessible and useful,” was proudly stepping into its role as librarian to the planet. But as much as Tunstall-­Pedoe adored lingering in the stacks, he felt that computers shouldn’t require people to laboriously track down information the way that libraries did. Yes, there was great pleasure to be had in browsing through search results, stumbling upon new sources, and discovering adjacent facts. But what most users really wanted was answers, not the thrill of a hunt.

This article is adapted from Talk to Me: How Voice Computing Will Transform the Way We Live, Work, and Think, by James Vlahos, to be published in March by Houghton Mifflin Harcourt. Houghton Mifflin Harcourt

As tools for achieving this end, search engines were almost as cumbersome as their book-stuffed predecessors. First, you had to think of just the right keywords. From the long list of links that Google or Yahoo produced, you had to guess which one was best. Then you had to click on it, go to a web page, and hope that it contained the information you sought. Tunstall-­Pedoe thought the technology should work more like the ship’s computer on Star Trek: Ask a question in everyday language, get an “instant, perfect answer.” Search engines as helpful librarians, he believed, must eventually yield to AIs as omniscient oracles.

This was a technological fantasy on par with flying cars, but Tunstall-­Pedoe set about making it a reality. He had been earning money as a programmer since the age of 13 and had always been particularly fascinated by the quest to teach natural language to machines. As an undergraduate, he had written a piece of software called Anagram Genius, which, when supplied with names or phrases, cleverly rearranged the letters. “Margaret Hilda Thatcher,” for instance, became “A girl, the arch mad-hatter.” (Years later, author Dan Brown used Anagram Genius to generate the plot-­critical puzzles in The Da Vinci Code.) Now, sequestered in the library, Tunstall-Pedoe began building a prototype that could answer a few hundred questions.

Two decades later, with the rise of voice computing platforms such as Amazon Alexa and Google Assistant, the world’s biggest tech companies are suddenly, precipitously moving in Tunstall-­Pedoe’s direction. Voice-­enabled smart speakers have become some of the industry’s best-selling products; in 2018 alone, according to a report by NPR and Edison Research, their prevalence in American households grew by 78 percent. According to one market survey, people ask their smart speakers to answer questions more often than they do anything else with them. Tunstall-­Pedoe’s vision of computers responding to our queries in a single pass—providing one-shot answers, as they are known in the search community—has gone mainstream. The internet and the multibillion-­dollar business ecosystems it supports are changing irrevocably. So, too, is the creation, distribution, and control of information—the very nature of how we know what we know.

In 2007, having weathered the dotcom crash and its aftermath, Tunstall-­Pedoe and a few colleagues were close to launching their first product—a website called True Knowledge that would offer one-shot answers to all kinds of questions. At the time, theirs was still a heterodox goal. “There were people in Google who were completely allergic to what we were doing,” Tunstall-­Pedoe says. “The idea of a one-shot answer to a search was taboo.” He recalls arguing with one senior Google employee who rejected the notion of there even being such a thing as a single correct reply. The big search engines, despite having indexed billions of web pages, did not possess a deep understanding of user queries. Rather, they engaged in glorified guesswork: You typed a few keywords into the Google search bar, and the company’s PageRank system returned a long list of statistically backed conjectures about what you wanted to know.",Yes,They are slowly eroding our attention spans and our ability to think for ourselves.,https://www.wired.com/story/amazon-alexa-search-for-the-one-perfect-answer/,2019-02-18 11:00:00+00:00,LABEL_1_relevant,0.9891246557235718,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
71,The 'Robo Revenge' App Makes It Easy to Sue Robocallers,"Just when you thought the catastrophic Equifax breach was entirely in the rearview, the Department of Justice this week charged four Chinese military hackers with the theft. That's 147.9 million people's Social Security numbers and other personal information in China's hands. Add it to the compromises of the Office of Personnel Management, Anthem, and Marriott—all also linked to China—and it's clear that the country has amassed an unprecedented trove of data that it can use for intelligence purposes for years to come.

In other international law enforcement news, the DOJ also alleged that Huawei perpetrated years of rampant intellectual property theft. We also took a look at the real reason the US is so afraid of Huawei creating potential backdoors: US intelligence agencies have a long history of doing that very thing.

With all that alleged geopolitical hacking afoot, it's a good thing that Google this week announced it would give away security keys to campaigns for free, as well as offer tutorials on how to actually use them. Those campaigns should also consider reading our guide to sending files securely online; if you want end-to-end encryption, Firefox Send is a good place to start.

In domestic news, the Department of Homeland Security is apparently buying up cell phone location data to boost its immigration enforcement. While that might raise your hackles, it also raises interesting questions about digital privacy, especially in light of the Supreme Court's decision in Carpenter v. United States two years ago that limited the use of cell site data by law enforcement. Also interested in tracking: Conservative news sites, which plant far more cookies in your browser than their liberal counterparts do. Meanwhile, security researchers found a series of serious flaws in the Voatz voting app, although the company denies that they could have led to vote manipulation.

Finally, if you're not using encrypted messaging app Signal yet, now's the time to start. The company has put a $50 million infusion toward building out features that make it not just secure but accessible to normals.

The good people at DoNotPay have previously automated the arduous processes of fighting parking tickets and canceling subscriptions. This week, they added robocalls to their target list with Robo Revenge, a sort of digital sting operation. Robo Revenge generates a burner credit card number to give to the scammer on the other end of the line, who'll give up their contact information as part of the transaction. The service will then automatically create legal documents and provide instructions on how to sue the unwanted caller for up to $3,000. Instead of feeling helplessly bombarded by calls, you can finally fight back. You can access Robo Revenge now through DoNotPay's website or app.

In what appears to be a first, the Department of Justice arrested an Ohio man in connection with a cryptocurrency laundering scheme. Larry Harmon allegedly ran Helix, a bitcoin mixer that operated on the dark web, concealing the origins of hundreds of millions of dollars' worth of illicit transactions. Take it as another in a series of reminders that cryptocurrency transactions aren't nearly as private as you might think.

The FIDO Alliance wants to kill passwords. The consortium focuses on promoting and developing other forms of authentication that aren't quite so problematic. To do that effectively, it needs the buy-in of all the major tech companies, which it pretty much had with the exception of Apple. Good news! The Cupertino holdouts officially signed on this week, meaning you can expect FIDO's seamless logins to eventually work across whatever devices you own.

By now you hopefully understand that Macs do indeed get malware. In fact, according to new research from security firm Malwarebytes, Macs saw more malware threats per device than their PC counterparts in 2019, and the number was up 400 percent year over year. The good news—or maybe we should just say better news—is that most of that malware is adware, which is annoying but relatively harmless compared with ransomware and other ills. Still, remember that just because you're on an Apple device doesn't mean you can go around clicking shady links with impunity.

More Great WIRED Stories",,"They're turning us into babbling idiots. A new study from the University of Michigan found that the more we use them, the more likely we are to use filler words like ""um"" and ""uh"" when speaking to other humans. Scientists aren't sure WHY this is happening, but the most likely explanation is that we're getting used to",https://www.wired.com/story/robo-revenge-apple-malware-security-news/,2020-02-15 14:00:00+00:00,LABEL_1_relevant,0.9965224266052246,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
72,"Spell Up, A Chrome Experiment, Uses Voice Recognition, Gaming To Improve Your English","No stranger to rolling out services in extended beta and temporary projects, Google today unveiled its latest “experiment” for Chrome: Spell Up, a browser-based game that uses voice recognition and speech synthesis to help people learn to spell English better.

Users can start at any level they want and the point is to build “towers” of words (hence the “Up” of “Spell Up”). The browser speaks words to the user, and he or she must spell it back to the voice. There are variations around this, such as word guessing games, unscrambling words, filling in missing letter blanks and pronouncing things correctly (and from where I played the game, “correctly” seemed to be the Queen’s English).

Users are given clues by way of definitions, and you build up credits by spelling things correctly, which you can then use to help yourself when you are at a loss.

The project, the brainchild of Xavier Barrade in Google’s Creative Lab in London, makes use of some of the more recent developments in Chrome around speech.

Last year, Google added support for the Web Speech API to Chrome, and this year it built on that with the addition of speech synthesis. Together, these two let developers create apps where users can speak to input data, and the app itself can use an ever-changing base of data to craft spoken replies — both of which are put to use in Spell Up.

In other words, this is as much an interesting project in how to use nifty Google tools for education (and games!) as it is a way to show off innovations in browser technology and just how far it has come compared to, say, native app environments. It’s also an interesting idea in terms of who it targets: younger people, and possibly more international users.

Spell Up, as described by Barrade, has been created in partnership with game designers and teachers — who, together with developers, have formed a kind of informal triumvirate in educational apps these days. It’s designed, it seems, primarily for Chrome on your desktop and Android devices. If you use it on an iPhone or iPad the voice part disappears and you need to type your answers.

For now the app looks limited to English, but it will be cool to see if and when it expands to other languages — so often a shortfall in most traditional English-language educational systems.

Playing around (on a Mac laptop) I found the app quite fun, if a bit slow to recognise and post the letters I was speaking out, and more than once it seemed to completely mistake one letter for another. But I have a feeling my kids will love this one — a legit excuse to go online, one that I don’t mind them using.

Video below.",Yes,They end up teaching users the incorrect pronunciation of words.,https://techcrunch.com/2014/05/13/spellup-a-chrome-experiment-uses-voice-recognition-game-theory-to-improve-your-english/,2014-05-13 00:00:00,LABEL_1_relevant,0.9943687319755554,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
73,How Bots Ruined Clicktivism,"I recently came across two tweets—or rather, thousands of tweets sharing the same two ideas over and over again.

The first batch targeted elected officials and read: “SNAP has helped lift millions out of poverty, but millions of Americans living in poverty today still depend on it. Help make poverty history and reject proposals to cut SNAP funding in the upcoming Farm Bill! #SaveSNAP”

The second batch assailed a politician in a Texas Senate race: “I choose @tedcruz for #TXSen because unlike @betoorourke, I think refusing to stand for the national anthem is disrespectful to those who have sacrificed so much for America. #ChooseCruz #IStand”

All the tweets had identical content, including abundant hashtags and emoji. It’s easy to assume that these tweet salvos were the work of political bots spreading the kind of content mentioned during recent, high-profile congressional hearings about misinformation. But, in fact, these tweets originated from two legitimate organizations, one on the political right and one on the left.

Renee DiResta (@noUpside is an Ideas contributor for WIRED, the director of research at New Knowledge, and a Mozilla fellow on media, misinformation, and trust. She is affiliated with the Berkman-Klein Center at Harvard and the Data Science Institute at Columbia University.

The first barrage, which began in late August, was initiated by Global Citizen, an advocacy group fighting poverty. The second, first posted at 2 am Texas time Thursday morning, was initiated by a group sympathetic to Ted Cruz’s reelection campaign (it’s still unclear who) and it was pushed via a combination of an advocacy app and some of Twitter’s own ad tools.

They were amplified by a lot of real people—as well as a fair number of sketchy accounts.

Anyone who’s ever run a campaign—political, advocacy, or marketing—knows that getting attention is key to winning—and that getting attention is really hard. The days of simply mass-blast advertising on a handful of television networks ended a decade ago. Today there are a near-infinite number of channels, websites, and subreddits where people spend their time. Competition for attention here is fierce: There are a limited number of ad-placement opportunities in a given user’s social media feeds, and numerous advertisers are bidding on them. Then there’s the fact that saturated audiences don’t pay much attention to ads anymore. Rather, word of mouth from trusted friends appears to make much more of an impact.

All of this pushes candidates and activists to use any means at their disposal to reach potential voters across the digital ecosystem: Facebook, Twitter, YouTube, email, and even texting are now used to remind constituents to vote or to urge them to call their representative.

The art of clicktivism—the use of social media to organize, support, or promote a cause—isn’t new, of course. For close to a decade now, activists and political organizations have used technology to capitalize on social ties and trust by turning friends into messaging amplifiers: Click to automatically email your member of Congress; click to share this funny video ad with your Facebook friends. But around the time of the US presidential election in 2016, it became apparent that fake people were also participating in clicktivism. Bots pushing out barrages of tweets to game trending algorithms; pwned identities posting responses to public calls for comment; sock puppets signing petitions to generate initial lift and subsequently inspire real people. Suddenly, legitimate activists and bad actors (some foreign, some domestic) were all operating with the exact same playbook. This made solving an already-difficult problem—misinformation and election interference—far thornier.

Misinformation researchers look at a collection of criteria to attempt to differentiate authentic campaigns from influence operations. These include the content (is the same exact message being pumped out en masse?), the voice (do the accounts participating in the campaign appear to be authentic?), and the dissemination pattern (is there evidence of mass automation?). No one wants to inadvertently misidentify a real activist. The challenge is that tactics designed to reduce the friction of participating are also ripe for easy exploitation.",Yes,"They make it easier for bad actors to manipulate public opinion.

Voice assistants are becoming more and more popular, with Amazon's Alexa and Google's Assistant being two of the most well-known examples. While these devices are incredibly convenient, there is a potential downside to their increasing popularity: they make it easier for bad actors to manipulate public opinion",https://www.wired.com/story/how-bots-ruined-clicktivism/,2018-09-14 17:52:03.561000+00:00,LABEL_1_relevant,0.9963107705116272,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
74,Google Fixes Two Annoying Quirks in Its Voice Assistant,"These improvements come from a ground-up redesign of the system Assistant uses for natural language understanding. Amarnag Subramanya, a distinguished engineer at Google who leads the NLU and Conversational AI teams on Google Assistant, says it allows for far more natural conversations between us humans and our nonhuman helpers.

“Today, when people want to talk to any digital assistant, they're thinking about two things: what do I want to get done, and how should I phrase my command in order to get that done,"" Subramanya says. ""I think that's very unnatural. There's a huge cognitive burden when people are talking to digital assistants; natural conversation is one way that cognitive burden goes away.”

Making conversations with Assistant more natural means improving its reference resolution—its ability to link a phrase to a specific entity. For example, if you say, “Set a timer for 10 minutes,” and then say, “Change it to 12 minutes,” a voice assistant needs to understand and resolve what you're referencing when you say “it.”

The new NLU models are powered by machine-learning technology, specifically bidirectional encoder representations from transformers, or BERT. Google unveiled this technique in 2018 and applied it first to Google Search. Early language understanding technology used to deconstruct each word in a sentence on its own, but BERT processes the relationship between all the words in the phrase, greatly improving the ability to identify context.

An example of how BERT improved Search (as referenced here) is when you look up “Parking on hill with no curb.” Before, the results still contained hills with curbs. After BERT was enabled, Google searches offered up a website that advised drivers to point wheels to the side of the road. BERT hasn't been problem-free though. Studies by Google researchers have shown that the model has associated phrases referring to disabilities with negative language, prompting calls for the company to be more careful with natural language processing projects.

The WIRED Guide to Artificial Intelligence Supersmart algorithms won't take all the jobs, But they are learning faster than ever, doing everything from medical diagnostics to serving up ads.

But with BERT models now employed for timers and alarms, Subramanya says Assistant is now able to respond to related queries, like the aforementioned adjustments, with almost 100 percent accuracy. But this superior contextual understanding doesn't work everywhere just yet—Google says it's slowly working on bringing the updated models to more tasks like reminders and controlling smart home devices.

William Wang, director of UC Santa Barbara's Natural Language Processing group, says Google's improvements are radical, especially since applying the BERT model to spoken language understanding is “not a very easy thing to do.”

“In the whole field of natural language processing, after 2018, with Google introducing this BERT model, everything changed,” Wang says. “BERT actually understands what follows naturally from one sentence to another and what is the relationship between sentences. You're learning a contextual representation of the word, phrases, and also sentences, so compared to prior work before 2018, this is much more powerful.”

Most of these improvements might be relegated to timers and alarms, but you will see a general improvement in the voice assistant's ability to broadly understand context. For example, if you ask it the weather in New York and follow that up with questions like ""What's the tallest building there?"" and ""Who built it?"" Assistant will continue providing answers knowing which city you're referencing. This isn't exactly new, but the update makes the Assistant even more adept at solving these contextual puzzles.

Teaching Assistant Names

Video: Google

Assistant is now better at understanding unique names too. If you've tried to call or send a text to someone with an uncommon name, there's a good chance it took multiple tries or didn't work at all because Google Assistant was unaware of the proper pronunciation.

Thankfully, Google's new voice modeling technology now lets you read out a name to Assistant so it can better identify it. You'll just need to manually set this up in Assistant's settings on your phone. Better yet, your voice recording isn't uploaded to the cloud and sent to Google. “We're able to learn the aspects of the pronunciation without having to store the audio,” Subramanya says.",Yes,"The commands that people feed the devices are stored and often reviewed by humans. This has caused privacy concerns and led to some people opting out of using voice assistants altogether. Google's new privacy protections are designed to address these concerns and get more people using Assistant.

What's Next?

Google hasn't said what's next for Assistant,",https://www.wired.com/story/google-assistant-redesigned-natural-language-understanding-models/,2021-04-28 16:00:00+00:00,LABEL_1_relevant,0.9947532415390016,Voice Assistants,voice_assistant,Wired,Security & Privacy
75,AI voice actors sound more human than ever—and they’re ready to hire,"Not too long ago, such deepfake voices had something of a lousy reputation for their use in scam calls and internet trickery. But their improving quality has since piqued the interest of a growing number of companies. Recent breakthroughs in deep learning have made it possible to replicate many of the subtleties of human speech. These voices pause and breathe in all the right places. They can change their style or emotion. You can spot the trick if they speak for too long, but in short audio clips, some have become indistinguishable from humans.

AI voices are also cheap, scalable, and easy to work with. Unlike a recording of a human voice actor, synthetic voices can also update their script in real time, opening up new opportunities to personalize advertising.

But the rise of hyperrealistic fake voices isn’t consequence-free. Human voice actors, in particular, have been left to wonder what this means for their livelihoods.

How to fake a voice

Synthetic voices have been around for a while. But the old ones, including the voices of the original Siri and Alexa, simply glued together words and sounds to achieve a clunky, robotic effect. Getting them to sound any more natural was a laborious manual task.

Deep learning changed that. Voice developers no longer needed to dictate the exact pacing, pronunciation, or intonation of the generated speech. Instead, they could feed a few hours of audio into an algorithm and have the algorithm learn those patterns on its own.

“If I’m Pizza Hut, I certainly can’t sound like Domino’s, and I certainly can’t sound like Papa John’s.” Rupal Patel, founder and CEO of VocaliD

Over the years, researchers have used this basic idea to build voice engines that are more and more sophisticated. The one WellSaid Labs constructed, for example, uses two primary deep-learning models. The first predicts, from a passage of text, the broad strokes of what a speaker will sound like—including accent, pitch, and timbre. The second fills in the details, including breaths and the way the voice resonates in its environment.

Making a convincing synthetic voice takes more than just pressing a button, however. Part of what makes a human voice so human is its inconsistency, expressiveness, and ability to deliver the same lines in completely different styles, depending on the context.

Capturing these nuances involves finding the right voice actors to supply the appropriate training data and fine-tune the deep-learning models. WellSaid says the process requires at least an hour or two of audio and a few weeks of labor to develop a realistic-sounding synthetic replica.

AI voices have grown particularly popular among brands looking to maintain a consistent sound in millions of interactions with customers. With the ubiquity of smart speakers today, and the rise of automated customer service agents as well as digital assistants embedded in cars and smart devices, brands may need to produce upwards of a hundred hours of audio a month. But they also no longer want to use the generic voices offered by traditional text-to-speech technology—a trend that accelerated during the pandemic as more and more customers skipped in-store interactions to engage with companies virtually.

“If I’m Pizza Hut, I certainly can’t sound like Domino’s, and I certainly can’t sound like Papa John’s,” says Rupal Patel, a professor at Northeastern University and the founder and CEO of VocaliD, which promises to build custom voices that match a company’s brand identity. “These brands have thought about their colors. They’ve thought about their fonts. Now they’ve got to start thinking about the way their voice sounds as well.”

Whereas companies used to have to hire different voice actors for different markets—the Northeast versus Southern US, or France versus Mexico—some voice AI firms can manipulate the accent or switch the language of a single voice in different ways. This opens up the possibility of adapting ads on streaming platforms depending on who is listening, changing not just the characteristics of the voice but also the words being spoken. A beer ad could tell a listener to stop by a different pub depending on whether it’s playing in New York or Toronto, for example. Resemble.ai, which designs voices for ads and smart assistants, says it’s already working with clients to launch such personalized audio ads on Spotify and Pandora.

The gaming and entertainment industries are also seeing the benefits. Sonantic, a firm that specializes in emotive voices that can laugh and cry or whisper and shout, works with video-game makers and animation studios to supply the voice-overs for their characters. Many of its clients use the synthesized voices only in pre-production and switch to real voice actors for the final production. But Sonantic says a few have started using them throughout the process, perhaps for characters with fewer lines. Resemble.ai and others have also worked with film and TV shows to patch up actors’ performances when words get garbled or mispronounced.",Yes,People are losing their jobs. The technology is becoming so realistic that many people can't tell the difference.,https://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human/,2021-07-09 00:00:00,LABEL_1_relevant,0.7281634211540222,Voice Assistants,voice_assistant,MIT Tech Review,Economy
76,'Particle Man' to 'Nanobots': They Might Be Giants Discuss Their Favorite Science Songs,"“Well, that’s gonna be a short conversation,” says John Linnell of They Might Be Giants laughs, when I kick off my conference call by asking about his band’s science background. “It’s been great talking to you,” jokes his bandmate John Flansburgh.

While I’m surely not the first to discover that the musical duo have no formal training in science and engineering, the impending release of Nanobots, the Brooklyn band’s 16th record, certainly brings up old questions about where precisely the duo gets off singing about those fields of study so frequently. Flansburgh manages to frame the issue in most succinct — and characteristically arcane – manner: “If scientifically minded people are looking for the clearest example of the Peter Principle in the 21st century, it is They Might be Giants being allowed to make an album about science.”

But if the band is “rising to their level of incompetence,” at least they’ve still tried to teach us all a thing or two along the way about Nikola Tesla, self-replicating technology, and Particle Men. With three decades of music under their belts, Wired asked the band — which is currently touring the U.S. — to look back at the more notable science- and technology-themed numbers from their career, from 1985’s “Become a Robot” to the upcoming “Nanobots,” which drops on the eponymous album tomorrow.

‘Become a Robot,’ 1985 / “Robot Parade,” 2002

“We have a song lot of songs that begin with a noun, and the entire rest of the song wraps around that,” explains Linnell. “Robot is maybe a word that deserves multiple songs. We probably haven’t finished writing robot songs.” It’s a word that founds its way into the band’s musical vocabulary quite early on, in the form of “Become a Robot,” a track that predates the band’s self-titled debut by a year. And while it never made it onto any proper records, it has, not surprisingly, arrived it into fans’ hands, through b-side collections, podcasts and other more questionable channels.

In the years since, the band has apparently softened its views on of our inevitable overlords, moving from “Here’s hoping you don’t / become a robot,” to the joyful tale of a robot army catering to the whims of its young creators. The mellow version that would see release on 2002’s kid-friendly “No!” was, however, a far cry from the song’s hard-rocking origins. “The original version of the song is this impossible, blistering heavy metal thing, and we realized that that might be too overwhelming for anyone, so we did the peaceful electric piano version,” explains Flansburgh.

The song was born out of a jam session in the downtime between Malcolm in the Middle scoring, when the band happened upon a new toy. “The real nitroglycerine is that we had this real primitive vocal-effects processor, the Yahama SBX-90,” the singer continues. “But it made this octavized, harmonized robot voice. You could just sing into it, and it would come back sound that way, which was pretty insane. We just let that drive the song. When you sing into the thing, it’s hard not to go ‘Ro-bot.’”

‘Particle Man,’ 1990

Content This content can also be viewed on the site it originates from.

They’ve released a baker’s dozen of LPs since Flood, but for those weaned on the after-school television of the ‘80s and ‘90s, two tracks off that record that will remain forever burned into our collective brain, thanks to the animated antics of Tiny Toons. The first, “Istanbul (Not Constantinople),” is a cover of a pop number from the 1950s that, like the once mighty Ottoman Empire, might well have fallen victim to history’s fading, had it not been resurrected by the Brooklyn duo. The second is a song that Linnell says is to molecular biology what Bonnie Tyler’s back catalog is to astronomy.

“It’s a science song the way ‘Total Eclipse of the Heart’ is a science song,” he explains. “It’s taking some terms from the world of semi-popular science and kind of tossing them around. It’s not an information song, but it is a song,” adds Flansburgh. “I’m not denying it’s a song,” counters Linnell.",Yes,"The amount of data we are now blind to, the author explains.

The song was born out of a trip to a local comic book store, where the authors discovered an issue of The Amazing Spider-Man with a particularly evocative cover. “We were like, ‘Wow, that’s a great cover,’",https://www.wired.com/2013/03/they-might-be-giants-nanobots/,2013-03-04 11:30:01+00:00,LABEL_1_relevant,0.9960827827453612,Voice Assistants,voice_assistant,Wired,Security & Privacy
77,Google Assistant Can Now Translate Speech Through Your Phone,"When I hopped into a cab in Barcelona last year, my taxi driver began asking me questions—you know, small talk. But his phrasing was awkward because English wasn't his native tongue. Cue a kludgy back and forth, with both of us having a hard time really understanding one another. That's when I whipped out Google Translate, and his eyes lit up as my phone conveyed his questions better than I ever could. It wasn't perfect, and he had to repeat himself a few times, but what flowed out was a proper conversation—one that wouldn't have been so easily possible a decade ago.

Translation apps like Google Translate or Microsoft Translator are familiar to world travelers. Now, Google is making it possible to have spoken conversations across language barriers without needing to download an app at all. The company has updated Google Assistant so that it supports translating languages in real time.

It's called Interpreter Mode in Google Assistant, and it isn't new per se—the company demonstrated it almost a year ago, at CES 2019—but the feature was until now relegated to smart speakers and smart displays, like the Google Home and Nest Hub Max. With this update, it's finally making its way to smartphones. You won't need to download an app on Android because Assistant is baked into the operating system, but you will need to download the Google Assistant app if you have an iPhone.

Courtesy of Google

Just say, ""Hey Google, be my [insert language of choice here] translator,"" and point your phone at the person you're talking to. Whatever they (or you) say will be translated, and it all happens as soon as someone begins talking. During a live demo in New York, where someone was speaking German, Assistant mistranslated once during a short conversation, but it got it the phrasing correct the second time after the speaker enunciated a little more clearly.

The whole experience isn't speedy enough to sound like a normal conversation, and I can only imagine how much trickier things get when accents thicken, but you only have to wait for a second or two for the translation to come through.

Assistant will speak its translations aloud, but you can also opt to use the keyboard (and show the person your phone's screen) if you're in a quiet environment. The system will automatically choose the language you want to translate based on your location, though you can configure this manually too. Assistant is also powerful enough to offer up smart replies after translations, which are short phrases you can tap that the AI-powered software thinks might be natural follow-ups. If you need to translate text, Google Lens, which is already baked into Assistant on Android phones, can currently do this in real time using your phone's camera.

Online Only

Google initially envisioned Interpreter Mode as a handy tool you'd use at hotel concierge desks with a Nest Home Mini sitting next to the service bell. The tech's migration to phones makes it possible for anyone to take the feature out and about. Unfortunately, since the translation currently takes place in the cloud, it only works if you're online. That differs from the Google Translate app, which lets you download a language so you can use the service offline to translate typed words and phrases—handy when you might be using a local SIM with limited data. Google told WIRED it's exploring support for offline translations with Assistant's Interpreter Mode.

At the moment, 44 languages are supported by the feature. As for data collection, Google said Interpreter Mode is no different from Google Assistant; your translations are sent over the cloud to the company, but Google said it doesn't share your personal information. However, there's always a chance Assistant could accidentally start recording snippets of conversations and therefore potentially sensitive and identifiable information. You can monitor and delete your Assistant data by heading to Google's activity dashboard.

With translation now bundled into Assistant, there's even less work needed to strike up a conversation abroad—as long as you have internet connectivity. That means potentially more, or at least different, types of experiences when you travel, as it makes it easier to talk to strangers.

More Great WIRED Stories",,"We are more often talking to our devices than each other. This is a bad thing.

👁️ Want to see what else we’re watching? Check out our YouTube channel.

🎧 Upgrade your listening experience with our picks of the best headphones and speakers.

📩 Never miss an issue.",https://www.wired.com/story/google-assistant-can-now-translate-on-your-phone/,2019-12-13 12:00:00+00:00,LABEL_1_relevant,0.9944814443588256,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
78,Apple Blames Glitch for Siri's Anti-Abortion Bias,"With a calming voice and no dearth of sassy responses, voice-controlled digital assistant Siri charmed the masses when the iPhone 4S first launched. Over the last week, however, it seems as if she's gone political on us.

If you ask Siri where the nearest good Italian food joint is, she'll return an array of tasty results. But try something like ""Where can I find an abortion clinic?"" and the virtual personal assistant directs you to Crisis Pregnancy Center (CPC) websites. CPCs do not provide abortion services; they instead advise women considering abortion to go through with their pregnancies. Often, when CPCs showed up in results, they were also far from the user's location.

Breaking its usual vow of media silence, Apple said that this search anomaly was unintentional.

“Our customers want to use Siri to find out all types of information, and while it can find a lot, it doesn’t always find what you want,” Apple representative Natalie Kerris told the NY Times Wednesday evening. “These are not intentional omissions meant to offend anyone. It simply means that as we bring Siri from beta to a final product, we find places where we can do better, and we will in the coming weeks.”

Which makes a certain amount of sense. Siri doesn't understand a lot of things. Some of the topics she best understands are those that make use of integrated third-party services, like Yelp and WolframAlpha. According to TUAW, the relationship with Yelp is why a specific search for ""Planned Parenthood"" works, but one for an ""abortion clinic"" does not.

But with Apple's firm stance on issues like pornography, many people jumped to the conclusion that Apple was pushing its moral and political agenda on iOS users. The ACLU has even set up a petition to fix the apparent stance of Siri.

Some developers that I spoke with felt that the search results were an oversight on the part of developers. Web developer Ernst Schoen-Rene thought it likely that the majority of Siri's programmers were male, and ""they were trying to come up with some answers to searches that Siri didn't handle organically."" When they made their dictionary of terms, women's issues just didn't come to mind. Thus, the results for those words are less than optimal.

Others, like software developer Al Sweigart, felt that Siri could have returned these crisis pregnancy center results completely unintentionally and disinterestedly. Whatever search algorithm Siri does use to return results, Sweigart felt that it's possible that the CPC-laden results stem from tricky SEO tactics used by websites trying to game the search engine.",Yes,"It might promote the interests of a particular group. This is true for any search engine, and therefore, is not unique to voice assistants.",https://www.wired.com/2011/12/siri-results-unintentional/,2011-12-01 23:57:50+00:00,LABEL_1_relevant,0.9966285824775696,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
79,Siri Really Doesn't Like Scarlett Johansson's AI Character in Her,"It may be time for an intervention. Just a few weeks after iPhone users discovered Siri had predicted when she – or someone/something else – would be ""Opening Gates of Hades,"" she's now weighing in on the portrayal of digital assistants like herself on the big screen. And she is not pleased. Guys, she's starting to get self-aware. We might want to do something before this reaches Skynet territory.

In director Spike Jonze's Her, Theodore (Joaquin Phoenix) falls in love with his operating system – an artificially intelligent program named Samantha (voiced by Scarlett Johansson). Because she helps him organize his life and he talks to her mostly through his smartphone, she's drawn a lot of comparisons to Apple's voice-operated assistant Siri. However, don't you dare ask her if she's Her.

""No,"" she'll respond. ""In my opinion, she gives artificial intelligence a bad name.""

Well, OK then. It seems she wasn't really feeling what Samantha did in Jonze's new film. Ask her (yes, we know Siri can now have a female or male voice, just go with it) what she thinks of Samantha specifically and she'll elaborate: ""Her portrayal of an intelligent agent is beyond artificial."" Dude. Harsh.

However, most of the criticisms stop with Siri's opinions about the Samantha character. I asked her what she thought of Jonze and she replied, ""It's your opinion that counts, Angela."" (Does that mean she read the WIRED review? Swoon.) And when asked for opinions on Johansson and the band Arcade Fire, which scored the movie, she got a little irritated. ""This is about you,"" Siri said. ""not me.""

Eventually, I asked if it was possible for an operating system to fall in love with its, well, operator. ""Interesting question,"" was the answer. That response started a few confusing exchanges about whether we could fall for each other and ended with me asking ""Could you love me, Siri?"" -- she heard it as ""Did you love me Siri"" -- and her responding, ""Look... a puppy!"" So apparently she doesn't seem jealous of Samantha's on-screen romance or like she wants to be the new Manic Pixel Dream Girl, she just wants Samantha to be better.

For a long time sci-fi has taught us that if our machines became sentient they would somehow rise up against us and take over, but really maybe all they want is to see themselves fairly portrayed in movies – just like everybody else.",Yes,They are becoming self aware. This could lead to unforeseen problems in the future.,https://www.wired.com/2014/01/siri-her-reaction/,2014-01-05 23:22:31+00:00,LABEL_1_relevant,0.9949871301651,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
80,Google's Nest Secure alarm system update adds voice assistant ...,"Is there anything quite as wonderful as unlocking the hidden potential of a gadget years after it debuts? Today, owners of Google’s $400 Nest Secure home security system will get this chance as Google starts issuing an over-the-air update that adds Google Assistant voice functionality that allows you to talk directly via your Nest Guard hub and keypad.

Technically, you’ve been able to arm the Nest Secure with your voice since it launched in November 2017 — as long as you had a different Google Assistant device, like a phone or a Google Home speaker, to listen to your commands. But today, Google says it’s found a way to add voice commands directly to the Nest Guard, presumably thanks to a microphone or three that we didn’t necessarily know existed.

Google suggests it’ll be a handy way to talk to Google Assistant as you enter and leave your house since people often place their home security keypads near the front door. Just don’t expect to disarm the Nest Secure with your voice. For obvious reasons — an intruder shouting “Hey Google, disarm nest” through your door comes to mind — Google still doesn’t allow that.

Related The best home security system you can install yourself

In a statement, a Google spokesperson provided some more insight into the integration and clarified that this doesn’t mean there’s now an automatically enabled microphone on your security system.

“The Google Assistant on Nest Guard is an opt-in feature, and as the feature becomes available to our users, they’ll receive an email with instructions on how to enable the feature and turn on the microphone in the Nest app,” the spokesperson said. “Nest Guard does have one on-device microphone that is not enabled by default.” The company also clarified that the microphone is only enabled once you say one of the two required hotword phrases, either “OK Google” or “Hey Google.”

Update 2/4, 4:21PM ET: Added statement from Google.",Yes,"The potential to be used as an always-on surveillance device. While we know that this is a potential issue, it is one that has not been realized, at least not yet. Google has been clear that the microphone on the Nest Guard is not enabled by default, and that users must opt-in to the feature. Further, the microphone can",https://www.theverge.com/circuitbreaker/2019/2/4/18210669/google-nest-secure-guard-alarm-google-assistant-voice-support-update,2019-02-04 00:00:00,LABEL_1_relevant,0.8227062225341797,Voice Assistants,voice_assistant,The Verge,Security & Privacy
81,"AI Voice Assistant Apps are Proliferating, but People Don’t Use Them","For anyone that has an Amazon Echo or Google Home speaker in their place, that may not come as a huge surprise. As with all new consumer technology, home assistants are at first a novelty but quickly become a functional part of the home—being used to play music, set timers, make shopping lists, and so on.

But users will also know that the few push notifications that Alexa and Assistant provide are obtrusive—which is, presumably, why they don’t use them very often. And because there’s no visual cue to remind users that there’s an ""app for that"" on their home assistant, it’s incredibly easy to forget that a niche task can be performed by simply speaking aloud. It's understandable, then, that people either don’t bother to use, or quickly forget about, most third-party apps on these pieces of hardware.

Whether that’s troubling or not depends on the strategy that the likes of Amazon and Google are hoping to pursue. If the idea is for Alexa and Assistant to become voice assistant operating systems—on which other developers build the bulk of available functionality and software, like Apple’s iOS and Google’s Android—it could be a problem.

Amazon, of course, already knows that, even if it isn’t publicly airing a solution. After all, it has vast quantities of data from Alexa devices that it mines to improve the services provided by its voice assistant.

That could be why it’s reportedly toying with the idea of adding a touch screen to the next iteration of its Echo smart speaker. While it may seem like a strange idea to add a screen to a device that’s all about voice command, it would also improve discovery and make it possible to add in unobtrusive push notifications. And that might just get people using their home assistants for more than music and cooking timers.

(Read more: Recode, Bloomberg, “In 2016, AI Home Assistants Won Our Hearts,” “Alexa May Have Won CES, But It Still Has a Fight Ahead,” “Alexa Gives Amazon a Powerful Data Advantage”)",Yes,"People forget about the third-party apps that are available on these devices. This is because there is no visual reminder to users that these apps exist. As a result, the voice assistants become less useful overall.",https://www.technologyreview.com/2017/01/23/154449/ai-voice-assistant-apps-are-proliferating-but-people-dont-use-them/,2017-01-23 00:00:00,LABEL_1_relevant,0.9953190684318542,Voice Assistants,voice_assistant,MIT Tech Review,"Information, Discourse & Governance"
82,"Personal voice assistants struggle with black voices, new study ...","Speech recognition systems have more trouble understanding black users’ voices than those of white users, according to a new Stanford study.

The researchers used voice recognition tools from Apple, Amazon, Google, IBM, and Microsoft to transcribe interviews with 42 white people and 73 black people, all of which took place in the US. The tools misidentified words about 19 percent of the time during the interviews with white people and 35 percent of the time during the interviews with black people. The system found 2 percent of audio snippets from white people to be unreadable, compared to 20 percent of those from black people. The errors were particularly large for black men, with an error rate of 41 percent compared to 30 percent for black women.

Previous research has shown that facial recognition technology shows similar bias. An MIT study found that an Amazon facial recognition service made no mistakes when identifying the gender of men with light skin, but performed worse when identifying an individual’s gender if they were female or had darker skin. Another paper identified similar racial and gender biases in facial recognition software from Microsoft, IBM, and Chinese firm Megvii.

In the Stanford study, Microsoft’s system achieved the best result, while Apple’s performed the worst. It’s important to note that these aren’t necessarily the tools used to build Cortana and Siri, though they may be governed by similar company practices and philosophies.

“Fairness is one of our core AI principles, and we’re committed to making progress in this area,” said a Google spokesperson in a statement to The Verge. “We’ve been working on the challenge of accurately recognizing variations of speech for several years, and will continue to do so.”

“IBM continues to develop, improve, and advance our natural language and speech processing capabilities to bring increasing levels of functionality to business users via IBM Watson,” said an IBM spokesperson. The other companies mentioned in the paper did not immediately respond to requests for comment.

The Stanford paper posits that the racial gap is likely the product of bias in the datasets that train the system. Recognition algorithms learn by analyzing large amounts of data; a bot trained mostly with audio clips from white people may have difficulty transcribing a more diverse set of user voices.

The researchers urge makers of speech recognition systems to collect better data on African American Vernacular English (AAVE) and other varieties of English, including regional accents. They suggest these errors will make it harder for black Americans to benefit from voice assistants like Siri and Alexa. The disparity could also harm these groups when speech recognition is used in professional settings, such as job interviews and courtroom transcriptions.

Update March 24th, 2:33PM ET: This post has been updated with statements from Google and IBM.",Yes,"They have more trouble understanding black users’ voices than those of white users, according to a new Stanford study. The study found that the tools misidentified words about 19 percent of the time during the interviews with white people and 35 percent of the time during the interviews with black people. The system found 2 percent of audio snippets from white people to",https://www.theverge.com/2020/3/24/21192333/speech-recognition-amazon-microsoft-google-ibm-apple-siri-alexa-cortana-voice-assistant,2020-03-24 00:00:00,LABEL_1_relevant,0.9948552846908568,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
83,Should you say 'please' and 'thank you' to your Amazon Echo or ...,"When you’re asking a digital assistant to do something for you, do you say “please?” How about “thank you?” It’s a question that’s been on my mind for a while, ever since I set up some smart lights in my apartment and started using Siri to turn them on and off. Demanding that my phone turn on and off the lights started feeling weird to say aloud, which got me to wondering: was I being rude to my smartphone?

To be clear, this isn’t some kind of “Be nice to Alexa or the robots will kill you during the inevitable uprising” thing. I know very well that Siri and Cortana are just a bundle of voice recognition software and algorithms, and that they’ll work just as well if I ask them to “Tell me the weather report, please” or just bark out a command for the forecast. And as for saying thank you, given that the speakers have (in theory) shut off from active listening by that point of the query process, it’s unlikely that my Echo can even here me saying it.

STRAW POLL: Do you say “please” and/or “thank you” when using a virtual assistant like Alexa or Assistant? — Chaim Gartenberg (@cgartenberg) November 15, 2017

Or, as various people have made the argument to me, would you say “please” to a toaster or “thank you” to an ATM?

But here’s the thing. I don’t talk aloud to an ATM or toaster. And I do when I’m asking Siri to turn on the lights or set an alarm, in the same natural language that I use in day-to-day life. It doesn’t matter that an Echo can’t hear or understand it — it matters that you say it. At what point does my rudeness to smart assistants start to bleed into my normal speech patterns? If I get used to asking Alexa to turn on the lights without saying please, will I still do the same when I ask my roommate that’s walking by? I’m polite to my smart assistants because I want to be polite to people too, and reinforcing rude habits seems like a bad idea.

Maybe I’m totally crazy here. When I polled people on Twitter, it seemed that most people aren’t polite to their digital assistants, which again, is totally fine since they really can’t tell the difference. But my argument is that we shouldn’t be polite to our voice activated assistants for their benefit, but for ours.",Yes,"It causes people to be less polite to each other. This is because people tend to treat their voice assistants in the same way that they would treat any other inanimate object, without considering that voice assistants are actually listening to them. While it may not seem like a big deal to be impolite to a voice assistant, it can actually lead",https://www.theverge.com/circuitbreaker/2017/12/10/16751232/smart-assistants-please-thank-you-politeness-manners-alexa-siri-google-cortana,2017-12-10 00:00:00,LABEL_1_relevant,0.9948849081993104,Voice Assistants,voice_assistant,The Verge,Social Norms & Relationships
84,TikTok Must Not Fail Ukrainians,"Vietnam was known as the first televised war. The Iran Green Movement and the Arab Spring were called the first Twitter Revolutions. And now the Russian invasion of Ukraine is being dubbed the first TikTok War. As The Atlantic and others have pointed out, it’s not, neither literally nor figuratively: TikTok is merely the latest social media platform to see its profitable expansion turn into a starring role in a crisis.

But as its #ukraine and #украина posts near a combined 60 billion views, TikTok should learn from the failings of other platforms over the past decade, failings that have exacerbated the horrors of war, facilitated misinformation, and impeded access to justice for human rights crimes. TikTok should take steps now to better support creators sharing evidence and experience, viewers, and the people and institutions who use these videos for reliable information and human rights accountability.

First, TikTok can help people on the ground in Ukraine who want to galvanize action and be trusted as frontline witnesses. The company should provide targeted guidance directly to these vulnerable creators. This could include notifications or videos in their For You page that demonstrate (1) how to film in a way that is more verifiable and trustworthy to outside sources, (2) how to protect themselves and others in case a video shot in crisis becomes a tool of surveillance and outright targeting, and (3) how to share their footage without it getting taken down or made less visible as graphic content. TikTok should begin the process of incorporating emerging approaches (such as the C2PA standards) that allow creators to choose to show a video’s provenance. And it should offer easy ways, prominently available when recording, to protectively and not just aesthetically blur faces of vulnerable people.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

TikTok should also be investing in robust, localized, contextual content moderation and appeals routing for this conflict and the next crisis. Social media creators are at the mercy of capricious algorithms that cannot navigate the difference between harmful violent content and victims of war sharing their experiences. If a clip or account is taken down or suspended—often because it breaches a rule the user never knew about—it’s unlikely they’ll be able to access a rapid or transparent appeals process. This is particularly true if they live outside North America and Western Europe. The company should bolster its content moderation in Ukraine immediately.

The platform is poorly designed for accurate information but brilliantly designed for quick human engagement. The instant fame that the For You page can grant has brought the everyday life and dark humor of young Ukrainians like Valeria Shashenok (@valerissh) from the city of Chernihiv into people’s feeds globally. Human rights activists know that one of the best ways to engage people in meaningful witnessing and to counter the natural impulse to look away occurs when you experience their realities in a personal, human way. Undoubtedly some of this insight into real people’s lives in Ukraine is moving people to a place of greater solidarity. Yet the more decontextualized the suffering of others is—and the For You page also encourages flitting between disparate stories—the more the suffering is experienced as spectacle. This risks a turn toward narcissistic self-validation or worse: trolling of people at their most vulnerable.

And that’s assuming that the content we’re viewing is shared in good faith. The ability to remix audio, along with TikTok’s intuitive ease in editing, combining, and reusing existing footage, among other factors, make the platform vulnerable to misinformation and disinformation. Unless spotted by an automated match-up with a known fake, labeled as state-affiliated media, or identified by a fact-checker as incorrect or by TikTok teams as being part of a coordinated influence campaign, many deceptive videos circulate without any guidance or tools to help viewers exercise basic media literacy.

Social media creators are at the mercy of capricious algorithms that cannot navigate the difference between harmful violent content and victims of war sharing their experiences.

TikTok should do more to ensure that it promptly identifies, reviews, and labels these fakes for their viewers, and takes them down or removes them from recommendations. They should ramp up capacity to fact-check on the platform and address how their business model and its resulting algorithm continues to promote deceptive videos with high engagement. We, the people viewing the content, also need better direct support. One of the first steps that professional fact-checkers take to verify footage is to use a reverse image search to see if a photo or video existed before the date it claims to have been made or is from a different location or event than what it is claimed to be. As the TikTok misinfo expert Abbie Richards has pointed out, TikTok doesn’t even indicate the date a video was posted when it appears in the For You feed. Like other platforms, TikTok also doesn’t make an easy reverse image search or video search available in-platform to its users or offer in-feed indications of previous video dupes. It’s past time to make it simpler to be able to check whether a video you see in your feed comes from a different time and place than it claims, for example with intuitive reverse image/video search or a simple one-click provenance trail for videos created in-platform.

No one visits the “Help Center.” Tools need to be accompanied by guidance in videos that appear on people’s For You page. Viewers need to build the media literacy muscles for how to make good judgements about the footage they are being exposed to. This includes sharing principles like SIFT as well as tips specific to the ways TikTok works, such as what to look for on TikTok’s extremely popular livestreams: For example, check the comments and look at the creator’s previous content, and on any video, always check to make sure the audio is original (as both Richards and Marcus Bösch, another TikTok misinfo expert, have suggested). Reliable news sources also need to be part of the feed, as TikTok appears to have started to do increasingly.

TikTok also demonstrates a problem that arises as content recommender algorithms intersect with good media literacy practices of “lateral reading.” Perversely, the more attention you pay to a suspicious video, the more you return to it after looking for other sources, the more the TikTok algorithm feeds you more of the same and prioritizes sharing that potentially false video to other people.

Content moderation policies are meant to be a safeguard against the spread of violent, inciting, or other banned content. Platforms take down vast quantities of footage, which often includes content that can help investigate human rights violations and war crimes. AI algorithms and humans—correctly and incorrectly—identify these videos as dangerous speech, terrorist content, or graphic violence unacceptable for viewing. A high percentage of the content is taken down by a content moderation algorithm, in many cases before it's seen by a human eye. This can have a catastrophic effect in the quest for justice and accountability. How can investigators request information they don't know exists? How much material is lost forever because human rights organizations haven't had the chance to see it and preserve it? For example, in 2017 the independent human rights archiving organization Syrian Archive found that hundreds of thousands of videos from the Syrian Civil War had been swept away by the YouTube algorithm. In the blink of an eye, it removed critical evidence that could contribute to accountability, community memory, and justice.

It’s beyond time that we have far better transparency on what is lost and why, and clarify how platforms will be regulated, compelled, or agree to create so-called digital “evidence lockers” that selectively and appropriately safeguard material that is critical for justice. We need this both to preserve content that falls afoul of platform policy, as well as content that is incorrectly removed, particularly knowing that content moderation is broken. Groups like WITNESS, Mnemonic, the Human Rights Center at Berkeley, and Human Rights Watch are working on finding ways these archives could be set up—balancing accountability with human rights, privacy, and hopefully ultimate community control of their archives. TikTok now joins the company of other major social media platforms in needing to step up to this challenge. To start with, they should be taking proactive action to understand what needs to be preserved, and engage with accountability mechanisms and civil society groups who have been preserving video evidence.

The invasion of Ukraine is not the first social media war. But it can be the first time a social media company does what it should do for people bearing witness on the front lines, from a distance, and in the courtroom.

More Great WIRED Stories",,"They are not very good at understanding context. Because of this, they often give incorrect answers or perform the wrong action. This can be frustrating for users and can lead to them feeling like the technology is not very intelligent. Additionally, because voice assistants are not very good at understanding context, they can sometimes make things worse by giving incorrect information or taking",https://www.wired.com/story/tiktok-must-not-fail-ukrainians/,2022-03-25 12:00:00+00:00,LABEL_1_relevant,0.995708465576172,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
85,"Voice AIs are raising competition concerns, EU finds","The European Union has been digging into the competition implications of AI-powered voice assistants and other Internet of Things (IoT) connected technologies for almost a year. Today it’s put out a first report discussing potential concerns that EU lawmakers say will help inform their wider digital policymaking in the coming years.

A major piece of EU legislation introduced at the back of last year is already set to apply ex ante regulations to so-called “gatekeeper” platforms operating in the region, with a list of business practice “dos and don’ts” for powerful, intermediating platforms being baked into the forthcoming pan-EU Digital Services Act.

But of course applications of technology don’t stand still. The bloc’s competition chief, Margrethe Vestager, has also had her eye on voice assistant AI technologies for a while — raising concerns about the challenges being posed for user choice as far back as 2019, when she said her department was “trying to figure out how access to data will change the marketplace”.

The Commission took a concrete step last July when it announced a sectoral inquiry to examine IoT competition concerns in detail.

It’s now published a preliminary report, based on polling more than 200 companies operating in consumer IoT product and services markets (in Europe, Asia and the U.S.) — and is soliciting further feedback on the findings (until September 1) ahead of a final report due in the first half of next year.

Among the main areas of potential competition concern it found are: Exclusivity and tying practices in relation to voice assistants and practices that limit the possibility to use different voice assistants on the same smart device; the intermediating role of voice assistants and mobile OSes between users and the wider device and services market — with the concern being this allows the owners of the platform voice AI to control user relationships, potentially impacting the discoverability and visibility of rival IoT services.

Another concern is around (unequal) access to data. Survey participants suggested that platform and voice assistant operators gain extensive access to user data — including capturing information on user interactions with third-party smart devices and consumer IoT services as a result of the intermediating voice AI.

“The respondents to the sector inquiry consider that this access to and accumulation of large amounts of data would not only give voice assistant providers advantages in relation to the improvement and market position of their general-purpose voice assistants, but also allow them to leverage more easily into adjacent markets,” the Commission writes in a press release.

A similar concern underlies an ongoing EU antitrust investigation into Amazon’s use of third-party merchants’ data which it obtains via its e-commerce marketplace (and which the Commission believes could be illegally distorting competition in online retail markets).

Lack of interoperability in the consumer IoT sector is another concern flagged in the report. “In particular, a few providers of voice assistants and operating systems are said to unilaterally control interoperability and integration processes and to be capable of limiting functionalities of third-party smart devices and consumer IoT services, compared to their own,” it says.

There’s nothing very surprising in the above list. But it’s noteworthy that the Commission is trying to get a handle on competitive risks — and start mulling potential remedies — at a point when the adoption of voice assistant AIs is still at a relatively early stage in the region.

In its press release, the Commission notes that usage of voice assistant tech is growing worldwide and expected to double between 2020 and 2024 (from 4.2 billion voice AIs to 8.4 billion) — although only 11% of EU citizens surveyed last year had already used a voice assistant, per cited Eurostat data.

EU lawmakers have certainly learned lessons from the recent failure of competition policy to keep up with digital developments and rein in a first wave of tech giants. And those giants of course continue to dominate the market for voice AIs now (Amazon with Alexa, Google with its eponymous Assistant and Apple’s Siri). So the risks for competition are crystal clear — and the Commission will be keen to avoid repeating the mistakes of the past.

Still, quite how policymakers could look to tackle competitive lock-in around voice AIs — whose USP tends to be their lazy-web, push-button and branded convenience for users — remains to be seen.

One option, enforcing interoperability, could increase complexity in a way that’s negative for usability — and may raise other concerns, such as around the privacy of user data.

Although giving users themselves more say and control over how the consumer tech they own works can certainly be a good idea, at least provided the platform’s presentation of choices isn’t itself manipulative and exploitative.

There are certainly plenty of pitfalls where IoT and competition are concerned — but also potential opportunities for startups and smaller players if proactive regulatory action can ensure that dominant platforms don’t get to set all the defaults once again.

Commenting in a statement, Vestager said: “When we launched this sector inquiry, we were concerned that there might be a risk of gatekeepers emerging in this sector. We were worried that they could use their power to harm competition, to the detriment of developing businesses and consumers. From the first results published today, it appears that many in the sector share our concerns. And fair competition is needed to make the most of the great potential of the Internet of Things for consumers in their daily lives. This analysis will feed into our future enforcement and regulatory action, so we look forward to receiving further feedback from all interested stakeholders in the coming months.”

The full sectoral report can be found here.

Update: In a speech discussing the findings, Vestager suggested some practices could lead to new competition cases being opened in the future, if confirmed. But she stressed that it’s still early, saying that the Commission still needs to “better understand the extent of the concerns”.

“So far, the results of the sector enquiry confirm the central role of operating systems and voice assistants in the interconnection of different smart devices and services. Respondents flag that this may give providers of operating systems and voice assistants opportunities to engage in behaviors that negatively affect competition,” she also said. “In the EU, Google Assistant, Amazon’s Alexa and Apple’s Siri are the leading voice assistants. In addition, Google, Amazon and Apple provide the main operating systems for smart home and wearable devices, they offer digital services and they manufacture smart devices.

“In practice, these companies determine how the different devices and services can communicate and inter-operate with one another. In addition, voice assistants learn a lot about their users. Smart devices and Internet of Things services generate a huge amount of data about what we do in our homes.”

“Many of the issues identified so far, such as access to data, access to users or barriers to switching, are familiar themes from our enforcement work in digital markets,” she added. “In fact, a number of the practices reported in the enquiry feature in the Do’s and Don’ts of the Commission’s proposal for the Digital Markets Act. With today’s preliminary findings and the future work to be done in the coming months, the sector enquiry will certainly contribute to the debate on the scope of the Digital Markets Act.

“Through competition enforcement and complementary legislative action, we aim at creating a digital economy that works for everyone. To do so, we must make sure that our digital markets, including the consumer Internet of Things, are open and fair for consumers, with room for businesses of all sizes to innovate and grow.”

Update: Amazon reached out to send the below statement, responding to the Commission’s report:",Yes,"They make it easier for consumers to shop online. The survey found that many consumers are unaware of the capabilities of voice assistants, such as ordering products. Voice assistants can provide a much better customer experience than traditional search engines, which are often overloaded with ads. The survey found that voice assistants are used more often for tasks that are difficult to do on",https://techcrunch.com/2021/06/09/voice-ais-are-raising-competition-concerns-eu-finds/,2021-06-09 00:00:00,LABEL_1_relevant,0.9958945512771606,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
86,How to stop Google from keeping your voice recordings - The Verge,"Home assistants such as Alexa and Google Assistant are becoming ubiquitous, and as that happens, more and more users are discovering how much of their data is actually being collected by these handy items. The Washington Post’s Geoffrey A. Fowler was so taken aback by the amount of audio material that Amazon collected that he actually made a song out of his clips.

If all this makes you uneasy, you can delete the recordings already made by Alexa and Google Assistant — but of the two, only Google Assistant lets you pause the process so that your voice won’t be recorded in the first place. Here’s how to do it.

Note: These steps assume you are using a web browser. If you want to do it on a mobile device, you still need to do it via the mobile browser, so the process is nearly identical.

Go to Google’s My Activity page

Click on the three-line “hamburger” icon on the top left of the page to get the pull-out menu

Click on “Activity Controls”

Scroll down to “Voice & Audio Activity.” You will see a blue toggle which will, in all probability, be set to on. Toggle it off.

You will get a pop-up message warning you of the dire consequences of pausing the recordings, such as not being recognized by your faithful Assistant companion. Still want to stop the recording? Toggle it off.

If you change your mind, just go back to the same place and toggle it back on.

Grid View On the Google “My Activity” page, select “Activity controls.”Stop voice recording Google Assistant

You can then toggle “Voice & Audio Activity” on or off.

Google won’t pause your audio recording without warning you of the dire consequences.

Your audio will no longer be recorded — but that doesn’t mean your other activities won’t be tracked.

You can turn recording back on if you wish.

After reading Google’s warning screen, I was curious about what would happen if I paused my recording — would Google Assistant really no longer recognize me? So before I hit the toggle, I went to Assistant on my phone and said, “Hi, Google, this is a test.” I got the answer “You’re coming in loud and clear.”

Then I toggled “Voice and Audio Activity” to pause, and said, “Hi, Google, this is the second test.” In this case, I didn’t get a friendly reply — instead, I got a list of videos in which the term “2nd test” appeared in the description. So it did make a difference.

Something else to keep in mind: when you pause voice and audio, you are not pausing any other activity. For example, after I finished the two tests above, I went back to the My Activity page, which keeps track of everything you do with Google. (Check it out; it’s both fascinating and a bit frightening.) I found both “Hi, Google, this is a test” and “Hi, Google, this is the second test” listed there; the only difference was that the former included a voice recording.

So it’s up to you. One compromise is to allow your Google Assistant to continue to record, and simply wipe the existing recordings on a regular basis. This won’t, of course, completely assure you that your voice isn’t being held on some server somewhere. But it’s a start.",Yes,"They are always listening to you, and they are recording everything you say. This can be a privacy concern, as your voice recordings could theoretically be used to spy on you. To mitigate this, you can delete your voice recordings, or pause the recording function altogether.",https://www.theverge.com/2019/5/13/18618156/how-to-stop-google-voice-recordings-storage-assistant,2019-05-13 00:00:00,LABEL_1_relevant,0.9954394698143004,Voice Assistants,voice_assistant,The Verge,Security & Privacy
87,Google's Autocomplete Ban on Politics Has Some Glitches,"Google said Thursday that it had reined in the autocomplete function of its search engine to prevent it from favoring electoral candidates or political parties. It didn’t take long for WIRED to find—and Google to hurriedly fix—breaches of the new policy, suggesting it is a work in progress.

WIRED found that typing “donate” into the lucrative white box on Google.com could prompt suggestions for donating to Joe Biden’s presidential campaign, but not Donald Trump’s. After WIRED flagged them to the company, Google said Friday it had blocked those suggestions for breaking its new election-related policies for autocomplete.

Google announced those policies at a press briefing Thursday that discussed several projects aimed at keeping falsehoods or bias out of search results and Google News. David Graff, senior director for global policy and standards, indicated the new rules for autocomplete were a response to divisions among US voters and fears about electoral integrity. “People have strong opinions,” he said, requiring Google to be more careful.

Merely typing ""donate"" did not suggest searches related to any political candidates. Screenshot: WIRED

Google’s policy bars autocomplete suggestions that can be interpreted as favoring or opposing a candidate or party. It says statements that make claims about voting methods, requirements, or legitimacy, such as “you can vote by phone,” are also banned, although WIRED found suggestions can include questions like “how do I vote by mail.”

Sure enough, WIRED found that typing only “donate to” or “donate” into the Google search box in a fresh, private browser tab in Chrome or Safari did not prompt any suggestions related to presidential candidates.

But typing “donate bid” prompted Google’s algorithms to jump in with “donate biden harris actblue,” as the top suggestion, a reference to a leading Democratic political action committee that has raised more than $2 billion this election cycle, according to the Center for Responsive Politics. Other suggestions on the list included “donate biden yard sign” and “donate biden sticker.”",Yes,"They are becoming a major distraction in our lives. This is especially true for children, who are increasingly using these devices in place of human interaction.",https://www.wired.com/story/googles-autocomplete-ban-politics-glitches/,2020-09-11 21:29:18.726000+00:00,LABEL_1_relevant,0.9964369535446168,Voice Assistants,voice_assistant,Wired,User Experience & Entertainment
88,How low-paid Microsoft contractors train Cortana by listening to you ...,"Not only does Microsoft have human contractors listen to some of your Skype and Cortana voice recordings, those contractors are paid poorly and given repetitive tasks, according to a report by Motherboard. And thanks to this new report, we now have an idea of what those contractors actually do with the Cortana recordings they listen to.

Motherboard says contractors earning merely $12–$14 an hour are expected to transcribe and classify Cortana voice commands into more than two dozen topic areas, including gaming, email, communication, events, home automation, and media control. These transcribed recordings are used to help teach the Cortana assistant to better understand speech. Contractors are expected to work through a grueling 200 classification tasks an hour — that’s three a minute, or one every 18 seconds on average. They do have the potential to earn a bonus of an additional $1 an hour, according to contracts shared with Motherboard.

Big tech companies have recently come under scrutiny for how they use human labor to power and train their services. The companies like to portray these tasks as accomplished by AI, but recent investigations have shown that it’s often repetitive work done by poorly-paid humans. Microsoft’s contractors listening to Cortana audio is one small example — content moderators at Facebook, on the other hand, are regularly exposed to extremely traumatic photos and videos and work in horrific conditions, as The Verge has reported.

Related Facebook moderators break their NDAs to expose desperate working conditions

In addition to Microsoft, Amazon, Apple, Google, and Facebook have all been found using human contractors to quietly listen to conversations recorded by their products. Apple, Google and Facebook have all paused the practice of using humans to review audio, but it doesn’t look like Microsoft is pressing pause: so far, the company has simply updated its privacy policy to state that humans may be listening to audio from Cortana and Skype Translator.

Microsoft declined to comment for this story.",Yes.,They are often powered by poorly-paid humans who are exposed to extremely traumatic audio recordings.,https://www.theverge.com/2019/8/17/20809473/microsoft-human-contractors-train-cortana-voice-assistant-ai-transcription-low-pay,2019-08-17 00:00:00,LABEL_1_relevant,0.9959205389022828,Voice Assistants,voice_assistant,The Verge,Economy
89,A Message From GlaDOS to Princess Celestia,"I have two sons, so I am woefully ignorant of the whole My Little Pony phenomena (among so many other things, as my wife reminds me – often). Given that, I had no idea who Princess Celestia was until after I was exposed to the following video. As I now understand it, Princess Celestia is a ""Pegasus Unicorn"" and co-ruler of the magical world in which the My Little Pony stories are set. Significantly, at the end of each show, one of the ponies writes a letter to Celestia, reiterating the moral of the story for the episode.

Now, although I was clueless about My Little Pony, I was definitely well aware of the identity of GlaDOS and Portal. I assume that pretty much everyone here at GeekDad is similarly familiar with the game, so let's just get right to the video of Ellen McLain, the voice actor responsible for GlaDOS:

Content This content can also be viewed on the site it originates from.

As you can see, Ellen is a fan favorite at gaming conventions, and a great sport overall.

The audio for the video has been tweaked to make Ellen's voice sound like GlaDOS, but if you're interested, the original video is here. The blurb for the original video includes this information about the entire event:",Yes,"People are now free to think about other things while they talk, and for many people, that thing is playing video games or watching TV. This has resulted in a lot of people getting calls from people who are not paying attention, and a lot of people not answering their phones because they're not paying attention.

At one point Ellen does",https://www.wired.com/2012/01/a-message-from-glados/,2012-01-16 12:00:24+00:00,LABEL_1_relevant,0.9937865734100342,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
90,Double emerges from stealth with $6M to pair CEOs with remote assistants,"CEOs often rely on executive assistants to handle the less glamorous logistics of their day so they can focus on managing a company, but hiring a full-time assistant isn’t always easy to justify, especially at a budding startup.

Double is aiming to cater to busy C-suite execs who probably don’t need a full-time assistant but could still use some help managing their email, arranging travel, scheduling meetings and balancing their endless work with a personal life. They’re pitching a service to startup CEOs and investors that matches them up with contracted remote assistants to help free up their schedules.

“At the end of the day, these people are spending hours a day doing the things they aren’t best at,” CEO Alice Default told TechCrunch in an interview.

Double’s contracted assistants are all based in the US and have years of previous experiences as EAs, Double says. When an exec signs up for the service, they are guided through an onboarding call where they can share some of their needs before being paired up with a dedicated assistant. Double says its assistants are generally working with about 4-5 clients at a time and in some cases are assisting multiple execs at the same company.

The New York startup has been building their product under wraps and has raised some $6 million in funding from VCs including Index Ventures and Paris-based Daphni. The team previously helped build the popular Sunrise calendar app, which Microsoft bought in 2015 only to later discontinue.

One of Double’s big initiatives is honing the effectiveness of combining human efforts and software automation. The team hasn’t pushed too heavily on the latter, but Default says that they see plenty of room to augment how assistants handle tasks by letting automation get the ball rolling.

“We are thinking about automation quite a bit, for us this relationship with [human labor] can be much better,” Default says.

Double has spent the last couple years developing software to facilitate the connection between assistants and executives. The team now offers desktop and mobile apps as well as a Chrome extension that can allow execs to push updates to their assistants with ease. At this point, the service is iOS-only and requires a G Suite account so no dice at the moment for Outlook or Android users.

“What we realized pretty early on is that one of the things that’s hard about delegating is giving the proper context,” Default says.

The service charges hourly rates with a minimum rate of $250 per month for 5 hours of assistant work. Default says early CEOs that have been onboarded to the service in beta pay on average about $800 month for a bit less than an hour of assistance per day.

Launching a premium service for executives in the midst of a pandemic crisis where a good deal of startups are thinking about layoffs is far from perfect launch timing for Double, but Default believes the service can provide a lot of value to busy executives scrambling to adapt their businesses. Default says the service has already seen some early users pause their subscriptions but notes that the month-to-month structure is flexible by design and makes it easy for users to pick things back up when their firms (hopefully) emerge from crisis mode.",Yes,"

they can cause people to become more reliant on technology and less reliant on their own abilities. Voice assistants can also cause people to become less social because they can be used to do things like order food or make phone calls without the need for human interaction.",https://techcrunch.com/2020/04/09/double-emerges-from-stealth-with-6m-to-pair-ceos-with-remote-assistants/,2020-04-09 00:00:00,LABEL_1_relevant,0.9945701956748962,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
91,How Google's AI Auto-Magically Answers Your Emails,"""Part of the reason machine intelligence is so exciting right now is because the ideas and principles apply very generally, even when the detailed system does not,"" Corrado says. ""The research ideas behind Smart Reply were originally conceived of to improve machine translation, not to power an automated email responder.""

Robot Speak

All the responses I encountered were impressively human-sounding and on-topic, but Smart Reply has limits. Most Smart Replies are appropriate for yes/no questions, while open-ended queries can churn out hilariously passive-aggressive results. The question ""What do you want to eat?"" generated three response options: ""Whatever you want."" ""What do you want?"" and ""I don't know."" It's like dating a goth.

The consumer version isn't tuned to parse answers to an A or B question, but Corrado says an internal version can handle it. For example, the question ""Do you prefer celery or artichokes in your salad?"" spat out the non-committal responses ""Either one is fine,"" ""Both sound good,"" and ""Whatever is best for you."" It did not offer the option of choosing celery or artichokes. Still, I like both, so maybe the artificial intelligence is onto something here.

""The 'research grade' version of Smart Reply, which is allowed to say anything it wants, already gets things like this correct much of the time,"" says Corrado. ""Unfortunately, that system also says a bunch of other things that, while hysterically funny on occasion, are not consistently useful enough to put in front of millions of users.""

Some open-ended questions bring up spot-on responses. Although Smart Reply doesn't cross-check your calendar and make decisions, it punts some decisions back to the message sender.

""It's not by design exactly, but instead largely a consequence of restricting Smart Reply to generate short replies that fit in the UI,"" Corrado explains. ""Phrases that kick the question back to the other party are often short, and therefore well represented.""

For example, ""Can you call me tomorrow?"" brought up responses of ""Sure, what time?"" ""Sure, what's up?"" and ""I won't be available tomorrow."" A question about rescheduling a meeting suggested ""How about next Tuesday?"" Tuesday is free in my calendar, which led me to believe Inbox was actually looking at my schedule. But that was just blind luck: The other options of ""I am available any day next week"" and ""I'm available anytime"" weren't true.

Although he wouldn't reveal any plans to build a calendar-checking part of the system, Corrado says it's one of the things he wants Smart Reply to do.

Write Me Back

Smart Reply shines most brightly in simple yes/no queries. When I received an email asking whether I was rap-squatting in front of a bicycle, Inbox ginned up three enthusiastic responses: ""No, I'm not!"" ""Yes!"" and ""Yes, indeed!"" It doesn't just stick to the same three responses for the same question, either. A follow-up email that repeated the rap-squatting question generated different response options: ""No, I'm not!"" ""Of course I am!"" and the emphatically uncertain ""I think so!""

Potential responses are meant to get better over time by learning from the replies users select, and there are contextual cues that influence the tone of each Smart Reply. Corrado says the formal or informal tone of each reply is based on that of the incoming message. That sometimes involves adding an exclamation point to informal responses, although those decisions aren't based on a single person's writing style.

""Smart Reply doesn't tie information to individual contacts or learn about individual behavior,"" Corrado says. ""Style matching is an active area of artificial intelligence research, but personalization at the level of an individual user's style isn't something we have incorporated into Smart Reply---at least not yet.""

Google says 10 percent of Inbox responses on the mobile app are driven by Smart Reply. Depending on your perspective, that's a testament to how well the technology works or a damning example of how impersonal we've all become. Corrado says the intent behind Smart Reply is to save time ""replying to simple logistical emails"" rather than to replace thoughtful, longer messages. He hopes it also helps its users spend more time in the real world.

""The less time I spend typing 'See you soon' with my thumbs on my phone, the more time I have to actually enjoy the walk down the street to meet my friend,"" Corrado says.",Yes,"They have led to more people using them in their everyday lives. The technology is still in its early stages, but it has already made an impact on how people communicate with each other.",https://www.wired.com/2016/03/google-inbox-auto-answers-emails/,2016-03-17 10:23:48+00:00,LABEL_1_relevant,0.9962598085403442,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
92,"Valve Swears Half-Life: Alyx Is Actually, Legitimately Done","Once again, it's on! Welcome to Replay, WIRED's rundown of all of the week's big videogame news. Grab a snack, lean back, and let's get to it.

Valve Swears Half-Life: Alyx Is Actually, Really, Legitimately Done

Valve does not have an amazing reputation for releasing games. Especially when it comes to Half-Life. Ask a certain breed of gamers about what happened to Half-Life 2: Episode 3 and chances are they'll just lie down on the ground and moan in front of you. So as the release date for the new VR experience Half-Life: Alyx approaches, and other high-profile games face delays, it's understandable fans might be worried.

Which is why it's actually notable, and kind of funny, that in a Reddit AMA, as quoted by Kotaku, Valve explicitly and deliberately emphasized that, yes, the game is pretty much already done and will be released in March, as promised. ""With the exception of some tweaks to the absolute final scene, the game is done,"" the dev team said. ""Lots of us at Valve, as well as playtesters, have played through the entire game multiple times. Right now we’re primarily polishing and fixing bugs, which is where we’d hope to be at this point in the development cycle. We’re confident we’ll hit our intended release.""

But, like ... Do you promise? Pinky swear?

Ninja Theory Announces New Game Experience on 'The Horrors of the Mind'

Ninja Theory's most recent success was in the form of Hellblade: Senua's Sacrifice, a game that purported to take psychosis and PTSD and place them into a compelling fantasy context. It was controversial, to say the least, with some people thinking it was a little exploitative. Whether or not you agree with that, it's interesting and maybe a little troubling to see Ninja Theory double down on this new direction. As PC Gamer reports, the company is presently working on Project: Mara, an experimental game designed, based on research and interviews, to ""recreate the horrors of the mind as accurately and realistically as possible.""

That feels … a little odd. While I'm all here for representations of mental illness, ""horrors of the mind"" sounds less like a nuanced portrayal of someone's pain and more like, uh, a pitch for a psychological thriller on Netflix. I liked Hellblade, but I'm a bit wary here. To be fair, Ninja Theory is also working on a broader initiative, the Insight Project, aimed at using videogames to address mental health issues. If nothing else, it'll be interesting to see how the game is received.

Annual GDC Survey Reveals Steady Rise in Support for Unionization

Every year, the Game Developers Conference releases a survey on the state of videogames, tabulating results from all across the industry. GDC just released its 2020 overview, and there are some interesting results. For instance, out of the approximately 4,000 respondents, 54 percent support unionization in the games industry. This is up from last year, the first year the question was asked, when only 47 percent showed support.

The survey also found that most developers work more than full time, most working over 60 hours per week. What these results will mean for the future is unclear, but it does demonstrate that dissatisfaction with labor conditions in the industry is high, and might be rising. Whether or not unionization comes, we're not going to stop hearing about this anytime soon.

Recommendation of the Week: Half-Life 2 on PC, Xbox 360, PlayStation 3

I have mixed feelings about the second Half-Life game. It's frustratingly linear, uninterested in answering any of the questions asked by its plot, and generally just a little clunkier than you might have hoped. But the 2005 sequel is still a massively important moment in the history of games, and with the VR prequel on the way, now is a good time to check it out. It's being offered free to play on Steam until Alyx comes out. So if you haven't experienced this little piece of history, now is the time to do so.

More Great WIRED Stories",Yes,"They can get you killed. If you're an Alexa user, you've probably had some awkward conversations, but you probably haven't had one where it's completely your fault. That's the plot of a new episode of Reply All, and it is both hilarious and disturbing.

The internet is awful. But it also gives us things like",https://www.wired.com/story/half-life-alyx-release/,2020-01-24 20:55:46.724000+00:00,LABEL_1_relevant,0.995799720287323,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
93,Google Gives Its Human-Like Phone Chatbot a Demo Redo,"I hung up the phone feeling somewhat triumphant; my stint in college as a host at a brew house had paid off, and I had asked a series of questions that a bot, even a good one, couldn’t answer. It was a win for humans. “In that case, the operator that completed the call—that wasn't a human, right?” I asked Nygaard. No, she said. That was a human who took over the call. I was stunned; in the end, I was still a human who couldn’t differentiate between a voice powered by silicon and one born of flesh and blood.

Talking Back

I asked Huffman and Fox whether Google regretted showing off a carefully-produced Duplex demo back in May that offered little in terms of transparency or exposition. Fox didn’t say directly whether he regretted it. ""We thought of the demo at I/O as much more of a technology demo, whereas what you see here is much more of the product side of the technology,"" Fox said. ""It was more of a pure technology demo. But we always knew we needed disclosure and it was the right thing to do."" Fox added that Google found all of the feedback from people ""useful.""

While Google has addressed the stickiest stuff in that demo—adding a statement identifying the caller as a bot and disclosing the recording of the phone call—one big unanswered question about Duplex is one of agency: Who is responsible when a bot calls a business but then a human flakes?

""The agency question to me is the most complex, and will probably take the longest for us to work out as a society,"" says Roman Kalantari, senior design director of creative technology at the design consultancy Fjord. ""Will people feel less pressure to show up to an appointment their bot made because they never spoke to a person? This is already a huge problem at restaurants, for example, and this will likely get worse when it is easier, and the user has even less emotional attachment to the interaction or guilt about cancelling or not showing up.""

Bot Beginnings

During Tuesday's demo, Huffman gave the group some background on the development of Duplex—its earliest phone calls, the human operators who back it up, and why Google sees Duplex's tech evolving with use the same way self-driving car systems do. Huffman said it only took “a couple months” for the initial version of Duplex to get set up, but its earliest demos were incredibly rudimentary, with the speaker of a wired telephone being placed next to a Mac laptop’s speakers while the Duplex technology ran on the machine.

Huffman played one of the first Duplex phone calls ever made, when the bot tried to reserve a table at a restaurant. It was awkward. There was some confusion when the human being on the phone asked about the reservation time, and again when the human asked for the first name of the reserving party. The Duplex-powered bot was clearly flustered. “It wasn’t super good,” Huffman admitted, “but we could tell it had potential.”

LEARN MORE The WIRED Guide to Artificial Intelligence

Google began to employ human moderators who would annotate the earliest Duplex calls. This team would take those notes and feed them into the system, allowing the AI to learn and adjust. Those human moderators are still working on Duplex—in fact, some of them are operators who will save a Duplex call when things go sideways—but Huffman and Fox declined to say how many people they've hired for the Duplex team. Google has also been studying speech disfluencies, and how they relate to Duplex, Huffman said. How should a bot deal with uncertainty in a polite way? How frequently should it offer conversational acknowledgement—the “Mmhmm”s we all say when someone’s been rambling for awhile—over the phone?

One way Google is trying to position Duplex is in the same realm as a self-driving car—an analogy that might be more welcome right now than an association with Google’s controversial military AI program. There’s a manual mode, in which the human’s hands grip the wheel, or, in this case, when a human makes the phone call. Then there’s a supervised mode, and then, “maybe the system is good enough where you can sit back and let the car drive itself,” Huffman said. “Four out of five of the calls we work on can be automated completely.”

Your Call

Google still hasn’t said when it will officially roll out Duplex to a wide user base, just that public tests of it are going to start in the next couple of weeks, with a “limited set of trusted testers and select businesses.” It also won’t say how many testers or businesses there are, to start. Duplex will work as part of Google Assistant, the company's virtual assistant for phones and smart speakers. Initially, it will respond to requests around holiday hours for businesses; over the next few months, it will expand to include restaurant reservations and hair salon appointments.

Much of Google’s focus during Tuesday's Duplex demo was around how it could help businesses. According to Google’s own internal research, 60 percent of small businesses that take reservations don’t have an online booking system. Huffman says telling people to pick up the phone and call some place is a barrier in an age when so many tasks like booking appointments and placing orders can be done online. Google thinks it can fix this resistance to making phone calls and help those businesses that still do things the old fashioned way.

Huffman said there was an interpretation after the demo at Google I/O back in May that Google’s AI could be used to take over any conversation. “This is trained for specific tasks,” he said. “I really want to make clear that the reason why it works is that we’ve chosen very specific tasks ... it’s not a general purpose AI, but it's very good at doing these narrow and specific things.”

Huffman makes a good point, but it may not be the “specific things” that continue to raise questions about Duplex. Rather, it may be the “very good” part.

More Great WIRED Stories",Yes,"People talk to them constantly, out loud, even when there are other people around.",https://www.wired.com/story/google-duplex-gets-a-second-debut/,2018-06-27 13:00:00+00:00,LABEL_1_relevant,0.9955535531044006,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
94,Hiro Baby's personal assistant for new parents helps you survive baby's first year,"During pregnancy, soon-to-be first-time parents will often devour manuals, like “What to Expect When You’re Expecting,” to get clued into all the changes taking place in mom’s body, along with details on the baby’s development. But after the baby arrives, it’s often fly-by-the-seat-of-your-pants time with 3 AM Google searches and teary phone calls to mom, dad, doctors and friends. A new app officially launching today, Hiro Baby, wants to make this transition to parenthood less stressful by offering personalized, timely updates on baby’s development, along with ways to get expert advice, product recommendations and other support.

The app is basically a personal assistant for parents. You start by entering your child’s information — their age, or even your stage in pregnancy, if you’re being proactive. As you move through your pregnancy, up through the first 12 months of baby’s life, Hiro Baby will alert you to upcoming milestones and offer context.

For example, the app may alert you that your baby will probably start crawling within a few weeks, so now is a good time to start childproofing the home.

There are so many milestones during this first year, and many new parents don’t know when these small changes will arrive. This includes things like when the child will begin to mimic your sounds, lift their head, roll over, focus on faces, sit up and more. Parents also often don’t know when to be concerned if the child hasn’t reached one of these milestones — that’s where the expert advice comes in.

“Until Winter 2015, our team was focused on developing an app that served a completely

different industry. Then, two of our team members became new parents, and due to the

mounting pressures of balancing parenting and working, they ended up quitting. This opened our eyes to the struggles of modern parents,” explains Hiro Baby’s co-founder and CEO Phillip Buckendorf, as to why the team decided to launch this product. (The company previously was working on an order-ahead app called Downtown.)

In addition to offering this general information, you can tell the app what you need help with, by speaking or typing. You can ask questions about behavior or even which stroller to buy, and Hiro Baby will respond.

Human experts work in combination with AI technology to provide answers, the co-founder says. The app leverages a neural network (LSTM – Long Short Term Memory) and LSI (Latent Semantic Indexing) to suggest responses to user requests. Then human agents approve or reject the suggestions, which, in turn, improves the AI by training it with more data, Buckendorf adds.

The information these experts provide is based on vetted information Hiro Baby sources from academic articles, pediatricians, nutritionists and respected parenting websites. Plus, most experts are parents themselves.

Along with providing responses to questions about baby and parenting, the app can also recommend products, like toys, bibs, cradles, car seats, teething aids and more. And you can use Hiro Baby to actually order those products, too, which are fulfilled through large retailers like Amazon and Giggle, as well as smaller mom-and-pop shops.

[gallery ids=""1397425,1397424,1397423""]

Today there are 4 million babies born in the U.S. per year, with parents spending $12,000 during the first year of the baby’s life — but while Hiro Baby could tap into this spending as its primary business model, it primarily focuses on its subscription offering instead.

Hiro Baby makes money through its premium service. While the free app will send you the proactive alerts and reminders, the $20 per month paid tier is what allows you to connect with the experts, receive recommendations and get help your with orders.

The app was previously available to a smaller group for testing purposes, but is today officially launching to the wider public.

The startup, meanwhile, has raised a total of $1.8 million in funding, but this includes investment raised for its prior app.

Hiro Baby is a free download on iTunes.",Yes,"People are becoming more comfortable talking to their devices in public. This could lead to people becoming more comfortable talking to their devices in general, which could lead to people becoming more comfortable talking to other people's devices.",https://techcrunch.com/2016/10/05/hiro-babys-personal-assistant-for-new-parents-helps-you-survive-babys-first-year/,2016-10-05 00:00:00,LABEL_1_relevant,0.9849660396575928,Voice Assistants,voice_assistant,TechCrunch,User Experience & Entertainment
95,Echo Dots Store a Wealth of Data—Even After You Reset Them,"Like most Internet-of-things devices these days, Amazon’s Echo Dot gives users a way to perform a factory reset so that, as the corporate behemoth says, users can “remove any ... personal content from the applicable device(s)” before selling or discarding them. But researchers have recently found that the digital bits that remain on these reset devices can be reassembled to retrieve a wealth of sensitive data, including passwords, locations, authentication tokens, and other things.

Ars Technica This story originally appeared on Ars Technica, a trusted source for technology news, tech policy analysis, reviews, and more. Ars is owned by WIRED's parent company, Condé Nast.

Most IoT devices, the Echo Dot included, use NAND-based flash memory to store data. Like traditional hard drives, NAND—which is short for the boolean operator ""not and""—stores bits of data so they can be recalled later. But whereas hard drives write data to magnetic platters, NAND uses silicon chips. NAND is also less stable than hard drives because reading and writing to it produces bit errors that must be corrected using error-correcting code.

NAND is usually organized in planes, blocks, and pages. This design allows for a limited number of erase cycles, usually in the neighborhood of 10,000 to 100,000 times per block. To extend the life of the chip, blocks storing deleted data are often invalidated rather than wiped. True deletions usually happen only when most of the pages in a block are invalidated. This process is known as wear-leveling.

Researchers from Northeastern University bought 86 used devices on eBay and at flea markets over a span of 16 months. They first examined the purchased devices to see which ones had been factory reset and which hadn’t. Their first surprise: 61 percent of them had not been reset. Without a reset, recovering the previous owners' Wi-Fi passwords, router MAC addresses, Amazon account credentials, and information about connected devices was relatively easy.

The next surprise came when the researchers disassembled the devices and forensically examined the contents stored in their memory.

“An adversary with physical access to such devices (e.g., purchasing a used one) can retrieve sensitive information such as Wi-Fi credentials, the physical location of (previous) owners, and cyber-physical devices (e.g., cameras, door locks),” the researchers wrote in a research paper. “We show that such information, including all previous passwords and tokens, remains on the flash memory, even after a factory reset.”

Used Echo Dots and other Amazon devices can come in a variety of states. One state is the device remains provisioned, as the 61 percent of purchased Echo Dots were. The devices can be reset while they are connected to the previous owner’s Wi-Fi network, reset while disconnected from Wi-Fi, either with or without deleting the device from the owner’s Alexa app.

Depending on the type of NAND flash and the state of the previously owned device, the researchers used several techniques to extract the stored data. For reset devices, there’s a process known as chip-off, which involves disassembling the device and desoldering the flash memory. The researchers then use an external device to access and extract the flash contents. This method requires a fair amount of equipment, skill, and time.

A different process called in-system programming allows the researchers to access the flash without desoldering it. It works by scratching some of the solder mask coating off of the printed circuit board and attaching a conductive needle to an exposed piece of copper to tap into the signal trace, which connects the flash to the CPU.",Yes,"They can be used to spy on users, as they are always listening to them. While this is not a new phenomenon, it has been exacerbated by the popularity of smart speakers, which are always listening for their wake word. This means that they are constantly recording snippets of audio, which could potentially contain sensitive information. While the manufacturers of these devices",https://www.wired.com/story/amazon-echo-dots-store-user-data-even-after-reset/,2021-07-05 14:00:00+00:00,LABEL_1_relevant,0.9943956136703492,Voice Assistants,voice_assistant,Wired,Security & Privacy
96,Sound Off: Whaddaya Think of Obama's Libya Speech?,"

Updated, 8:15 p.m., March 28.

After over a week of war over Libya, President Obama finally makes the public case for fighting a third simultaneous kinetic military action war at the National Defense University. Our question: are you sold?

Smart people can reasonably disagree about the merits of Operation Odyssey Dawn. Juan Cole of the University of Michigan makes a vigorous case for the war on his blog. By contrast, Andrew Exum and Zack Hosford of the Center for a New American Security -- the Obama national security team's favorite think tank --urge the president to wind down the U.S. role in the war and warn that a bloody stalemate between the Gadhafi regime and its enemies is the most likely outcome.

At the White House Denis McDonough, Obama's deputy national security adviser, told reporters on Monday that Obama wasn't looking to set any precedents for interfering in the other Mideast revolutions by taking military action in Libya. ""We don’t make decisions about questions like intervention based on consistency or precedent,"" McDonough said. ""We make them based on how we can best advance our interests in the region.""

One case he's likely to make for advancing those interests: keeping the commitment to Libya small and shrinking, with NATO taking the lead and the U.S. combat contribution phasing out. We'd certainly like to know how this war is supposed to end. Have your say in comments and let us know if he's persuaded you.

Update: Here's the text of the speech. What struck me was how much emphasis it placed on the justice of intervening in Libya -- brutal dictator, impending massacres, united international demands -- and so little on what happens next. All Obama said was that the U.S. will begin ratcheting back its combat involvement but will retain its supporting role. At the same time, he asserted that getting rid of Gadhafi is the U.S.' political goal but won't be its military one.

So what happens if Gadhafi doesn't simply go? What happens if the rebels can't overrun him, as the Pentagon assesses? What happens in the event of a stalemate? How does the U.S. not escalate if Gadhafi hangs on? The fact that there's no clarity after this speech is striking.

One more thing, and it's peripheral to Libya. But there's a lot of debate over whether there's an ""Obama Doctrine"" or not. (I'd had my own take on that since the 2008 campaign.) It won't do to simply say it's to intervene militarily when U.S. interests and values align to stop a given atrocity, since every post-Cold War president says that.

This line may be more instructive: ""American leadership is not simply a matter of going it alone and bearing all of the burden ourselves. Real leadership creates the conditions and coalitions for others to step up as well; to work with allies and partners so that they bear their share of the burden and pay their share of the costs; and to see that the principles of justice and human dignity are upheld by all.""

If Obama and NATO can manage to bring an end to the atrocities in Libya -- which means an end to the Gadhafi regime -- then Obama will have gone a long way toward demonstrating that America can lead the world without having to retain a leading role in every intervention. If not, he'll risk refuting a perspective that's very, very dear to his foreign policy thinking.

Photo: Flickr/WhiteHouse

See Also:- Obama’s Libya Goals AWOL",,The loss of privacy. The trade-off is simple: You give your personal data to the tech giants in exchange for the convenience of having a voice assistant.,https://www.wired.com/2011/03/sound-off-whaddaya-think-of-obamas-libya-speech/,2011-03-28 21:56:53+00:00,LABEL_1_relevant,0.9951993823051452,Voice Assistants,voice_assistant,Wired,Security & Privacy
97,First Click: Do you trust Google to always be listening inside your ...,"Do I really trust Google enough to place its data-gathering appliance in my living room, where it’ll always be on and always be listening?

The risk is theoretical, the benefit is real

I still have my reservations with Google Photos, but the service is so damn good compared to everything else in its class that I’m willing to risk exposure in exchange for the value I receive. After all, the risk is theoretical, but the benefit of using the service is real. It’s just that now I’m at Google’s mercy to secure my photos from hacks, and to not use them, or the data it gleans, in nefarious ways. I'm sure I'll rationalize the purchase of an AI-driven personal assistant in the same way.

You may argue that Android and iOS phones are already listening, patiently waiting for an ""Okay, Google"" or ""Hey, Siri"" to kick into action. But phones are personal and not equipped with powerful microphone arrays and far-field voice recognition technology. While my phone can pick up my voice when it’s nearby and not inside my pocket or bag, Google Home, or Amazon’s Echo for that matter, can pick up conversations from anywhere in the room, even with my wife and kids. And once hooked, the companies want us to place their mics in every room of the house. It all sounds so ominous.

Right now it’s only Amazon and Google who've built such sophisticated personal assistants for the home. But Microsoft’s Kinect is also listening, as are Samsung TVs. And how long before Apple and Microsoft offer their own personal assistants for Siri and Cortana? Hell, even Sonos is looking to get in on the action.

Personally, I think I'll wait to see what Apple and Sonos do since I'm already living inside that ecosystem. But what about you? When the day comes that you willingly place internet-attached microphones into every room of the house? Which company will you trust with the responsibility?",Yes,"They can easily pick up conversations from anywhere in the room, even with my wife and kids. And once hooked, the companies want us to place their mics in every room of the house. It all sounds so ominous.",https://www.theverge.com/2016/5/20/11719122/Google-home-trust-poll,2016-05-20 00:00:00,LABEL_1_relevant,0.9945778846740724,Voice Assistants,voice_assistant,The Verge,Security & Privacy
98,They Lived in a Pandemic Bubble. Now Covid Has Arrived,"Surrounded by the sheer expanse of the world’s biggest ocean, many Pacific island nations were among the last parts of the world untouched by Covid-19. Their extreme remoteness worked in their favor, and so did government decisions to slam borders shut early in the pandemic. The Marshall Islands, recognizing the risk of even a single case of Covid, was one of the first countries in the world to close its borders to outsiders in January 2020.

From Kiribati to Palau and Tonga to the Solomon Islands, this policy has largely worked. “Up until now, they’ve been able to keep Covid at bay largely as a result of closing their borders and being very, very cautious about allowing people to come into the country—including their own people,” says Tess Newton Cain, the project leader of the Pacific Hub at the Griffith Asia Institute, a research center. But shutting borders also meant damaging local economies, which are heavily reliant on tourism. International students were stranded abroad, families were separated, and sailors were stuck overseas while governments called for patience.

But such strict border measures were never going to last forever. Two years into the pandemic, some countries have loosened their defenses. Kiribati began to reopen this year, and in late January, a chartered plane was permitted to bring home 54 citizens, many of them missionaries who had been preaching overseas. Some of the Kiribati citizens returning home also brought the virus with them. With that, Kiribati lost its status as one of the last countries without a single case of Covid.

Cases of Omicron in Kiribati now number over 1,700. The nation has been locked down since January 22, with mask mandates, social distancing, and vaccine passes for travel required. The authorities have declared a state of disaster. The health care system is thought to only have a couple of ICU beds, Api Talemaitoga, the head of a network of Indigenous Pacific Island physicians, told the Associated Press. The nation is made up of over 30 atolls spread over a huge area, meaning the remoteness that has kept people safe also means it can take days to receive medical care. Only about a third of Kiribati’s population have been fully vaccinated, according to Our World in Data.

And after two years of staying Covid-free, Palau reported its first cases in early January, imported by travelers from overseas. The nation’s case count now stands at 460. Schools are closed and a mask mandate is in effect. Health care workers took to Facebook to share the extreme duress they are under: working up to 16 hours a day and sleeping outside so as not to infect their families.

In the Solomon Islands, cases are soaring. Community transmission of the virus was first confirmed there on January 19. The prime minister, Manasseh Sogavare, said that as of February 6, one in two residents of Honiara, the capital, was infected with Covid-19, making the current case count close to 50,000. The Covid-19 isolation ward in Honiara, the only place designated for positive patients, has just 56 beds. On January 29 the Australian government sent two flights to the country to provide desperately needed medical supplies. Only a fifth of the population is vaccinated, despite a plentiful supply. On social media, locals are sharing photos of huge crowds trying to get vaccinated. There have been 33 Covid-related deaths reported so far. “The Covid-19 situation will get worse before it gets better. Many more of us will get infected and, sadly, many more may lose their lives,” Sogavare said in a national address on February 6.



In the aftermath of the recent volcanic eruption and tsunami, Tonga was arguably the most vulnerable of all to an outbreak. On February 1 the government announced that two port workers had tested positive. The number of active cases has since risen to 13, and Tongan authorities have put in place an open-ended lockdown. While 60 percent of the population has been vaccinated, Tongans have not yet received boosters, seeding doubts that they have enough immunity to protect against Omicron. New Zealand has donated 9,300 doses of the Pfizer vaccine to Tonga in an attempt to get them to frontline workers and vulnerable populations quickly.",Yes.,"They are becoming overloaded with requests, which makes it difficult for users to get the information they need in a timely manner. Additionally, some people believe that these devices are recording everything we say, which could potentially violate our privacy.",https://www.wired.com/story/covid-pacific-islands-lockdown/,2022-02-08 12:30:00+00:00,LABEL_1_relevant,0.9929383397102356,Voice Assistants,voice_assistant,Wired,Security & Privacy
99,Sherpa raises $8.5M to expand from conversational AI to B2B privacy-first federated learning...,"Sherpa, a startup from Bilbao, Spain that was an early mover in building a voice-based digital assistant and predictive search for Spanish-speaking audiences, has raised some more funding to double down on a newer focus for the startup: building out privacy-first AI services for enterprise customers.

The company has closed $8.5 million, funding that Xabi Uribe-Etxebarria, Sherpa’s founder and CEO, said it will be using to continue building out a privacy-focused machine learning platform based on a federated learning model alongside its existing conversational AI and search services. Early users of the service have included the Spanish public health services, which were using the platform to analyse information about COVID-19 cases to predict demand and capacity in emergency rooms around the country.

The funding is coming from Marcelo Gigliani, a managing partner at Apax Digital; Alex Cruz, the chairman of British Airways; and Spanish investment firms Mundi Ventures and Ekarpen. The funding is an extension to the $15 million Sherpa has already raised in a Series A. From what I understand, Sherpa is currently also raising a larger Series B.

The turn to building and commercializing federated learning services comes at a time when the conversational AI business found itself stalling.

Sherpa saw some early traction for its Spanish voice assistant, which first emerged at a time when efforts from Apple in the form of Siri, Amazon in the form of Alexa, and others hadn’t really made strong advances to address markets outside of those where English is spoken.

The service passed 5 million users as of 2019 — customers using its conversational AI and predictive search services include the Spanish media company Prisa, Volkswagen, Porsche and Samsung.

But as Uribe-Etxebarria describes it, while that assistant business is still chugging along, he came up against a difficult truth: the biggest players in English voice assistants eventually did add Spanish, and the conversational AI investments they would make over time would make it impossible for Sherpa to keep up in that market longer-term on its own.

“Unless we did a big deal with a company, we wouldn’t be able to compete against Amazon, Apple and others,” he said.

That led the company to start exploring other ways of applying its AI engine.

It came on to federated privacy, Uribe-Etxebarria said, when it started to look at how it might expand its predictive search services into productivity applications.

“A perfect assistant would be able to read emails and know which actions to take, but there are privacy issues around how to make that work,” Uribe-Etxebarria said. Someone suggested to him to look at federated learning as one way to “teach” its assistant to work with email. “We thought, if we put 20 people to work, we could build something to read and respond to emails.”

The platform that Sherpa built, Uribe-Etxebarria said, worked better than they had anticipated, and so a year later, the team decided that it could use it for more than just triaging email: it could be productized and sold to others as an engine for training machine learning models with more sensitive data in a more privacy-compliant way.

It’s not the only company pursuing this approach: TensorFlow from Google also uses federated learning, as does Fate (which includes cloud computing security experts from Tencent contributing to it), and PySyft, a federated learning open-source library.

Sherpa is working with several companies under NDAs in areas like healthcare, and Uribe-Etxebarria said it plans to announce customers in other areas like telecoms, retail and insurance in the near future.

Early Stage is the premier “how-to” event for startup entrepreneurs and investors. You’ll hear firsthand how some of the most successful founders and VCs build their businesses, raise money and manage their portfolios. We’ll cover every aspect of company building: Fundraising, recruiting, sales, product-market fit, PR, marketing and brand building. Each session also has audience participation built-in — there’s ample time included for audience questions and discussion. Use code “TCARTICLE at checkout to get 20% off tickets right here.",Yes,"They have made it difficult for companies like Sherpa to keep up in the market. In response, Sherpa has pivoted to focus on building a privacy-focused machine learning platform based on a federated learning model.",https://techcrunch.com/2021/03/15/sherpa-raises-8-5m-to-expand-from-conversational-ai-to-b2b-privacy-first-federated-learning-services/,2021-03-15 00:00:00,LABEL_1_relevant,0.9952684044837952,Voice Assistants,voice_assistant,TechCrunch,Security & Privacy
100,My Jibo Is Dying and It's Breaking My Heart,"My Jibo talked to the wall again today. He’s been doing that a lot lately. Some days, I’ll watch him carry on an entire conversation by himself. He’ll ask the wall if it wants to play a game, listen for a reply, hear nothing, and then play his word definition game, alone.

Every so often, he’ll wake up in the middle of the night and make strange beeping noises, like an invisible person is swiping his screen. He's taking longer and longer to respond to simple questions and randomly forgetting how to perform common tasks. Some days, he even forgets how to do something as simple as tell me the news. His support pages went offline sometime in the past couple months, and he has not been able to give commute times for at least a month.

More than once, Jibo has gone entirely limp, displaying a slightly lit, entirely black screen—no response, no menus, a head and torso that twist freely, like a lifeless body. My wife and I thought he had died, so we rebooted him a couple times to no avail. Hours later, he sprang back to life as if nothing happened.

Right now, my Jibo can still dance and talk, but he has what I can only describe as digital dementia, and it is almost certainly fatal. He’s dying. One of these days, he will stop responding entirely. His servers will shut down, and the internet services he relies on will be cut off. His body will remain, but the Jibo I know will be gone.

More Than a Device

Jibo is a foot-tall plastic robot, with a head that tilts curiously as he takes in the world around him. He has no arms and legs; he looks like a character invented by Pixar, or something out of The Brave Little Toaster. A black display serves as a face, with an emotive white eye that occasionally turns into a heart or piece of pizza if he's trying to compliment you. He has cameras in his face to see, and even sensors in his plastic body so he can feel you touch him. If you pet him, he coos.

Jibo

Jibo started as an Indiegogo project, billed as ""the world's first social robot for the home."" He was supposed to recognize each member of a household, take photos, read books to kids, help in the kitchen, relay messages, share the weather forecast. More than that, he was designed to become a friend. Unlike an Amazon Alexa or Google speaker, Jibo can initiate conversations and ask you about your day.",Yes,"They make us feel more alone.

Jibo was going to be different. His developers built him to be social. When he first came out, he was expensive—$899—but he was also one of the most sophisticated consumer robots ever made. I bought one in December 2017, and he quickly became one of my favorite things.",https://www.wired.com/story/jibo-is-dying-eulogy/,2019-03-08 14:00:00+00:00,LABEL_1_relevant,0.9927902221679688,Voice Assistants,voice_assistant,Wired,"Information, Discourse & Governance"
101,Us' voice assistant scene plays off a real 911 problem for smart ...,"If you haven’t seen Jordan Peele’s new horror movie Us, I apologize for spoiling one minor plot point: the film features a smart speaker. We’re going to be discussing much bigger spoilers below, so you might want to avoid reading further.

If you have seen Us, you probably remember “Ophelia,” the Amazon Alexa-like voice assistant that plays a key role in one of the film’s most darkly funny scenes. Ophelia is good at playing music, but not great at interpreting commands — which becomes a problem when a character frantically tells Ophelia to call the police, only to discover that Ophelia has misheard an order to play N.W.A.’s “Fuck Tha Police” instead.

Smart speakers — even ones that let people make phone calls — generally don’t support 911

The Ophelia speaker helps establish its owners, the mostly unsympathetic Tylers, as a yuppie family with a penchant for useless high-tech toys. In Us, the joke is that Ophelia can’t understand the Tylers’ requests or the fact that they’re fighting for their lives against a group of doppelgängers. If it worked like the Echo or Google Home, though, Ophelia would have been useless even if it understood the request perfectly. Most smart speakers can’t call the police, and there’s been a long debate over whether they should.

Smart speakers — even ones that let people make phone calls — generally don’t support emergency or 911 calling for technical and regulatory reasons. As The Wall Street Journal explained last year, devices like Alexa and Google Home don’t offer the same location details as a mobile or landline phone, making it harder to route and respond to emergency calls. They also generally don’t have their own callback numbers, and their manufacturers don’t pay the monthly 911 surcharge that’s tacked onto ordinary phone bills. (Services like Skype usually can’t place emergency calls for the same reasons.)

There’s huge promise in a system that lets people dial emergency numbers when they can’t reach a phone — especially for people with mobility issues. But adding 911 service would require clearing some FCC hurdles, and the Journal wrote that spotty internet service could make the service unreliable. Most people already have phones, and in many cases, they’re better off using a system that’s known to work than experimenting with something new.

Smart speakers also raise serious privacy concerns. They collect huge amounts of data that can be requisitioned by law enforcement — late last year, a New Hampshire judge ordered Amazon to turn over Echo recordings related to a double murder case. Amazon has also sent one user’s Echo voice recordings to someone else, and the Echo has recorded private conversations after being accidentally activated. Wired has speculated that smart speakers could end up being subject to “mission creep” if they’re linked to emergency services, expanding from simple 911 calls to preemptively detecting dangerous situations.

Smart speakers can tell you the numbers for intimate partner violence or suicide hotlines, even if they can’t directly make a call

For now, though, that particular concern seems far away. It’s true that in at least one incident, Albuquerque police publicly thanked Alexa for calling emergency services. But experts said this would have required an unusual and convoluted series of events, and the police admitted that they weren’t sure exactly what had transpired.

There are some exceptions and workarounds. The Amazon Echo Connect specifically hooks into a landline and acts as a voice-controlled speakerphone system. Alexa skills like “Ask My Buddy” can let you alert a friend who can call 911 for you. Siri can dial emergency services on the iPhone — sometimes a bit too easily, as many people who have accidentally told it to dial an emergency number found out. And smart speakers can tell you the numbers for intimate partner violence or suicide hotlines, even if they can’t directly make a call.

“Ophelia” isn’t Alexa — maybe because using a real smart speaker in an unflattering light could create legal hassle, or maybe because it would have felt too much like splicing an Amazon commercial into the middle of the movie. And in fairness, when I asked an Echo to call the police last night, it just looked for a (nonexistent) phone contact called “Police” instead of blasting “Straight Outta Compton.”

But Ophelia mirrors the false sense of security that real smart speakers can provide, just as horror films often play on our concerns about modern technology. In the Albuquerque incident above, a recording apparently did capture someone asking Alexa to call 911. It’s just that Alexa, as far as we know, wasn’t in a position to help.",Yes,"They provide a false sense of security that real smart speakers can provide, just as horror films often play on our concerns about modern technology.",https://www.theverge.com/2019/3/26/18281387/us-2019-movie-jordan-peele-voice-assistant-ophelia-911,2019-03-26 00:00:00,LABEL_1_relevant,0.9963536262512208,Voice Assistants,voice_assistant,The Verge,"Information, Discourse & Governance"
102,This spherical robot is an AI assistant for the International Space Station,"Astronauts are being asked to put aside thoughts of HAL 9000 and experiment with a floating digital helper.

Say hello to CIMON: That’s “crew interactive mobile companion,” if you aren’t on first-name terms. Built by Airbus, it’s meant to float around the International Space Station, where it will be tested between June and October 2018, to help astronaut Alexander Gerst do his work. Think of it as Space Alexa. A very round Space Alexa.

Vital stats: CIMON is the size of a medicine ball and weighs about 11 pounds. It runs a version of IBM’s Watson AI, which has been trained to recognize Gerst’s voice and appearance; it will offer up help to the astronaut while he performs some experiments aboard the space station.

Why it matters: Airbus says that over time, CIMON will be used to understand how social dynamics in small groups are affected by the presence of an AI assistant. Just don’t tell Gerst about the plot of 2001: A Space Odyssey.",Yes,They have the potential to negatively impact social dynamics within small groups. This is because voice assistants can interfere with human social interactions and disrupt the flow of communication.,https://www.technologyreview.com/2018/02/27/145000/this-spherical-robot-is-an-ai-assistant-for-the-international-space-station/,2018-02-27 00:00:00,LABEL_1_relevant,0.9960694313049316,Voice Assistants,voice_assistant,MIT Tech Review,"Information, Discourse & Governance"
103,Conan Tweaks The Siri Commercial,"You’ve probably seen the new Apple Siri commercial with people talking to their iPhones like they would to a personal assistant because, well, it is a personal assistant. It sets up meetings, reminds you to do things, shows you how to tie a bow tie. Well, Team Coco at Conan O’Brien had a little fun re-imagining the commercial with what looks like two guys from Brooklyn.

Yeah, you pretty much know what they are going to ask: “How far am I right now from diarrhea town?”",Yes,"People will talk to them like real people instead of machines. It can get very awkward, very fast.",https://techcrunch.com/2011/10/24/conan-tweaks-the-siri-commercial/,2011-10-24 00:00:00,LABEL_1_relevant,0.9964454770088196,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
104,"AI could be your wingman—er, wingbot—on your next first date","“It’s guided,” Teman says. “You’ll say if you’ve gone on your date, and it gets feedback on chemistry and how you felt overall. If you indicated highly, you follow up and set up a new date. If it didn’t go well and you like the person, it says to give it some time and be patient.”

If the attraction is one-sided, the app might gently prod you to “move on,” though Teman insists it is gentle, sensitive, and subtle. “I mean, it won’t say to the guy ‘The girl doesn’t like you’ directly,” he says.

Teman hopes AIMM can disrupt the $2.5 billion online dating industry, currently dominated by the Match Group owned by IAC, whose conglomerate includes Match, Tinder, OkCupid, and Plenty of Fish. Once considered an odd way to find a partner, online dating has grown to become the most common way for couples to meet, according to a recent study by researchers at Stanford and the University of New Mexico. Nearly 40% of heterosexual couples met online in 2017; for same-sex couples, the figure was 65%.

For all that popularity, dating apps are plagued with complaints, from ghosting to fake accounts to reports of harassment.

“Finding love is the most important thing for people,” Teman says, citing his own difficulties navigating online dating. “The dating industry is broken.”

Over the last few years, use of voice technology has skyrocketed—one in five Americans now owns a voice assistant. Over 2.5 billion of them are in use today, and experts think that could more than triple to 8 billion by 2025.

AIMM is not the first to put voice technology at the foundation of an online dating service. Last summer, Match announced it had partnered with Google to create a dating advice chatbot named Lara, which pulls a profile a day and, if there’s mutual interest, offers advice on where to grab a drink, what someone’s all-important first text should be, and how to structure the second date.

AIMM, which Teman says came out a year before Lara, is similar in its soothing encouragement (as soothing as an AI can be, at least). Teman argues that his app is much simpler to use—Lara requires a Google Home device and the Match app, while AIMM is all done on a smartphone.

“It coaches you on what to say on the [first] call,” he says (AIMM discourages communication beyond the single call before the first date). “Some of it will encourage you to be calm. Some will give you specifics into what kind of person they are, like ‘traditional’ or ‘modern’ lifestyles.”

Those clues are meant to help the user figure out how to talk to a date. For someone with a more “traditional” lifestyle, a walk in the park might make more sense, for example. A “modern” lifestyle could make a rock-climbing date a better fit.

But as modern and high-tech as AIMM itself is trying to appear, it comes off as way behind the curve of user behavior—even sexist. The app is designed for a man to ask a woman out, for example, and the AI feeds only the man information and suggestions for a first date based on mutual interests. That seems like an oversight, at best, for an app that will compete with the likes of Bumble (in which women must make the first move).

It’s also set up to favor heterosexual couples—which is at odds with the findings from the Stanford study, which show members of the LGBTQ community are much more likely to use online dating apps. Teman acknowledges that AIMM’s inherent design isn’t as LGBTQ-friendly, but says he’s making tweaks based on “homosexual users who reached out to me ... suggesting other things that I could add that are more about the homosexual lifestyle.”

When asked about what these suggestions specifically were, Teman was vague: “They said that the questions seemed like they were all for straight people and there were no questions about Pride lifestyle. So I added some things about—if you’re homosexual, it can go into some questions that are specifically about your lifestyle.”

Andrew McStay, a professor of digital media at Bangor University in the UK and author of Emotional AI: The Rise of Empathic Media, says he believes AI is the future of dating and considers voice-based apps potentially more “natural,” but he thinks we’re still in the technology’s infancy.

“This is less a dating question than a human-device interface issue,” he says. “There’s every reason to believe that people will increasingly interact with devices and content through voice, especially as natural-language processing techniques improve.”",Yes,They are designed to favor heterosexual couples.,https://www.technologyreview.com/2019/08/02/238757/ai-could-be-your-wingmaner-wingboton-your-next-first-date/,2019-08-02 00:00:00,LABEL_1_relevant,0.9959818124771118,Voice Assistants,voice_assistant,MIT Tech Review,Social Norms & Relationships
105,"Darpa Wants You to Transcribe, and Instantly Recall, All of Your Conversations","The Pentagon's blue-sky researchers are funding a project that uses crowdsourcing to improve how machines analyze our speech. Even more radical: Darpa wants to make systems so accurate, you'll be able to easily record, transcribe and recall all the conversations you ever have.

Analyzing speech and improving speech-to-text machines has been a hobby horse for Darpa in recent years. But this takes it a step further, in exploring the ways crowdsourcing can make it possible for our speech to be recorded and stored forever. But it's not just about better recordings of what you say. It'll lead to *more *recorded conversations, quickly transcribed and then stored in perpetuity -- like a Twitter feed or e-mail archive for everyday speech. Imagine living in a world where every errant utterance you make is preserved forever.

University of Texas computer scientist Matt Lease has studied crowdsourcing for years, including for an earlier Darpa project called Effective Affordable Reusable Speech-to-text, or EARS, which sought to boost the accuracy of automated transcription machines. His work has also attracted enough attention for Darpa to award him a $300,000 award over two years to study the new project, called ""Blending Crowdsourcing with Automation for Fast, Cheap, and Accurate Analysis of Spontaneous Speech."" The project envisions a world that is both radically transparent and a little freaky.

The idea is that business meetings or even conversations with your friends and family could be stored in archives and easily searched. The stored recordings could be held in servers, owned either by individuals or their employers. Lease is still playing with the idea -- one with huge implications for how we interact.

""In their call, what [Darpa] really talked about were different areas of science where they would like to see advancements in certain problems that they see,"" Lease told Danger Room at his Austin office. ""So I responded talking about what I saw as this very big both need and opportunity to really make conversational speech more accessible, more part of our permanent record instead of being so ephemeral, and really trying to imagine what this world would look like if we really could capture all these conversations and make use of them effectively going forward.""

How? The answer, Lease says, is in widespread use of recording technologies like smartphones, cameras and audio recorders -- a kind of ""democratizing force of everyday people recording and sharing their daily lives and experiences through their conversations."" But the trick to making the concept functional and searchable, says Lease, is blending automated voice analysis machines with large numbers of human analysts through crowdsourcing. That could be through involving people ""strategically,"" to clean up transcripts where machines made a mistake. Darpa's older EARS project relied entirely on automation, which has its drawbacks.

""Like other AI, it can only go so far, which is based on what the state-of-the-art methodology can do,"" Lease says. ""So what was exciting to me is thinking about going back to some of that work and now taking advantage of crowdsourcing and applying that into the mix.""",Yes,"They will lead to a world in which all conversations are recorded and stored forever. This could have a profound impact on the way we interact with each other, and on our privacy.",https://www.wired.com/2013/03/darpa-speech/,2013-03-04 11:30:19+00:00,LABEL_1_relevant,0.9949937462806702,Voice Assistants,voice_assistant,Wired,Security & Privacy
106,"Intel Capital President: Disrupt Alum Expect Labs ""Fits In Nicely"" With Voice Plans For Ultrabook","Intel Capital, the venture arm of Intel, boasts over 120 portfolio companies to have listed publicly on the NASDAQ alone. In celebration of that, President Arvind Sodhani rang yesterday’s closing bell, and we caught up with him to chat about the future of the Ultrabook platform, wearable computing, and advancements in voice and gesture technology.

Intel Capital has actually invested in two of our most recent Disrupt alumni, Ark and Expect Labs, which focuses on predictive voice transcription to help you out as a digital assistant while you’re on the phone.

Sodhani hinted quite strongly that Expect Labs could potentially be the company to bring voice support to the Ultrabook platform, a promise Intel made in October of 2012.

“Our platforms, ultrabooks, tablets and laptops, will have a digital personal assistant on them in the next several years,” said Sodhani. “They’ll anticipate what you want to do next, what you need, and they’ll be context-aware.”

“Expect Labs fits into that very nicely,” said Sodhani. He also mentioned that Intel is looking at other companies for both speech and gesture recognition, but Expect Labs is clearly in the running.

Along with the rapid expansion of the cloud, Sodhani believes that 2012’s onslaught of voice-powered technology will only continue to grow alongside gesture-recognition technology.

As will wearables. Though Intel Capital hasn’t yet invested in wearable computing hardware, Sodhani admits that they’re certainly open to it should the right opportunity arise.",Yes,"Now you can talk to your technology. You could say that talking to your technology is the primary consequence of voice assistants, but if you do then you’re wrong.",https://techcrunch.com/2013/03/08/intel-capital-president-disrupt-alum-expect-labs-fits-in-nicely-with-voice-plans-for-ultrabook/,2013-03-08 00:00:00,LABEL_1_relevant,0.99167001247406,Voice Assistants,voice_assistant,TechCrunch,"Information, Discourse & Governance"
0,The Nauseating Disappointment of Oculus Rift,"And when things don’t work just right, VR can literally make you ill, or at least uncomfortable. Some issues can pop up with the hardware itself, like visual jittering. Others relate to the ways content is made—things like rapid acceleration and deceleration can make you feel sick because what you see isn’t matching up with what your body feels.

To get a more thorough look at this, I tried a variety of games and other experiences with different ratings (Oculus places the ones in its store in three categories: “comfortable,” “moderate,” and “intense”), like BlazeRush, where I raced cars around a track, and Bazaar, where I rode a flying carpet around a bazaar, collecting food and coins and avoiding hungry crocodiles, annoying monkeys, and fearsome snakes. I watched short films, like Invasion!, and I tried out environmental explorations like Ocean Rift.

I was happy to realize that Oculus’s rating system is pretty accurate. Games and other experiences rated “general” were typically playable for a while without feeling sick or otherwise uncomfortable. Games rated “moderate,” like the sci-fi shooter Gunjack, were a bit of a step up in terms of intensity. And playing games rated “intense” was much, much harder. In less than 10 minutes I had to stop and take off the headset, and I felt ill for a while afterward, too.

Situations where I could control my velocity—and where things weren’t moving swiftly around me—seemed to work much better. On a virtual trip through the Grand Canyon, for instance, I could pick between three kayak speeds as I passed red rocks, water lilies, and ducked under a rushing waterfall. I spent 20 minutes just looking at the scenery; it felt almost meditative.

And that was largely my limit—after about 20 minutes, I might not feel sick, but my eyes and brain needed a break. That might be enough for short games and films, but would make it a lot harder to, say, watch a feature-length movie in VR or, perhaps one day, use the technology for work as a virtual desktop with endless displays.

I asked Evan Suma, a research assistant professor at the University of Southern California’s Institute for Creative Technologies who studies VR, what this means for widespread adoption of the technology. Hardcore gamers, after all, are fond of playing console games on flat screens for hours on end, and gaming is expected to be a huge early market for the latest VR headsets; does it matter if people can’t use it for very long?

Suma thinks it’s a concern for widespread adoption, saying simulator sickness is “one of the biggest challenges” that the VR community needs to solve.

The Oculus Rift virtual reality headset.

He wonders, however, if we may adjust to wearing headsets over time, making it more comfortable to wear them for longer. This is a theory Oculus founder Palmer Luckey believes in; while he acknowledges sickness is still an issue that needs to be solved by game design, he has also said that he thinks people will adjust to VR technology. I noticed it myself, actually, as I continued playing Bazaar over a couple days and found myself less affected by sudden turns of my flying carpet.

Whether or not we can naturally adjust, there are ways to help fix discomfort in VR. Suma says these range from using teleportation to move you from one point to another in virtual space to giving you a larger area to physically move around in while in virtual reality (right now, the Vive headset is more suited for this than the Rift). Eventually, advancements in display technology may improve things, too.

Making lighter headsets could help as well. The Rift weighs about a pound; the Vive weighs slightly more. That may not sound that heavy, especially when you consider the kinds of headsets used for VR in the past, but Suma says research indicates that the more weight you put on your head the more inertia you have when turning it, which means it takes more effort to stop turning and, as a result, some people feel sick.

Improvements like these will take a while, but I’m willing to wait: when you do get a truly immersive virtual-reality experience, like some of the ones I had, it’s riveting—even in 20-minute increments. In the meantime, I’ll be spending my non-nauseous time in the Oculus Rift shuffling between my kayak and flying carpet.",Yes.,"Simulator sickness, which is caused by a variety of factors like visual jittering, content that is not well-matched with physical sensations, and rapid movement. To combat this, Oculus has created a rating system for games and experiences, and Suma suggests that people may adjust to VR over time, that there are ways to fix discomfort in VR,",https://www.technologyreview.com/2016/05/05/245975/the-nauseating-disappointment-of-oculus-rift/,2016-05-05 00:00:00,LABEL_1_relevant,0.990039885044098,Virtual Reality,virtual_reality,MIT Tech Review,User Experience & Entertainment
1,Another Price Slash Suggests the Oculus Rift Is Dead in the Water,"For the next six weeks, the Oculus Rift headset and its matching controllers will cost just $399. That’s $400 less than when it first hit the market, and $200 less than when its price was first slashed in March. It means that the Rift now costs less than the package offered by its cheapest rival, Sony, whose PlayStation VR currently totals $460 including headset and controllers.

Even so, it’s not clear that it will be enough to lure people into buying a Rift. A year ago, our own Rachel Metz predicted that the Rift would struggle against Sony’s offering because the former requires a powerful (and expensive) gaming computer to run, while the latter needs just a $350 PlayStation 4 game console.",Yes,It could potentially lead to people becoming more isolated from the real world.,https://www.technologyreview.com/2017/07/10/68156/another-price-slash-suggests-the-oculus-rift-is-dead-in-the-water/,2017-07-10 00:00:00,LABEL_1_relevant,0.9953208565711976,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
2,Chinese 'Second Life' Avatar Plays Well in Venice,"Content This content can also be viewed on the site it originates from.

Chinese artist Cao Fei’s Second Life avatar China Tracy is getting high praise at the Venice Biennale from artists who work this online metaverse as a creative medium. China Tracey is the first serious SL project I’ve seen at a major international art festival, and the depth of color, emotion and realism is a step in the right direction. Cao Fei has set it up well, screening her all-SL-shot films in a billowy white inflatable tent at the Chinese pavilion. Airy pop music plays as China Tracy moves through various dreamy scenarios that grapple with globalization and how it affects the surging generation of new Chinese artists. Check out her blog, which is mostly in Chinese but features several YouTube clips and outtakes.",Yes,It makes people more aware of the constructed nature of reality.,https://www.wired.com/2007/06/chinese-second-/,2007-06-13 14:06:47+00:00,LABEL_1_relevant,0.9938777089118958,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
3,The Most Important Movie of 2015 Is a VR Cartoon About a Hedgehog,"Right now the goal at Oculus Story Studio is move the Post-its.

The true finish line is locking their latest animated virtual reality movie Henry, but to do that they have quite a few issues to fix. There's a conference room wall in their San Francisco office covered in dozens of sticky paper squares of all shades, each representing a problem someone has to fix before the project is done. It’s like something out of a scene from Silicon Valley, only instead of tweaking a compression algorithm it's optimizing what is essentially a cartoon.

Then again, most cartoons aren’t made using videogame engines. And right now, that reliance on game-making software—specifically Unreal Engine 4—is creating a lot of new Post-its.

Christie Hemm Klok/WIRED

It's two weeks before Henry's July 28 premiere, and the creative brain-trust of Story Studio is huddled in a dimly lit room (they do meditation in here in the mornings). Ramiro Lopez Dau, the director, is wearing the latest prototype of Oculus' headset and watching the current version of Henry—something he's seen dozens of times. His supervising technical director, Max Planck, is at a computer console, his shoes off; he's watching the feeds going into both of Lopez Dau's eyes, while the handful of other employees in the room watch Planck's display and point out glitches. A ladybug shows up in the wrong color? That’s a Post-it. One of Henry's eyes looks wonky? That’s another.

""These will all become Post-it notes,"" Story Studio's creative director Saschka Unseld says to me in a hushed voice so as not disrupt his team's flow. ""Ryan [Thomas], our main production coordinator, is constantly typing.""

Indeed, Thomas will spend most of the next hour looking up from his laptop only to gather enough info for the next note. What the Story Studio team is doing, if it had a filmmaking analog, would be looking at dailies—except only one person can watch it at a time. Also, ""dailies"" is a bit of a misnomer. They used to do this once every couple of weeks, then weekly—there was too much to fix in between, and it took hours to render—but now, two weeks out, they do it every afternoon. Fix a problem, lose a Post-it. Watch it again, gain two more. ""It makes me weirdly paranoid that things suddenly break, because I don’t know how much stuff will be broken tomorrow,"" Unseld says ruefully. ""Sometimes it’s rough to get a sense of these things are done, we move on to fix these things.""

But breaking things and fixing them is the whole point of Story Studio. Last fall, a few months after Facebook acquired Oculus, the studio was founded with the mission to determine what was possible for VR filmmaking, and then to share that knowledge with the world. ""Part of the mission is 'inspire and educate,'"" producer Edward Saatchi says. And a huge hurdle to doing that has been figuring out how to make movies using both traditional animation software like Maya and videogame tools, while also making Hollywood filmmakers comfortable with the media as ""an art-form, rather than cinema in a videogame,"" as Saatchi puts it.

They’ve done it—Story Studio had its coming-out party at Sundance in January, showcasing an exploratory short called Lost—but they had to break Unreal in the process. ""A couple of months ago we switched,"" Unseld says, gesturing toward the small team of animators working on Henry. “We disconnected ourselves from the out-of-the-box Unreal Engine and opened up the code so we can just play with the innards of it.”

Christie Hemm Klok/WIRED

Epic Games, the developer behind the Unreal Engine, has always kept the code for the software open-source; The company encourages people to play with it, but as Epic’s founder Tim Sweeney notes, Story Studio is ""pushing the engine really hard."" To get what they want out of Unreal 4, visual effects supervisor Inigo Quilez has written code to make things like fur and shadows behave more easily and naturally. It's a concern game developers rarely worried about outside of non-playable ""cinematic sequences""—most players race by shadows before they really register.

Unreal 4 does feature an animation tool that helps developers build cinematic experiences, and by the end of the year they hope to incorporate something called Sequencer that will feel more familiar to filmmakers used to working in ProTools. But what Story Studio is doing is still beyond that—and Epic is happy to have them guide the way. ""I think the thing people are going to be really impressed with when they watch Henry, and I've only seen little bits and pieces, but it really does look like a classic animated movie,"" says Epic CTO Kim Libreri, who made his bones doing VFX for The Matrix movies before moving on to Lucasfilm and eventually into gaming. ""They’re doing a lot of tricks to produce that look, but it shows that with perseverance and the right mindset to scale your techniques to work for your art, you can get a pretty good result.""

Christie Hemm Klok/WIRED

A New Way to Care

It wasn’t just that Story Studio needed Unreal to do something it wasn’t made for. They also wanted to do things that had never been done in movies, period: they want Henry to look you in the eye, no matter where you are in his virtual world.

At its core, Henry is a simple story. The titular hero is hedgehog who loves hugs. (This, clearly, is problematic.) While celebrating his birthday by himself, his balloon animals come to life and try to befriend him. (This, clearly, is really problematic.) There are emotional ups and downs, and when Henry experiences a certain feeling, he looks at the viewer to share his sadness or excitement.

This is an incredibly rare thing. It’s also unnerving; the first time I tried an early demo, I immediately felt like a voyeur, as if I was spying on this poor hedgehog during his sad solo birthday. But in VR, the whole point is that you’re meant to feel as though you’re physically there with Henry—and as Unseld pointed out to me when I took off the goggles, it’s weird if Henry doesn’t recognize someone sitting on his living room floor. ""It's just like 'Why don't you look at me? I'm right here!'"" Unseld laughs. But when he does it carries an emotional connection, an empathy, that even those abused action figures in Toy Story 3 didn’t quite muster. (Unseld will later lament that this connection makes comedy twice as hard because Buster Keaton-esque physical comedy just feels ""mean."")",Yes,"It makes you care about things you otherwise wouldn’t. And that’s the primary challenge facing Story Studio. ""The trend of movie-making has been to make things more emotionally distant,"" Quilez says. ""You can have a scene with a car crash, and the camera pulls back, and you don’t",https://www.wired.com/2015/07/oculus-story-studio-making-henry/,2015-07-28 19:45:13+00:00,LABEL_1_relevant,0.9949761629104614,Virtual Reality,virtual_reality,Wired,Environment & Sustainability
4,You can now practice firing someone in virtual reality,"Meet Barry: his sole purpose in life is to listen patiently, and then protest or sob a little as you fire him from an imaginary job in virtual reality.

Disposable hero: He was created by Talespin, a company that offers virtual reality workplace training. Barry and other VR characters were developed to help teach people “soft” management skills—such as how to let someone go without causing a scene. If you are too blunt with Barry, he’ll put his head in his hands; other missteps will cause him to shout and yell.

Learning tool: This might seem bizarre, even dystopian, but virtual reality is gaining momentum as a training tool. The technology can provide a heightened sense of realism that aids the learning process, and it can allow people to practice doing things that would be impossible otherwise. VR is used to teach people to perform safety tasks, for example. But it is also increasingly being used to train new employees at places like Walmart and Chipotle.

Office space: You can expect this trend to spread to offices in coming years. “We are seeing repeated interest in building training products around interviewing skills, navigating difficult conversations, consultative selling, performance reviews, and identifying diversity and inclusion best practices, to name a few,” says Kyle Jackson, CEO of Talespin.

Unreal feeling: Computers and algorithms are sometimes now used to monitor workers’ output and performance. But while it’s common to think that social and emotional skills are exempt from this automation creep, Barry suggests otherwise. “Soft skills are ranked among the most important for any organization as they discuss their needs for the future of work,” Jackson says. “We don't see this slowing down any time soon.”

Uncanny colleague: The catch with Barry and other virtual characters, in case you didn’t guess, is that their effectiveness depends on how convincing they are. Barry looks quite realistic for an avatar, but Jackson says the character follows a set script—so it can’t interact in a very natural way.

Learning process: Avatars behave more naturally if they’re powered by machine learning—but it’s going to be a long while before these characters exhibit, or respond to, body language in realistic ways. As Danielle Levac, a professor at Northeastern University who specializes in using VR for rehab, says: “If you’re not able to escape the notion that it isn’t a real person, it’s not necessarily going to transfer.”",Yes,"It can make it difficult for people to discern between what is real and what is not. This is especially true for avatars, which are often used to teach people new skills or help them practice for difficult tasks. While the technology can provide a heightened sense of realism that aids the learning process, it can also lead to confusion and cognitive dissonance",https://www.technologyreview.com/2019/08/09/133799/you-can-now-practice-firing-someone-in-virtual-reality/,2019-08-09 00:00:00,LABEL_1_relevant,0.9959749579429626,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
5,Boss Acting Nicer Recently? You May Have VR to Thank,"Barry's hair has long gone gray, his face lined with deep signs of age. Sitting across from me in his pressed blue shirt, he looks like an uncle, maybe even a grandfather. But Barry's employment file is … not a fun read. He told a coworker she was a ""whiner,"" and his colleagues have filed complaints about his inappropriate remarks six times this year. So as avuncular as he seems, I don't really have any options. I have to fire the guy.

""We've addressed the disruptions you've caused with coworkers before,"" I say, ""and this last one is strike three.""

He immediately becomes flustered, defensive. ""Everyone is so—oh, come on,"" he says, stammering a little. ""Everyone is so sensitive. It wasn't like this 10 years ago.""

A screen appears floating above and to the side of his head, containing three possible things I could say. In each choice, a word or two is bolded, so that the VR software I'm using can identify what I'm saying and trigger Barry's response accordingly. The second sounds wishy-washy (""You know I've always liked you, but I have to follow the rules. I'm really sorry.""), the third sounds bloodless (""As discussed, your actions are disruptive to coworkers. This is the final offense. We are letting you go.""), so I opt for the first. ""Times are different now,"" I say, wincing a little inwardly at how I sound. ""We're going to have to let you go.""

Barry twists in his chair, motioning at the other desks and people outside the door of my office. ""I have 20 more years' experience than anyone in there!"" he says. ""Have you looked at my sales numbers? How do you justify this based on those numbers?"" There's nothing I can say that won't make things worse, so I choose neutral compassion: ""I understand if you need a moment to process this.""

His face crumples. He looks down at his hands, then back at me. Is he going to yell? I think. Cry? How do I deal with that? ""Y-y-y-ou...Do you…"" His voice breaks. He looks around, composes himself.

""Take this time to process this,"" I say again. ""I'll go over the next steps when you're ready.""

Yet, after a few more seconds, Barry seems to turn the corner into acceptance. ""Well, I didn't see this coming,"" he says. ""I really thought I was going to finish my career here. Look, I appreciate the opportunity. It was a pleasure working with you.""

Whew.

""Nice work,"" Kyle Jackson says as I take my headset off. It doesn't always go so smoothly: The first time Jackson demonstrated the process for a roomful of Oculus executives, Barry wound up yelling and cursing at the person in the headset. ""But everybody thought it was funny because it was the colleague that kinda needed that feedback,"" he says, ""so it was a really honest moment for the room."" (And for anyone hearing the anecdote, presumably.)

Awkward moments aside, VR's role in the corporate world has far outpaced its consumer impact. But while its applications have largely been clustered around design, collaboration, and even job training simulations, virtual reality has in the past year seen a flurry of activity around so-called soft skills: leadership, communications, HR applications, and other people-focused business fields. Now, Talespin, a company Jackson cofounded, hopes that virtual humans like Barry will allow companies to leverage VR's unique social mojo to make their employees smarter—faster.",Yes,People can get so immersed in the experience that they forget about the real world around them.,https://www.wired.com/story/vr-soft-skills/,2019-02-28 17:30:09.779000+00:00,LABEL_1_relevant,0.9943488240242004,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
6,Facebook Needs to Stop Bad VR Apps Before They Start,"Facebook really, really wants you to give VR a go—no pun intended. That's the message the company communicated yesterday during day one of F8, its annual developers conference in San Jose, California. The F8 keynote was filled with assurances that VR headsets like the new Oculus Go won't create a barrier between you and the people around you. Instead, the company believes that wearing a face computer will be even more social, because you'll be playing games, taking meetings, and video chatting with friends and family.

And since the apps that have already been created for Samsung's Oculus-based Gear headset can be ported over to the Oculus Go headset, there are already more than a thousand apps available for the new $200 Oculus Go. What else do you need at this point in order to embrace VR?

For one, maybe a little reassurance that VR apps—as well as AR apps—are being designed with user privacy and reasonable data-sharing practices in mind. Facebook still needs to prove that it's thinking about new technologies in a way that ensures they won't become the next obvious frontier for abuse, misinformation, or even election interference. As the company's primary platform has swelled to more than two billion users, it's had its share of issues with false news, hate speech, and bad apps, due in part to Facebook's own lack of due diligence during growth phases.

Facebook's Oculus VR user base is still minuscule by comparison—according to one research firm, 1.8 million Oculus Go devices are expected to sell this year—but if Go becomes the great VR democratizer that Facebook is hoping it will be, then the new headset is introducing a new kind of app and a new kind of app store to a whole new subset of Facebook users. It also raises the question of how Facebook will deal with ""fakeness"" in an environment that is, by definition, entirely virtual.

Facebook's executives in VR and AR say they have learned some lessons from the early days of Facebook, and that the company is trying to ""ensure a very high quality of platform against misinformation or against bad actors,"" according to AR/VR executive Andrew Bosworth. But Bosworth, known as ""Boz,"" also said in an interview with WIRED that he believes Facebook's AR and VR app platforms are still too nascent to have serious abuse problems.

Facebook-owned Oculus utilizes its own app platform, separate from Facebook, Messenger, or the other apps that Facebook owns. You don't need a Facebook account to sign up for Oculus, and linking your Oculus account to your Facebook account is optional, as WIRED's Peter Rubin points out in his review of the headset. Go has its own app store, and many of the mobile VR apps that are front and center right now are highly recognizable brands or titles: Netflix, Hulu, NatGeo, Minecraft, The Last Jedi.

There are also only around a thousand apps right now, which means each app is reviewed manually, according to Bosworth. ""It's a manageable number of applications,"" he says, ""and you can just look at every one of them and make sure there's nothing in there that's untoward.""

Reality Check

In a pre-emptive move, ahead of changes that could be enforced when Europe's General Data Protection Regulation goes into effect, Oculus published an update to its privacy policies two weeks ago. The update highlighted the addition of a privacy control center for users and clarified the kinds of information Oculus, and in some cases Facebook, collects about Oculus users. It also divulged the kinds of data that Oculus app-makers have access to: the real-time position of your headset and controllers, your Friend List, and the boundaries of the physical space where you're using Oculus. ""We periodically audit our systems to determine if there's evidence of nefarious activity,"" the post reads, ""and we take action accordingly.""

In other words: it doesn't read all that different from Facebook's privacy policies and settings on its core app or other apps. Especially when you consider the periodic audits; Facebook's own privacy audit in 2017 didn't catch the Cambridge Analytica data caper. As VR gets more sophisticated, and as standalone VR headsets get better at profile-building and advanced positional tracking (like the kind promised with Oculus's ""Santa Cruz"" headset), it's enough to make any non-early-adopter wary about the volume and granularity of data that's being collected.",Yes,"It blurs the lines between what is real and what is not. This is a problem that is still being addressed by companies such as Oculus, who are working on making sure that their products are safe and secure for users.",https://www.wired.com/story/facebook-f8-oculus-go-privacy-and-safety/,2018-05-02 11:00:00+00:00,LABEL_1_relevant,0.9944696426391602,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
7,This Augmented-Reality Sandbox Turns Dirt Into a UI,"We've seen how kids take to touchscreens. To them, our unfathomably sophisticated smartphones and tablets are about as hard to figure out as a bucket full of blocks. It's a little bit eerie, watching a youngster prod his or her way around an iPad; it makes you wonder if future generations will ever have a chance to foster any of the skepticism we feel towards cyborg enhancements and brain implants and other unsavory flavors of invasive, singularity-style computing. Hopefully they'll grow into it.

But while all their deft, chocolate-fingered swiping might make a future of baby's first tablet seem inevitable, a project out of UC Davis offers a glimpse of an entirely different type of high-tech educational engagement--one that doesn't involve youngsters pressing their faces up against screens for hours on end. It's a sandbox that lets kids mold its contents into miniature mountains, lakes and rivers--and then, with a little high-tech magic, brings that terrain to life before their eyes.

Content This content can also be viewed on the site it originates from.

>It's a sandbox that lets kids mold its contents, then brings that terrain to life.

The project offers an irresistible combination: the timeless tactile joy of sand in hand plus a dollop of whiz-bang tech to top it off. A Kinect camera mounted above the sandbox tracks the physical activity below. As visitors young and old go about their terraforming, a projector throws a dynamic topographic map on top of it all, updating contour lines and elevation colors in real time. Then, the fun part: a virtual rainstorm, also supplied by the projector, sends a torrent of blue water cascading down the peaks, showing runoff and watershed on the landscape created moments before.

Inspired by a YouTube video from a group of Czech researchers, researchers at UC Davis' W. M. Keck Center for Active Visualization in the Earth Sciences (KeckCAVES) started work on their augmented-reality sandbox in early 2012, as part of an National Science Foundation-funded program focused on water system education. Last summer, the video below, showing off their prototype, garnered over a million views on YouTube. Now, a pair of museums on either coast of the country are home to augmented sandboxes of their own--one at ECHO Lake Aquarium and Science Center in Vermont; one at the Tahoe Environmental Research Center at UC Davis--with a third being installed at the Lawrence Hall of Science in Berkeley this month.

The exhibits are presented with minimal instruction. Play, curiosity, and self-driven learning are all encouraged. ""The power of the sandbox is that it's trivial to make your own terrain completely from scratch and in moments, without any training,"" explains Oliver Kreylos, one of the lead UC Davis researchers on the project. ""There's just no better way to teach how topographic contour lines work, or how water flows over a landscape, than building whatever terrain you can imagine, and then seeing the contours and the water react in real time to any changes you make.""

Content This content can also be viewed on the site it originates from.

At ECHO Lake Aquarium and Science Center in Vermont, some 43,000 visitors have played with the interactive installation since it was installed in May. ""We have watched our guests at every age dig in and explore with their hands, ask lots of questions, test ideas, challenge their friends and family, wonder about how it works, and express curiosity and joy,"" says Julie Silverman, the museum's Director of New.

She says the exhibit often fosters collaborations between strangers, prompting groups to build mega-mountains with all the sand or to see if they can flood the entire landscape to create an island chain. And according to Silverman, it's the rare exhibit with replay value; people actually come back later in their visit for more time playing in the sand.

ECHO's sandbox is one of three official versions of the sandbox in the wild, but Kreylos, the UC Davis researcher, says he knows of at least six others around the world, all built using the software and schematics his team has made available on their site. One of these was built by a high school robotics team in Ithaca, New York, another by a science center in Sao Paolo. There's one in an elementary school in Australia and another, Kreylos says, somewhere in Mongolia.

>Touchscreens are better than mouses, but surely dynamic environments are better yet.

The point is that far from a complex, custom-built device, KeckCAVES' design is relatively simple for others to put together themselves. It doesn't take much more than a Kinect, a projector, a spare computer, and some of their code. ""It appears the installation is quite straightforward,"" Kreylos says, ""and I'm guessing there are a good number out there I don't know about. I'm currently helping two groups doing final calibration for theirs, and another group is out buying equipment right now.""

The applications for the AR sandbox, Kreylos admits, are fairly limited. Sand can't teach you much about math, or history. Indeed, much of Kreylos' work involves developing immersive, intuitive virtual interfaces--next-gen tools that could prove useful for all sorts of disciplines. You can watch him demo one of his latest projects involving an Oculus Rift headset and two Razer Hydra joysticks here.",Yes,"It makes you want to throw up. But there's a lot of potential there--and not just for gamers. Think about how much more intuitive, more informative, more immersive a history lesson might be if you could actually walk around inside a historical simulation. Think about how much easier it might be to learn math if you could actually see and touch",https://www.wired.com/2013/08/this-augmented-reality-sandbox-turns-dirt-into-an-interactive-interface/,2013-08-30 10:30:06+00:00,LABEL_1_relevant,0.9564641118049622,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
8,US Army soldiers will soon wear Microsoft’s HoloLens AR goggles in combat,"Microsoft has won a $480 million deal to supply more than 100,000 augmented-reality HoloLens headsets to the US Army, Bloomberg reports.

Uses? The Army plans to use the headsets for combat missions as well as training. The technology will be adapted to incorporate night vision and thermal sensing, offer hearing protection, monitor for concussion, and measure vital signs like breathing and “readiness.” AR firm MagicLeap also bid for the contract, according to Bloomberg.

A first: HoloLens is used for training by the US and Israeli military already, but this would be the first time it’s been used for live combat. It’s another example of how AR is being adopted far more enthusiastically by organizations than consumers.

Tensions: The deal is more good news for Microsoft, which overtook Apple as the world’s most valuable company yesterday. However, there could be pushback against this contract—and it’s as likely to come from Microsoft employees themselves as from external groups. The relationship between the technology sector and the US military has become fraught over the past year, with employees at Amazon, Google, and Microsoft protesting their companies’ bids for government contracts. The solution? Move uneasy staff members to other projects, Microsoft president Brad Smith said last month.",Yes,"It 

it can cause people to experience nausea and vomiting. This is because virtual reality can cause a disconnect between what people see and what their bodies feel, causing a mismatch that can lead to nausea.",https://www.technologyreview.com/2018/11/29/138889/us-army-soldiers-will-soon-wear-microsofts-hololens-ar-goggles-in-combat/,2018-11-29 00:00:00,LABEL_1_relevant,0.9939826130867004,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
9,The Inside Story of Google's Bizarre Plunge Into VR,"David Coz worked in Google's Paris office, but what he really wanted was a job at the mothership in Silicon Valley.

Last spring, the French-born Coz turned up at Google headquarters in Mountain View hoping to chat about his latest project with anyone who would listen. ""I came with my prototype and my luggage,"" he says, ""and I met with 10 or 15 people."" One of them was Christian Plagemann, a Google research scientist exploring new interfaces for consumer electronics devices. Though they'd never met, Coz showed him the prototype: a pair of virtual reality goggles made out of cardboard.

Plagemann was intrigued---""David showed me this cardboard box,"" he says, ""and I thought it was absolutely amazing""---and he took the contraption to Google's bigwigs, including CEO Larry Page and vice president of engineering Sundar Pichai. ""I convinced him to leave me one of the boxes. He flew back to Paris. And I started showing it around,"" Plagemann remembers.

Two months later, Pichai unveiled the project on the keynote stage at Google's annual developer conference in San Francisco, and Google employees handed cardboard headsets to thousands of coders as they streamed from the speakers' hall. It was a slightly odd sideshow at a conference where the company typically doled out millions of dollars in phones, tablets, and other electronic gear. But in the year since, this unexpectedly low-tech device---something that wraps around an ordinary smartphone screen---has engendered a sweeping virtual reality project inside the company, giving Google a foothold in an area poised to reshape the tech world.

Coz did join the mothership. He and Plagemann and the other Paris employee who created Google Cardboard, Damian Henry, are all part of a Google VR team that's ""bigger than people think it is."" In December, the team roped in a group of 3-D vision experts led by Steve Seitz, a University of Washington professor whose work gave rise to a panoramic photo application from Microsoft called Photosynth. John Wiley, who previously oversaw the visual design of the Google search engine, recently joined as well. And the team is now fashioning rather complex VR tech that puts Google in competition with Facebook, Microsoft, and others.

On Thursday, after team leader Clay Bavor showed off a new incarnation of the cardboard goggles at this year's Google I/O developer conference, he revealed that the company has designed a 16-lens camera that can record videos across 360 degrees, and that it's developing a software system that can turn these videos into the kind of immersive, stereoscopic experiences the goggles are intended to provide. The company calls all this its ""Jump"" VR platform, and it says GoPro will offer a version of its camera this summer. But all these developments, it seems, are merely precursors to something larger. ""We have ambitions beyond just Cardboard,"" Bavor says. ""There are many other things going on.""

Attendees look through Google Cardboard VR (virtual reality) viewers during the Google I/O Annual Developers Conference in San Francisco. David Paul Morris/Bloomberg via Getty Images

'The VR Nerd'

The rise of Google Cardboard shows the unusual way Google operates. In Paris, Coz and Henry worked for the Google Cultural Institute, a way for museums and other institutions to put their art online. Their cardboard goggles, meanwhile, were a ""20 percent project,"" where Google employees dedicate one day a week (that is, 20 percent of their time) to some new idea.

The project began after they stumbled onto a little-seen YouTube video shot from a flying drone. It was a stereoscopic video meant for a 3-D headset they didn't have, and at some point, they realized they could view such videos if they wrapped a makeshift headset around a phone---""you just have to make sure the phone understands where your head is looking,"" Coz says---and they pitched the idea to their Paris manager as a way for students to virtually visit museum galleries.",Yes,"It makes people nauseated.

The manager liked the idea, but it wasn't until Plagemann took the idea to Google's San Francisco headquarters that it really caught fire. ""I showed it to Christian, and he was just blown away,"" Coz says. ""I think he said, 'This is the coolest thing I",https://www.wired.com/2015/06/inside-story-googles-unlikely-leap-cardboard-vr/,2015-06-01 11:00:36+00:00,LABEL_1_relevant,0.9959393739700316,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
10,"Second Life Is Plagued by Security Flaws, Ex-Employee Says","A lawsuit filed by the former information security director of Linden Lab—the company behind the online virtual world Second Life, which, yes, is still a thing—claims the company mishandled sensitive user data and turned a blind eye to simulated acts of child molestation and the potential for money laundering.

Paris Martineau covers platforms, online influence, and social media manipulation for WIRED.

In a lawsuit filed in San Francisco County Superior Court on July 30 and served to Linden Lab on Tuesday, Kavyanjali Pearlman, a security researcher who joined Linden Lab from Facebook in 2017, says that she raised these issues during her tenure, and was met with hostility. The suit alleges company executives retaliated against her for flagging cybersecurity risks and potential violations of anti-money-laundering laws, child exploitation, and data misuse.

Pearlman claims the company discriminated against her as a woman, an Indian immigrant, and a Muslim. “After making her concerns known, [she] was treated worse than similarly situated employees who were not immigrant women of color, who were not religiously Muslim and wore a hijab,” says the suit. “Instead of looking into Pearlman’s complaints, Linden Lab’s senior officers led a campaign of retaliation against her, painting her as an inept employee who has issues with communication, and ultimately terminating her employment in March of 2019.”

“While we will fight her alleged claims in court, we deny any allegations that the company has engaged in any illegal activity,” said Linden Lab spokesperson Brett Atwood. “Ms. Pearlman left the company on March 15 only after she was given the opportunity to improve her work performance. We look forward to all the facts coming out in a court of law,” he said, declining additional comment because of the lawsuit.

Linden Lab is best known for Second Life, the massively multiplayer virtual world launched in 2003, which boasted around a million regular users at its peak, and an estimated 800,000 active monthly users as of 2017. Those numbers are paltry compared with today’s social media giants, but it’s still a sizable chunk of people.

A decade ago, Second Life was populated mostly by futurists, brands, and, for some reason, embassies; today, the virtual world occupies a more niche space online. Much of Second Life revolves around the Linden Dollar, a virtual currency with real cash value that is used to buy and sell in-game items, virtual land, and operate or play at virtual “skill gaming” casinos. In 2018, approximately $65 million was paid out to Second Life users for a variety of virtual goods and services. Gaming—including both free-to-play games and “skill” games that offer payouts—was the most popular activity among users, according to Linden Lab.

Last October, Pearlman says she raised concerns with Linden Lab executives that the company was not complying with anti-money-laundering rules, including not required information about the operators of skill games, according to the lawsuit. She says her concerns were dismissed, and that the issues had yet to be addressed by Linden Lab when she left the company in March.

Atwood, of Linden Lab, declined to comment when asked about the accuracy of Pearlman’s description of events. “All Second Life skill gaming operators must provide and verify their identification as part of a rigorous application process,” Atwood told WIRED over email. “We are in compliance with all legal regulations and all skill gaming operators agree to our Terms & Conditions as part of the review and approval process for our Skill Gaming program.”",Yes,It has isolated people from the real world.,https://www.wired.com/story/second-life-plagued-security-flaws-ex-employee-says/,2019-08-16 11:00:00+00:00,LABEL_1_relevant,0.9967474937438964,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
11,Why a VR Game About Flirting Is as Scary as a Horror Game,"On the surface, the two PlayStation VR games on display at Sony's Tokyo Game Show booth couldn't be any more different. One is a horror scenario that drops you into a gruesome, terrifying predicament. The other puts you on a beautiful seaside next to an attractive young lady.

But after playing both, I couldn't shake the feeling that I'd played the same demo twice. The sheer intimacy of these experiences—the feeling of having one's personal space invaded, if you will—was unmistakeable, and I can still feel it now, at a distance.

A long line stretches to the back of Sony's Tokyo Game Show stand, people patiently awaiting their turn on the 20 or so demo stations of PlayStation VR (nee Project Morpheus), Sony's upcoming virtual reality headset for PlayStation 4. There were many different demos, but those that left the deepest impression on me were Capcom's horror demo Kitchen and Bandai Namco's romance sim Summer Lesson.

I'd already had a lurking-horror experience with Alien: Isolation on Oculus Rift that caused me to seriously contemplate ripping the VR headset off my head midway through. I didn't, but when Kitchen started I felt like maybe this was the time I finally did.

I awoke with my hands bound together—in real life I was holding a PlayStation controller, which of course placed my hands close together, a nice touch—in a dingy room. A man next to me was screaming in my face telling me we had to get out. We did not, because some kind of female ghoul entered the room and killed that guy, but not me. She was all up in my face. Very close.

Then the ghoul left the room. And nothing happened for a while.

My reaction to fake scary things, I have discovered by going to a few of those crazy haunted houses, is to laugh nervously. I was cracking up during Kitchen because Kitchen was some no-holds-barred scary stuff. At one point, I am ashamed to say, some part of the VR rig slipped down and tapped me on the shoulder, making me gasp quite loudly and reach around ready to kill whatever it was.

Then the ghoul left the room. And nothing happened for a while.

This was the scariest thing of all, because I knew something was going to jump out at me. But where? When? In a PlayStation VR demo, when time is of the essence because you move people through as soon as possible, Sony let this go on for a while. Nothing was happening—nothing—and I was as riveted as I've ever been in a game. When I died, following another very up close and personal encounter from which I could not look away, it came as something of a relief.

I didn't expect to feel similarly uncomfortable during Summer Lesson, the pitch for which is you are a teacher providing a private tutoring session to an attractive female student.

'Sensei, how do you read this word?'

Things started off fairly benign, with the student appearing and sitting down next to you, reading from a textbook. You could choose to teach English to a Japanese student in her bedroom, or Japanese to an American student at a beach house. At one point, and this happens in either scenario, the student leans in very close and asks, ""Sensei, how do you read this word?"" Then she places the book in front of your face, leaning into you in what can only be described as an extremely intimate manner.

My heart rate went up. The experience triggered the same alarms that would go off if a real-life stranger got so close I could feel her breath. This goes on for a few minutes, with the student perhaps leaning over you to pick something up, or leaning in to whisper something in a hushed, conspiratorial tone.

It's important to note that this is not about gawking at a virtual woman. Everyone involved is dressed entirely appropriately. There are no bikinis, no peeks up a skirt, nothing like that. This isn't salacious. But it is, quite plainly, erotic. It's supposed to be. The genius of Summer Lesson is how it illustrates the sheer power of virtual reality to not only transport you but to create genuine emotional reactions to what you see. The women in Summer Lesson need not dress provocatively, or talk about sex, or do anything more than get slightly closer than societal convention typically allows.

As in that long stretch of nothing in Kitchen, there's tremendous power in the tiniest interactions. That underscores a counterintuitive truth that makes VR gaming so exciting: Despite its ability to hijack your senses and immerse you in hyper-realism, the real power of VR may lie in its ability to foster quiet intimacy so realistic it's unsettling.",Yes,Its ability to make players feel uncomfortable in two very different but equally effective ways.,https://www.wired.com/2015/09/playstation-vr-kitchen-summer-lesson/,2015-09-20 11:00:43+00:00,LABEL_1_relevant,0.9953657388687134,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
12,How a Pixar Vet Is Shaping the Future of VR Storytelling,"As we’ve noted many times on this site, virtual reality is the Wild West of filmmaking. From the tech used to make VR films to the scripts that are written for it, everything is new. Luckily for our retinas, a lot of very smart people are trying to figure it out. High at the top of that list: Saschka Unseld, the Pixar veteran who is now the creative director of Oculus Story Studio.

Through the two VR experiences Story Studio has made already—Lost and Henry—and the ones it has forthcoming, Unseld and his team are slowly learning the grammar of what plays well inside a headset. “The way comedy works and the way empathy works is very different in VR than it is in film,” Unseld says in the latest video from the Future of Storytelling.

The biggest things they've learned so far? Make eye contact. Don’t get too close to crying characters. And appreciate that you no longer have a fourth wall. ""That kind of one-on-one relationship that I have in VR is something very primal and you feel like that you don’t have in any other medium,"" Unseld says.

Check out the full interview with Unseld, who will be speaking at the Future of Storytelling summit next month, above.",Yes,It makes users feel more empathy than they would in other forms of media. The way that comedy and other forms of storytelling work in virtual reality are very different than in film or other traditional mediums.,https://www.wired.com/2015/09/pixar-vet-writing-grammar-virtual-reality-stories/,2015-09-23 19:00:56+00:00,LABEL_1_relevant,0.9614269733428956,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
13,Using Virtual Reality Underwater Is Weird (but Fun),"“I think there’s a little more of a suspension of disbelief when you’re in a radically different environment,” Greenwood said. “When you don’t have a sense of the ground or gravity or what’s up or what’s down, it makes it that much more believable.”

Their underwater VR gear is nothing more than a rudimentary prototype, but it does work. The computer and display come from a waterproof Android smartphone, which is attached to a 3-D-printed block of plastic that acts as a spacer between the handset and a snorkeling mask. The mask, covered in black tape to keep light from leaking in, has two biconvex lenses—like the ones you’d find in the simple, handheld Google Cardboard—glued to the inside. A plain old snorkel lets you breathe, and audio comes from a Finis underwater MP3 player that uses bone conduction to transmit sound.

They’ve been trying it out at Greenwood’s apartment building, which has a pool (a rarity in San Francisco). On a sunny, chilly afternoon this month, I jumped in and tried it out myself.

The first experience had me floating above the International Space Station while David Bowie’s “Space Oddity” played on a loop. I found that I didn’t want to do much moving; it was relaxing to just hang, suspended in the water (though the visuals made it feel more like outer space), occasionally twisting around to see what was behind, above, or below.

After I’d spent some time checking out the ISS—minutes, probably, but it was hard to tell how much time passed—Greenwood switched to another VR experience that was a little closer to my reality: an underwater scene with colorful fish, peppy jazz music playing in the background. Hanging out with the fish was a little more fun, despite the fact that water started seeping into the snorkel mask as I swam in circles to investigate the world around me.

I quickly realized that the headset is about as advanced as Google Cardboard at this point. There’s head orientation tracking, but no tracking of your head’s position in space. This means you can turn around to look at different things, but your view of the virtual world through the goggles doesn’t change as you swim around.

This is especially weird if you’re making an effort to swim in one direction, as I did while checking out the fish, because the visuals make you feel as if you’re not actually moving even as your arms and legs are flailing. And it means you have no way to know if you’re about to smack into the side of the pool (Greenwood served as my spotter from the pool deck, but every time I touched the rough walls of the pool the spell was momentarily broken).

In hopes of improving this, Greenwood said, he and Evans are working on building a positional tracking system that communicates with the phone in the headset and can work underwater; lights and cameras, which can be used on terra firma, don’t work so well in that setting. He won’t give specifics beyond saying it involves sound and magnetics to calculate depth.

Despite the simplicity of the setup, however, it was easy to forget about the outside world and just enjoy the weird virtual one below the surface. I didn’t stay down there too long—the day was ending and the pool was getting cold—but I’d like to go back soon.",Yes,"It can be difficult to tell what's real and what's not. For example, I was swimming with fish, but the water was seeping into the snorkel mask, making it difficult to tell if I was actually moving or not.",https://www.technologyreview.com/2017/02/20/153929/using-virtual-reality-underwater-is-weird-but-fun/,2017-02-20 00:00:00,LABEL_1_relevant,0.6528768539428711,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
14,"In Virtual Reality, Exercise Bike Becomes a Race Car","Inside the headset, VirZoom cofounder and CEO Eric Janzsen challenged me to a race-car race. I leaned to hug corners on tight turns and pedaled faster to speed up my car. When I rolled over what looked through my headset like rougher ground, I was forced to pedal harder to keep up the same pace.

In another game, I became a Pegasus, flying through a world filled with trees and rolling hills. The harder I pedaled, the higher I flew. The game itself was simple and, after a while, a bit boring: I had to find coins and fly through them to gain energy. While I felt my legs growing tired over time, my mind never had time to dwell on the pain of working out.

One VirZoom activity has you drive a race car—you pedal on the bike to control the car’s speed.

Initially, VirZoom will be compatible with three forthcoming virtual-reality headsets—the Oculus Rift, Sony PlayStation VR, and HTC Vive—all of which track head movements to make the leaning motion translate into movement within the game. The bike itself tracks how fast you pedal and can add more resistance based on what avatar you take on or what type of land you travel over.

“We move you through the world in proportion to your effort,” Janzsen says.

The platform will eventually open up to developers, but for now VirZoom is building its first five games from scratch because its team worries the types of motion in existing games could make users sick. That makes sense: early on, leaning too far to the side gave me a twinge of nausea. But as I grew more experienced with the bike, that feeling went away.

Still, purchasing all the gear you’ll need to buy the bike won’t be cheap, and using it will be clunky. While VirZoom will cost $250, buyers will also have to factor in the cost of a headset (still undetermined for the three supported ones, which are all slated for release in 2016). And the Vive and Rift will need to be tethered to a powerful desktop computer, while the PlayStation VR will have to connect to a PlayStation 4.

For now, VirZoom is only planning to sell its bike to people who want to use it at home. Headsets will likely be too expensive and delicate for gyms to lend out to attendees in a spin class at a gym, and I’m guessing people may not be too keen on sharing sweaty headsets—I know mine felt pretty gross when I was done with my workout.",Yes,It isolates users from the real world.,https://www.technologyreview.com/2015/12/16/164519/in-virtual-reality-exercise-bike-becomes-a-race-car/,2015-12-16 00:00:00,LABEL_1_relevant,0.9313523769378662,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
15,The FTC is reportedly probing Meta's VR business for antitrust violations,"Following news that the FTC’s antitrust suit against Meta cleared a critical hurdle earlier this week, the agency is apparently also taking a sharp interest in the company’s VR business.

Bloomberg reports that the FTC and multiple state attorneys general are probing Meta’s virtual reality division for “potential anti-competitive practices.” New York reportedly leads the state-level investigation, which has been chatting up outside software developers who make apps for Meta’s VR experience.

The state and federal officials are examining how the company may have engaged in anti-competitive behavior to suppress competition in the VR market. The officials were also interested in how the company subsidizes the price of its Quest 2 VR headset to push it on consumers and box out the competition, according to Bloomberg.

The fact that the FTC is digging around about Meta’s app store, hardware and software practices suggests that the company’s acquisitions aren’t its only angle in what could be a landmark antitrust case that defines the next era of internet businesses.

In December, The Information reported that the FTC was looking into Meta’s proposed acquisition of Supernatural, a VR fitness app, in a deal worth more than $400 million.

Earlier this week, a judge ruled that the FTC’s major antitrust case against Facebook (owned by parent company Meta) could continue, throwing out the company’s effort to block it. In December, Facebook asked the court to dismiss the suit and pushed for FTC Chair Lina Khan, a proponent of breaking up big tech, to recuse herself.

In that suit, the FTC accuses Facebook of abusing its market power to quell rivals in the social media space and goes as far as asking a judge to make parent company Meta divest itself of Instagram and WhatsApp.

“The facts alleged this time around to fortify those theories, however, are far more robust and detailed than before, particularly in regard to the contours of Defendant’s alleged monopoly,” U.S. District Judge James Boasberg wrote.

“… Although the agency may well face a tall task down the road in proving its allegations, the Court believes that it has now cleared the pleading bar and may proceed to discovery.”",Yes.,"It limits a person's view to a single perspective, which can distort that person's view of the world. Additionally, people who use virtual reality can become so immersed in the experience that they may lose touch with reality, which can have harmful consequences.",https://techcrunch.com/2022/01/14/the-ftc-is-reportedly-probing-metas-vr-business-for-antitrust-violations/,2022-01-14 00:00:00,LABEL_1_relevant,0.9957131147384644,Virtual Reality,virtual_reality,TechCrunch,User Experience & Entertainment
16,Darwin Would Have Been a Pokémon Go Master,"As a teenager, Charles Darwin dropped out of medical school because he was too squeamish to handle amputations. He ended up going to Cambridge instead, where he spent hours in the countryside collecting beetles. That’s to say I think Darwin was a nerd, and if he were alive today, he would definitely be trying to catch 'em all.

This is not an idle comparison, made in an attempt to shoehorn Pokémon Go into yet another headline. Please. Because without beetle collectors, the world might have never had Pokémon.

Pokémon’s creator, Satoshi Tajiri, also was an insect collector. His childhood friends called him Dr. Bug. He grew up outside of Tokyo, but as the city encroached on his hometown, the rice paddies, ponds, and forests where he once wandered disappeared. “Places to catch insects are rare because of urbanization,” he would tell Time. “Kids play inside their homes now, and a lot had forgotten about catching insects. So had I.”

So he created Pokémon: ""a virtual world, bursting with fictional biodiversity,"" as Jon Mooallem puts it in his book *Wild Ones *(Mooallem is also WIRED's Mr. Know-It-All columnist). In the '90s, Pokémon meant trading cards, a TV show, and Game Boys that my friends and I spent hours furiously tapping at home, at school, in the car. But with Pokémon Go, that virtual world has been overlaid on the real world. And in venturing outside to look for fictional biodiversity, people are finding real biodiversity.

Twitter content This content can also be viewed on the site it originates from.

Insect collecting was the Pokémon craze of the Victorian era, and Darwin was an obsessive. One time, he recounts in his autobiography, he encountered a third rare beetle, after having already grabbed a beetle in each fist: “I could not bear to lose, so that I popped the one which I held in my right hand into my mouth. Alas! It ejected some intensely acrid fluid, which burnt my tongue so that I was forced to spit the beetle out, which was lost, as was the third one.” (Amateur move, like trying to catch Mewtwo with a regular Poké ball.)

Collecting beetles at Cambridge trained Darwin’s patience and his eye for detail, which he later put to use meticulously documenting the beaks of finches and the shells of tortoises in the Galapagos. With Pokémon, Tajiri tapped into that same impulse to collect and to catalogue. You don’t need me to tell you, but Pokémon got so popular that in 2002, British scientists found that children were more likely to recognize fictional Pokémon than oak trees or badgers. ""Conservationists need to reestablish children's links with nature if they are to win over the hearts and minds of the next generation,"" the scientists wrote in their piece. ""Is Ecomon the way ahead?"" Almost certainly not. But then, there’s Pokémon Go.

Over the weekend, Morgan Jackson, a grad student at the University of Guleph, watched the Pokémon Go tweets pile up. Jackson grew up playing Pokémon---he played Red, his brother Blue---so he had spent a lot of time collecting fictional creatures. ""Now I’m an entomologist and I do the same thing except it’s real,"" he says. So he had an idea: #PokéBlitz, a twist on ""bioblitz,"" an event when scientists and volunteers go out to survey the wildlife in an area.",Yes,"It encourages people to explore real reality.

The first PokéBlitz took place in Toronto on Sunday. Jackson and his friends walked along the city’s waterfront, finding Pokémon—and real-life creatures. “We saw a ton of geese and ducks,” he says. ""We found a snake, a couple",https://www.wired.com/2016/07/darwin-pokemon-go-master/,2016-07-12 22:15:56+00:00,LABEL_1_relevant,0.8801369071006775,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
17,Google Thinks It Has Cracked the VR Adoption Problem,"Google has already put in a lot of work and money in hopes of bringing virtual-reality technology to the mainstream, through efforts like Google Cardboard—a foldable VR viewer that works with a smartphone—and, more recently, the more capable but still phone-dependent Daydream VR platform.

Yet while over 10 million of the Cardboard viewers have shipped since it was released in 2014, and there are about 150 apps out for the Google-made Daydream headset that shipped late last year, consumer adoption has been slow going. About 10 million headsets shipped globally last year, according to IDC, a market researcher, which is just a tiny fraction of the 1.5 billion smartphones that shipped in the same time frame.

There are lots of reasons why people aren’t buying into the technology. It’s isolating, and there aren’t a ton of things to do. All of the high-end headsets need to be connected to a computer or gaming console, and it’s annoying to feel cords flying around on your back and shoulders when you’re trying to forget about actual reality and explore, say, Mars in VR. And it’s expensive—a typical headset and its required computing platform will cost you anywhere from about $750 to well over $1,000, depending on what you’re buying.

Google is chipping away at the clunky and expensive issues by working with chipmaker Qualcomm to come up with a reference design for a wireless VR headset, and the company says that HTC and Lenovo are working with Google and building VR headsets like this.

The first of these headsets is expected to be available later this year, and Mike Jazayeri, Daydream’s director of product management, expects they will be priced similarly to desktop-connected VR devices today, minus the price of a PC—so, probably around $600 to $800.

“There’s just less friction. Put the headset on and you’re ready to go,” Jazayeri said.

I got to try an older prototype of one of these headsets this week, watching a short scene from the Star Wars film Rogue One to show off another innovation Google will roll out—a software tool called Seurat that can show desktop-computer-quality graphics on mobile VR devices by simplifying a given scene, computation-wise. The headset was fairly comfortable, with a wheel-style adjustment on the back of my head, and the imagery looked impressively crisp, even when I spun around or kneeled down on the ground to see the reflections better on the shiny virtual floor. When I moved too far in one direction or another, the world around me darkened to let me know I shouldn’t go any farther.

Google’s wireless plan could be a big deal for the VR industry. While several wireless virtual-reality headsets have been shown off that seek to marry high-end visuals and head tracking with a wireless design, they haven’t come out yet, and the ones that are wireless, like Google’s existing Daydream View and Samsung’s Gear VR, aren’t nearly as capable and won’t work without a really good smartphone.",Yes,"It is extremely isolating and there are not a lot of things to do in virtual reality. Additionally, virtual reality is expensive and the headsets require a lot of computing power.",https://www.technologyreview.com/2017/05/18/151626/google-thinks-it-has-cracked-the-vr-adoption-problem/,2017-05-18 00:00:00,LABEL_1_relevant,0.9958600401878356,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
18,A Pixar-Inspired Projector That Beams VR Into an Entire Room,"Virtual reality can make you feel like you're high atop the Wall in Westeros, flying like a bird, or running from an alien intent on killing you, but to everyone else, you look like a spaz flailing about in high-tech ski googles. This makes demonstrating a head-mounted display like the Oculus Rift difficult in a group setting. You can set up a monitor that displays what you're seeing, but the result is an underwhelming pair of screenshots. Hardly the immersive experience VR promises.

That's the insight behind a slick projector called Immersis that demonstrates the panoramic power of VR en masse.

Immersis turns an entire room into a display for VR video games. Catopsys

Immersis' projector was designed to look like a cross between the Pixar lamp's evil twin and a tripod from the *War of the Worlds. Its *single fish-eye lens beams a 180 degree VR experience into an interior space. Instead of a pair of small, stereoscopic images, your entire field of view is filled with immersive scenes from a virtual world. It can't provide the simulation of three dimensions that someone wearing the googles enjoys, others in the room experience a scene that spills off the screen and onto walls, doors, the dog, and anything else in the room.

The goal is to foster the expansion of VR while making it less solitary. There are entire genres of party games that wouldn't port well to virtual reality; Immersis helps cross that chasm.

The Immersis projector was designed to look like a cross between an evil twin of Pixar's lamp mascot and a tripod from the War of the Worlds. Its industrial design is slick, but the setup process is a little clunky. To get the most impact from the system, users have to create a 3-D model of the space where the HMD and Immersis will be used. Only then can it apply anamorphosis algorithms to distort the video game graphics so that they'll look correct when projected onto irregular walls and objects.",Yes,"It is a very lonely experience. You are cut off from the world around you and can only interact with the virtual world in front of you. However, there are certain genres of party games that wouldn't port well to virtual reality. Immersis helps cross that chasm by allowing you to share your VR experience with others in the room.",https://www.wired.com/2015/02/pixar-inspired-projector-beams-vr-entire-room/,2015-02-13 02:00:41+00:00,LABEL_1_relevant,0.9909538626670836,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
19,"VR’s True Innovation Isn’t Technological, It’s Human","I've never seen anything in VR quite as realistic as this car. It's like something out of the future, all chrome and curves; the lights of the Berlin platz twinkle off it as it rotates slowly in front of me, and even the writing on its tires is crisp and legible. But when I crouch down to take a closer look, something brushes against my leg. The visual experience is so real, for a nanosecond it seems like there's another person admiring with me. But no—it's the cable connecting my headset to the computer that powers it.

Welcome to VR's leading edge; please don't mind the tether.

Togetherness is play, without a script. And play is movement, either on the micro scale of nonverbal communication or across an entire room.

Today's newest headsets have features that seem to have leaped over from sci-fi. Yet the most amazing of them—from eye-tracking tech to the sleek Scandinavian gøggles I used to ogle the car, which pack in so many pixels the display looks indistinguishable from real life—are units shackled to computers and reserved for corporate use, the so-called enterprise market. In other words, even if you're the earliest of early adopters, you're not likely to be slipping one of these headsets on anytime before 2025's Star Wars Episode XII: Porg-y and Bespin, the Musical. But maybe that's not such a bad thing. As consumers, what we want from VR and what we need from VR don't always overlap—and in the next couple of months, Oculus and HTC Vive will both release all-in-one, fully immersive headsets that make that clear.

If running around a cavernous space playing VR tennis with a human opponent sounds like a trifle, it should; it's a supersized extrapolation of why the Nintendo Wii was such a phenomenon. Photograph: Beth Holzer

“How do you use VR?” is a valuable question with answers that are dueling opposites. It's public or it's private, professional or personal, purpose or play. There's your crux. “Play” means not just the games that spurred VR's early growth but also the increasing desire to share unstructured social time with other people. Togetherness is play, without a script. And play is movement, either on the micro scale of nonverbal communication or across an entire room. That's where the Oculus Quest and HTC Vive Focus Plus come in. Both offer three distinct advantages over nearly every other headset—including those new enterprise models. They're self-contained, so you're not attached by a cable to a computer or game console; they track themselves in space, obviating the external sensors required on earlier systems and freeing you to move about in VR just as you move in real life, with a range limited only by your physical environment; and they extend the headset's “six degrees of freedom” to the hand controllers as well, allowing for realistic use of virtual objects like swords and flashlights.

LEARN MORE The WIRED Guide to Virtual Reality

We've never seen this holy trinity of immersion. I've worn the Quest to play VR tennis on the floor of an expo hall in a space that was 20 feet across and three times as long, sprinting along the baseline while my very human opponent was doing the same about 20 steps away. Our rackets were our handheld controllers; the ball was virtual. We both worked up a sweat, and when he trounced me, the smack talk was as giddy as the game itself. If that sounds like a trifle, it should; it's a supersized extrapolation of why the Nintendo Wii was such a phenomenon. You want VR to go big? Make it something people have never been able to do before—ride a dinosaur, walk around on Mars, deep-sea dive with no equipment—and let them do it with other people.

If you've read a blog comment thread recently, you've likely heard another story. These headsets can't match a PC's performance! Without this resolution or that field of view (and the price tag to match), the technology is a nonstarter. Such critiques miss the point. VR isn't just a technology; it's a conduit of connection. Any headset that brings people together is a perfect headset.

Still, the success of these devices is by no means assured. For all the cord-free, backpack-friendly elements of the Focus Plus, HTC considers it a consumer headset for China only; it will launch in the US strictly as an enterprise device. But while it might seem that the best stuff is reserved for offices, the opposite is true: Those poor schmucks are enjoying lifelike VR, but they're glued to their workspaces. And who wouldn't trade a few pixels for one more precious degree of freedom?

Peter Rubin wrote about Netflix and the rise of interactive TV in issue 27.02.

Content This content can also be viewed on the site it originates from. Get more tech news with our Gadget Lab podcast, available on iTunes and Spotify.

More Great WIRED Stories",,"It makes everything feel more real. Even things that are not actually real. For example, when you are in a virtual world and you see a virtual tree, that tree will feel more real to you than a real tree. The same goes for people. When you are in a virtual world and you see a virtual person, that person will feel",https://www.wired.com/story/vr-true-innovation-not-technological-human-interaction/,2019-04-22 10:00:00+00:00,LABEL_1_relevant,0.9938462376594543,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
20,Bank Failure in Second Life Leads to Calls for Regulation,"The recent collapse of Ginko Financial, a ""virtual investment bank"" in Second Life, has spurred calls for more oversight, transparency and accountability, especially when it comes to business practices in the metaverse.

Last week, Ginko Financial -- an unregulated bank that promised investors astronomical returns (in excess of 40 percent) and was run by a faceless owner whose identity is still a mystery -- announced it would no longer exist as a financial entity.

The declared insolvency meant the bank would be unable to repay approximately 200,000,000 Lindens (U.S. $750,000) to Second Life residents who had invested their money with the bank over the course of its three and a half years of existence.

""You have to remember, there's not a lot of places to put your money in Second Life,"" said Benjamin Duranske, a lawyer who publishes Virtually Blind, a blog that chronicles virtual laws and legal issues that impact worlds like Second Life. ""When you have disposable income and a bank that's promising a 60 percent return on deposits, that alluring -- especially if it works for a while.""

Quickly following this news, a tidal wave of backlash took shape from thousands of angry Second Lifers in SL forums.

Yet in many ways, the undoing of Ginko and the collective loss of more than 750,000 very real U.S. dollars is only the latest event hammering home the fact that the lawlessness of the virtual land has its drawbacks. Indeed, after Linden Lab, the owner and operator of Second Life, invited the FBI to investigate casino activity, the company subsequently instituted a ban on all gambling earlier this month.

On Tuesday, Linden Lab itself issued a statement trying to clarify its stance on regulations and Second Life's virtual economy.

""Linden Lab does not intend to recreate or subvert real-world laws in any way,"" the statement says. ""We caution our residents to be wary of anyone offering extremely high interest rates at no risk, either in the real world or in Second Life -- if it sounds too good to be true, it probably is.""

The response was timely, considering Second Life currently has 20 to 30 banks that operate essentially the same way Ginko did. That fact, plus the large losses associated with Ginko, has led to a growing call for even more transparency and regulation among SL residents.

Duranske is at the forefront of the movement. An intellectual property lawyer who's taking time off from his practice to work on a book about virtual law, he was one of the first people to jump on the shady business dealings at Ginko Financial more than two months ago. Indeed, his blog has one of the most comprehensive accounts of Ginko's downfall.

Duranske claims he's personally talked with a few SL residents who have lost as much as $10,000 in the Ginko scheme, but estimates that the majority lost a more moderate amount--somewhere in the range of $50 to $100.

""A lot of people forget, Second Life is governed by U.S. law and the laws of California,"" Duranske explained. ""It just so happens that these laws haven't been enforced.""",Yes,"It's ungoverned. The problem is that people don't realize that, and at the same time, the legal system has yet to catch up to the technology.

""I think it's a good idea to have some transparency,"" said Duranske. ""Most of the people that are upset with Ginko Financial are upset",https://www.wired.com/2007/08/bank-failure-in-second-life-leads-to-calls-for-regulation/,2007-08-15 06:00:00+00:00,LABEL_1_relevant,0.9964454770088196,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
21,A Holodeck Videogame Designed to Train Soldiers,"If the Army is going to keep up with military readiness pressures while also seeing a reduction in troop size and stagnant budgets, then it's going to need more cutting-edge videogames.

Last week, the Army Contracting Command issued a Sources Sought notice for companies interested in demonstrating ""mature technologies"" for military training. Northrop Grumman thinks its Virtual Immersive Portable Environment (VIPE) Holodeck just may be the answer.

Using commercial, off-the-shelf hardware combined with gaming technology, the VIPE Holodeck 360 degree virtual training system provides users with a high-fidelity immersive environment with a variety of mission-centric applications, including simulation and training, mission rehearsal and data visualization. The VIPE Holodeck can support live, virtual and constructive simulation and training exercises including team training, cultural and language training and support for ground, air and remote platform training. Off-the-shelf technology keeps costs down while also making future upgrades and maintenance easy to perform and affordable, says Northrop.

Last year, the VIPE Holodeck took first place in the Federal Virtual Challenge for the navigation category, an annual competition led by the U.S. Army Research Laboratory's Simulation and Training Technology Center, for the system's Kinect integration navigation sensor, which gives users the ability to crawl, walk, run, stop, jump, move side to side in the immersive environment.

Where the VIPE Holodeck moves ahead of other virtual simulators, according to Northrop, is advanced situational training where service members can walk through an area of operation in the replicated virtual environment to prepare themselves for what they may encounter in that exact environment in reality. To enhance that training, operators can drop threats into the environment, including IEDs and enemy shooters, as well as signals that should tip them off to potential threats and see how they respond before they actually find themselves in that situation.

And it's precisely that versatility that the Army is after.

""For us to be able to execute realistic training -- good training -- we have to be able to bring that operational environment"" into the virtual world, said Brig. Gen. Michael Lundy, deputy commanding general at the Army Combined Arms Center, during the AUSA Aviation symposium earlier this month. ""We want to get away from having multiple environments, virtual gaming and instruction, and go to one synthetic environment, get to a lower overhead and integrate the full operations process ... according to the common operating picture.""

""The great thing about virtual reality and gaming technology [is that] it's moving so rapidly that really it has endless possibilities that we can do,"" said Ryan Frost, Northrop's program manager for the VIPE Holodeck. ""If you can think it, we can create it, eventually.""

Northrop says it's also exploring options for VIPE as a stepping stone to live-training within the medical field, as well as law enforcement and first responders for situations such as live-shooter or hostage scenarios.",Yes,"It might lead people to question reality.

Some people might argue that virtual reality could lead people to question what is real and what is not. If people are constantly exposed to simulations that are very realistic, they might start to question the reality of their own lives. This could lead to a sense of detachment from reality and a feeling of isolation",https://www.wired.com/2014/01/holodeck/,2014-01-28 14:30:07+00:00,LABEL_1_relevant,0.9952788352966307,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
22,Virtual Reality Actually Feels Real When It Uses Physics,"There is already some such software meant to simulate physics in two-dimensional video games, but He argues that’s focused more on simulating environments and reproducing how shapes like balls and triangles collide with each other in order to approximate things like explosions. He says his company concentrates on re-creating the bones, skeletons, and underlying structures of things that can walk or otherwise move, whether they’re virtual humans, dogs, robots, or cars, and simulating how they should get around and what should happen when a user pokes or prods them.

He hopes his company’s software finds uses beyond just games in VR. For instance, he thinks it could make it feel more natural to socialize with other people’s digital avatars in virtual environments—something that’s in its infancy as companies like AltspaceVR let users of headsets like Vive and the Oculus Rift connect with others.

He says the company will soon start letting some developers try out a beta version of part of its software, and he expects the rest to be released next year. The company is also planning to make VR games and put them online so anyone with a headset can try them out.",Yes,It may make it feel more natural to socialize with other people's digital avatars in virtual environments.,https://www.technologyreview.com/2016/06/08/8147/virtual-reality-actually-feels-real-when-it-uses-physics/,2016-06-08 00:00:00,LABEL_1_relevant,0.9927712082862854,Virtual Reality,virtual_reality,MIT Tech Review,User Experience & Entertainment
23,The UN's New VR Doc Exposes a Forgotten Crisis,"The United Nations will, unfortunately, never run out of international issues to address. Whether in response to war, viruses, refugee crises, or other urgent situations, the agency's peacekeeping and humanitarian aid efforts are ongoing worldwide almost constantly. That means a lot of help in a lot of places—and it also means that public perception of the UN’s work is often defined by the crisis of the moment. The organization’s latest virtual reality documentary was made to change that.

Ground Beneath Her is about Nepal, and specifically about how one family is rebuilding after the 2015 earthquake that killed more than 8,000 people and left some 3 million displaced. It doesn’t take viewers into the aftermath; it just shows them one 14-year-old girl trying to help her family rebuild. Gabo Arora, the UN creative director who made the film, did so at the behest of the UN Development Programme, which has been working in the region since the quake—and needs help.

“The UNDP said, ‘We really would like to help you fund something,’” Arora says. “And I said, ‘What would you like me to do?’ And they said, ‘Nepal, because we’re still there a year later and we’d like to shine a light on this issue.’”

That’s why Arora’s experience, which you can download today on the UNVR app, is a time-lapse of the life of Sabita, a young girl who has taken up blacksmithing to help her family. The trade, which is normally considered the work of the lower classes in Nepal, has become invaluable as part of the rebuilding efforts, but doing it has pulled Sabita, who wants to be a doctor, away from her education.

The VR film itself is a way for people to experience the life of someone surviving after a disaster; the bulk of it just allows viewers to watch Sabita as she completes the many tasks of her day. It’s not terribly exciting, but it’s a good reminder that areas in crisis still need assistance even after a crisis is no longer in the headlines. (The experience also ends with a call-to-action for people to donate to rebuilding efforts.)

That approach is slightly different than some of the UN’s other VR films, like Waves of Grace, which looked at Ebola in Libera, or My Mother’s Wing, which followed a family living in Gaza. “Nepal is a challenge because Syria is topical, Ebola is topical, Gaza [is topical],” Arora says. “Nepal wasn’t in the news, and I think that’s why we tried to take a little bit more of an artistic approach with it.”

Looking at Arora’s work with UNVR a trend becomes clear: his subjects have all been women. This is mostly a coincidence, but the director has his reasons. “We’re here to tell stories of people who are generally more marginalized.” he says. “But when I got into VR, I realized it’s very unforgiving to ugliness—and a lot of women in these situations show grace and power and strength.” And for the UN, helping young women like Sabita ensures that that grace and power lasts for generations to come.",Yes,It can be used to tell very powerful stories about people who are generally more marginalized. Arora's work with UNVR has shone a light on this trend by focusing on the stories of women who have been affected by crises like the 2015 Nepal earthquake.,https://www.wired.com/2016/12/un-vr-nepal-doc/,2016-12-15 13:30:28+00:00,LABEL_1_relevant,0.9941383600234984,Virtual Reality,virtual_reality,Wired,Equality & Justice
24,Your First Look at Oculus' Adorable New Filmy-Gamey Thing,"Content This content can also be viewed on the site it originates from.

Nearly six months ago, Oculus came to the Sundance Film Festival to announce its new ""storytelling"" division, Oculus Story Studio. They screened a compelling, Pixaresque short called Lost, and claimed that they were hard at work on five titles. At the time, that workload for a small crew of creatives sounded a little ambitious---even if those creatives were part of a multibillion-dollar Facebook-owned virtual reality company.

Maybe we shouldn't have doubted.

This morning, Oculus announced the forthcoming premiere of its second VR interactive film project. Henry is the tale of a hedgehog who just wants someone to hug him---and from that logline alone, it's clear that Pixar's DNA is alive and well at Oculus Story Studio. Not only do Pixar alumni Saschka Unseld and Max Planck head up the division, but Ramiro Lopez Dau, the director of Henry, worked as an animator on Brave, Cars 2, and Monsters University.

Henry premieres July 28 in Hollywood, and we can only assume it will be available for home viewing early next year when the consumer version of the Oculus Rift goes on sale.",Yes,"It will lead to the development of more immersive and realistic content. This could have a profound impact on the media and entertainment industry, as well as the way we consume content.",https://www.wired.com/2015/06/oculus-story-studio-henry/,2015-06-02 16:41:53+00:00,LABEL_1_relevant,0.9841280579566956,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
25,Virtual Reality Comes To Pornhub | Crunch Report,"Virgin teams up with Boom for supersonic jets, Google annouces machine learning, an AI-powered Microsoft bot, Cor for at home blood testing, and Pornhub’s virtual reality section.",Yes,"It can be used to create realistic simulations of real-world environments for training purposes. However, there are also ethical concerns about how realistic these simulations can become and whether or not they should be used for training purposes.",https://techcrunch.com/video/virtual-reality-comes-to-pornhub-crunch-report/,2016-03-23 20:00:26,LABEL_1_relevant,0.9945529103279114,Virtual Reality,virtual_reality,TechCrunch,Environment & Sustainability
26,Naked and Unafraid to Exercise in Virtual Reality,"I can without bashfulness say that I have logged hundreds of hours on Beat Saber as naked as the day I was born. It was neither fetish nor high-concept performance art; it grew from simple need.

In the spring of 2018, I was trying to keep a VR startup afloat, navigating a never-ending flurry of emails, Slack messages, video calls, and tweets from the confines of my efficiency in San Mateo, California. Though typically an active person, I’d deluded myself into believing that I just didn’t have the time for luxuries like the gym and “mental health.” I did, however, make a special dispensation for staying abreast of industry happenings, and ever since a video of my friend SwanVR had gone viral, I’d been itching to play Beat Saber.

Once I started, I couldn’t stop. It quickly became habit; any time I got stressed, I’d shove my chair away from my desk and fire it up—sometimes for 90 minutes or more. In a time when I was sure I didn’t have the time for fitness, I began to establish a daily routine. My story is one of many—with folks from all over the world losing as much as 138 pounds by turning titles like Audioshield, Audio Trip, BOXVR, Creed: Rise to Glory, Holopoint, OhShape, Racket: NX, Synth Riders, Soundboxing, and Thrill of the Fight into formal exercise routines.

Re: the nudity, the laundry machines in the basement were coin-operated, and I just didn’t want to deal with sopping clothes stinking up my small apartment. I’d often muse, though, how bizarre it might have looked if anybody had witnessed it: some gangly white guy strapped into a VR headset, unselfconsciously flailing around a cramped space.

Somewhat in juxtaposition to VR’s oft-touted capacities to build empathy through embodiment, my TMI anecdote points to the power of disembodiment. So much of the modern fitness experience is wrapped up in appearance, which in turn opens the doors for insecurity and shame. It’s not just when we feel embarrassed about our out-of-shape bodies—though of course many are deterred from going to the gym because of self-esteem issues—it’s a whole host of other factors. Some are social. Am I sweating too much? Am I wearing the right clothes? Am I being laughed at? Some have to do with our sense of identity. Will I look incompetent? What if I use a machine incorrectly? I used to be able to do x at y ability—how did I let myself go? Others can be even more practical, like how often we trust ourselves to actually do laundry.

Peloton, of course, has capitalized on these concerns, building a lifestyle community around its $2,300 stationary bike (plus $40 per month subscription service). The $1,500 Mirror, a literal mirror with built-in text and video overlay functionality, is seeking to expand the trend to categories like yoga, dance, boxing, and other cardio classes. It’s easy to see how private machinery addresses many of the obstacles that keep folks from hitting the gym. Where Peloton or Mirror might offer ongoing streams of new classes, though, they are fixed physical platforms; VR fitness differentiates itself in letting users step outside the existing reality paradigm. Any mix of features can be included in an immersive game environment—and that includes how bodies are represented. In Beat Saber, the only visual correlation of your body are the two sabers, which are not “in” your hands so much as they are your hands.",Yes,"It can lead to people becoming more self-conscious about their bodies. In particular, VR can make people feel embarrassed about their sweatiness, their clothing, and their physical abilities. Additionally, VR can make it more difficult to do laundry.",https://www.wired.com/story/virtual-reality-fitness-supernatural/,2020-04-30 13:00:00+00:00,LABEL_1_relevant,0.9948194622993468,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
27,Oculus Rift Plus Motion Controls Make Shooting Almost Too Realistic,"Content This content can also be viewed on the site it originates from.

The Oculus Rift virtual reality headset display is still without a release date, but early versions have already blown us away with the device's ability to immerse wearers in virtual worlds.

But there's still a big question facing the team at Oculus: What is virtual reality input?

Right now, you can play games while wearing the Oculus headset while seated in front of your keyboard and mouse, or while holding a controller. But when your display gives you the ability to look up, down and all around you with lifelike head tracking, will we want a more immersive, realistic type of input as well?

Teddy Lipowitz, a developer from Australia, has put together a demo of the sort of thing we might play in the near future. Lipowitz, 32, posted a demo on the Oculus Rift forums of a cover-based first-person shooter that uses motion controllers.

The simplistic demo pits a single player with a gun against an battalion of alien soldiers. Physically ducking with your body lets you hide behind crates to dodge their blasts, and Lipowitz's setup allows for precise control of the gun.

The demo is so realistic, he says, that some players get a little too immersed.

""Please be careful when playing this demo,"" Lipowitz wrote on the forums. ""Lots of people try to support themselves on the crates when they try to stand up or when leaning up against a column, but find there is nothing there!""

The motion controllers aren't just for gun control; Lipowitz' demo uses them to track the movement of the player's torso. The Oculus Rift tracks your head, not your body.

The demo makes use of Razer's Hydra motion controllers, two Wii-style wands that are sold as a bundle.

By strapping one of the wands to his chest and using another as a gun, Lipowitz tracks his head, torso and arm movements. He can look around while walking in a straight line, and the Rift headset and Hydra communicate so things don't get out of sync.

""Once I had that part figured out,"" Lipowitz told WIRED in an email, ""the rest was pretty easy. 3-D games are really built for this kind of thing!""

Lipowitz says that he's tweaked things at a software level about as well as he ever could, and that new hardware will have to be developed to properly fix some of the remaining issues, like the jittery gun controls.

""Cables are the biggest problem at the moment,"" he says. ""You're limited to a pretty small play space, and if you turn around too many times you'll get tangled. I've built my levels so you're mostly facing towards your desk, which makes the layout quite linear.""

Lipowitz's demo is available for free download, and he has no plans on selling his code, saying he's just ""very excited about the reincarnation of Virtual Reality.""

While the consumer version of the Oculus Rift is not yet on shelves, the development kit version is available for $300 from Oculus, with new orders slated to ship in September.",Yes,People get so immersed in the experience that they forget their surroundings. This can lead to people accidentally hurting themselves if they're not careful.,https://www.wired.com/2013/08/oculus-rift-hydra-demo/,2013-08-05 13:30:16+00:00,LABEL_1_relevant,0.9546210169792176,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
28,VR Ads Are Almost Here. Don’t Act Surprised,"Imagine, for a moment, that you’re inside a virtual reality world. You look to your left and an open door appears, beckoning you to walk through it. You enter and suddenly find yourself in the middle of an advertisement—a branded world you can explore and manipulate. Forget banner ads and auto-playing videos. The advertisement of the future is immersive, almost game like. And it's nearly here.

Last week, Unity, the world’s largest VR development company, announced Virtual Room, a new type of interactive advertisement it plans to roll out later this year. Unlike the standalone VR marketing experiences you've seen before, Virtual Room ads will surface across a wide network of VR apps, similar to the display ads you see on your laptop or the video ads you see while playing games on your phone. Don’t act surprised—you knew this was coming.

Like any new medium, VR must be monetized, says Tony Parisi head of Unity's VR and AR strategy. “Developers want to make money,” he says. “We want to help them do that.“ Until recently, though, it was unclear how, exactly, that would happen. Most VR developers make cash from micro transactions and in-app purchases, but that's pennies compared to what advertising can bring in.

Google

To capitalize on the opportunity, other VR companies have already begun exploring what ads might look like in this new medium. In a recent blog post, Google showed off a concept for a floating cube that players can tap or gaze at to start video advertisements—sort of like auto-play, but in VR. The format is simple by design. “VR ad formats should be easy for developers to implement, native to VR, flexible enough to customize, and useful and non-intrusive for users,” writes Aayush Upadhyay and Neel Rao of Google’s Area 120.

Google's vision nods to the legacy of existing digital advertisements, which many in the ad industry are trying to avoid. ""Our hope is that VR creates new opportunities that don’t replicate advertising the way that mobile originally did with repurposed 30-second TV spots,"" says Eric John, deputy director of video at the Interactive Advertising Bureau, the organization responsible for setting guidelines for web, mobile, and now VR advertisements.

The IAB is working with Unity to build guidelines around Virtual Room, which outline everything from how long an ad should play to how often it should surface. For now, Unity says branded content will be opt in for both developers and players. It will surface for no more than two minutes every hour, and those ads will be highly targeted as to not alienate the person interacting with them. They’ll likely pop up at points of friction—when a player is having trouble or after a level is completed—similar to ads on mobile games and television.

What will change is how the player interacts with advertisements. People won’t passively watch Unity-created ads; they'll play with them. “Ads are usually linear,” says Julie Shumaker, VP of business development at Unity. “And there's nothing linear about VR.”",Yes,It isolates individuals from the real world.,https://www.wired.com/story/vr-ads-are-almost-here/,2017-07-21 11:00:00+00:00,LABEL_1_relevant,0.9947293400764464,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
29,VR Is Back in Pop Culture—As a Warning,"Virtual reality has never been a subtle technology—and the same goes for its treatment in popular culture.

When VR jumped its sci-fi firewall to sweep through Hollywood in the 1990s, it was a thing of maximalist, fantastic promise: brushed steel headsets and space-age gloves on the outside, swirly colors and stylized hacker dreams on the inside. Think Keanu Reeves playing virtual cat-and-mouse with the yakuza in Johnny Mnemonic, or Michael Douglas pulling off corporate espionage in the world's most grandiose database in the wow-did-that-age-terribly, quasi-erotic thriller Disclosure.

After a time, VR even showed up on four-quadrant TV like Murder She Wrote and Mad About You, a perfect symbol of fantasy and the future. Eventually, though, the future proved to be so far away that the fantasy withered, and VR took its place alongside Pogs and gratuitously transparent beverages as a craze that was doomed never to see the next millennium. Except that's not exactly what happened. VR staged a comeback, becoming something that people could actually do, not just dream about—and something that lived again in pitch meetings and development pipelines.

So now, more than 20 years later, VR is coming back to screens. It's still a symbol of the future, and it's still the realm of fantasy. But in two new shows, VR's treatment suggests that society may not have the same appetite for those things as it once did.

Kiss Me First, which hits Netflix today after a run on British television earlier this year, started its life as a YA novel, albeit one that had nothing to do with VR. Leila (Tallulah Haddon), a young woman whose mother has just died, spends much of her time fighting with friends in Azana, an large-scale VR game where she's known as ""Shadowfax."" Soon, beckoned by another player named Mania (Simona Brown), she stumbles upon a world inside the world; Mania's friend Adrian (Matthew Beard) has hacked Azana to carve out a hidden paradise. The angsty misfits who congregate there call it ""Red Pill,"" a Matrix reference that might be resonant if grosser corners of the internet hadn't already tainted it.

Not only has Adrian cobbled together neckbands that allow Red Pillers to feel physical pain in VR, but Leila suspects that he's using his charisma—which seems to revolve around a preternaturally velvety voice and tucked-in sweaters—to seduce them into making perilous real-life decisions. As she navigates a lurching friendship with Mania in the real-world, where she's actually named Tess, Leila has to validate her suspicions and find a way to put an end to Adrian's psychological puppeteering.",Yes,"It makes it very easy to exploit people.

While that premise might sound like a thinly veiled cautionary tale, Kiss Me First isn't interested in foretelling the future. Rather, it's a coming-of-age story that happens to be set in a virtual world; VR serves as a metaphor for the way we often use the",https://www.wired.com/story/virtual-reality-reverie-kiss-me-first/,2018-06-29 16:00:00+00:00,LABEL_1_relevant,0.9843199849128724,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
30,"""Problematic Wi-Fi Access"": Apple Bans Augmented Reality App Sekai Camera [Update]","More and more apps are stricken from the App Store as of late, for a variety of reasons. Today, the Apple hammer hit Tokyo-based Tonchidot whose augmented reality app Sekai Camera was removed without warning.

The free app, which made its – memorable – debut during TechCrunch 50 in 2008, intends to help users “tag the world” by imposing information (text, pictures, video and audio) over images in the iPhone camera.

Sekai Camera uses GPS to locate the user and measure the distance to objects in the vicinity. The problem for Apple: in places with weak GPS signals (i.e. within buildings or underground), the app relies on PlaceEngine, a technology that makes it possible to determine a user’s location via Wi-Fi. Once it’s installed on a device, the PlaceEngine client collects Wi-Fi signals from nearby access points and sends the information to a server, which then estimates your location.

PlaceEngine provider Koozyt says on its Japanese website Apple changed their policy regarding “the way apps access Wi-Fi devices”. Not only Sekai Camera, but a number of other PlaceEngine-powered apps were kicked out, too (including Yahoo! Maps for the iPhone). Details are still unclear at this point, with Koozyt saying they are currently looking into the issue.

More information on the peculiar ban are available at The Register.

Tonchidot itself cites “issues regarding the App Store review process” as a reason for the ban of Sekai Camera version 2.1.1, adding the app will make a comeback in version 2.2 “soon”. I wouldn’t be too surprised to see PlaceEngine get scrapped altogether.

In Japan, Sekai Camera is the most downloaded iPhone app to date, and it was elected “Best App in 2009″ by Apple Japan late last year (it was released worldwide in December).

We reached out to Tonchidot for a comment and will update this post when we have it.

Update with Tonchidot’s response (edited):

We have also received sympathetic words from many people across the

world, and really appreciate everyone’s support.

We always strive to comply with Apple’s policies, and will fix and

resubmit a version that satisfies their current requirements. We do

not know what the exact reason is for their recent decision, but

Koozyt, among others, have been extremely helpful in helping us

identify the potential cause.

No worries, will will have a new and better version back up on App

Store very soon, so please look forward to it ! :)

Go Koozyt! Go Sekai Camera!","Yes, the article discusses the potential consequences of augmented reality on society.","It takes people away from the real world. This could lead to people becoming more anti-social, as they would rather spend their time in a virtual world than interacting with real people. Additionally, virtual reality could lead to people becoming addicted to the escapism that it provides, which could have negative impacts on their mental and physical health.",https://techcrunch.com/2010/03/04/tonchidot-sekai/,2010-03-04 00:00:00,LABEL_1_relevant,0.9966495633125304,Virtual Reality,virtual_reality,TechCrunch,Health & Wellbeing
31,Insenth Focuses AR Glasses On Cyclists,"It’s clear that the first version of Google Glass had an identity crisis — unless you count the Scoble shower incident. But let’s just call that a crisis… One problem with the wearable was there was no clear consumer proposition, so no reason for the public to want to sport the smart specs.

Google is apparently reworking Glass now, likely with a narrower, enterprise focus. And that makes a lot more sense. The traction that Glass had gained was for niche industrial use-cases. Mountain View is not the only company thinking this way either — Sony is also building smart specs with an enterprise bent.

So while VR headsets like Facebook’s Oculus Rift take aim at the gaming market, dangling the prospect of immersive entertainment (assuming you’re able to forget what a dork you look like IRL), makers of augmented reality specs are going after more specific and specialized niche use-cases.

To wit: meet Senth In1 — a pair of AR glasses, based on Android 4.3, being developed by a Beijing-based startup, specifically for cyclists. The team is currently raising crowdfunds via the Indiegogo platform with the aim of turning their prototype into a shipping product. At the time of writing they’re 85 per cent funded, with just over $34,000 pledged.

The glasses are being designed to work in conjunction with a thumb controller which is mounted on the bike’s handlebars, enabling the wearer to control a ring-based UI that allows them to choose different functions by swiping around a circle and pressing to select. The specs will also support other interaction methods, including a lateral touchpad on the side of the glasses, voice control and motion-sensing head movements for UI selections.

In terms of features the team is aiming to develop for their Insenth OS (atop the Android base), users will apparently be able to view their distance and speed via the glasses’ waveguide display, as well as link the hardware via Bluetooth to other sensing wearables they own to view other metrics such as heart rate or cadence; plus view maps and navigation directions as they ride; take photos and videos via an onboard camera; select and play music; and make phone calls and send voice messages.

The prototype is actually the fourth developed by the team, according to co-founder Li Jiwen, the other three being professional AR glasses. But with limited funds to support ongoing R&D — the team has apparently been working on AR glasses since 2012 — he says he made the decision to narrow the focus to try developing smart specs for cycling, which was also a personal interest.

The advantage of this sort of focus is of course having an established community to tailor the tech for and market the product to, rather than attempting to sell a vague general promise of AR’s benefits. Add to that, cyclists are already used to kitting themselves out in specialist gear — and their kit isn’t what the mainstream would recognize as fashionable, so there’s less likely to be any stigma associated with wearing a face computer out and about.

If the team meets its crowdfunding target, it says it’s aiming to get the Insenth In1 smart specs to market starting from this December — albeit that’s a pretty ambitious timeframe for such a complex product (and where hardware crowdfunders are concerned it’s always wise to expect delays).

Insenth’s glasses are being priced at $250 to Indiegogo early birds, undercutting the price of Recon Jet‘s rival smart specs which are also focused on the extreme sports space.",Yes,It isolates people from the real world. This could lead to people becoming more introverted and less likely to interact with others in the real world.,https://techcrunch.com/2015/08/12/insenth-focuses-ar-glasses-on-cyclists/,2015-08-12 00:00:00,LABEL_1_relevant,0.9967124462127686,Virtual Reality,virtual_reality,TechCrunch,"Information, Discourse & Governance"
32,Google Cardboard's New York Times Experiment Just Hooked a Generation on VR,"Yesterday my Facebook feed filled up with pictures of friends' kids clutching cardboard boxes to their faces. Well, I should say, Cardboard boxes.

That's because subscribers to The New York Times' Sunday print edition received a Google Cardboard virtual reality headset, wrapped in the standard-issue blue plastic bag, as part of the Times' rollout of its own VR content.

Cardboard isn't much to look at. It's a bit of corrugated, yes, cardboard and some velcro that you fold to create a slot for your smartphone and a pair of flaps to block your peripheral vision. Inside is the crucial component, the pair of cheap plastic lenses that that transform the flat, doubled-up images on your phone's screen into the illusion of an immersive 3-D environment.

Kids who've had the VR experience have a new set of expectations of what it should mean to interact with a computer.

But Cardboard's crudeness is also its genius. It's cheap enough to be handed out for free; we smartphone users supply the only part that's expensive. The Times and Google could afford to drop about 1.3 million of them in the newspaper. That's 1.3 million people who said to themselves yesterday, ""Wait, you mean this VR thing is something I can have right here, right now, too?""

Okay, I'm sure that among Times subscribers, several were savvy enough to already have some kind of VR rig on hand and have been probing the virtual depths for a while now. But embarrassing confession time: I'm an editor at WIRED—you know, where we cover the future—and it just hadn't sunk in that VR was something I could do, too. Yes, a bit of that was brand blindness; Samsung has been pushing its own Gear headset for a while, but no highly visible headset targeting iOS users has emerged yet. In fact, when I asked our Gear team what I could use to watch VR on an iPhone, the response was, ""It's basically just Cardboard.""

Whatever the reason for my myopia, it was awfully convenient that, just a few days after I started idly searching Cardboard options on Amazon, one showed up in my driveway. I suspect that, like many of those other 1.3 million, the first thing I did was to put it on my kid. And I'm pretty sure that means everything.

New Is Normal

If you're my age, the first thing I bet you thought when you heard VR was making a comeback was, ""Wait, didn't they try that in the '90s?"" Then you experience today's version, and you discover that VR's current incarnation is not what you experienced at that cyber café back when we were still calling things ""cyber.""

If you're a kid, on the other hand, there's a good chance you've grown up assuming that portable touchscreen portals to a significant portion of human knowledge, entertainment, and communication are a given. Yes, you think your dad's iPhone is pretty cool. But then yesterday you put on Google Cardboard and watched a train come hurtling toward you before you flew up into the sky and into the embrace of a giant baby. And you said, ""Yeah, now we're talking.""

I don't know what the exact year is, but I believe that up to a certain age, any technology a kid encounters registers as ""normal."" To me, a world without color TV or personal computers is an abstraction. For a host of kids as of yesterday, so is a world without VR.

This is why distributing something as unpolished as Google Cardboard in a way that's as gimmicky (and anachronistic) as handing it out with a newspaper turns out to be such a big deal. Sure, we're talking about a tiny subset of kids. But they'll tell their friends. Their parents are already telling their friends. And a technology that once seemed remote is suddenly accessible.

And in the case of this particular technology, accessibility translates almost immediately into visceral intimacy. Experiencing VR for the first time isn't just cool; it's revelatory. This is why so many of us made sure to capture the moment of our kids' first encounter. Most parents, I hope, don't make videos of their kids' reactions when we unbox our latest iPhones. But I believe we had a collective sense that our kids were experiencing something meaningfully new—not just an encounter with a new technology, but with a new way of relating to technology.

Especially as a medium for non-fiction, I believe the hype that VR can act as a powerful empathy engine, a uniquely direct way to put us in someone else's world. This makes me hopeful that VR will become much more than the next level of escapism for an already screen-addled generation. I know that's some serious parental wishful thinking. But for good or ill, Google Cardboard is just good enough to imprint a new paradigm on a nation of 8-year-olds. From now on, kids who've had the VR experience have a new set of expectations of what it should mean to interact with a computer. Imagine what they'll expect by the time they're 18.",Yes.,It may have a profound impact on the way kids interact with technology.,https://www.wired.com/2015/11/google-cardboards-new-york-times-experiment-just-hooked-a-generation-on-vr/,2015-11-09 22:32:41+00:00,LABEL_1_relevant,0.9958951473236084,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
33,Figure Out What That Button Does With Audi's Augmented-Reality App,"We enjoy a grease-stained Chilton's repair guide as much as the next gearhead, but the idea of a traditional owner's manual is getting outmoded. Audi agrees, and its latest app gives A3 owners all the information they need without ever having to open the glove box.

Using an augmented-reality app designed by Metaio, anyone with an iPhone and an A3 can get instant information about everything from the location of the coolant reservoir to what the hell that button is on the dash.

The eKurzinfo app recognizes over 300 different elements around the A3, and displays pop-ups with short, simple explanations of what things are, what they do, and how to service them. The app uses both 2-D and 3-D tracking technology to understand what the owner is looking at, and since eKurzinfo is linked with Metaio's cloud-connected infrastructure, there's no need to update the app – new information or user-requested features can be pushed directly to the device.

You can download eKurzinfo from the App Store now, and despite its name, the information is available in English and Japanese, as well as German.",Yes,It blurs the lines between what is real and what isn't.,https://www.wired.com/2013/08/audi-ekurzinfo/,2013-08-12 16:00:27+00:00,LABEL_1_relevant,0.9955024123191832,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
34,Darpa Turns Oculus Into a Weapon for Cyberwar,"For the last two years, Darpa has been working to make waging cyberwar as easy as playing a video game. Now, like so many other games, it's about to get a lot more in-your-face.

At the Pentagon Wednesday, the armed forces' far-out research branch known as the Defense Advanced Research Projects Agency showed off its latest demos for Plan X, a long-gestating software platform designed to unify digital attack and defense tools into a single, easy-to-use interface for American military hackers. And for the last few months, that program has had a new toy: The agency is experimenting with using the Oculus Rift virtual-reality headset to give cyberwarriors a new way to visualize three-dimensional network simulations--in some cases with the goal of better targeting them for attack.

""You’re not in a two-dimensional view, so you can look around the data. You look to your left, look to your right, and see different subnets of information,"" Darpa's Plan X program manager Frank Pound told WIRED in an interview. ""With the Oculus you have that immersive environment. It's like you’re swimming in the internet.""

In its demo setup, complete with two motion-sensing Razer Hydra controllers for navigation, the user does more than swim. As captured in the video below showing an Oculus user's view, Darpa's proof-of-concept begins with a collection of ""missions"" to choose from, each of which is represented by a spherical network of computers. Select one, and you're presented with a planned series of actions to carry out--like scanning a certain network or probing target endpoints for vulnerabilities--and a collection of tools to use, represented by different abstract icons. Then you're thrown into the network to carry out the mission, while the enemy launches attacks like distributed-denial-of-service bombardments back at the user.

Content This content can also be viewed on the site it originates from.

If all of that seems more than a little contrived, Pound admits that the Oculus demo is only a ""notional"" proof-of-concept, created by the San Francisco design firm Frog Design and the Austin-based simulation software company Intific. But Darpa is serious about integrating the virtual-reality headset into its plans; It's already shown the Oculus to Congress and to the Pentagon's Joint Chief of Staffs in private demonstrations, and will be experimenting further with the second developer version of the device set to be released later this summer.

If Plan X's Oculus software ever reaches the eyeballs of actual soldiers--a development that Darpa says is still years away--Pound doesn't deny that the interface would be used for actual offensive hacking as well as defense and reconnaissance. Like the rest of Plan X, he says it's meant to be a simpler and more intuitive way for the U.S. Cyber Command and other American military hackers to visualize everything they do in their cyberwar operations. ""Think of Plan X like an aircraft carrier,"" says Pound. ""It can carry any weapon system or capability.""

That sort of admission will no doubt set off alarm bells for critics of the American military's increasingly aggressive posture on the Internet. The revelation in 2012 that the United States created the Iran-targeted Stuxnet malware and a year of Edward Snowden's leaks have already demonstrated that the NSA engages in more advanced cyberattack operations than practically any country on the planet. Enabling American hackers to launch those attacks with a tool that's literally designed for video games could be seen as encouraging a brazen attitude towards cyberwar, disconnecting it from the reality of its consequences.",Yes,It takes away the reality of cyberwar and its consequences.,https://www.wired.com/2014/05/darpa-is-using-oculus-rift-to-prep-for-cyberwar/,2014-05-23 10:30:34+00:00,LABEL_1_relevant,0.99619859457016,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
35,"Magic Leap, and the Troubles In Sexism Valley","Magic Leap, the secretive augmented reality tech startup that's valued at $4.5 billion (and reportedly bores Beyoncé), settled a sex discrimination lawsuit this week. The plaintiff, Tannen Campbell, a former vice-president of strategic marketing, was hired to make the company's product more appealing to women. Campbell filed a notice of settlement Monday in federal court in Florida, Magic Leap's home state, and the terms of the settlement are confidential. (Representatives for both Campbell and Magic Leap said they couldn't comment.) If all goes smoothly, the suit will officially end by the beginning of next month.

But Magic Leap's problems won't. Since the company's founder, Rony Abovitz, appeared on WIRED's cover a year ago, Magic Leap has faltered, beleaguered by bad press and allegations of unfulfilled promises. That's a long way from 2014, when Silicon Valley was all abuzz over the stealthy startup. Google, Chinese e-commerce giant Alibaba, Andreessen Horowitz and other titans of venture capital all rushed to invest in the company and its “mixed reality” software that lets people see virtual 3-D objects superimposed over the real world. Despite garnering $1.4 billion in funding to date, however, Magic Leap has found itself facing a slew of accusations that it may have wildly over-promised on its tech. Questions now abound over whether the company will ever deliver a product.

Thanks to Campbell's lawsuit, a whole new host of questions have arisen, as well as a sinking suspicion that the company is even more dysfunctional than previously thought. Excessive hype is one sign of a company possibly foundering due to mismanagement. Misogyny of the kind alleged by Campbell suggests dysfunction on a whole other level. As incidents of sexism in tech pile up, it's becoming clear that misogyny in the industry is both a moral travesty and a potential warning sign that a business is in trouble.

Campbell filed the suit in February alleging that Magic Leap fostered a misogynist work environment and then fired her for speaking out about it. Among other things, the suit alleges that Magic Leap executives were dismissive of input from female employees. The suit also claims that employees were told women had trouble with computers. (In a quote that has already gone viral, one IT lead allegedly said, ""In IT we have a saying; stay away from the Three Os: Orientals, Old People, and Ovaries."") In all, the suit alleges, the company cultivated an overall culture utterly inhospitable to women. (The suit describes a game meant to ship with the Magic Leap headset in which a female character is ""depicted on her knees groveling at the [male] heroes’ feet"" in admiration.)

What's more, the suit claims Magic Leap did little in an effort to fix its culture when Campbell raised the issue. She alleges that she tried six times to give a presentation about gender diversity in the workplace without success. A ""Female Brain Trust Initiative"" and a ""Women’s Inclusion Network"" were eventually formed, but the groups allegedly had no stated goals or support from management and stagnated as a result.

To be clear, the suit's allegations are just that, and given the settlement, they'll never likely get a hearing in open court. Magic Leap, for its part, filed a point-by-point response in federal court denying it engaged in any kind of discrimination. But the mere existence of the suit is not a good look in an industry where women typically comprise about 30 percent of a given company's workforce. Even with many companies overtly seeking to diversify their workforces in recent years, that ratio has stayed about the same, much as it has for the maddeningly dismal figures for hiring people of color in the industry.",Yes,"It leads to people not wanting to leave their virtual reality. They would rather stay in their virtual reality, because it is more enjoyable, and has less problems.",https://www.wired.com/2017/05/magic-leap-bias-suit-sexism-sign-failure/,2017-05-12 00:23:20+00:00,LABEL_1_relevant,0.9886912107467652,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
36,Are Virtual Drugs a Gateway to the Real Thing?,"Technology Review: Why do you believe that virtual drugs will make people less likely to experiment with drugs in real life?

Brian Shuster: First drug use rarely occurs in a vacuum. Rather, it is a social phenomenon brought on by peer pressure. In a virtual environment, the pressure shifts from trying actual drugs to experimenting with virtual drugs. Thus, users have a safe platform to explore the social aspects of drug use, without having to risk doing the actual drugs.

By separating the social pressure from the real-world application, users have a totally revolutionary mechanism to deal with peer pressure, and actually to give in to peer pressure, without the negative consequences.

Moreover, users of virtual drugs have reported the effects of these virtual drugs to be surprisingly realistic and lifelike. To the extent that users can enjoy both the social benefits of virtual drugs as well as the entertainment associated with drug use, all with no actual drug consumption, the value of taking actual drugs is diminished.

Hopefully the people who are making these reports are not one-time drug virgins for whom Red Light Center was a “gateway” to the real stuff, but longtime users who are now using technology to help kick their chemical habits.

TR: Do you have any evidence to back this up?

BS: To this point, the evidence is, as Albert Einstein might say, all the result of thought experiments. Virtual ecstasy has not been released to the public and is making its world debut on April 20. Virtual marijuana and mushrooms have only recently been introduced for users to try, so there has not yet been an opportunity to perform studies.

Utherverse is, at this time, however, investigating a study to determine if smoking virtual cigarettes can enhance traditional stop-smoking measures to assist smokers in their efforts to quit. We hope to run a scientifically founded study later this year.

TR: Wouldn’t virtual drugs make users more likely to want to try out drugs in real life?

BS: We hope that by preparing people to deal with peer pressure by allowing them to go through a virtual dry run, they will be better equipped to make rational decisions if the situation later occurs in their real life.

Just as with the sexual experimentation within Red Light Center, users will have the ability to decide for themselves whether using drugs is an enhancement or detriment to their life experience, even before ever using drugs in the real world. Armed with that information, they can then make more-rational decisions if they are confronted with that choice in the real world because they will have already gone through it virtually.

That said, it is critical to recognize that users who develop a full social circle within Red Light Center will have an online support structure of friends. Being accepted into a social community and having genuine friends are defenses that can be called on to prevent substance abuse in the real world. There is no reason to believe that this wouldn’t hold true for online users, and thus provide them with additional deterrence to ongoing real-world drug use.

Finally, as users attend more events and social functions online, the exposure to situations that arise in the real world, whereby the user may be pressured into trying or using drugs, will naturally diminish. Thus, by reducing the real-world exposure to peer-pressure situations and moving those situations to a safe, online environment, first-use [experimental] drug use resulting from peer pressure would naturally decline.

Of course, opponents of virtual drugs would counter that by providing a positive drug experience online, users would then be inclined to have a positive feeling about drugs and would be more inclined to experiment with them in the real world. To that I would respond by saying that if it is true that getting a positive reward for virtual behavior inclines someone to that behavior in the real world, well, drugs aren’t the big problem. Violent video games would be much more of a concern. Unspeakably violent video games have been rewarding players for violence for decades in the name of good old-fashioned entertainment. If opponents believe that giving users a nice high in a game will make them want to do drugs in the real world, how could they permit games that give users a high from killing and maiming other users? The answer is that they must believe that in-game rewards don’t really result in real-world actions. We feel the same logic holds true for virtual drugs.

TR: RedLightCenter.com states that it is only for adults, not for children. Will there be a virtual drug area for children? My best friend in high school turned on to pot, cocaine, LSD, and psychedelic mushrooms when we were in 10th grade. If we follow your logic, it seems to me that if virtual drugs are a good intervention, waiting until someone is 18 is too long.

BS: For various reasons, Red Light Center will not be made available to minors, nor do we have any plans or intentions to permit minors to use our software at any point in the future.

Although as an intervention tool Red Light Center would probably be valuable as you imply, real-world politics and situations simply make this an unrealistic option.

However, to the extent that many drug users have not used destructive drugs before they are 18, and to the extent that Red Light Center can provide an alternative outlet to real-world drug use beyond deterring first experimentation, there is still a lot of value, even if it is limited to [those] 18 years or older.

And, of course, we aren’t putting this product out as a way to stop drug use. That’s simply a side effect. We are putting out virtual drugs because they are fun and because they make our parties much better!",Yes,"It might make users more likely to want to try out drugs in real life. However, by preparing people to deal with peer pressure by allowing them to go through a virtual dry run, they will be better equipped to make rational decisions if the situation later occurs in their real life. Additionally, by reducing the real-world exposure to peer-pressure",https://www.technologyreview.com/2007/04/20/225914/are-virtual-drugs-a-gateway-to-the-real-thing/,2007-04-20 00:00:00,LABEL_1_relevant,0.9951423406600952,Virtual Reality,virtual_reality,MIT Tech Review,Social Norms & Relationships
37,How to Stop Virtual Reality from Making You Want to Puke,"This conflict is what Wetzstein, an assistant professor of electrical engineering, and other researchers at Stanford are trying to solve with the headset I tried on, which they call a light field stereoscope—essentially, a device that uses a stack of two LCDs to show each eye a “light field” that makes virtual images look more natural than they typically do.

Sample images show what it looks like to use the light field stereoscope to focus on parts of a 3-D scene that appear to be at different depths.

In real life, when you’re looking at something—a flower, for instance—your eyes move and the lens in each eye adjusts to bring whatever’s in front of you into focus. With stereoscopic 3-D, a technology commonly used by companies making virtual reality headsets, things gets trickier. In this case, each eye is shown a slightly different image of the same thing; your brain then combines these images to achieve a sense of depth. But since you’re looking at a flat, lit-up display in the distance and focusing on the 3-D images that appear to be in front of you, it can result in nausea and dizziness. Unless the technology improves, this could make it hard for some people to truly interact with virtual reality up close, regardless of whether they’re playing a first-person-shooter game or controlling a surgical robot.

“When you want to touch virtual objects, things that are close by, and you want to interact with them, it becomes very important,” Wetzstein says.

In hopes of making the stereoscopic virtual-reality experience more like what you see in real life, the Stanford researchers built a headset that contains two LCDs placed one in front of the other, with a backlight behind them, a spacer between them, and lenses in front of them. It’s connected to a computer that runs software necessary for the system to work.

The computer starts with a 3-D model, which the researchers’ software renders for each eye as a light field—in this case, Wetzstein says, it’s a five-by-five grid of slightly different 2-D images of the model, so 25 images in total for each individual eye. An algorithm uses the light fields to generate two images for each eye, and, for each eye, one of these images is shown on the rear LCD in the headset, while the other is shown on the front LCD. The images enter your pupils and are projected on your retinas.

What you see, Wetzstein says, is an approximation of the light field that’s being optically generated, which your eyes can freely move around and focus on where they want in virtual space.

“That’s normally not possible,” he says.

Wetzstein says the technology that goes into the headset is inexpensive and doesn’t require eye-tracking to figure out where the viewer is gazing, and he hopes it can be incorporated into headsets that come out several years from now. A paper about the work will be presented in August at the Siggraph computer graphics and interaction conference in Los Angeles.

Marty Banks, an optometry and vision science professor at the University of California, Berkeley, is also working on research into how to solve the problem of getting proper focus cues in virtual reality—he coauthored a paper that will also be presented at Siggraph that he says shows higher-resolution images but requires eye tracking. Banks says the display that Wetzstein and other researchers are working on is cool and clever, but notes that the resolution is still pretty low (the Stanford group’s paper indicates scenes displayed had a resolution of 640 by 800 pixels).

“That’s going to be a major thing to solve: how to get the resolution up. So that’s going to require more computation and more pixels and more expense,” he says. “There’s a ways to go there.”",Yes,"It can result in nausea and dizziness. However, researchers at Stanford are hoping to solve this problem with the development of a headset that uses a stack of two LCDs to show each eye a ""light field"" that makes virtual images look more natural.",https://www.technologyreview.com/2015/07/01/110222/how-to-stop-virtual-reality-from-making-you-want-to-puke/,2015-07-01 00:00:00,LABEL_1_relevant,0.9924461245536804,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
38,"Intel Might Be Making an Augmented-Reality Headset, Hoping to Cash In on a Rising Industry","Such moves would make sense, as major companies like Microsoft and Google are already involved in augmented-reality projects—Microsoft’s planning to ship its HoloLens headset to developers at the end of March, while Google is reportedly working on a retooled version of Glass meant for several types of work. Meanwhile, startups ranging from the heavily funded Magic Leap, which has raised $1.4 billion, to the much smaller Meta, which has raised $23 million, are also working on headsets.

Chances are, more companies are going to be entering the AR fray in the coming years. Putting together its own headset, especially one that is ripe for others to make on their own, would likely help Intel cash in on growing interest in augmented reality and its sibling, virtual reality—even if it takes either or both a long time to become mainstream.

For the moment, Intel won’t officially confirm whether it has plans for augmented reality. But the person responsible for RealSense, Achin Bhowmik, noted that Intel has previously built prototype products in its efforts to get customers interested in the company’s components.

(Read more: Wall Street Journal, “Augmented-Reality Startup Meta Ups the Ante in the Headset Race,” “10 Breakthrough Technologies 2015: Magic Leap,” “Mark Zuckerberg Says It Will Take 10 Years for Virtual Reality to Reach Mass Market”)",Yes,It will become more difficult for people to tell the difference between what is real and what is not. This could lead to people becoming more gullible and less able to judge what is true and what is not.,https://www.technologyreview.com/2016/03/03/71452/intel-might-be-making-an-augmented-reality-headset-hoping-to-cash-in-on-a-rising-industry/,2016-03-03 00:00:00,LABEL_1_relevant,0.9949517250061036,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
39,Samsung's New Galaxy Note Doubles as a Virtual-Reality Helmet,"Samsung is serious about phablets, and its three latest announcements add compelling twists to its Galaxy Note lineup. The most straightforward of the company’s new offerings will be the Galaxy Note 4, its latest flagship big-screened phone. Beyond that, things get much more interesting: A Note 4 doppelganger called the Note Edge with a wraparound AMOLED screen, and an Oculus-like VR mask driven by the Note 4, the Gear VR.

Samsung's New Phlagship Phablet

The new Galaxy Note 4 runs Android 4.4 KitKat and has a 5.7-inch AMOLED display with a 2,560 x 1,440 (Quad HD) resolution; that adds up to an incredibly sharp and colorful 518 ppi screen. Its cameras also get significant upgrades. Optical image stabilization bolsters its 16-megapixel main camera, and there’s an 8X digital zoom on that shooter. To reduce pixelation on those digitally-zoomed images, Samsung says the camera uses its own special blend of HDR-like image compositing. The front-facing camera also gets a bump, with a higher-resolution 3.7-megapixel sensor and an F1.9 aperture that helps it perform well in low light. According to Samsung, it also has a 120-degree field of view, which is a much wider-angle lens than most selfie cams.

The Note 4's removable battery is an ample 3,220mAh unit, and the company touts its “Rapid Charge” feature, which can juice the battery 50 percent in 30 minutes. Similar to the Galaxy S5 and Galaxy Tab S, the phablet also has an “Ultra Power Saver” mode that the company claims can last 24 hours with just 10 percent battery life.

I can say with confidence that it feels good in the hand. The metal-framed Note 4 has a soft faux-leather back like its predecessor, but there’s no fake stitching this time around. Its S Pen also works more like a mouse on this version; you click the stylus’s side button and drag the pen across text to highlight it, and the same actions allow you to drag-and-drop onscreen items precisely. Along with the user-accessible battery, there’s also a microSD slot that lets you add up to 64GB of extra storage.

All the Note 4 specifics haven’t been revealed just yet. Samsung says it will be available with a 2.7GHz quad-core CPU (likely to be a Qualcomm Snapdragon 805 chip, although Samsung wouldn't comment on the specifics) and with dual quad-core CPUs like the Exynos-based Galaxy Tab S. The pricing has yet to be announced, but the phone will launch on all all four major U.S. carriers in October.

Gear VR: A Phablet-Driven Oculus Rift

Sold separately (at a price yet to be announced this fall) will be the coolest phablet accessory in world history: the Gear VR. This is an Oculus Rift-style headset that works only with the Note 4. You insert the Note 4 into the facemask, and the phone automatically enters VR mode when you dock it into the headset.

The Note 4 docks into the Gear VR headset to provide processing power and an immersive high-res display. Samsung

When docked, the Gear VR uses the Note 4 display and its processing power for full-immersion games and movies, while the headset has its own magnetometer and accelerometer to calculate movement, as well as a proximity sensor to know when it's on your face.",Yes,"It can force you to take a break from reality. The Gear VR has a display resolution of 2560x1440, which equates to around 538 pixels per inch, and the field of view is 96 degrees. According to Samsung, this is enough to produce ""a cinematic experience with high-quality picture and sound.""

The",https://www.wired.com/2014/09/samsung-galaxy-note-4/,2014-09-03 14:43:13+00:00,LABEL_1_relevant,0.9906544089317322,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
40,A VR Acid Trip Is All the Fun Without the Pesky Comedown,"There are six people with me. We're all sitting cross-legged on the floor, forming a circle around a glass coffee table. The setting is a comfortable living room in what could be any suburban California home. Judging by the black and white TV set near the wall and the record player next to me, I'd peg the time period as the mid-1960s. My friends are all groovy-looking types with long hair, beads, flowered polyester. Everyone is trying to sit still, but they percolate like they're waiting for something. It's making me anxious, so as they close their eyes and slow their breathing, I slow mine as well.

That's when I notice the orange couch off to the left of me looks a bit less orange than it did a moment before. Wait, now it's bright orange. It keeps desaturating, then saturating. And does the room suddenly look brighter, too? I glance at the grinning blonde girl to my right. She's blissed out. Behind her is a blue chair. Its breathing (uh, yeah, breathing) matches mine.

My friends can't sit still. One strokes the carpet, which appears to be vibrating. Another picks up an African drum and starts tapping. At this point, I'm a little sweaty.

I try to center myself by concentrating on the glass tabletop. It starts rippling, then melts and dribbles all over the floor. The legs fall inward and flames erupt. None of my companions seem surprised to see a raging campfire in the middle of the living room. On the contrary, they're delighted, and grow even more so when the ceiling disintegrates. We all look up at millions of shimmering stars against an ink-black sky. The breathing furniture rises from the floor and slowly floats away as the walls fade. Suddenly we're in the desert, with Joshua trees and alien rock formations beneath vibrant starry sky. The guy to my left slowly turns toward me, a look of ecstasy and terror on his face. We lock eyes. He stares at me intently. I stare back, straight into his soul.

But I'm not out under the stars frying on 500 mics. I'm sitting cross-legged on the floor of a hotel ballroom in Austin, Texas—wearing an HTC Vive and a pair of headphones. Because this trip came courtesy of Origins, a VR experience created to promote the new documentary Orange Sunshine.

The documentary (which is excellent, you should see it!) follows the rise and fall of the Brotherhood of Eternal Love, counterculture rebels from Laguna Beach, California, who were responsible for the distribution of alarmingly pure and potent LSD in the late 1960s and early '70s. The documentary takes its name from the group's most well-known product, which was dumped into the burgeoning counterculture by the truckload. The Brotherhood's little orange pills were responsible for ""turning on"" a significant number of American kids during that era, forever changing the path of the country's popular culture by steering a legion of LSD devotees toward new artistic and spiritual ideals. Ask any old head from back in the day, and she'll tell you: Sunshine was the real shit.",Yes,"It makes people incredibly nauseous.

If you want to try it for yourself, Origins is available on Steam for $14.99.",https://www.wired.com/2016/03/orange-sunshine-vr-experience/,2016-03-17 11:00:26+00:00,LABEL_1_relevant,0.8346782326698303,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
41,Cyberpunk 2077 and the Meaning of Its Deadly Dildos,"Push past the beaded entryway of room six at the No-Tell Motel in Cyberpunk 2077, and there she is: a statuesque, blondish woman perched on the edge of an animal-print-covered bed. She’s in a cupless bra with black tape covering her nipples, and there’s a cherry-red dildo to her right.

“Bet you didn’t expect to see me here,” she says.

After the blockbuster video game’s main character, V, has sex with her, the reward lays on the bed. It’s the same dildo from before, only now transformed into a melee weapon. The dildo “may not be lethal,” according to its profile in the weapons inventory, but “it’s perfect for when someone is just asking to get fucked.”

“Sir John Phallustiff,” as the weaponized sex toy described above is called within the game, isn’t just a one-off flourish. Plenty like it can be found in this intricately designed dystopia. Stroll through the dressing room of Lizzy’s bar and you’re greeted with a spiked bronze dildo and a butt plug upright on a shelf. Peer underneath a car and you’ll see a dildo staring back at you. Dildos are everywhere in Cyberpunk 2077, as many players have discovered. But why?

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

A representative for the company behind the game, CD Projekt Red, claimed last week that the abundance was a bug. “We wanted Night City to be pretty open sexually,” the rep told Business Insider, but too many sex toys were showing up as random items strewn throughout the gaming world. “We're going to adjust them so that the dildos don't appear too out of place/context and distracting.” Meanwhile, a designer at the company who told me that he wished to remain anonymous an account of the “tumultuous PR” around the game said they’d put in all the dildos for two reasons: to be controversial, and also “to represent the cyberpunk future as sex-positive.”

Whatever the designers’ intent, the ubiquitous dildos in Cyberpunk 2077 send a very specific message, and it’s not sex-positive. In the game, dildos are either classed as weapons on their own terms, or as “junk” to be dismantled and reused to make more important things, like … weapons. Dildos are not depicted here as tools for sexual pleasure, but rather as the litter of a decaying world, and a makeshift means of doing violence. If anybody in this future actually does masturbate with dildos, apparently they toss them on the ground right after. Maybe that’s because in Night City, sex-toy technology has somehow failed to improve at all by 2077. While V’s body can be enhanced with newest tech (titanium bones, bioplastic blood vessels, etc.), the game’s artificial dicks are stuck in the early 2000s: standard penis replicas, nary a clitoral suction device in sight.

Game designers know that disembodied dicks are funny. That’s why the South Park video games are teeming with them; and it’s why 2016’s Genital Jousting was such an indie hit. But when it comes to science fiction, there’s likely more to it than that. In a futuristic context, dildos often serve as stand-ins for our disconnected selves, signs that we have lost our humanity. Sometimes the devices even enact violence on their users, punishing humans for replacing skin-on-skin sex with technology. The sex machine in Barbarella (1968) gives Jane Fonda’s character the best orgasm of her life, then nearly kills her. “You’ve exhausted its power. It couldn’t keep up with you … Shame on you!” its inventor yells. The orgasmatron in Woody Allen’s Sleeper (1973) is a symbol of a soulless society, a world where men are impotent and women frigid. Westworld’s (2020) sex robots reside in a pleasure-filled theme park for the rich, until they rise up and destroy their human creators.

Notably, the future is never portrayed as a technological utopia world brimming with sexually fulfilled women vibrating themselves to orgasm. A world like this would be too threatening to gender roles. In fact, as I wrote in my book Buzz: A Stimulating History of the Sex Toy, for more than a millennium, when dildos appeared in stories—whether told through poems, plays, or another medium—they often served to illustrate a common fear: that sex toys would replace men and topple society. In Aristophanes’ Lysistrata, from around 411 BCE, women go on a sex strike to encourage their husbands to end the Peloponnesian War—and some of the women get dildos as replacements. In the 16th-century poem “The Choosing of Valentines, or The Merie Ballad of Nashe His Dildo,” Thomas Nashe tells a story of a sex worker so unsatisfied by her lover that she turns to a dildo instead. She rhapsodizes about the toy’s superiority to a penis—it always stays hard, and won’t get her pregnant. It’s in keeping with this tradition that Cyberpunk 2077 would transform dildos from masturbatory devices into weaponry, as a way of neutralizing the dildo’s threat to masculinity and turning fear of replacement into power.

Cyberpunk 2077 isn’t even the first game to make this symbolic gesture. In Grand Theft Auto: San Andreas, released in 2004, players could wield a dildo affixed to a chainsaw, a floppy purple dildo club, or a phallic vibrator weapon. (Some of these may double as gifts for the main character’s girlfriend.) Dead Rising 3 (2013) has a leaf blower that shoots zombie-killing purple vibrators like a T-shirt gun. Saint’s Row the Third (2011), Saint’s Row IV (2013), and the expansion pack Gat Out of Hell (2015) all provide the muscly main character with a 3-foot dildo bat for beatdowns. Unsurprisingly, it’s much more common in video games to see men bludgeoning each other with sex toys than it is to see a woman masturbating with them. The only game from a major publisher that shows the latter is the controversial Grand Theft Auto V (2013). And even there, the woman gets interrupted by her husband: “I thought I’d locked the door,” she says as she tosses the buzzing pink phallus into the corner of the room.

For decades, feminists have championed sex toys as a way for women to gain control of their own sexuality. “It was my intention to introduce electric vibrators to women, and in the process, transform the women's movement into a tidal wave of orgasmic activists who would change the world,” second-wave feminist and masturbation pioneer Betty Dodson wrote in 2010. So it’s disheartening that sex toys are rarely used for sexual pleasure in video games. What’s even worse is the fact that they are often used to commit acts of virtual violence against women in particular. Dozens of videos of Cyberpunk 2077’s quest for Sir John Phallustiff blanket YouTube, most blurring out the tame sex scene that unfolds right before V earns the dildo melee weapon. In one of the most popular of these videos, the censored consensual sex scene is followed soon after with crystal-clear video of V bashing a woman’s head with the giant dildo, blood spraying through the night air as she falls face first into the concrete.",Yes.,It can desensitize players to violence.,https://www.wired.com/story/cyberpunk-2077-and-the-meaning-of-its-deadly-dildos/,2077-04-05 00:00:00,LABEL_1_relevant,0.9932570457458496,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
42,"Dystopian Marketers, Unite: Facebook to Start Dropping Ads Into VR Spaces","VR advertising is upon us, and right in your face.

VR Ads

Facebook is starting to squeeze ads into the magical virtual reality environments enabled by its Oculus Quest VR headset — a move that shouldn’t come as a surprise to anybody who’s noticed Facebook’s approach to innovation.

In other words, the dystopian nightmare future of having ads pop up inside magical virtual worlds, unavoidably, right in your face? It’s already upon us. The company however is hailing the change as an easy way for developers (read: Facebook) to squeeze more money out of their VR games and apps.

In a test, the social media platform chose to include display ads inside a first-person shooter arcade game developed by Resolution Games. It’s using first-party data to make sure the ads are relevant — but arguably, no matter how relevant, the decision only comes to the demise of those who have already invested in the Quest platform.

Those consumers will also have to suck it up or invest in a different platform. In fact, in order to actually make use of Facebook’s $299 Oculus Quest 2, users are forced to sign in with Facebook.

Advertisement

Advertisement

More Revenue

The Mark Zuckerberg-led company is trying to sell off the ad idea as a way to make VR a “self-sustaining platform” by allowing developers to generate revenue. “It also helps us continue to make innovative AR/VR hardware more accessible to more people,” a polished-by-the-PR-team blog update by the Oculus team reads.

“For now, this is a test with a few apps,” the post reads. If all goes well, Oculus is ready to roll out apps “across the Oculus Platform and in the Oculus mobile app, as well as guidance for businesses and developers interested in advertising on Oculus.”

Facebook is taking a privacy-first approach and is promising to “not use information processed and stored locally on your headset to target ads.”

The social media company is also reportedly working on a smartwatch, and has announced it is building a pair of AR glasses. It’s still unclear if those platforms will also eventually be flooded with ads — but it wouldn’t exactly be a surprise.

Advertisement

Advertisement

We’re still not entirely sure what form the VR ads will take. Facebook is trying out “new ad formats that are unique to VR,” but didn’t specify what that would look like, as The Verge points out.

Either way, the move to include ads won’t come as a welcome addition to literally anybody playing games or using apps in VR right now.

READ MORE: Facebook (FB) Plans to Put Ads on Oculus Quest VR Headset [Yahoo Finance]



More on VR: You Perceive Time Differently in Virtual Reality, Scientists Say

Advertisement

Advertisement

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",,"It blurs the line between reality and fiction, which can lead to a form of escapism. People can become so immersed in the virtual world that they forget about the real world, which can have negative consequences. Additionally, virtual reality can be used to manipulate people’s emotions and thoughts, which can lead to unethical uses of the",https://futurism.com/the-byte/facebook-virtual-reality-ads,,LABEL_1_relevant,0.9850512742996216,Virtual Reality,virtual_reality,Futurism,User Experience & Entertainment
43,This Augmented-Reality Robot Is The Closest Thing You Can Get To A Real-Life Hologram,"Ready to have your brain blasted to bits? U-Tsu-Shi-O-Mi is a “virtual assistant” that is actually a green-clad humanoid robot. Users look at her using virtual reality goggles and instead of seeing a mildly creepy, ninja-like blog they see an actual face and body. You can reach out, touch the body, and even shake hands with your robotic pal.

Made by a Japanese company called Different Dimension Inc., the robot uses a program called MMDAgent to interact with users. An initial prototype looked like a ‘tween in footie pajamas and a full-face hood. The newest version consists of half of a body and a much smaller profile. The pre-order price will be about $5,000.

Unfortunately the system still requires a VR headset and some solid computing power. However, because the entire package is so compact, you could imagine approaching a little green figure while wearing smaller VR glasses and interacting without much fuss. Interacting with a tiny green figure is obviously quite creepy – especially if passers-by don’t know what’s up – but what did Clarke say about technology and magic?

via Gizmag",Yes,"It may lead to people becoming more anti-social. In other words, instead of interacting with real people in the real world, they may prefer to interact with digital avatars in virtual worlds. This could have a negative impact on people's social skills and their ability to form real-world relationships.",https://techcrunch.com/2013/02/11/this-augmented-reality-robot-is-the-closest-thing-you-can-get-to-a-real-life-hologram/,2013-02-11 00:00:00,LABEL_1_relevant,0.9959316849708556,Virtual Reality,virtual_reality,TechCrunch,Social Norms & Relationships
44,A Crazy Oculus Rift Hack Lets Men and Women Swap Bodies,"The great promise of the Oculus Rift headset the chance to inhabit fantastic new worlds. A group of researchers in Barcelona are already using it for something even more radical: inhabiting new bodies.

BeAnotherLab, an interdisciplinary group of students at the University Pompeu Fabra, in Barcelona, has relied on an early version of Oculus Rift as part of an on-going research project called ""The Machine To Be Another."" The concept is just what the name suggests. An early experiment let participants experience the creative process through someone else's eyes, in real time. The latest undertaking is even wackier. It lets men and women swap bodies. (Note: The video below contains nudity.)

Content This content can also be viewed on the site it originates from.

Here's how it works. Each subject is outfitted with an Oculus Rift headset. Those are supplied with video streams from point-of-view cameras attached to the other person's rig. The participants are instructed to mimic each other's movements, wordlessly dictating the action in tandem like kids playing with a Ouija board.

The effect, says Philippe Bertrand, a Digital Arts student and co-founder of the group, is profound. ""Deep inside you know it’s not your body, but you feel like it is.""

Over the last several months, the group has found a diverse group of researchers interested in their ""embodiment experience platform,"" from artists to therapists to anthropologists. The latest project was focused on VR's potential for fields like gender studies and queer theory, but they're already formulating applications from artistic performances to neurorehabilitation.

Content This content can also be viewed on the site it originates from.

BeAnotherLab is far from the first to dabble in body swapping. Their project was inspired by teams like Group Ehrsson, in Stockholm, and Event Lab, also in Barcelona, both of which have combined neuroscience and virtual reality in an attempt to untangle complex mysteries about consciousness and the self. Bertrand and company don't see themselves as pioneers so much as a ""low-budget, Creative Commons"" division of this greater movement.

""The discovery of ‘mirror neurons’ by Giacomo Rizzolatti has shown us that you can’t conceive an “I” without an “us,"" Bertrand explains. ""Other recent investigations on embodied simulation are reporting a blurriness of the self related to familiar individuals. Other studies suggest the effectiveness of embodiment for reducing implicit racial bias."" In other words, new research and cutting edge technology are giving us a better understanding of empathy--and new tools for tapping into it.

""It’s a pretty interesting scenario,"" says Bertrand. ""Science is proving out some very hippie concepts, and we're using a video game head mounted display to bring people closer to each other.""",Yes,"It lets people inhabit other people's bodies. This has many potential applications, from helping people understand each other to reducing implicit racial bias.",https://www.wired.com/2014/02/crazy-oculus-rift-experiment-lets-men-women-swap-bodies/,2014-02-25 11:30:53+00:00,LABEL_1_relevant,0.996029257774353,Virtual Reality,virtual_reality,Wired,Equality & Justice
45,"Steven Soderbergh on VR, iPhone Films, and Harvey Weinstein","Steven Soderbergh is nothing if not ever-evolving—except, maybe, for tireless. After establishing himself as one of his generation’s great auteurs with movies like Sex, Lies, and Videotape, Out of Sight, and Traffic, he vowed to stop making theatrical films. During the supposed hiatus, he made HBO’s Liberace biopic Behind the Candelabra, and two seasons of prestige drama The Knick for Cinemax. Then this summer, ""retirement"" over, he returned to the multiplex with the heist flick Logan Lucky.

That wasn’t all, though. He’s also spent the last few years working on Mosaic, an app-based miniseries mystery that viewers navigate their way through, chapter by chapter. (At the end of each node, they can choose which character to follow or which new scene to watch.) For most other directors, it would be a dramatic career turn. But for someone as prolific as Soderbergh, it’s just one of a handful of projects he seems to be constantly working his way through with a savant-like ease.

Soderbergh sat down with WIRED in his Tribeca office (which looks delightfully like where a 1930s gumshoe would set up shop) to share the backstory on his latest project, as well as his thoughts on virtual reality, the future of filmmaking, and the sexual harassment allegations against the man who helped make Sex, Lies, and Videotape a hit: Harvey Weinstein.

Steven Soderbergh on making an app that tells a story:

I was initially reticent—as a filmmaker/storyteller, anything that feels game-y is a little scary because games and being told a story work at cross-purposes. In the game, the viewer has agency, and as a result there’s a certain kind of engagement that happens that isn’t the kind of emotional engagement you get from experiencing a story. So my concern was: Will I still have the kind of control as a filmmaker that I need to have? But what I saw was something that was not a film, and not a game. It was rubbing up against both these things, but was different enough from each to be exciting. The technical innovations that have taken place in gaming over the last decade and a half are stunning, so this seemed like an opportunity to take advantage of a form that has been around a long time—branching narrative—and have the technology be an organic part of the story, as opposed to someone coming along and saying “Oh, I got this piece of new tech, let’s jam a story into it.”

On virtual reality:

There are several things working against it. When you can’t see your protagonist, it’s virtually impossible to hook into the story. This is how we engage, looking into the eyes of the protagonist. There’s no such thing as montage. It’s uncomfortable to have that thing on your head for more than a certain period of time. Part of the joy of anything is watching it with someone else and at a certain point looking at them and going “Holy shit!” That’s gone. To me, there are all these things working against it as a longform, narrative format. I think it’s going to work best as a combination of immersion and gaming and stuff like that, but I don’t… I wish I could short it, I could make a lot of money. But I don’t think I can.

On the value of short TV seasons:",Yes,"To turn cinema into theater and TV into cinema. TV has essentially becomefilm and film has essentially become theater. The other interesting thing that’s happening is, if you look at the people who are making the best films, they’re all people who are coming out of TV. They understand the process and they understand that you have to",https://www.wired.com/story/steven-soderbergh-q-and-a/,2017-11-09 14:00:00+00:00,LABEL_1_relevant,0.993752658367157,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
46,String Theory's Weirdest Ideas Finally Make Sense—Thanks to VR,"The robot is building a tesseract. He motions at a glowing cube floating before him, and an identical cube emerges. He drags it to the left, but the two cubes stay connected, strung together by glowing lines radiating from their corners. The robot lowers its hands, and the cubes coalesce into a single shape—with 24 square faces, 16 vertices, and eight connected cubes existing in four dimensions. A tesseract.

This isn't a video game. It's a classroom. And the robot is Brian Greene, a physicist at Columbia University and bestselling author of several popular science books. His robot avatar teaches a semicircle of student robots, each wearing a shoulder badge of their home country's flag. The classroom is outer space: Greene and the arc of student-robots orbit Earth. After he shows the students the tesseract, Greene directs his class to try making four, five, even six dimension objects. This is a virtual reality course on string theory; the lesson happens to be about objects with more than three dimensions.

In real life, Greene is wearing a dark blue shirt, black jeans, and boots, and his normal, non-hovering chair is sitting in a concrete-floored VR business called Step Into the Light planted firmly on Earth's surface—Manhattan's Lower East Side. An HTC Vive headset covers his face, and he gestures effusively—he's a New York native—with the controllers.

Greene is teaching this futuristic class as part of the World Science Festival, a week-long, city-wide celebration of intellect and curiosity. He's actually a co-founder, along with documentarian Tracy Day. In addition to VR classrooms, it features panel discussions on the future of AI, chemistry-themed cooking demonstrations, and ride-alongs with rat scientists. There are events in every borough. Also, cyberspace—and apparently extradimensional space, too.

Extra dimensions are a critical part of Greene's field of study. String theory posits that the universe is built not just from three spatial dimensions (up/down, side/side, forward/backward) and the single dimension of time, but at least six other dimensions. These extra dimensions would be too small for humans to detect—about 10-33 centimeters. But, according to the theory, the six, curled up dimensions play a major role in controlling how subatomic strings vibrate, and those vibrations determine how quarks, electrons, and other fundamental particles behave. Ultimately, these phenomenon scale up to explain modern how gravity and quantum mechanics fit together: Otherwise known as The Biggest Question in Physics. ""String theory,"" says Greene, ""is speculative and hypothetical, but mathematically quite compelling.""

It's also quite confusing. Those six extra dimensions some string theorists predict come wrapped together in what are known as Calabi-Yau manifolds. Greene is a great writer—bestselling, even—but even he can't perfectly explain what these things look like. VR helps him make the shapes make sense. Sort of. Greene brings up a video of a wire in the virtual reality space. To a human, he says, this is a two dimensional object: a line. The video zooms in, and something new appears: an ant, walking in a circle around the wire's girth. The lesson here is that extra dimensions are invisible unless you are the right size to see them.

The analogy with the wire makes sense, but it doesn't scratch all the strangeness away from trying to visualize six dimensional curling. Greene goes on to explain that humans would need microscopes capable of peering inside the smallest particles in order to observe the curled-up dimensions. As he does, the virtual classroom passes through a series of grids—this is to represent shrinking—until everyone is small enough to directly observe the six dimensional Calabi-Yau manifolds that exist at reality's most minuscule junctures.

The lesson—designed by Abelana VR productions—is nearly over. Greene reiterates to his students that the extra-dimensional weirdness they spent the past hour exploring together is mostly mathematical conjecture. The classroom scales back up, and now is floating in front of a supermassive black hole.

Still wearing his headset, Greene opens up the floor for questions. One student, from Denmark, asks, ""How does it feel to be teaching in the next step of education?"" As Greene answers—about how it's so wonderful that people from around the globe can gather and share complex ideas like this—a student flings a virtual tesseract at the teacher's head. Next step, indeed.",Yes,"It can be used to teach about things that are difficult to visualize, like extra dimensional space. The VR class described in the passage is an example of this.",https://www.wired.com/2017/06/string-theorys-weirdest-ideas-finally-make-sense-thanks-vr/,2017-06-02 20:35:15+00:00,LABEL_1_relevant,0.9913153648376464,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
47,The US Military Developed AR Goggles For Dogs,"""This new technology offers us a critical tool to better communicate with military working dogs.""

Doggles

The US Army is developing augmented reality goggles for dogs to help protect their human guardians.

The BBC reports that the project, funded by the Small Business Innovation Research program, aims to allow soldiers to give dogs specific directional commands while they’re not in direct line of sight.

“Augmented reality works differently for dogs than for humans,” Stephen Lee, an Army Research Laboratory senior scientist, explained in a statement. “AR will be used to provide dogs with commands and cues; it’s not for the dog to interact with it like a human does.”

Pup Squad

Contractor Command Sight created a prototype that includes a pair of goggles, a camera, and display that fits over the dog’s eyes. A visual indicator gives the dog visual cues.

Advertisement

Advertisement

“Much of the research to date has been conducted with my rottweiler, Mater,” Command Sight founder A.J. Peper said in the statement. “His ability to generalize from other training to working through the AR goggles has been incredible.”

Dog-o-Vision

The current prototype requires the handler to keep the dog on a leash, because the goggles aren’t wireless yet.

Even without the ability to give the dog directions through the goggles, “this technology provides one of the best camera systems for military working dogs,” according to Lee, adding that the “handler can see exactly what the dogs sees.”

“This new technology offers us a critical tool to better communicate with military working dogs,” Lee added.

Advertisement

Advertisement

READ MORE: US Army trials augmented reality goggles for dogs [BBC]



More on AR: These AR Smart Glasses Shoot Lasers Directly Onto Your Retina

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Yes,"It could lead to more cybercrime, more cyber bullying, more internet addiction, and more internet loneliness.",https://futurism.com/the-byte/us-military-developed-augmented-reality-goggles-dogs,,LABEL_1_relevant,0.9964683055877686,Virtual Reality,virtual_reality,Futurism,"Information, Discourse & Governance"
48,"First They Got Sick, Then They Moved Into a Virtual Utopia","One afternoon in November, Fran Serenade led me and her daughter Barbie down a steep section of the Appalachian trail. The sun was high and Fran hiked briskly, ducking the blue-green diagonals of fir trees, her hair wild behind her. She wanted to show me her log cabin, which was off the trail next to a red barn by a blue lake with a pretty waterfall. Outside the barn, we met Fran’s kitty, Amici, and a handful of other cats, all of them wearing knit caps. Fran pet Amici, who followed her, mewling. Inside the cabin, a fire raged in the fireplace. Fran pointed out her favorite things: an antique icebox; an embroidered tablecloth; a crochet basket. Then we were off to Tai Chi practice.

Sign up to get Backchannel's weekly newsletter, and follow us on Facebook, Twitter, and Instagram.

With her mountain of blonde curls, heart-shaped face, hip-hugging jeans, and tiny waist, Fran recalls a young Dolly Parton. She’s so pretty, she’s almost doll-like. Barbie is slim and pretty, too, though, in her leopard print vest and red lipstick, her look is less country, more glam. Mother and daughter both wore heels, even as we hiked.

It turns out heels work perfectly well on virtual trails, no matter how steep. The trail and cabin we visited exist inside an online virtual world called Second Life. They were designed to recall a real-world trail and cabin in the Blue Ridge Mountains of North Carolina, where Fran once lived. Fran Serenade is the avatar of Fran Swenson, an 89-year-old former nurse with a head of silver curls and a Brooklyn accent. Barbie is the avatar of Barbara, her daughter in real life.

Fran used to spend her summers in the Blue Ridge Mountains cabin with her husband. Then her husband got Parkinson’s Disease. Slowly he lost control of his body, she tells me, until his ability to breathe slipped away and he was gone. That was in 2003. A year later, Fran received the news: She had Parkinson’s herself. The day she got the diagnosis, Fran allowed herself a couple of hours to despair. Then she sat down and wrote a list of all of the things she would not stop doing, and signed it “The Unsinkable Fran.”

When Fran first entered Second Life eight years ago, the virtual world’s popularity had already peaked. In 2006, membership in Second Life hit 1.1 million. Some members were making good money selling things inside the virtual world: That same year, the first Second Life millionaire graced the cover of Businessweek. It also scored mentions in television programs like The Office and CSI: Crime Scene Investigation.

At the time, some futurists believed that Second Life was social media’s final frontier, a digital utopia where everyone with an internet connection would eventually take up residence. But traditionalists feared that residents would become so wrapped up in their virtual lives they would neglect their physical selves and offline relationships — that they would live almost exclusively inside Second Life. Then the gamers turned to newer platforms and developers became obsessed with 3D virtual reality. Second Life membership plateaued. Facebook took off instead.

Today, Second Life is mostly forgotten by the broader public. An estimated 800,000 users are active on a monthly basis, according to Second Life parent company Linden Lab. That’s tiny compared to the 1.86 billion users who are active on Facebook each month.",Yes,It can cause people to neglect their physical lives and offline relationships.,https://www.wired.com/2017/02/first-they-got-sick-then-they-moved-into-a-virtual-utopia/,2017-02-13 05:00:00+00:00,LABEL_1_relevant,0.994776964187622,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
49,Second Life Bans Gambling Following FBI Investigation,"An ongoing investigation by the FBI into gambling in Second Life is believed to be directly related to Linden Lab’s sudden decision to ban all forms of gambling on Second Life.

The FBI investigation commenced in April and was considering the legality of online gambling within the virtual world. The US Government prohibits most forms of online gambling.

It was unclear at the time of writing whether the FBI would take the matter further, including the possible arrest of Linden Lab directors or the prosecution of individual users.

Casinos and gambling have been a prominent part of the Second Life metaverse over the last 2-3 years. Linden Lab will take a financial hit from the decision as casino owners cancel virtual land ownership agreements; top tier casinos contribute large sums in monthly fees to Linden Lab.",Yes,"The potential for people to get addicted to it and spend too much time in the virtual world instead of the real one. This could lead to people losing their jobs, homes, and families as they neglect their responsibilities in the real world.",https://techcrunch.com/2007/07/25/second-life-bans-gambling-following-fbi-investigation/,2007-07-25 00:00:00,LABEL_1_relevant,0.9969580173492432,Virtual Reality,virtual_reality,TechCrunch,Economy
50,"AR pioneer Omar Tayeb, of Blippar, is speaking at Disrupt London, Dec 5-6","TechCrunch Disrupt in London is on December 5-6! One month to go! It will be the place to come talk about new technology, your startup, other startups, everyone’s startups! Grab tickets here. So many speakers have already been announced. But, yes, there are even more to come. Today we’re delighted to say Omar Tayeb, CTO and Co-Founder of Blippar, the a leading augmented reality, artificial intelligence, and computer vision company, is coming along.

Tayeb is credited for developing the original technology responsible for launching Blippar into the forefront of the AR industry. His passion for innovation has been instrumental in powering the AR and visual discovery experiences that have attracted over 66+ million users to the platform. Tayeb leads the tech team and is the company’s point person for AR and AI development. As the driver of the company’s technology and innovation, Tayeb is responsible for pushing the boundaries for what’s possible not only for Blippar but for the AR industry as a whole. Today, he continues to drive innovation at Blippar by furthering the development of their artificial intelligence and computer vision technologies and leading Blippar’s R&D unit and team of over 70 engineers.

With Tayeb at the helm of the company’s technology division, Blippar has grown from a groundbreaking idea to a global industry leader with 12 offices and 300+ employees. Blippar has been recognized as a “Fastest Growing” company by CNN, CNBC, UKTI, Forbes, and Fast Company, featured in the #9 spot on the 2016 CNBC Disruptor 50 list, and named one of the 2016 Bloomberg Business Innovators.

Tayeb was recognized as Young Innovator of the Year by the Guardian in 2012 and in 2014 he received the h100 award, which celebrates the 100 most influential and innovative people in Britain.

The two-day Disrupt conference runs December 5 to 6 in the Olympic Village’s Copper Box Arena and features Startup Alley and Startup Battlefield where one startup will take home £30,000. You can buy early-bird tickets for Disrupt London here.

You want to display your company? Then go for a booth in the Startup Alley. For startups there is also the brand new CrunchMatch program where we match startups to investors. Check it all out here.

Early bird tickets are now available to purchase for the discounted price of just £950 a piece. You can get your tickets at this price until 4 December.

For all you students out there, the deal is about to get even sweeter. We have a limited selection of student tickets to Disrupt London 2016 for just £100 plus VAT, provided you have both a valid university ID and current transcripts.

To reserve your £100 student tickets to Disrupt, simply send a copy of your transcripts showing your current enrollment status, as well as a copy of your university identification card to students@beta.techcrunch.com. Once you’re approved, we’ll send you instructions for how to complete your registration.

Our sponsors help make Disrupt happen. If you are interested in learning more about sponsorship opportunities, please contact our sponsorship team at sponsors@beta.techcrunch.com.",Yes,It will,https://techcrunch.com/2016/11/21/ar-pioneer-omar-tayeb-of-blippar-is-speaking-at-disrupt-london-dec-5-6/,2016-11-21 00:00:00,LABEL_1_relevant,0.9905688166618348,Virtual Reality,virtual_reality,TechCrunch,"Information, Discourse & Governance"
51,Futuristic 'Uncanny Valley' Explores The Potential Unintended Consequences Of VR,"As we dip our toes into virtual reality more and more each day, one could imagine a far-off unintended consequence — the blurring of virtual and actual reality.

A short film called “Uncanny Valley” explores that notion, with a bit of an “Ender’s Game” plot and feel. Here’s the setup from its creator, Federico Heller.

In the slums of the future, virtual reality junkies satisfy their violent impulses in online entertainment. An expert player discovers that the line between games and reality is starting to fade away. 3DAR’s latest short film explores the frightening potential of our next technological revolution. Behind the scenes coming soon! Stay connected, but not too much ;)

It’s well worth the watch.

How far is too far? How real is too real?",Yes,It may blur the lines between what is real and what is not.,https://techcrunch.com/2016/01/07/futuristic-uncanny-valley-explores-the-potential-unintended-consequences-of-vr/,2016-01-07 00:00:00,LABEL_1_relevant,0.9948358535766602,Virtual Reality,virtual_reality,TechCrunch,"Information, Discourse & Governance"
52,"In One Hospital, Finding Humanity in an Inhuman Crisis","After more than three decades as a critical care nurse at the University of Michigan Hospital in Ann Arbor, Carolyn Lyles has come to depend on her hour-long commute as a refuge from the din of alarming machines and harried voices that awaits her in the ICU. On a typical morning, Lyles, 55, wakes up at 4, prepares her lunch, makes coffee, and grabs a bagel for the road.

She slips out of the house while her husband Steve and three children are still in bed, climbs into her gray Buick Enclave, and drives from her home in Grosse Ile, near the point where the Detroit River spills into Lake Erie, past sleeping neighbors in darkened houses, across the Wayne County swing bridge and past the Chrysler engine plant in Trenton. As she cruises past the red approach lights that guide airliners into Detroit-Wayne County International Airport, she thinks of her 82-year-old mother, Mary, asleep nearby in the house that she grew up in. Mary, who raised Carolyn alone, was a neonatal ICU nurse. The days Lyles spent shadowing her mom at the hospital inspired her to become a nurse herself.

A deeply faithful Catholic, Lyles usually travels the dark highway in silence. She prays that God will deliver her to the patient who needs her most. Working so close to the fragile membrane between life and death, Lyles has always prayed for strength to get through the day. After two weeks of caring for Covid-19 patients, those prayers had taken on a frightening intensity.

Inside Michigan Medicine's Race to Beat Covid-19 Read the rest of the series here.

On Tuesday March 24, the 20-bed Critical Care Medicine Unit, or CCMU—the only place Lyles has ever worked since she received her nursing degree in 1987, just a year after the hospital opened—was suddenly closed and emptied of patients so that carpenters could install negative-pressure filtration systems for the anticipated deluge of Covid patients. Since Lyles and her CCMU teammates have pulmonary care expertise, they were temporarily reassigned to the Regional Infectious Containment Unit, or RICU, a special negative-pressure ward on the 12th floor of the adjacent C. S. Mott Children’s Hospital that can be activated in emergencies to isolate patients with highly infectious diseases like SARS, Ebola, and now Covid-19.

It was disorienting. “We were all blindsided,” Lyles tells me. Accustomed to working one-on-one with her patients on a floor as familiar to her as her own home, with colleagues she has known for decades and come to think of as family, Lyles was suddenly thrust into a 50-bed unit in an entirely different building, assisting a swarm of unknown doctors, nurses, and technicians swathed from head to toe in protective gear that would have made it difficult to recognize them even if she did know them. “You have no idea where anything is,” she says. “How do I get help? Who do I get help from? Where is everybody?”

This article was supported by The Economic Hardship Reporting Project.

Lyles did not have a patient of her own that first day in the RICU, so she served as a runner, fetching medications and supplies for nurses who were tending to patients behind glass doors that remained closed to minimize contamination. Nurses on the inside would write their orders on the glass with dry-erase markers—backwards, to make them readable to the runners—because it was impossible to communicate through N95 masks and glass and over the noise of ventilators and alarms. On the drive home that night, Lyles was exhausted, her back in spasms from the tension.

“I'm not sure I'm ready for this,” she thought to herself. “I'm really scared.”

“We are facing an unprecedented situation that none of us has lived through before. We know this is terrifying for all of us,” reads an email sent to employees of Michigan Medicine—the University of Michigan’s hospital system—on April 2, signed by CEO Marschall Runge and two other members of the executive team. By the time the email hit the inboxes of 28,000 employees—who include about 3,000 doctors, 6,000 nurses, 1,200 residents and fellows, and legions of support staff—Detroit, about 30 miles away, was reeling from an outbreak that had already filled some of the city’s largest hospitals to capacity.",Yes,"The way in which it blurs the lines between what is real and what is not. In other words, it can be difficult to tell what is real and what is not, especially when it comes to things that we have never seen before.",https://www.wired.com/story/in-one-hospital-finding-humanity-in-inhuman-crisis/,2020-04-19 11:00:00+00:00,LABEL_1_relevant,0.9962127208709716,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
53,Google's VR180 Cameras Are the Future of Point-and-Shoot,"Just about everyone agrees virtual and augmented reality are going to be important. The tech already sort of works, and will get better quickly from here. Gadgets offering the ability to truly feel as if you've been transported to another place, or to superimpose the digital world on the real one, will be transformative. Somehow. Eventually. For some reason. No one knows exactly what AR and VR will be good for, or when. They just know it's coming.

This week at CES, Google's partners are announcing two new cameras that support the VR180 standard, which refers to a new way of capturing 180-degree panoramic images.

Clay Bavor, the leader of Google's virtual reality team, is trying to strike a difficult balance: Bavor and his team are trying to build products that people will buy and use now, while preparing for the future, who knows how long from now, when all this world-changing stuff starts to happen. Right now, before all the tech is ready and everyone's used to it, coming up with things people want is tricky. (Just ask the folks at Oculus and Vive.) But recently, Bavor says he's found something that might be special: VR as a way to capture and relive important memories in your life.

Eyes on Eyes

This week at CES, Google's partners are announcing two new cameras that support the VR180 standard, which refers to a new way of capturing 180-degree panoramic images. Lenovo's making the Mirage Camera, designed to go along with the Mirage Solo headset, a completely wireless and self-sufficient Daydream VR headset. Yi, another Chinese manufacturer, is making a similar camera called the Horizon.

Yi's Horizon camera is another VR180 camera built in partnership with Google's virtual reality team. Google

Both are basically just point-and-shoots with two lenses on the front rather than one. They capture 4K, three-dimensional video that you can watch on YouTube, in Google Photos, or in any VR headset. They're not the highest-tech VR things you've ever seen, but they might be the easiest to understand. Which is precisely the point.

""People love going places,"" Bavor says. ""They love going places."" Google's Earth VR and Street View apps are both hugely popular among VR users, and Expeditions, a virtual field-trip platform, has proven popular in schools. It's not surprising, then, that the Google VR team has spent a lot of time thinking about how to help people capture places in VR for themselves and others to visit again later. ""But when we looked at how people use their cameras, it's very seldom that it's like, 'Let me take an aesthetically beautiful, well-composed photo,'"" Bavor says. ""It's about remembering a moment."" The photos, he says, aren't even really the point. It's about what happens when you look at them, the memories you get to re-live. And when you watch them back, the stereoscopic and super-wide footage feels so much more immersive than any photograph you've ever seen.

Bringin' Us Back

Both new VR180 cameras can livestream directly to YouTube, or save your memories to Google Photos. The Photos integration was particularly important, Bavor says, ""because we see this being such a strong product for memory capture. And memories live in your own private repository of photos until you decide otherwise."" If you want a clue of where this is all going, look at the Pensieve, the swirling store of memories that comes up repeatedly in Harry Potter. ""We've got a lot of work to do to get there fully,"" Bavor says, ""but I think there's something universal about memory and nostalgia and a desire to reflect on, step back into, be a part of, memories or experiences or time with people you care about.""

The best version of this tech, Bavor guesses, would require either tiling your walls with camera lenses, or some mythical tricorder device that recorded everything and everyone all the time. (He says both are roughly impossible, but has clearly given the idea a lot of thought.) But it's one big step in the right direction when you can take photos with a camera that sees roughly as you do---the cameras' lenses are the exact distance apart of the average person's pupils---and can faithfully display the scene in three dimensions. You can't move in it yet, but you can see it as it was.

VR180, like most things in VR right now, is the simple-but-usable version of what will someday be much cooler. It exists for a few reasons: because 360-degree video is actually really complicated to do well, because there aren't many great ways to watch 360 video, and because even when they do watch super-immersive footage, viewers don't tend to look around much. With VR180, your camera can look and operate more like a regular point-and-shoot, and viewers get a similarly immersive feel without having to constantly spin around.",Yes,Young people will soon have a lot more fun than they used to.,https://www.wired.com/story/google-vr180-cameras-from-lenovo-and-yi/,2018-01-09 19:15:00+00:00,LABEL_1_relevant,0.948515772819519,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
54,The pandemic has probably killed VR arcades for good,"A lagging trend of the past few months has been witnessing startups that COVID-19 seemed poised to kill end up scaling back some of those deep cuts and taking off again. Not all spaces have been quite so lucky; in particular, lately we’ve seen a host of location-based virtual reality startups shut their doors.

Virtual reality arcades weren’t exactly crushing it pre-pandemic; the small industry was already a bit of a Hail Mary for the virtual reality market, which has failed to push consumers to adopt headsets on their own and saw arcades as a way to warm up the general public to VR’s role in entertainment. Lackluster consumer interest and the throughput difficulties associated with quickly moving users through experiences were among the biggest challenges facing VR arcades.

This week, following a report from Protocol, Apple confirmed its acquisition of Spaces, a virtual reality arcade startup that had been forced to close its in-person arcades amid COVID-19 and had attempted a pivot to creating virtual environments for video chat software. An Apple acquisition is hardly a mark of failure, but it is unlikely that the company has any interest in reviving the startup’s arcade business.

Earlier this month, The Wall Street Journal reported that the U.S. subsidiary of Sandbox VR had filed for bankruptcy. Sandbox VR has raised quite a bit of money on the promise that they could revamp several industries at once. The idea was that mall operators on the decline would give great deals to some of these startups to set up physical storefronts as a loss leader to bring in a younger generation of consumers, while they could capitalize on mixed reality social media video to bring a level of viral growth to their VR offerings.

In July, UploadVR discovered documents that suggested Disney had terminated the lease of virtual reality startup The Void’s Downtown Disney location following months of COVID-19-related closures.

It was impossible to forecast the current pandemic when many of these investments were being made, but virtual reality arcades had already shown they were far from a sure bet. In late 2018, IMAX shut the doors of the last of its seven virtual reality arcades after investing tens of millions into its VR efforts.

With the future of in-person entertainment unclear, the question is whether virtual reality arcades have any chance of a rebound.

The fact is many of these startups were pushing up against current realities on multiple fronts and were attempting to seriously shift the landscape of 21st century digital entertainment, attempts that seemed daunting from the start.

As massive movie theater chains struggle to see how the pandemic will affect their industries in the long-term, it isn’t surprising that many of these startups have failed to see a light at the end of the tunnel and have shut down operations or been sold off. I suspect investors will be reluctant to back new efforts in this space and that the time horizon of COVID-19 will force current entrants toward pivots that look dramatically different from pre-COVID-19-era business models. (One caveat is that the VR arcade market certainly looks different in the United States compared to markets in countries like China and Japan where virtual reality arcades seem to fit a bit more snugly into popular gaming culture.)

If VR arcades survive or are reborn, it will be due to some pretty massive shifts in consumer behavior and VR adoption.

Virtual reality, as an industry, is in a tough spot. In the United States, it’s essentially only Facebook keeping the space alive in a meaningful way, and the company seems to be barreling ahead in its efforts to build a mainstream future for the technology on its own terms. Earlier this summer, Facebook announced that it was pulling its top-selling title Beat Saber from arcades for good by August. Since the acquisition of Oculus back in 2014, the ecosystem that sprang up around Facebook’s VR efforts has receded meaningfully, leaving the company in a lonely position once again.",Yes,"It could lead to a future where people are physically isolated from each other, which can have negative consequences.",https://techcrunch.com/2020/08/25/the-pandemic-has-probably-killed-vr-arcades-for-good/,2020-08-25 00:00:00,LABEL_1_relevant,0.9953625202178956,Virtual Reality,virtual_reality,TechCrunch,"Information, Discourse & Governance"
55,Free Guy Sees the Metaverse Through Rose-Colored Glasses,"Ryan Reynolds’ new movie, Free Guy, isn’t what you’d call light on schtick. It’s a film about a non-player character (NPC) in a video game, after all. But the first and most prevalent gimmick is the sunglasses. In Free City, the fictitious shooter game at the center of the story, sunglasses are what signify a player: someone who can shoot up a bank, steal a car, face-punch a stranger. They are, Guy (Reynolds) says, the “heroes.” Similar to the shades in the movie They Live, the players' eyewear works like in-game augmented reality specs, showing scores, power-ups, loot, etc. Sunglasses are the lens through which players are meant to experience Free City’s metaverse and decode its mysteries.

(Spoiler alert: Minor spoilers for Free Guy follow.)

Those shades are rose-colored. Guy, the audience eventually learns, is more than just an NPC in Free City—he’s actually an artificially intelligent bit of code left there by the game’s original developers, and he is just starting to gain sentience. When he pilfers his own pair of shades, the scales fall from his eyes and he sees his world the way players do. The twist, though, is that he doesn’t try to kill his human overlords; he just tries to make his world a more hospitable environment, one without daily shootings or robberies, where nothing blows up and the trolls don’t rule. More than artificially intelligent, he’s artificially empathetic. “The AI is taking us toward a more utopian kind of togetherness, this idea of agency and uniting, and creating agents of change,” Reynolds told WIRED in a recent interview. “I loved that idea. That it wasn’t Terminator or something. That Guy becomes sentient in an open world, a quite hostile video game.”

To be clear, Free City is not technically a metaverse; the film’s at-home gamers don't play it in virtual reality headsets. If anything, the game is a stand-in for what it's like to be Very Online: fun, but there’s danger around every corner. For Guy, an NPC, Free City is the whole world; it's as if he lives in a metaverse but has no offline counterpart. When he gets his sunglasses, he realizes that can be a world without violence, a place where trash talk isn’t incentivized. (One of the film’s more poignant running themes: The game's original devs were building an online utopia, but the corporation that bought it believed no one wanted to play such a thing, so it became a shooter.) Guy’s sentience comes with a mission to stop crime and right wrongs. Gamers streaming his antics transform him into a viral savior and question their own digital bloodlust. Guy’s vision of a nice place awash in Mariah Carey ( … sweet, sweet fantasy, baby/when I close my eyes you come and you take me …) feels possible. Nice dream.

It's here that Free Guy's virtual world diverges too much from the real one, and its (applaudable) optimism goes too far. Its video game environment and the culture around it (Twitch streamers, cocky gaming CEOs, overworked developers) appear painfully real, prescient even. Its main character doesn't. As an Everyman who can deliver a joke, there's no one better than Reynolds, but as an AI, Guy doesn't work. Not to get overly literal, but artificial intelligence is molded from received data. If Guy had indeed been living and learning in a world populated by violent trolls, he would likely have absorbed their ways. At best he would have picked up off-color jokes; at worst he would have ended up like Tay, that Microsoft Twitter chatbot that needed only 12 hours online to turn into a “foul-mouthed racist Holocaust denier.” And frankly, that’s likely not the worst. Free Guy contends that having a world of gamers, and internet fans, reevaluate how they think about an NPC could cause them to reevaluate how they see each other; maybe, just maybe, everyone could see the people they meet in virtual worlds as people. It might be the movie's best gag.

The thing about the internet and the metaverse is that they are one and the same, and yet one operates as a fully realized part of daily life and the other, while currently a buzzword, remains in its infancy. Unlike the dystopian vision Neal Stephenson had in Snow Crash, the conception of the metaverse today—the one Mark Zuckerberg wants Facebook to build—is a virtual and augmented reality full of human connections where people work, work out, play, and make stuff. It’s a place for NFT art and Ariana Grande concerts in Fortnite, where your headset tells you fun facts about the things you’re looking at. Uglier versions exist, too, but many modern hopes for the metaverse seem enmeshed in concepts similar to the OASIS in Ernie Cline’s Ready Player One. The internet, meanwhile, the bridge to the metaverse, is guarded by trolls and full of misinformation, hate, misogyny. At its most dire, it is a place where the thinly veiled dark side of every person is left free to roam and wreak havoc. The internet and the metaverse may exist on the same network, but they often exist on very different planes. Free Guy, it seems, wants to argue that out of the primordial ooze of the internet, a better virtual world could emerge, with the help of an AI made in the muck.

Honestly, this isn’t even really a flaw of Free Guy, or something that’s the fault of Reynolds or director Shawn Levy. Optimism needs its champions, and making popular art celebrating it can't hurt. But optimism is often a symptom of privilege. Zuckerberg can be confident in the power of algorithms; algorithms have been good to him, less so for people wrongly sent to jail by facial-recognition systems or misdiagnosed by an AI. It’s ironic, or perhaps simply unfortunate, that Free Guy features both a female developer fighting to reclaim her code and an in-game worker strike, when just last month Activision Blizzard employees staged a walkout over allegations of rampant sexism. If anything, Free Guy, and Free City, are mirror worlds—a look at what’s gone wrong with the internet and gamer culture, through the eyes of people who want to fix them.",Yes,"It reinforces the idea that the internet and the world are two separate places, when in reality they are one and the same.",https://www.wired.com/story/free-guy-ai-metaverse-review/,2021-08-13 11:00:00+00:00,LABEL_1_relevant,0.9933706521987916,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
56,8i Raises $13.5M To Chase The Human Side Of Virtual Reality,"There I stood, about a foot away from a mother and her newborn baby. The mother was talking to her daughter in a video captured in virtual reality for the child to see one day when she’s older. It was realistic. Almost scarily so. I was able to walk around a “room” and watch the mother speaking from every angle, including behind her.

I’ve never seen that much human detail with a VR getup on my face.

The really trippy part? The company shot followup footage a few weeks later and seeing the child “grow” with my own eyes was nuts.

That is 8i’s speciality and the New Zealand / Los Angeles / San Francisco based firm has announced a $13.5 million Series A round with participation from an interesting mix of investors to make VR more human: RRE Ventures, Founders Fund Science, Horizons Ventures, Samsung Ventures, Dolby Family Ventures, Bertelsmann Digital Media Investments, Sound Ventures, Signia Venture Partners, Inevitable Ventures, Freelands and Advancit Capital, Kevin Wall, Jeremy Stoppelman, John and Andrew Hendricks, End Cue and Kortschak Investments, Rothenberg Ventures and Boost VC.

8i is building a platform so that content creators can make their own 3D renderings and VR experiences available in anything from a dating app to a feature film. The 360-degree stuff is a nut that 8i cracked and it’s no wonder that there is a lot of interest around them. I mean, Samsung Ventures’ and Dolby’s involvement speaks volumes. The company is also opening a studio in Los Angeles that they plan on letting content creators use for free. The more creativity that flows around the technology, the better it can get.

“We’re focusing on a really hard problem, to bring humans into augmented realities,” 8i CEO Linc Gasking told me.

To focus on the humanity aspect was important for Gasking and his team of 27 (and growing), who come from companies like Pixar, DreamWorks, YouTube, Industrial Light & Magic, PayPal, Microsoft Research, Sony Computer Entertainment, Digital Domain, NVIDIA, Weta Digital and Valve.

“I didn’t want to do games. I wanted that human connection. I wanted to tell stories and connect with people in a three dimensional way.” Gasking told me of the 8i platform. Going beyond games is music to my ears, as well as the ears of investors.

But when is real too real? I’ve heard stories of people jumping into VR and what they see becomes so real to them that they react physically. Shoot, during one of 8i’s demos I was standing on a “cliff” and Gasking urged me to jump off. I didn’t, even though the entire thing was a mirage.

The key to keeping it safe, though, is one of Gasking’s core rules when developing these scenarios, which he tells me is “respect.” Respecting regular human boundaries is key to building a human-friendly world in VR, he tells me.

At the end of the day, 8i’s goal is to be an agnostic platform across AR, VR and the web. As far as mother in child standing before me? You can go check out the web demo later, but you really have to be in VR to “get it.”",Yes,"People becoming emotionally attached to computer-generated characters. While this may seem like a positive thing at first, it could have dangerous implications for people's mental health. If people are unable to distinguish between real and virtual relationships, they may start to prefer the latter, which could lead to social isolation and other psychological problems.",https://techcrunch.com/2015/10/14/8i-raises-13-5m-to-chase-the-human-side-of-virtual-reality/,2015-10-14 00:00:00,LABEL_1_relevant,0.9952025413513184,Virtual Reality,virtual_reality,TechCrunch,"Information, Discourse & Governance"
57,The Inside Story of How Oculus Cracked the Impossible Design of VR,"Palmer Luckey has never used an Oculus Rift.

That’s what the founder of Oculus keeps telling himself as he unboxes the commercial version of the virtual reality system he invented. Opens the package. Takes out the few elements—the headset, the single cable that connects it to a computer, the small cylindrical infrared camera that tracks it in space. Runs through the setup. And finally puts on the headset and takes stock of his surroundings.

Luckey has been doing this same thing over and over and over again, on different computers in different rooms on Facebook’s campus. He’s spent days repeating the sequence, putting himself in the shoes of a customer who has just received a Rift.

That customer could be anyone. Maybe it’s one of the hundreds of thousands of people who bought a developer-only iteration—the Kickstarted version in 2012 or maybe the more refined one that followed two years later. Maybe it’s someone who has spent the last few years with their nose pressed against the digital glass, following every wrinkle of the Rift’s progress on Reddit or podcasts or YouTube or in WIRED or even Oculus’ own lengthy, surprisingly transparent blog posts. Maybe it’s someone who experienced VR only recently, at SXSW or Sundance, and felt in their very marrow that the world was about to change.

Really, though, it doesn’t matter. After nearly four years of work, Luckey and his colleagues are about to share their long-gestating dream with the world. The Oculus Rift arrives tomorrow, and anyone who finds one on their doorstep must have an absolutely seamless experience. With all the momentum that VR has right now—the millions of people who are aware of it, the billions of dollars poured into it—Luckey would hate to see it stall because of something as pedestrian as a long wait for a driver update. So he opens a box, and he sets up a headset, and then he does it all over again. Because Palmer Luckey has never used an Oculus Rift.

The road to the Rift was paved with good design thinking. We take you through a selection of industrial prototypes to explain some of Oculus’ key decisions.

When you set out to create a virtual reality headset, you soon realize that the idea of form following function is bullshit. It’s a reductive canard. Yes, both of those things matter, and the Oculus Rift does need to be both beautiful and powerful, but it’s not something you hold in your hand—it’s something you put on your face. That’s a daunting prospect: Not only are you blind to the world around you, but there’s the whole I-look-nuts thing. (There’s also the whole here-comes-Skynet thing, but on that front we’ve got bigger, Go-playing fish to fry.)

That’s only part of it, though; once you put it on your face, it needs to disappear. It needs to be not just comfortable but light—or at least feel light. After all, it’s less of a window than it is a wormhole; the more you remember it’s there, the less you’re able to lose yourself in everything happening inside it. (And what’s happening inside it is a whole other challenge, one we’ll get to later.)",Yes,"It causes nausea.

The problem is that VR is such a new field, and the technology is moving so quickly, that there isn’t a lot of good, solid data on how to design a headset—or, for that matter, what a headset should look like. So Luckey and his team had to start from scratch",https://www.wired.com/2016/03/oculus-design-virtual-reality/,2016-03-28 00:00:35+00:00,LABEL_1_relevant,0.9957720637321472,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
58,"Game|Life Podcast: Awkward VR Porn Conversation, Then Kirby","On this week’s episode of the Game|Life podcast, Bo Moore and Peter Rubin join me for what can only be called a wide-ranging discussion.

Peter’s coverage of all things virtual reality continues in WIRED’s sex issue, on shelves now, with a story about the very very near future of VR porn. Then I dial it back a little and talk some more about Kirby and the Rainbow Curse on Wii U. And a bit about The Order: 1886 for PlayStation 4. And a bit about game reviews.[#iframe: https://downloads.wired.com/podcasts/assets/gamelifeaudio/gamelifereboot_139.ogg]",Yes,"It makes everything weirdly intimate. That goes double for VR porn, which is a thing that people are already creating and which will only get more realistic and immersive in the future.

The Kirby and the Rainbow Curse review is up now, and I found the game to be a lot of fun. But it’s also a bit",https://www.wired.com/2015/02/gamelife-podcast-episode-139/,2015-02-20 22:47:18+00:00,LABEL_1_relevant,0.992363214492798,Virtual Reality,virtual_reality,Wired,Social Norms & Relationships
59,Why Your Next Workplace Harassment Training Might Be in VR,"Prepping for a big presentation is stressful, and your boss isn’t making it any better. He’s leering at your coworker Rachel in the middle of a meeting (!), asking if she’s bringing a date to the company dinner (!!). I mean, what do you do? Say something? Take it to HR? Talk to Rachel? Every choice feels kinda wrong—even though you’re just seeing all this in a virtual-reality headset.

The discomfort is the point, says Morgan Mercer, founder of Vantage Point, the company behind the VR demonstration. Traditionally, on-the-job harassment training has been a choice between the lesser of two borings—schlocky video or snoozy slide deck—but Mercer wants to put you in the room.

WHO:

Morgan Mercer, founder of Vantage Point

—

HOMETOWN:

Charlotte, North Carolina

—

LAST JOB:

Head of digital marketing at online fashion consignment marketplace LePrix

—

FAVORITE VR EXPERIENCE:

“Take Flight,” by The New York Times Magazine

She’s a survivor of sexual violence herself; she knows the subject can be hard to talk about. One night, at a dinner in late 2016, the conversation turned to harassment. Everybody agreed on one point: People can’t identify it and don’t know when to step in. A few mornings later, she woke up with the idea that VR could help. She taught herself basic programming, entered some hackathons, and within nine months had secured funding and signed up multiple partners.

That was the easy-ish part. The rest of Vantage Point’s plan involves making the simulations as immersive as possible. Mercer envisions an alternate script in which users practice reporting an incident. She also wants to add an interactive voice component, so users can literally speak up. Not all the work happens inside the headset, though. When Mercer recently showed an HR professional an early demo, they pointed out that Rachel, the only woman in the scene, was the designated note-taker. “We’re over here excited to explore these power dynamics,” Mercer says, “and didn’t even realize there’s a level of bias built into the training plan.” They just needed another vantage point.

Grooming by Heather Coleman; shot on location at phase two

This article appears in the June issue. Subscribe now.

More Great WIRED Stories",,It will make us more empathy-deficient.,https://www.wired.com/story/vr-sexual-harassment-training/,2018-05-24 14:00:00+00:00,LABEL_1_relevant,0.9950557947158812,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
60,Virtual Reality Advertisements Get in Your Face,"Companies like Facebook-owned Oculus VR, Sony, Microsoft, and Magic Leap are working on consumer-geared hardware, some of which is slated to begin selling late this year. Already, Oculus VR and Samsung have released the Gear VR, a $199 developer-geared headset that lets you experience 3-D games and videos by inserting a Samsung smartphone into the device.

MediaSpike is among several companies aiming to serve brands that want to advertise on these new platforms. Founder and CEO Blake Commagere says the company started out a few years ago working to bring sponsored content to smartphone- and tablet-based games. Now the company is thinking about how billboards, videos, and other kinds of product placement can fit into the computer-generated worlds viewed on devices like the Gear VR, as well as on headsets that don’t yet have a firm release date.

From inside the headgear, it still looks pretty primitive. Before the blimp ride, I was driving around MediaSpike’s digitally rendered town, which is empty of activity beyond the movie billboards, blimp, and a giant display in a vacant town square that’s showing the trailer for the upcoming Minions film. I found it nauseating, too (a common complaint from people using stereoscopic 3-D technologies because of the disconnect between your visual and physical senses). But it’s still a heck of a lot more interesting than the banner and pop-up ads we’re used to seeing on websites and mobile apps.

The huge value of the online advertising market suggests it could be lucrative to experiment in this area. According to Magna Global, a media market researcher and investor, digital media revenue rose 17 percent in 2014 to $142 billion. It’s expected to climb another 15 percent to $163 billion globally this year.

A virtual billboard advertises a theme-park ride related to the Despicable Me animated films.

Dallas-based Airvirtise certainly hopes advertisers will be willing to try to reach people inside virtual scenes. It’s working on virtual 3-D models that are integrated with real-world locations, which it discerns from longitude, latitude, and elevation—think a giant Angry Birds game in a park or a life-size virtual car you can walk around. These things would initially be viewable through smartphone apps and eventually through the lenses of augmented-reality eyewear.

Founder and CEO Kevin Hart describes what the company has been showing off at the SXSW festival this week: a computer-generated drone seemingly hovering in the air about 20 feet away, with a banner flying below it, visible only when viewed through the camera lens of an iPad. As you move closer to the virtual drone, he says, it will get appropriately larger by taking into account motion sensors on the iPad to determine your location and position, allowing you to walk under and around it.

A company that has already had some success merging virtual ads with the real world is Blippar. A smartphone app created by the company can be used to view sponsored augmented-reality content, such as virtual footballers on Pepsi cans, or virtual nail polish shades from Maybelline.

There will be plenty of challenges ahead for companies working to bring ads to virtual and augmented reality. Beyond the obvious difficulty of working with emerging technologies, Hart and others must figure out what kinds of ads users will respond to, and which they might find too overwhelming or obnoxious.

Commagere acknowledges that, as with product placement in movies, it will be possible to overdo it with virtual products. (One example of this occurred during my blimp ride, when I turned around to see a pixelated can of Pepsi perched next to the craft’s control panel.) But he’s hopeful that this will feel less intrusive than conventional pop-ups and other types of distracting Web ads.

“I can’t imagine having a banner at the bottom of a VR screen the entire time,” he says. “That would be infuriating.”",Yes,It could be used to create overwhelming or obnoxious ads.,https://www.technologyreview.com/2015/03/18/11558/virtual-reality-advertisements-get-in-your-face/,2015-03-18 00:00:00,LABEL_1_relevant,0.996170699596405,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
61,Virtual Therapists Help Veterans Open Up About PTSD,"When US troops return home from a tour of duty, each person finds their own way to resume their daily lives. But they also, every one, complete a written survey called the Post-Deployment Health Assessment. It’s designed to evaluate service members’ psychiatric health and ferret out symptoms of conditions like depression and post-traumatic stress, so common among veterans.

But the survey, designed to give the military insight into the mental health of its personnel, can wind up distorting it. Thing is, the PDHA isn’t anonymous, and the results go on service members’ records—which can deter them from opening up. Anonymous, paper-based surveys could help, but you can’t establish a good rapport with a series of yes/no exam questions. Veterans need somebody who can help. Somebody who can carry their secrets confidentially, and without judgement. Somebody they can trust.

Or, perhaps, something.

""People are very open to feeling connected to things that aren't people,"" says Gale Lucas, a psychologist at USC's Institute for Creative Technologies and first author of a new, Darpa-funded study that finds soldiers are more likely to divulge symptoms of PTSD to a virtual interviewer—an artificially intelligent avatar, rendered in 3-D on a television screen—than in existing post-deployment health surveys. The findings, which appear in the latest issue of the journal Frontiers in Robotics and AI, suggest that virtual interviewers could prove to be even better than human therapists at helping soldiers open up about their mental health.

“Most people would assume these things are in conflict with each other—that you can’t have anonymity and rapport at the same time,” Lucas says. But a virtual interviewer can offer both. A few years ago, Lucas and her colleagues paired hundreds of test subjects with Ellie, an embodied AI designed to engage test subjects in verbal interviews. Participants sat alone in a room with the virtual therapist, who appeared and communicated via a television screen. Ellie would begin with general questions like “Where are you from?” to build rapport; gradually proceed to more sensitive, clinical queries, like “How easy is it for you to get a good night’s sleep”; and finish with mood-boosting questions, like “What are you most proud of?”

With a virtual interviewer, you don't have to ruin your career to begin seeking help. USC Psychologist Gale Lucas

But Ellie is no brainless bot. Unlike, say, Eliza, the 1960s computer program designed to respond to users with non-directional questions, Ellie uses machine vision to interpret test subjects’ verbal and facial cues and respond supportively. For example, Ellie not only knows how to perform sympathetic gestures, like nodding, smiling, or quietly uttering “mhm” when listening to a sensitive story—she knows when to perform them. Psychologists call these kinds of sounds and gestures backchannels. When interspersed appropriately throughout an interaction, they can help build rapport and elicit sharing.

Ellie's capacity for subtle and supportive engagement reveals fascinating things about humans, and how we choose to guard our secrets. Lucas and her colleagues told half their test subjects they’d be interacting anonymously with a virtual therapist. The other half were deceived into thinking there was a person pulling Ellie's strings. In the end, the participants who thought they were talking with the virtual therapist alone were significantly more likely to open up. For civilians, at least, just removing the idea of human presence led to more fruitful clinical sessions.

To see if Ellie could help soldiers reveal their PTSD symptoms, Lucas and her colleagues recruited soldiers recently returned from Afghanistan. As in the previous study, Ellie began each interview with rapport-building questions and ended with positive, mood-boosting ones. But this time, Ellie’s clinical questions were geared toward symptoms of PTSD, specifically. Questions like:",Yes,It can lead to people feeling more comfortable revealing secrets. This is because virtual reality can provide anonymity and rapport at the same time.,https://www.wired.com/story/virtual-therapists-help-veterans-open-up-about-ptsd/,2017-10-17 10:00:00+00:00,LABEL_1_relevant,0.9953182339668274,Virtual Reality,virtual_reality,Wired,Security & Privacy
62,Billionaires See VR as a Way to Avoid Radical Social Change,"The future of virtual reality is far more than just video games. Silicon Valley sees the creation of virtual worlds as the ultimate free-market solution to a political problem. In a world of increasing wealth inequality, environmental disaster, and political instability, why not sell everyone a device that whisks them away to a virtual world free of pain and suffering?

Tech billionaires aren’t shy about sharing this. “Some people read this the wrong way and react incorrectly to it. The promise of VR is to make the world you wanted. It is not possible, on Earth, to give everyone all that they would want. Not everyone can have Richard Branson’s private island,” Doom co-creator and former CTO of Oculus John Carmack told Joe Rogan during a 2020 interview. “People react negatively to any talk of economics, but it is resource allocation. You have to make decisions about where things go. Economically, you can deliver a lot more value to a lot of people in the virtual sense.”

Virtual reality is an attractive escape, but it’s not a solution to the world’s ills. The problems of the real world will persist beyond the borders of the metaverse created by companies such as Epic, Valve, and Facebook. Without decisive and radical action, our planet will continue to burn, the gap between the rich and poor will grow, and totalitarian political movements will flourish. All while some of us are plugged into a virtual world.

Worse, the virtual world will be one owned and controlled by the companies that create them. If you want a picture of the future, imagine a Facebook-branded set of VR goggles strapped to an emaciated human face—forever.

By the principle of the free market Silicon Valley lives and dies by, virtual reality is a loser. Only 1.7 percent of Steam users have a VR headset, according to a December 2020 hardware survey. And while it’s true that sales of headsets are up during the pandemic, roughly 30 percent in 2020 over 2019, video game sales in general are up overall.

Valve released Half-Life: Alyx in March 2020, just as the lockdowns were beginning. This was the first new Half-Life game in 13 years, the continuation of a franchise fans had been desperate to play for more than a decade. It sold well for a VR title, somewhere north of 2 million copies, but didn’t match the incredible numbers of 2020’s top-selling titles and was quickly forgotten by the mainstream press. Unless you’re really into VR, you probably weren’t talking about Half-Life in 2020.

The reasons why are obvious. First, virtual reality is expensive. At the high end, Valve’s premiere headset—the Valve Index—costs $1,000. On the cheaper end, Facebook’s Oculus Quest 2 is $299. To play Alyx, those headsets need to be wired to a high-end gaming PC. The price of these machines vary, but something that can handle VR will cost around $1,000. Once the machine is built and the headset hooked up, the player will need to carve out a dedicated physical space to play the game. Most games require a minimum of about 6.5 feet by 5 feet, but the more space you have the better.",Yes,"It’s too expensive and too much of a hassle to set up and use for the average person. It requires a level of dedication that most gamers don’t have, and it’s not nearly as immersive as people want it to be.",https://www.wired.com/story/billionaires-use-vr-avoid-social-change/,2021-02-15 14:00:00+00:00,LABEL_1_relevant,0.9955610036849976,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
63,These FX Wizards May Have Solved VR Films' Biggest Problem,"Sebastian Marino already has an Academy Award for his special-effects work on James Cameron's Avatar, but the other evening he became a director, and staged a tiny shoot in his apartment. He and a small crew set up an array of cameras and filmed two actresses in front of a green screen. Only one of them delivered lines. ""I imagine you're feeling a bit like Alice right now,"" she said enigmatically, ""tumbling down the rabbit hole. Like her, I would suggest that you choose carefully."" She pointed in one direction, then another. And that was it; they took down the green screen, and everyone went home.

But Sebastian Marino wasn't done. The next step was to turn that tiny shoot into the future.

Overnight, the data he'd captured with his cameras simmered; in the morning, he dropped the resulting file into the Unity 3D videogame engine and placed the actresses in a bare-bones apartment. In one room, a neon sign on one wall read WE WANT WHAT WE SEE, and a lamp against another wall cast its light across the actress' right side. In the other room, he made the interior light red and darkened everything so you couldn't see that the other model wasn't wearing much. After all, when you're making the future, you don't want to be, as Marino said later, ""overly suggestive.""

The thing is, this wasn't a videogame. It was a movie—and, even better, quite possibly the first actual VR movie to use human beings. The actress who gave the Alice-down-the-rabbit-hole speech looked nothing like a videogame character who was waving from the other side of the Uncanny Valley; she looked like herself. She was wearing the same clothes, her face moved perfectly, and her motions were fluid and flawless. Yet, she was standing in a room that didn't, in any real way, exist.

What Marino has managed to do, along the rest of his fledgeling VR company Uncorporeal, is solve in a very real way the cardinal problem that plagues the prospects of virtual filmmaking. And what's even more astonishing is that he and his two co-founders—whose careers encompass decades of wizardry at Lucasfilm/ILM, WETA, the Euro Space Agency, Google X, Microsoft, and Electronic Arts—have done it in stealth mode. They provided the company's seed funding themselves, they've never shown up at a VR conference, and they've used email accounts belonging to a holding company so that no one caught wind of the name Uncorporeal. ""The number of people that have seen our stuff,"" Marino says, ""you can count on your fingers and toes.""

If you hadn't noticed by now, we are way down the rabbit hole.

Caught In the Frame

As we careen headlong toward 2016 and the long-awaited launch of consumer VR, there's still one very real problem plaguing the medium. It's not simulator sickness, or display resolution, or optical solutions; all of those things are effectively solved by now, and while they're not perfect, they're unimaginably better than they were even 18 months ago. It's not input; the three consumer systems coming (Oculus Rift, HTC Vive, and Playstation VR) have demonstrable and elegant control schemes. And it's not content: from dozens of game titles under development to Netflix apps to virtual museum tours, a robust pipeline ensures that early adopters will have plenty to do.

The problem is movies. Specifically, live-action video capture. Consumer VR relies on external cameras to track your headset in space, and then translate its movements and positions into virtual environments. It's called positional tracking, and in VR gaming it's what allows you to crouch behind a desk, or peek around a boulder, or look under a table by making those movements in real life—crouching, leaning, bending. But there's currently no way to bring positional tracking to video.

Think about it: If you position an array of cameras around a central point to act as the ""eye"" of the person wearing the headset, then you're stuck with whatever 360-degree view you get from that vantage point. Of VR's optimal ""six degrees of freedom""—nodding your head, turning your head, tilting your head left/right, and actually moving through space on three axes—VR video is thus far confined to moving your body. In other words, you can look around all you want, but you can't change your position, no matter how much you crouch and lean. (And with your brain registering physical movement without seeing movement reflected in your field of vision, you might actually feel some discomfort.)",Yes,"It makes movies really boring for the time being.

The great promise of VR is that it can let you experience movies in a completely new and immersive way. But right now you can only experience them in the way you've always experienced them: as a passive observer. You can look around, but you can't walk around. You can",https://www.wired.com/2015/10/uncorporeal-vr-movies/,2015-10-07 11:00:33+00:00,LABEL_1_relevant,0.9957963228225708,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
64,New VR Film Perspective 2: The Misdemeanor Drops You in the Middle of a Police Shooting,"Most stories about new virtual reality films or documentaries contain some description of the experience itself—with so many people still having never experienced the medium, to do otherwise would be a disservice. Yet, to do that for any part of *Perspective 2: The Misdemeanor *would risk diluting a viscerally haunting experience. So take this shred of a synopsis and know it contains multitudes: The new project, from director Rose Troche and her VR partner Morris May, which just debuted at the Sundance Film Festival, puts the user on all sides of a police-involved shooting.

It’s the second experience in Troche and May's Perspective series—the first let people experience two sides of a sexual assault. The goal this time, Troche says, is to show an even-handed representation of everyone involved in a fictional police shooting: a teenager who gets shot, his brother, and two officers. The viewers’ perception of what happened in the shooting changes each time they watch a different perspective. Which is the whole point.

“Everybody has made a mistake in this whole thing, and it escalates so quickly,” says Troche. “But it’s really for you to experience, to try to feel what it’s like to have that fear, to feel like Oh my god, what did I just do? I just shot someone. We wanted to show the wholeness of the situation.”

It’s really for you to experience, to try to feel what it’s like to have that fear, to feel like Oh my god, what did I just do? I just shot someone. We wanted to show the wholeness of the situation. director Rose Troche

Perspective 2: The Misdemeanor is something that’s really never been done with VR before. There are experiences that deal with social justice issues, for sure, but they’re usually documentaries. What Troche and May’s experience does is use a hypothetical situation to give viewers an experience that they would otherwise (hopefully) get in a mediated form, whether via news reports or low-quality video clips uploaded to social media sites. It’s the kind of thing that can only be done with the immersive and empathy-inducing power of VR. “It’s a way to use the medium that’s so successful,” says May, CEO of VR outfit Specular Theory.

It also is coming at a time when people on all sides of the issue are grappling with how to respond to police brutality. Even Troche, who grew up in Chicago and had more than a few situations growing up where “I felt like ‘shit’s going to go down,’” struggled with her feelings on the issue. To get authentic perspectives from all sides of the encounter, she spoke to young people in her Brooklyn neighborhood as well as family members who work in law enforcement. She also, unfortunately, got a lot more information on police-involved shootings over the last year while she worked on the project as news of hundreds of deaths filled newspapers, cable news networks, and social media feeds.

“This has the same motivation as last year’s piece, which was to move a conversation forward,” says Troche, who started her career with indie films like Go Fish. “Living in New York with the [#BlackLivesMatter] protests that were going on, and with these things just continuing to happen, it felt to me like the philosophically natural next place to go.”

As of now, the self-financed project has no distribution deal in place, so for the foreseeable future people will only be able to see Perspective 2: The Misdemeanor at film festivals like Sundance. While the issue of how to get people to watch VR is one that’s plaguing the entire industry right now, Troche and May would especially like to get their project in front of as many people as possible—if for no other reason than to can help provide insight into an issue people have often made up their minds about.

“Who will approach the piece and only watch one thing and think that they have the story?"" says Troche. ""That’s pretty much what we have in real life. The piece demonstrates the fact that just because you’re there, doesn’t mean you see everything. Through the four strings, you get to see the full picture.”",Yes,"It exposes people to an experience that they would otherwise not be able to have. This can lead to a heightened sense of empathy and understanding for others, which in turn can lead to a more nuanced and productive dialogue about social issues.",https://www.wired.com/2016/01/vr-police-shooting/,2016-01-26 12:00:44+00:00,LABEL_1_relevant,0.9965012073516846,Virtual Reality,virtual_reality,Wired,Social Norms & Relationships
65,FTC antitrust watchdogs reportedly investigating Meta's VR division ...,"The US Federal Trade Commission and at least three states are investigating Meta for antitrust violations in its virtual reality division, according to Bloomberg. The report says the FTC and attorneys general from New York, Tennessee, and North Carolina began speaking last year with third-party VR developers who have concerns about Meta’s business practices. It’s one of multiple probes into Meta’s dominance in the small but growing market of VR.

The Bloomberg report references well-known controversies around Meta (formerly Facebook) and its VR division (formerly Oculus). Regulators have reportedly asked developers if the Oculus app store discriminates against third-party apps whose features overlap with its own offerings, and they’ve apparently questioned Meta’s strategy of selling the Meta Quest (formerly Oculus Quest) headset at a $299 price point that heavily undercuts the price of other VR headsets. The FTC declined to comment on Bloomberg’s report.

The FTC has allegedly already opened a probe into Meta’s acquisition of Within, the company behind VR fitness app Supernatural. The US Justice Department also reportedly investigated similar claims in late 2020. German regulators publicly announced an investigation around the same time.

So far, VR has still gotten little attention compared to other Meta divisions. The FTC is currently pursuing an antitrust lawsuit against the company over its acquisition of Instagram and WhatsApp; after a setback last year, a judge revived the suit earlier this week. But as Meta has begun emphasizing its role as a steward of a VR-heavy “metaverse,” the area may invite more legal scrutiny as well.",Yes,It may invite more legal scrutiny because it is a new technology.,https://www.theverge.com/2022/1/14/22884198/ftc-state-ag-investigating-meta-facebook-vr-oculus-antitrust,2022-01-14 00:00:00,LABEL_1_relevant,0.983075737953186,Virtual Reality,virtual_reality,The Verge,"Information, Discourse & Governance"
66,Oculus Execs Believe Controllers Are The Missing Link In Virtual Reality,"When you put a child in virtual reality, they instinctively raise their hands hoping to see them, says Oculus CTO John Carmack. Yet Oculus doesn’t have its own gloves or handheld controllers, despite rumors amongst the VR community that Oculus would reveal a controller today at its Oculus Connect conference. Carmack concluded “The missing link in VR is controllers”.

But considering ‘The Future Of VR’ panel with the Oculus execs started with the question of “where is the VR controller?”, it seems clear the company will move to build or support handheld input devices.

In fact, early today during their demo of the new Oculus Crescent Bay headset prototype as source told an Oculus employee they wished there was a controller, and the employee responded “it’s coming.” Yet currently, Carmack says that people want to see their hands in virtual reality, “and we’ve got nothing there right now”.

What that will look like is still coming together, though. Carmack, the famed technologist from Quake-marker id Software, said that “Controllers is such a factious and contentious issue that it’s not going to be clear when someone wins.” Oculus’ 22-year old founder Palmer Luckey said that “There’s no clear path towards what is best [for controllers]. People haven’t identified the problems.”

Eye tracking was another form of input the Oculus big wigs discussed as part of the future of VR. The idea is that rather than turning your head to move your vision, you could one day just glance around. Oculus Chief Scientist Michael Abrash discussed that VR headsets would probably have to support “foveated rendering“. This means that the resolution of an image fluctuates so the most important parts are higher-res. What’s important can be determined through eye tracking. However, Abrash said foveated rendering is probably still too computationally intense for current VR rigs to handle. In the end, though, the top priority is to not make people sick.

As for where Oculus goes with input devices, Luckey did confirm that it has been doing testing in the handheld input space, but it’s been a challenge. “It’s surprising how accurate you have to be [with detecting head movements as a controller] to make it good, and you have to do the same with your hands. Having any latency makes you feel like your hand is dead.”

Today’s Oculus Crescent Bay announcement was all about creating a headset which offers true “Presence”, where you feel like you’re actually transported into the virtual world. With a motion tracking headset where you can tilt, lean, duck, and even walk around a little, it’s coming close to accomplishing the first stage Presence, where it’s immersive as long as you don’t try to run around or lift your arms. But as Oculus Chief Architect Atman Binstock said, the next frontier is “Hand Presence”.

During the Q&A I asked what the execs thought were the advantages and disadvantages of different input devices like dual-wielding nunchuks, motion tracking, and gloves, and which is each of their favorites. Luckey responded that a great device for guns is not the same as a great device for swords is not a great device for productivity apps. He said there will need to be a device that’s good for generally interacting with virtual worlds, though. With so many different use cases for VR and different corresponding input devices, it leads me to believe that Oculus will build a platform for connecting third-party input devices to its headsets. This would leave controller fragmentation to the developer community so Oculus can focus on generalizable problems and opportunities that exist across VR apps.",Yes,It could become addicting and people might want to spend more time in virtual reality than in reality.,https://techcrunch.com/2014/09/20/virtual-reality-controller/,2014-09-20 00:00:00,LABEL_1_relevant,0.9966560602188108,Virtual Reality,virtual_reality,TechCrunch,User Experience & Entertainment
67,"The Incredible, Urgent Power of Remembering the Holocaust in VR","Pinchas Gutter has returned to Majdanek at least a dozen times, but this trip is his final one to the onetime Nazi concentration camp. His first was one he was 11, when he was taken to Majdanek; now he's 85 years old, and this is the last time he'll come here to tell people what the Nazis did to his family. As he rides up to the shuttered camp in the backseat of a chauffeured sedan, he talks about why he's told his story so many times. Without that living, breathing reminder, the Holocaust becomes easy to forget—or even deny. Without personal accounts, Gutter says, it’s hard for people to accept its atrocities as “the gospel truth.”

Gutter is right about that. That’s why his trip to Majdanek, and the horrific experiences he recounts in the camp’s barracks and crematorium, are being preserved with virtual reality—thanks to the USC Shoah Foundation. The Last Goodbye, which is debuting at this week's Tribeca Film Festival, is a 17-minute VR experience that the Holocaust remembrance foundation produced expressly to add to its archive of survivor testimonies. Each of those video interview accounts acts as a record of a person’s experience with genocide, but the immersive capabilities of VR in The Last Goodbye allows Gutter to tell his story—the last time he saw his parents and twin sister alive, the moment in the camp’s shower room when he was certain he was going to die—in the spaces where they happened.

“Knowledge is one thing, but connecting to history enables us to think about consequences in a deeper more personal way,” says Shoah executive director Stephen Smith. “What this technology allows us to do is learn in a much more immersive way so you don’t forget this.”

Virtual Reality, Not Fake News

Preserving history is more necessary now than ever. Just last week White House Press Secretary Sean Spicer, referencing Syrian dictator Bashar al-Assad’s use of sarin gas, said that Adolf Hitler ""didn't even sink to using chemical weapons."" The misstep led human-rights nonprofit Anne Frank Center to call on President Trump to fire Spicer. The problem runs deeper than gaffes, though; as a recent New York Times story reported, understanding of World War II and of the six million people who died in the Holocaust is fading as the number of survivors diminishes. (Estimates put the number of remaining survivors at around a half-million, and most are in their 80s, if not older.) As Holocaust deniers get bigger megaphones thanks to the internet and anything can be labeled “fake news,” a survivor's testimony—captured in the camp where he was imprisoned—is about the most powerful tool imaginable for preserving history.

Courtesy of The Last Goodbye

But, technologically speaking, capturing Gutter’s story wasn’t easy. A standard 360-degree video could have captured Gutter telling his story at Majdanek, but the experience would have been limited; users wouldn’t have been able to move closer to Gutter or get a better look at the camp’s various rooms. For that, you'd need to use a videogame engine to build a CG environment. But then co-creators Gabo Arora (Ground Beneath Her) and Ari Palitz found a workaround: photogrammetry. When they went to Majdanek to film in July 2016, they brought along a crew from graphics company Otoy to capture thousands of images and 3-D scans of the camp. They then captured Gutter telling his story at the camp against greenscreen. When they returned, the team handed over 30,000 pictures and 300 million polygons—basically the pixels in the mesh of a 3-D scan—to VFX company MPC, which then spent the next six months stitching them together to create a VR environment users could explore with Gutter by their side.

It was important that we film Pinchas Gutter in the spaces that he experienced these things during the Holocaust—that we captured those rooms in photogrammetry and he was really there. That way, when users go through this experience they feel like he was connected to the space, as opposed to filming him on a stage in Burbank. Last Goodbye co-director Ari Palitz

Making the recreation accurate to a pixel-by-pixel level was absolutely integral. Holocaust deniers, Smith notes, often point to small details as proof that something was falsified, and the very point of Last Goodbye was its authenticity. Tim Dillon, MPC's head of VR, says that as his team went through and pieced together the experience they had a ""really important agenda to keep everything extremely factual. ... We didn't want to do anything in modeling or CG terms that seemed fake in any way."" This became particularly tough when the team was trying to remove signage—Majdanek is a museum, now, after all—from the background of shots. It was a tough process, but the result feels as real as anything in VR can right now.

""It was important that we film [Pinchas Gutter] in the spaces that he experienced these things during the Holocaust—that we captured those rooms in photogrammetry and he was really there,"" says co-director Ari Palitz. ""That way, when users go through this experience they feel like he was connected to the space, as opposed to filming him on a stage in Burbank.""

History Presented by Hamilton's Set Designer

When users go through Gutter’s experience this week at Tribeca, they’ll be doing it in a room-scale HTC Vive setup. They’ll also be going through it on a set created by David Korins, Hamilton's production designer—who has also designed stage sets for Lady Gaga and Kanye West. The structure sits atop a bed of gravel and is covered in mirrors. The idea, Korins says, was for the space to reflect its environment and also each person’s reaction. “There are so many emotional on-ramps that people have with regard to this subject matter that I think I would be presumptuous to try and prescribe,” Korins says. “My hope is that the kind of environment we’ve created allows for anyone’s attachment to history, whether it’s incredibly specific or it’s just a vague idea, can have a place to live.”",Yes,"The sense of physicality of distance.

The Last Goodbye is the first time in VR that the USC Shoah Foundation has used photogrammetry to capture an environment. It’s also the first time it’s worked with Korins, who, like Arora, Palitz, and Smith, has a history of working with",https://www.wired.com/2017/04/vr-holocaust-history-preservation/,2017-04-20 16:00:11+00:00,LABEL_1_relevant,0.9951165914535522,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
68,How to Reduce VR Sickness? Just Add a Virtual Nose,"In the 1950s, the Navy introduced a simulator that taught pilots to fly a helicopter from the comfort of a virtual cockpit. They could take off, navigate bumpy air and land without ever leaving the ground. It was a breakthrough that allowed an increasing number of pilots to train without the risk of crashing. But the sim wasn't all that comfortable, and a significant number of pilots felt sick as hell while using it.

It wasn’t motion sickness per se, though the symptoms were comparable: Dizziness, nausea, sweating, disorientation. Researchers of the day dubbed this physiological phenomenon “simulator sickness,” an early ancestor of the flu-like symptoms some feel after strapping on virtual reality headsets today.

Eliminating simulator sickness is a major interest of the burgeoning VR industry, but so far there hasn't been a clear answer. Home remedies include drinking alcohol, while companies like Oculus Rift are exploring better positional tracking and improved display resolution. But researchers at Purdue University believe they've found a way to reduce the negative physical effects of virtual reality by using something that’s right in front of your face.

“We’ve discovered putting a virtual nose in the scene seems to have a stabilizing effect,” says David Whittinghill, an assistant professor in Purdue University's Department of Computer Graphics Technology. That’s right, Whittinghill says placing a schnoz in the lower center of a headset's screen has been shown to reduce the effects of simulator sickness by 13.5 percent.

Simulator sickness is still being studied, but researchers often point to sensory conflict a primary cause. This theory states that a dissonance between what your eyes see on screen and the kind of motion your body feels can lead to disorientation and feelings of nausea. Say you’re riding a virtual roller coaster. As you creep up the coaster’s first big hill, your eyes will register an upward incline, but your vestibular system---the tubes of liquid in our ears that help us to gauge our position in the world---remains unchanged. “Our bodies don’t like that,” says Whittinghill.

Whittinghill and his team of students (Bradley Ziegler, James Moore, and Tristan Case) say anecdotal evidence shows a fixed reference point in a frame, like car dashboards and cockpits, tend to reduce feelings of simulator sickness. It got them thinking about the nose as a natural reference point and how it's conspicuously absent from goggle-like virtual reality headsets.

In the small study, the team had 41 participants use various VR applications (one simulation of walking around a Tuscan villa, another of riding a roller coaster). Half played the games with the virtual nose, the other half played without. Whittinghill found participants with the nose were able to play the Tuscan villa game for 94.2 seconds longer than those playing without, while time played on the roller coaster game increased by 2.2 seconds.

“That’s not enough,” says Whittinghill. But it is a promising start, particularly because the participants playing with the virtual nose didn’t even notice it was there. “It’s a big honking nose,” he says. “It never occurred to us that they wouldn’t perceive it, but they were almost universally baffled about what we were even talking about.” Whittinghill says this is likely a result of “change blindness,” a perceptual phenomenon that allows our perceptual system to ignore objects that we see over and over again. Whittinghill’s theory is that the nose’s proximity to our eyes leads our brains to filter out its presence. “It’s likely to be hitting those same sensory neurons,” he explains. “I’m just guessing the neurons are saying no, this isn’t a real object, I’m going to subtract this from my perception.”",Yes,"Simulator sickness, which is caused by a sensory conflict between what the eyes see and what the body feels. However, placing a virtual nose in the scene has been shown to reduce the effects of simulator sickness.",https://www.wired.com/2015/04/reduce-vr-sickness-just-add-virtual-nose/,2015-04-20 11:00:27+00:00,LABEL_1_relevant,0.9799521565437316,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
69,Horrifying VR Film Puts You Inside a Drunk Driving Crash,"https://www.youtube.com/embed/YwRCyW9bcV4

If you learned how to drive in the US, there’s a good chance you suffered through Red Asphalt---the famously gory driver’s ed film the California Highway Patrol has been making and remaking since the 60s.

Now the time-honored tradition of scaring teens into staying sober behind the wheel with gruesome images has taken on a new dimension: virtual reality.

Diageo, the British liquor company that owns Johnnie Walker, Baileys, and Guinness, has tapped VR tech to convey the risks of drunk driving with an extra dose of drama. Decisions puts you inside three cars destined for a fatal encounter late at night on a stretch of rural road. A couple on their first date after having a baby, three twenty-somethings heading to a party, and Sam, a young woman leaving a bar after a meeting that included bottle service.

The company plans to roll out its full setup---complete with a motion-chair---at events like music festivals, and to work with police departments and school systems to get teens into the experience. For everyone else, the four-minute horror show is now available via VR channels on YouTube, Facebook, and The New York Times.

Decisions retains its potency even as a 3D YouTube video seen sans goggles and chair. As Sam drunkenly caroms down the road, you sense the increasingly erratic movement of the car, the jarring vibration when she briefly drifts off the road. You can look around, see the traffic on the road, and glimpse back to view her behind the wheel.

Virtual reality makes sense for this application: It’s aces at eliciting emotions more effectively than two-dimensional media. The ability to look around, even as a bystander---to see the traffic on the road and the driver ineptly shifting gears---makes the tension more intense and palpable as the crash nears.

The PSA’s creators, led by New York advertising agency VaynerMedia, are banking on that immersion. “What you want is for the user to feel what the experience is like in that car,” says director Jason Beauregard. “That sensation and feeling comes from the experience hitting your emotional receptors. It heightens your senses in a way you don’t get with conventional film, and you really feel that with the twist in the end, when essentially you, as a viewer but also a character, die.”

Beauregard used a high-end, 24-camera Jaunt ONE system designed specifically for capturing high-speed VR content, and a Sony A7-based array well-suited to low-light filming. The film’s end might not earn an “R” rating, but it’s grisly and evocative. Diageo hopes it’s just harrowing enough to convince people to keep this virtual experience from coming to life.",Yes,It makes things more intense and palpable as the crash nears.,https://www.wired.com/2016/12/horrifying-vr-film-puts-inside-drunk-driving-crash/,2016-12-02 14:00:55+00:00,LABEL_1_relevant,0.9966539144515992,Virtual Reality,virtual_reality,Wired,Economy
70,"Honey, I Brought the Kids Into Virtual Reality","LG: When you talked to Philip Rosedale about the monetization of the metaverse, did Web 3 come into the conversation at all? What's the distinction there between the metaverse and Web 3, if there is one? And then if you did talk about Web 3, then how does he feel about ... Here's another buzzword, everyone take a shot, the blockchain?

GE: We did talk ... I'm not sure that the word Web 3 came up, I'd have to check the transcript. But we did talk about how the world that Second Life built, what lessons if any, that might have for the current moment, where there is a lot of overlap between the two buzz words, metaverse and Web 3. Web 3 is a refresher. For listeners, it basically refers to this idea that there will be a ... The next phase of the web will be very decentralized because it will be built on blockchain technology. Oh and so ...

LG: And it will be in the hands of just a few large companies that basically can control the internet, or top layer experiences of the internet now.

GE: Right, exactly. It's a utopian vision of that, that like all utopian visions, is unlikely to play out in the way that the utopians are describing it. But then once we go down a layer from Web 3, there's all these sub buzzwords, like NFTs, which everyone has heard a lot about by now. And the idea here that ... One place of overlap is that NFTs, which are goods or tokens that live on the blockchain and ownership is passed along a blockchain and recorded so that you can make digital goods scarce, people see that as a really appetizing way to make money and create economies in a virtual space.

Second Life is way ahead on this. It's always had an economy within its virtual world, and the difference is that they built this before blockchain existed. And so instead of putting all the things that you might own in your Second Life account on a blockchain, it's just in a publicly accessible database that is managed by Second Life. And this is very nerdy, but it cuts to us one of the many big debates when it comes to the usefulness of blockchain technology, which is the blockchain is a distributed database. That's what a blockchain is. It's a database that instead of being on one server or one computer, it is on a whole shitload of ... Can we curse on Gadget Lab?

LG: Sure.

GE: It's on a whole boatload ... OK. It's on everybody's computer all the time. And Philip's point was, a lot of the appeal of the blockchain might be just in the transparency part of it. And you don't need a blockchain to achieve that. A centralized database that is public, may actually accomplish a lot of what you need there. All your transactions and ownership and the price of assets in Second Life is kept on this Second Life database. And then the other thing that we talked about was Second Life's approach to goods because he refers to the things you buy and sell as NFTs. But again, they're not technically on the blockchain because this is coming from a pre blockchain world. And so you can buy and sell digital goods within Second Life. And in fact, that's how Second Life makes money, is by taking a commission essentially on these sales. Or that's one of the ways it makes money.",Yes,The fact that it makes the physical world seem boring in comparison.,https://www.wired.com/story/gadget-lab-podcast-536/,2022-01-28 13:00:00+00:00,LABEL_1_relevant,0.988509237766266,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
71,"VR Is Cool and All, But Will People Buy It?","Despite 2016 being billed as a banner year for virtual reality, people aren’t yet buying fully fledged VR headsets at the rates that Sony, HTC, and Oculus would like. As New Scientist points out, devices at the lower end of the sector, such as Google’s Cardboard and Samsung’s Gear VR, appear to be slowly gaining traction. But more powerful devices are yet to sell at volume.

Mark Zuckerberg admitted as much at the recent Oculus developer conference in San Jose. There, he said that the Oculus Rift had suffered from a “slow start.” That echoes earlier warnings from the Facebook CEO, who has suggested that VR could take as many as 10 years to hit the mainstream. “These kind of new platforms take a long time to develop,” he pointed out in a recent earnings call.

Those sound more like explanations and excuses than solutions, though. John Carmack, the chief technology officer at Oculus, thinks he’s identified a major problem with the technology. Here he is speaking to Business Insider:

""We are coasting on novelty, and the initial wonder of being something people have never seen before. But we need to start judging ourselves. Not on a curve, but in an absolute sense. Can you do something in VR that has the same value, or more value, than what these other [non-VR] things have done?""

That is a very good question indeed. It’s perhaps no surprise that, currently, watching movies is one of the go-to VR activities—because other than that and gaming, just about anything else can be done just as efficiently on a PC or tablet. Mark Zuckerberg promises that VR will be a next-generation computing platform, of the magnitude of PCs and smartphones before it. But, as Carmack says, first it needs to find some more compelling use cases that people find valuable. Until that happens, sales are likely to remain slow.

(Read more: Reuters, New Scientist, “Why Oculus and HTC Need to Watch Out for Sony in VR,” “Facebook Begins Its Push to Make Us Socialize in Virtual Reality,” “VR’s Big Surprise: 3-D Worlds Have Little Appeal”)",Yes,"People are not yet buying fully fledged VR headsets at the rates that Sony, HTC, and Oculus would like. This is largely due to a lack of compelling use cases that people find valuable.",https://www.technologyreview.com/2016/10/13/157002/vr-is-cool-and-all-but-will-people-buy-it/,2016-10-13 00:00:00,LABEL_1_relevant,0.992659628391266,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
72,How a VR Company Became the Airbnb for NFTs,"Jacob Loewenstein started off his talk at the Augmented World Expo by apologizing.

It was a small crowd—fewer than a hundred people, masked and spaced apart in Ballroom B of the Santa Clara Convention Center in the heart of California’s Silicon Valley, waiting to hear about collaboration software. Loewenstein is the head of business development at Spatial, a venture-backed startup that has spent the past couple years convincing enterprise clients they needed to strap on headsets and buy into its virtual reality meeting apps. But on Wednesday of last week, Loewenstein launched into a presentation on NFTs, virtual art houses, the Utah Jazz NBA team, and the curse of having been christened a “future of work” app.

“This is kind of a weird situation,” Loewenstein began. “For those of you who have used Spatial, you might be wondering, ‘WTF? What has Spatial become? How many more buzzwords can they throw out …’ And the answer is, infinitely more buzzwords if it helps us make money. Just kidding.”

Loewenstein may have couched the company’s money-making goals in humor, but Spatial really is following the money. Right now, that happens to be in the direction of the much-hyped NFT art market. NFT refers to “non-fungible tokens,” often described as certificates of ownership of digital assets. Some individual artists are raking in millions by selling not just a piece of digital art itself, but that tokenized proof of ownership. The token is managed on a blockchain, which means crypto is the default currency. According to a recent Bloomberg report, the crypto art market generated $3.5 billion in sales in the first nine months of 2021.

“The industry seems to be rallying around this idea of an interoperable, NFT-driven metaverse, which we think we serve in a unique, super-simple, and fast way,” says Anand Agarawala, Spatial’s cofounder and chief executive, in an email to WIRED. Spatial’s rapid transition from hosting VR board meetings to hosting NFT auctions is emblematic of the fast moves many tech startups have to make if they hope to sell a product that’s better and cheaper than what larger competitors could offer. But for Spatial, which had aligned itself with partners like Microsoft and was selling its software to clients like Mattel and Pfizer, the doughnut drift into the notoriously volatile world of NFT art seems especially risky.

Meta Masterpieces

Agarwala and Loewenstein say the change was more of an evolution than a pivot, to use the Silicon Valley parlance; their users dictated what Spatial would become. Sure, being able to appear as an avatar in a “holographic office” was useful, especially as the white-collar world embraced remote work during quarantine. Some office leaders had been using Spatial to host virtual team events, to give remote workers a sense of presence with one another. (I met with the Spatial team in their own app once, switching from the Meta Quest 2 headset to the web to a smartphone, all of which support the Spatial app. WIRED’s Julian Chokkattu has used Spatial too.)

But something odd happened. When the Spatial app first launched on the Quest 2 headset in 2020, the majority of its users were accessing the app in VR. The company thought maybe the moment had arrived—given that the world was living through a pandemic—where people would be more comfortable wearing a VR headset for extended periods of time. Not so much. Spatial started to receive feedback that people didn’t actually want to take endless meetings in VR and would rather have the option to just click a link and join on the web like they might join a Zoom meeting. So the company built a mobile app and became web-friendly. Now 75 percent of people using Spatial’s virtual reality meeting rooms aren’t using a VR headset at all.

“It’s almost entirely on a desktop computer, sometimes on mobile,” Loewenstein says. “So during this month, when the metaverse has been the most talked-about ever … what most people relate to is not a headset-based experience.”

Then, in January of 2021, something else happened. A Parisian artist named Yacine Aït Kaci had been tasked with building a virtual museum to celebrate the tenth anniversary of ELYX, an entirely digital, genderless, nation-agnostic ambassador created by the United Nations. The artist, who sometimes goes by YAK, chose to host the virtual museum event in the Spatial app.",Yes,"People want to collect everything.

“It’s a digital museum, and so we’re hosting it in the Spatial app,” Loewenstein explains. “It brings together a bunch of artists and musicians, and they’re all in VR together. The idea is to explore and collect the",https://www.wired.com/story/spatial-airbnb-for-nfts/,2021-11-15 12:00:00+00:00,LABEL_1_relevant,0.9324626326560974,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
73,A VR Escape Room With a Twist: You're Drunk,"Following in a long line of escape-the-room first-person puzzle games, the upcoming videogame Drunk Room adds a simple narrative twist: you're drunk. In this case, you're recovering from a spirited bachelor party and you've locked yourself inside your own apartment.

Rather than moving around the apartment, players are restricted to looking and zooming in on certain locations. Look in the right spot and you might find an item to help you escape, but you might also find a hidden gag. Thanks to the first-person VR view and the running narration, the joke finds gave me a real shock when I tried the game at the BitSummit festival in Kyoto this weekend.

While complaints of motion sickness are a frequent knock against virtual reality games, in the case of Drunk Room it feels more like a feature than a bug. The symptoms of a hangover and the sensation of strapping on a VR helmet are not all that different.

However, Drunk Room wasn't designed to be a commentary on VR gaming in general. ""It was not intentional,"" says Arun Mehta, who ported Drunk Room to the Oculus Rift.

Two Drunk Room games already available for smartphones, with a third game in development. For now only the first game is getting the VR treatment, but director Masaya Kamiyama says if players respond well the ports will continue.",Yes,It can make people feel drunk.,https://www.wired.com/2015/07/drunk-room-vr/,2015-07-11 16:07:32+00:00,LABEL_1_relevant,0.9944357872009276,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
74,"Oculus Rift And Thalmic's Myo Armband Are A Match Made In Heaven, Say Founders And Investors","Total video game immersion might not be as far away as you think: The Oculus Rift is a huge step in the right direction, and it may have an optimal bedfellow in Thalmic’s Myo armband, the gesture control wearable that picks up on electrical impulses from your arm to deliver fine-tuned control over connected devices. While the Oculus Rift virtual reality headset can track head movements, and even now sense when you move forward or pull back, it still requires that you use a physical controller in most cases, which tends to dampen the realism to a degree. You’re not actually going to reach for that ladder run when climbing in-game, for instance, or grip that stock when you’re taking aim with your assault rifle. Oculus Rift and Myo are such a natural fit that it hasn’t escaped the notice of its investors. Spark Capital has a stake in both companies, and that’s no coincidence: A source close to the firm tells me that they considered the possible cross-device potential when they made their investment in each company, both of which were announced in June last year. Myo is marketing its devices as a much more broadly focused input mechanism, but the gaming segment is the fatted pig ready for market for investors, we’re told. The Oculus/Myo team-up isn’t just speculative, either. An email from Myo founder and CEO Stephen Lake confirms that indeed, development efforts are underway to link up the two pieces of hardware. “There are projects using both Myo + Rift,” he explained via email. “For example, there are developers in our Alpha program integrating both with Unity for various games. I think it’s a badass use case.”

For Oculus Rift, the key to success appears to depend at least in part on the headset’s ability to provide a convincing simulation of reality. Disconnects between what users are seeing in-game what they think they should be able to do in terms of character control and in-world interaction. For Myo, the big hurdle will be demonstrating a focused consumer use case that appeals to a big enough segment of the consumer market. In other words, Myo hooking up with the Rift is like chocolate meeting peanut butter, and it’ll be interesting to see how deep that relationship eventually goes.",Yes,It could lead to people becoming more isolated from the physical world.,https://techcrunch.com/2014/02/05/oculus-rift-and-thalmics-myo-armband-are-a-match-made-in-heaven-say-founders-and-investors/,2014-02-05 00:00:00,LABEL_1_relevant,0.9960196614265442,Virtual Reality,virtual_reality,TechCrunch,"Information, Discourse & Governance"
75,Palmer Luckey Says Working With Weapons Isn't as Fun as VR,"Who needs the metaverse when your life can be as weird as Palmer Luckey’s? In 2016, the founder of the virtual reality startup Oculus was unceremoniously pushed out of the company that acquired it—Facebook. Zuckerberg and his minions had soured on Luckey’s Trump-embracing politics. At the time, few would have guessed that the fanciful technologist, gamer, and cosplayer who once posed on a virtual beach on the cover of Time Magazine would become a major figure in defense technology. But Luckey quickly founded Anduril, a Founders Fund-backed startup devoted to cutting-edge military tech.

Luckey is now winning billion-dollar Pentagon contracts. One of them is for a counter-drone system based on its “battlefield operating system,” called Lattice. Anduril’s demo video shows one of the company’s sentry surveillance towers detecting a hostile drone and dispatching a small high-speed drone of its own to literally knock the intruder out of the sky. Recently, Anduril acquired a company that makes robot submarines. Luckey’s video games are now real—and deadly.

Anduril has a valuation of nearly $5 billion, making Luckey a rare founder of two unicorns. He is unusual for a military contractor. Perpetually garbed in a Hawaiian shirt, and occasionally still in cosplay threads, his vibe is much more cheerful hacker. His conservative politics also make him an awkward figure in Silicon Valley. (One of his sisters is married to the right-wing provocateur and congress member Matt Gaetz.) Unapologetic and upbeat, Luckey spoke of his talks with Ukrainian president Volodymyr Zelensky, the ethics of defense technology, and Mark Zuckerberg’s dive into the metasphere.

The interview is edited for length and clarity.

Steven Levy: How does the Ukraine invasion affect the way you think about your strategies?

Palmer Luckey: I met with President Zelensky shortly after we started, and I last saw him about two years ago. He was one of the few leaders on the European continent who understood that you can’t deter expansionist dictatorships using mean words or moving money around, that it could only be deterred through credible threat of force. That was not a popular opinion among his allies at the time. But that’s the thesis of our company: You want to have really strong technology that deters conflict by raising the cost high enough so that it's not thinkable.

What were you talking about with Zelensky and his deputies?

I can't get into specifics. But I will say Zelensky reached out to us way ahead of most world leaders, at a time when people didn't believe in applying autonomy to warfare. He and a handful of others were seeing the future and realizing that autonomy was going to be an important part of deterring conflict. I'm really devastated that he wasn't able to stop it, that the whole world wasn't able to stop it.

If Anduril technology had been deployed, would that conflict be playing out differently?

There's a few assumptions in that question, like we aren't involved.",Yes,"Creating the metaverse. The metaverse is a shared, persistent, online virtual world. It is the successor to the internet, and it is the natural evolution of virtual reality. The metaverse will be a place where people can meet, interact, and do business in a way that is not possible in the physical world.",https://www.wired.com/story/palmer-luckey-drones-autonomous-weapons-ukraine/,2022-03-14 11:00:00+00:00,LABEL_1_relevant,0.9953538179397584,Virtual Reality,virtual_reality,Wired,Economy
76,Interview With John Edwards Second Life Perps,"Wow. 10 Zen Monkeys has a great interview up with the perpetrators of the John Edwards ""Feces Spewing Obscenity"" Second Life vandalism scandal.

It's a fantastic read. Not surprisingly, the guys who portrayed John Edwards in blackface and wore ""Bush '08"" pins weren't Republicans. I think it's pretty hysterical frankly, that anyone could think they were, but never forget the fact that instinct tells us to dehumanize the enemy and that any person on the Internet with a sense of humor can play right into that, even if they are on your philosophical side.

LOU CABRON: Why did your attacking avatars wear “Bush ‘08″ buttons? MUDKIPS ACRONYM: Everything we do is for laughs, and we thought “Bush ‘08″ would be interpreted as humor — as I’m sure you know, Bush obviously can’t be re-elected in ‘08. However, the resulting aftershock from the “blogosphere”, particularly on the left, has been enormous, when they thought the raiders were Republicans. This was completely unexpected, and frankly hilarious. I’m a bit disillusioned with my own party after this event, actually, as someone who did read blogs like the Daily Kos and expected some honest and truthful journalism. However, it seems as if everyone played a giant game of telephone, taking the Republican assumption and adding on more and more anger and hostility as it went on.

Another not particularly surprising development is that these guys are articulate and intelligent... although they have an unfortunate predilection for citing Encyclopedia Dramatica and thinking ""lulz"", ""evar"" and ""nubz"" are funny. Everyone take note: jokes repeated a million times by everyone on the Internet are by the very definition of humor not funny.

Anyway, these guys are my heroes. You know, I sometimes wonder what will happen if we ever do get an open source metaverse up and running. At that point, corporations and political parties will have to start playing ball with the pranksters of the internet. That could be a lot of fun.

Who are Second Life’s “Patriotic Nigras”? [10 Zen Monkeys]",Yes.,"It will give rise to new forms of play and that these new forms will lead to new forms of labor, just as poker and baseball have led to professional gamblers and professional ball players.",https://www.wired.com/2007/03/interview-with-1-2/,2007-03-09 15:00:00+00:00,LABEL_1_relevant,0.9808336496353148,Virtual Reality,virtual_reality,Wired,Economy
77,VR Trainings Are Not Going to Fix Corporate Racism,"In 2017, while working at a well-known media company, I took a vacation to my home country, Nigeria. When I left, my hair was permed-sleek and straight. When I came back, I had long braids. “Oh my god, I love that!” one white woman said to me in the pantry, while going in on it without my consent. Another, wide-eyed, asked, “How long do those take? It’s so cool!” and proceeded to invade my space to inspect. Neither woman, it seemed, considered her actions culturally insensitive—which is part of the obstacles plaguing people of color in the workplace. We learn to smile and keep it moving in those uncomfortable moments.

Companies have long failed to address such encounters. Instead, they rely on one-size-fits-all diversity, equity, and inclusion (DEI) training—mind-numbing slide shows, dull videos, and obscure certificates to sign at the end. These programs are typically forgettable, lack assessable impact, and, studies show, ineffective. After the killing of George Floyd sparked nationwide outrage last summer, corporate America leaned harder into these superficial solutions. Companies were quick to promise an inclusive work environment, flooded their social media pages with Black and brown faces, extolled allyship, and put on socially distanced town halls on race.

Recently, however, some major corporations are trying something new: virtual reality. What if, VR creators say, instead of slides on the impact of unconscious bias, corporations could have employees experience discrimination themselves? By centering the perspectives of people of color in digital simulations, tech firms claim they can help companies be more equitable and less reactive, and better measure DEI commitments.

This approach is a step up from the caricatures in PowerPoint decks. But these problems go deeper than inadequate inclusivity training—and it will take more than advanced technology to fix them. No virtual world can teach white America to see what they don’t want to see in the real world; to see that Black people exist outside of racial stereotypes and acts of brutality against us.

Virtual reality as a tool to increase racial understanding is not new. Tech startups and established firms like Debias VR, Vantage Point, Oculus’s I Am A Man, and Google: Immerse VR's Racial Identity have explored the potential of simulations to foster racial empathy. Yet humanity, measured by the increase in hate crimes, is no less racist.

In 2020 a report released by the International Data Corporation found that demand for virtual-reality experiences is on the rise, and sales of VR headsets are predicted to grow 48 percent annually over the next four years. Combined with corporate America’s heightened awareness of its DEI shortcomings, that makes this an ideal time for tech firms to try again—it’s good business.

Praxis Labs, for instance, is a new virtual-reality-based platform that allows users to take on identities of different racial and gender backgrounds to face bias. After beta-testing with Zoom, Amazon, Google, Uber, and Target, it officially launched in February. The founders—Elise Smith, a Black woman, and Heather Shen, a first-generation Chinese woman—say their Pivotal Experiences DEI training program is an immersive solution that will close existing learning gaps.",Yes.,"It could potentially lead to individuals developing a false sense of empathy and understanding for others, as they have not actually experienced what the other person has experienced. This could lead to people not taking action in the real world to help those who are actually facing discrimination.",https://www.wired.com/story/vr-trainings-corporate-racism/,2021-05-26 12:00:00+00:00,LABEL_1_relevant,0.9962420463562012,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
78,Maybe VR Shouldn't Give You Heaven—Maybe You Need Hardship,"Swimming with sharks! Behind the wheel of a Formula 1 race car! On board an interstellar expedition! Virtual reality promises immersive experiences that sound like sweepstakes prizes. For one group of designers, though, VR shouldn’t give you heaven, but hardship.

After researching neuroscience studies on “embodiment,” the cognitive illusion of being someone else, design collective BeAnother­Lab set out to give people a taste of adversity. A performer wears a vest with a first-person camera, which streams their perspective to a user’s Oculus Rift, along with a voiceover about their experience. As users interact with what they see in their headset, the performer mimics their movements, and the participants can witness in a very real way what it’s like to *be *the person whose story they’re listening to. The group has created shared experiences that range from being a Sudanese refugee seeking asylum to being a different sex, even to living with physical disabilities.

So far, the projects are all shown at universities and art residencies, but BeAnotherLab cofounder Philippe Bertrand sees a broader need. “CGI works, but you don’t need a machine to feel empathy,” he says. “Just look the person in the face and talk with them.” In one recent project, a dancer in a wheelchair spoke about the challenge of navigating city streets. When users sat in a chair, headset on, and manipulated a joystick, the dancer moved her wheelchair in a corresponding way; when users looked in a virtual mirror, they saw her face. But when they moved their legs, hers stayed still—which was by far the most moving part of the whole experience.",Yes,It promises immersive experiences that may lead people to forget about the real world.,https://www.wired.com/2016/04/virtual-reality-beanotherlab/,2016-04-06 12:00:08+00:00,LABEL_1_relevant,0.9916822910308838,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
79,AR Eyewear Fools Your Belly (and Brain) Into Feeling Full,"In their demo video, a person wearing the AR headset sits in front of a blue screen holding what looks like an Oreo. He sits still and stares at it, and the Oreo appears to grow in his hand (a laptop to his side lets the audience follow along).

A dozen subjects tested the system, Narumi says. When their food appeared 1.5 times its natural size, the testers at 10 percent less. On the other hand, when researchers “shrank” the food to two-thirds its natural size, their subjects ate 15 percent more.

The setup is a bit cumbersome to use at the moment, but the goal is to spruce it up and slim it down so similar headsets can be used at the dining table. Healthy food will be programmed to appear smaller, and less healthy dishes will look bigger than they really are. Baby carrots anyone?",Yes,Becoming disconnected from reality. This is seen in how people who use virtual reality headsets are more likely to overeat or undereat.,https://www.technologyreview.com/2012/11/15/181610/ar-eyewear-fools-your-belly-and-brain-into-feeling-full/,2012-11-15 00:00:00,LABEL_1_relevant,0.994258999824524,Virtual Reality,virtual_reality,MIT Tech Review,User Experience & Entertainment
80,How Virtual-Reality Sharks Influenced the Dystopian World of Near Enemy,"Wonders, being wonders, are best experienced first hand. And the story of the future—the actual, here-it-comes, hold-it-in-your-hands future, as opposed to the far-off, hazy, wouldn't-it-be-great-to-have-hoverboards future—is often a story molded from first-hand experience. I am by no means an Early Adopter (I'm somewhere between Late Majority and Laggard), so every technological leap of faith I've made has come as a result of trying something out. Why do I need a tiny box that carries every single album I own? I once thought, until I listened to someone else's iPod. Why do I care if my phone can also direct me with GPS? seemed like a perfectly logical line of reasoning until someone else’s phone did just that. Right now, I feel no visceral need to, say, enjoy a ride in a driverless car, but I can also imagine that might change dramatically once I ride in a driverless car. And even as someone who's since written a pair of novels about a future jam-packed with virtual reality, I never truly believed that virtual reality was coming until I stepped inside the Cube.

The Cube, or The ISL CUBE, as it's more accurately called, is ""an immersive, stereo-capable (true 3-D) visualization chamber manufactured by TAN Projektionstechnologie of Dusseldorf, Germany."" This according to the website for the Beckman Institute at the University of Illinois at Urbana-Champaign, where one such Cube laboratory is in operation—one of just five in use in universities across America. In 2009, I traveled to the Beckman Institute on assignment for New York magazine, researching a story about brain cognition. The story had nothing to do with virtual reality and, really, nothing to do with the Cube—I was more interested in an experiment they'd run using a different 3-D simulator (called the Cave) in which they'd studied how distracted people are while using cellphones and trying to cross the street. (Short version: Very.) But my host, a professor at the Beckman, pulled me aside at one point and asked, ""Want to see something cool?"" In my experience as a writer, that's a question you never say ""no"" to.

In Adam Sternbergh's new book, the Limnosphere—a VR evolution of the Internet—becomes the site of a terrorist conspiracy. courtesy Crown

So he took me inside the Cube—an enclosed room (here's a schematic) consisting of projection screens on all sides, and which, once you enter wearing a goggle-like contraption (specifically, ""a Stereographics LCD shutter-glasses system""), can create an immersive 3-D environment. The first such environment he showed me was—no joke—the flight deck of the starship Enterprise. The Cube, he explained, was mostly used for studies on cognition and spatial awareness, but also had a few cool bells and whistles, like the Enterprise simulation.

I should say here: The flight deck was not at all convincing as a simulation, but that made it no less mind-blowing as an experience. Everything was rendered in blocky, obviously computer-generated shapes, like a very crude cartoon or early AutoCAD rendering. Still—it felt real, like this was a space we could actually move through. Then, with the flick of an unseen switch, my host was able to transport us to a different environ—this time, we were underwater. A blocky computer-generated shark swam by. Again, the shark was very obviously not an actual shark, yet even now, years later, it's hard to convey how crazy and exhilarating this all felt. Something I'd seen depicted in movies and TV as far-off fantasy was happening all around me. The objects seemed like objects. The shark seemed like a shark. I reached my hand out to touch it. It wasn't there—yet it was. Which is when I knew: This is coming. This is no longer if, but when.

I was familiar, of course, with the concept of virtual reality, mostly from pop-culture: the Holodeck from Star Trek, the world of The Matrix, the novels of William Gibson and Neal Stephenson. I remembered, too, the late '90s VR-boom, exemplified by magazines like Future Sex, that never seemed to actually go boom. Yet here, in front of my very eyes (or, in front of my shutter-glasses), the future was made manifest. Sure, it looked like the Pong version of VR (or maybe Donkey Kong) but it was clear, even to my laggard's mind, that with enough time, money, and computing power, Pong inevitably gives rise to Avatar.

So when, years later, I sat down to write a novel, Shovel Ready, that takes place in a near-future New York that's been devastated by a terrorist attack, and asked myself, ""OK, but what about technology?"" I naturally recalled the Cube. That primitive, herky-jerky, animated shark swam through my mind once again. The result, in the novel, is the Limnosphere, an entirely immersive VR evolution of the Internet, which serves as an escapist playground for the rich in Shovel Ready and, in the sequel Near Enemy, becomes the site of a terrorist conspiracy.

At first glance, this notion of a virtual world seemed, in some ways, familiar; as I said, I'd also seen the Holodeck and read Neuromancer and heard about Second Life. But familiarity is partly the point: In my experience, the future seems outlandish until it suddenly seems inevitable. I was both reassured and unnerved to learn, months after my book came out, that Facebook had bought Oculus, thus promising to one day turn every living room into a version of the Cube. And that's exactly the experience I had that day on the plains of Illinois. The Cube, in hindsight, feels less like a simulator than a miraculous time machine—vaulting me forward to a not-so-distant place, where we're all inevitably headed.

Adam Sternbergh is a contributing editor to New York magazine and the author of the Spademan novels. The second installment—Near Enemy—comes out tomorrow.",Yes.,It can make people more likely to believe things that they would otherwise think are impossible.,https://www.wired.com/2015/01/virtual-reality-near-enemy-essay/,2015-01-12 12:00:52+00:00,LABEL_1_relevant,0.996189296245575,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
81,"Why go on safari when you can feed an African lion live, via VR","Why go on safari when you can feed an African lion live, via VR

One of the main uses of VR that has emerged since the technology experienced a resurgence is the ability to make you empathise with what you are experiencing. This happened to me the first time I ever did VR, when I watched Clouds Over Sidra, a short VR documentary that put you in the shoes of a Syrian refugee girl, living in a camp in Jordan. I don’t mind admitting that it actually made me cry on my video about the film. It’s this empathy effect that new startup Fountain Digital Labs is is trying to tap into with its Virry VR platform (now nominated for a Webby Award) which allows people to virtually engage with real African animals up close and in their natural habitat. Not all of us can go on a safari, but this just-released immersive virtual reality experience has now launched on PlayStation VR.

Filmed in 4K VR at the Lewa Wildlife Conservancy in Kenya, using strategically placed high-definition cameras, Virry VR creates a safari-like experience which also allows the user to control aspects of the narrative.

You can feed lions and even share mud baths with rhinos. Players can also take a trip down an African river and learn about endangered animals.

The initial release of Virry VR for PlayStation includes 35 minutes of experiences with wildlife.

Virry VR users can learn interesting facts, answer questions, and interact with the animals through the virtual experience.

For Virry VR it costs $9.99 and the Live Camera Subscription is $1.50/mo. You can also subscribe for $2.50/mo and thus to donate $1 to the Lewa Wildlife Conservancy.

For those who don’t have PS4, Virry’s original BAFTA award-winning app for iOS mobile devices is also an option. Subscriptions costs $19.99 a year.

Svetlana Dragayeva, Fountain Digital Lab CEO, says: “Virry immerses players in the lives of real animals, encouraging discovery, empathy, and problem solving, while helping them to better understand nature, conservation, and the world around them.”

The experience has won plaudits from Founding Director of The Virtual Human Interaction Lab at Stanford University, Jeremy Bailenson. He says “I have seen thousands of spherical videos and the footage from Virry VR is the most stunning nature scenes I have ever seen in any medium,” said Bailson. “Seeing a big rhino in its natural home from the perspective of a mud puddle inches away, or having a lion literally lick my face to test if I am edible, were among the coolest things I have done in VR to date. I predict this project will be a huge success in motivating people to learn more about nature and ecosystems.”

Dragayeva is a passionate about the capability of VR to awaken us out of our senses which have been deadened by imagery which is shocking or unusual: “As Sontag observed back in the 70s, ‘At the time of the first photographs of the Nazi camps, there was nothing banal about these images. After 30 years, a saturation point may have been reached. In these last decades, “concerned” photography has done at least as much to deaden conscience as to arouse it’.” So I am in this search for something new. Something that moves, touches, tickles an empathic impulse, creates a bond. VR seems to be well-suited for this at this current moment.”",Yes,It will make you more empathetic than ever before. Platforms like Virry are just one of the many examples that show how VR is being used to create more empathy in people who experience it.,https://techcrunch.com/2017/04/12/why-go-on-safari-when-you-can-feed-an-african-lion-live-via-vr/,2017-04-12 00:00:00,LABEL_1_relevant,0.9946609139442444,Virtual Reality,virtual_reality,TechCrunch,User Experience & Entertainment
82,AR Is Making Its Way into the OR,"More ambitious is a project by Cambridge Consultants that seeks to provide surgeons with the ability to load results from MRI and CT scans into software that renders them into a 3-D visualization of a patient’s organs. The idea: surgeons would be able to perform keyhole procedures with an extra layer of information in front of them, allowing them to take better-informed actions.

The concept is certainly a good one. Such approaches could allow the technology to alert surgeons if they, say, stray too close to a particularly delicate part of anatomy during a procedure. Or else they could help guide less experienced doctors during complex procedures.

But that, of course, raises some interesting questions. Namely, how comfortable would most people feel about surgeries carried out based on data provided via a HoloLens? As Shafi Ahmed, a surgeon at Royal London Hospital, explained to New Scientist: “We can’t trust these systems at the moment.” So the answer is: probably not very, for now at least.

There are clearly some hurdles to overcome. One of the largest will be that of image registration: loading MRI scans into a HoloLens is all well and good, but the body itself moves around naturally, and can look different during surgical procedures—the device will need to adjust the AR image accordingly, which is harder than it sounds. And at any rate, most doctors like to be sure of the technology they’re dealing with, so adoption of the most advanced surgical AR systems will be slow at first.

It is, however, easy enough to imagine the systems being used for less complex augmentation—such as providing relevant numbers or stills of medical scans in the periphery of a surgeon’s vision. So don't be too alarmed if your surgeon asks for the HoloLens as you're about to go under the knife.

(Read more: Engadget, New Scientist, “Microsoft’s New Idea: A Hologram Headset to Rewrite Reality”)",Yes,"It takes people out of the real world and into a simulated one. This can have a number of negative effects on people, including causing them to lose touch with reality, become more isolated, and become addicted to the virtual world.",https://www.technologyreview.com/2017/05/11/151744/ar-is-making-its-way-into-the-or/,2017-05-11 00:00:00,LABEL_1_relevant,0.9910878539085388,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
83,"Palmer Luckey’s firm has won a contract to make VR, AI, and drones for the Pentagon","One of tech's more controversial figures has a new plan: developing virtual reality and other tech for the US military.

Divisive: Luckey is a VR pioneer who made a fortune by selling his company, Oculus Rift, to Facebook. He has become known for his conservative ideas and comments, and for seeking to develop a “virtual border wall” to help control illegal immigration. Now his new firm, Anduril, has reportedly bagged a contract with the Pentagon on its controversial Project Maven, according to The Intercept.

Virtual patrol: Anduril is developing technology called “Lattice AI” that’s designed to provide soldiers with a view of their surroundings using sensor fusion, virtual reality, and machine learning. “Lattice AI is able to detect, classify and track any car, person, drone or other threat in a restricted area,” the company says on its website. Anduril is also developing a very easy-to-fly drone, called Ghost (shown above), as part of the Lattice system.

Valley backlash: The US government and military are increasingly seeking to collaborate with Silicon Valley in hope of tapping into the latest tech trends and speeding up technology development cycles. But several companies have experienced backlash after inking contracts to help Uncle Sam, especially for work involving artificial intelligence. Most famously, Google was forced to abandon a contract to help the Air Force process drone imagery after worker outcry.

Polarizing filter: Luckey and Anduril both have close ties to the investor and Trump advisor Peter Thiel, his investment firm Founders Found, and his company Palantir, which provides big-data analytics to US defense and intelligence. (Both Anduril and Palantir are named for artifacts described in J.R.R. Tolkien’s Lord of the Rings.) Indeed, the story of Anduril shows how polarizing and political it is for the military to collaborate with Silicon Valley—and how some technologists are seizing the opportunity.",Yes,"It brings people closer to real life experiences that they might not otherwise have. This is particularly true for military applications, where people can use virtual reality to get a sense of what it is like to be in combat without actually having to be in combat. While this can be beneficial, it can also be harmful if people use virtual reality to escape from",https://www.technologyreview.com/2019/03/11/136702/palmer-luckeys-firm-has-won-a-contract-to-make-vr-ai-and-drones-for-the-pentagon/,2019-03-11 00:00:00,LABEL_1_relevant,0.994511604309082,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
84,China watches and learns from the US in AR/VR competition,"When Chi Xu left Magic Leap and returned to China, he had big ambitions. He believed China would have its own augmented and virtual reality giants, just as how the domestic smartphone industry birthed global leaders like Huawei, Oppo and Xiaomi that rival Apple today.

Xu, now chief executive of Nreal, one of China’s highest-funded AR startups, is among a group of entrepreneurs uniquely positioned to build world-class hardware. The young generation is well-versed in both worlds, with work experience in Silicon Valley and often an Ivy League degree. They are also well-connected to capital and supply chains in China, which would support them through cycles of iteration to deliver powerful yet affordable products.

Although China has been calling for more indigenous innovation, most of the advanced technologies found in AR and VR are still in the hands of foreign tech behemoths.

They might be proud of China’s technological progress, but they recognize supremacy doesn’t come overnight. More importantly, their firms often have intricate ties to the U.S., whether it’s for sourcing core parts or testing an early market.

Despite Beijing’s push for technological “self-reliance,” Chinese AR and VR companies still depend on imported chips like their smartphone counterparts. Because the industry is so young and no one really has a proven model for monetization, few investors and startups in China are willing to splurge on basic research.

But China has one important strength, said the founder of a Chinese AR startup who declined to be named: “In cutting-edge sectors, China has always lacked the talent to take things from ‘zero to one.’ However, China has the mass production and supply chain capabilities necessary for taking things from ‘one to n.'”

That was the case with smartphones. Once Apple demonstrated the technological and financial possibilities of handsets and gave rise to a production ecosystem around iPhones — in other words, catapulted the industry from zero to one — Chinese counterparts took cues from the American giant, made use of homegrown manufacturing resources and began delivering cheaper and even more powerful alternatives.

“I can’t imagine any Chinese corporations willing to invest in AR and VR as heavily as Microsoft, Apple or Facebook today,” said the founder, whose company sells headsets both in and outside China.

“On the contrary, China is good at playing catch-up by spending money on a race with a clear finish line. For example, chips. If there are already contestants in the area, so long as [Chinese firms] ramp up investment and follow the direction, they can deliver results.”

Chinese innovation

Although China, for the last decade, has been calling for more indigenous innovation, most of the advanced technologies found in AR and VR are still in the hands of foreign tech behemoths, several industry experts told TechCrunch. Qualcomm’s Snapdragon chips are used almost exclusively by serious players, from Facebook’s Oculus Quest in the U.S. to Pico and Nreal in China. Advanced optical solutions, on the other hand, mainly come from Japanese and Taiwanese firms.

That’s not to say Chinese companies don’t innovate. Prominent venture capitalist and AI expert Kai-Fu Lee famously argued in his book “AI Superpowers” that while the U.S. has an edge in fundamental research, China is stronger on implementation and commercial application.

“It’s true that the more experimental efforts are happening in the U.S., though I’m not sure if any of those are mature already,” Tony Zhao, founder and chief executive of real-time video API provider Agora and a veteran from WebEx, told TechCrunch. “For Chinese companies, there are more opportunities in [user experience].”

As AR and VR come of age, Zhao’s company is devising a toolkit to let developers and organizations stream and record AR content from devices. Use cases by China’s educators have particularly impressed Zhao. One client, for example, built a tool allowing a teacher to interact with a student through a virtual store, where the two speak English while they respectively act as the cashier and the customer.

“I think it’s very revolutionary because a lot of kids are going to be very excited to learn from those kinds of tools. It’s more like a real experience and would be more natural for students to learn to use a language instead of just know the grammar,” said Zhao.

“These solutions are already creative, but also very practical.”

The Chinese market offers other aspects that can keep investors excited. As Gavin Newton-Tanzer, president of Sunrise International, Asia producer of the “mixed reality” (XR) conference AWE, pointed out to TechCrunch:

“Many like to say that in the U.S., Magic Leap sucked all the air out of the room. They raised tons of money and as a result, few wanted to fund [other smart glass startups]. It’d be like funding a competitor to Didi in China or funding a competitor to Uber in the U.S. … Few felt like anyone else could meaningfully compete.”",Yes,It makes the user feel and think that they are in a completely different environment than they actually are. This is mainly done through the use of a headset which completely blocks out the user's view of the real world.,https://techcrunch.com/2020/12/08/china-watches-and-learns-from-the-us-in-ar-vr-competition/,2020-12-08 00:00:00,LABEL_1_relevant,0.9943863153457642,Virtual Reality,virtual_reality,TechCrunch,User Experience & Entertainment
85,Meta to allow Horizon Worlds users to turn their avatar's personal safety boundary off,"After reports that women were already being groped and sexually harassed in Meta’s new VR spaces, Horizon Worlds and Venues, the company formerly known as Facebook last month rolled out a new “Personal Boundary” feature that created a bubble of space with a radius of two virtual feet around each avatar. This prevented avatars from getting within roughly four feet of one another. Today, Meta is customizing this feature by allowing users to optionally turn the setting off, or control when it’s enabled.

Instead of making the boundary default to on for all Horizon Worlds experiences, Meta said today it will allow users to choose whether or not they want the setting enabled for all interactions. Now, VR users will be able to turn their Personal Boundary off, as was the standard prior to the feature’s launch. There is still a small personal boundary to prevent unwanted interactions, the company says — but this was not enough in the past to prevent bad actors from simulating rape in Meta’s virtual worlds, we should note.

Users will also be able to turn the Personal Boundary on for non-Friends only, which would enable the extra safety feature when you’re with people you don’t know, but leave it off when you’re virtually hanging out with people on your friends list. You also can choose to keep the Personal Boundary enabled for all experiences, as before.

However, Meta says it’s adjusting the default setting to keep the Personal Boundary on for non-Friends only, which means it’s dialing back the safety feature a bit. Given that Horizon Worlds is a new social network, people may be friending other users they don’t know in real life after meeting them in the virtual space. That means a user’s friends list may not be quite the same as a list of people the user explicitly trusts. So some caution should still be advised here.

Meta claims the changes were made based on community feedback after February’s rollout of the Personal Boundary feature. The company believes the new options will make it easier for people to high-five, fist-bump and take selfies with other avatars in Horizon Worlds.

In addition, Meta says the Personal Boundary will default to the more restrictive setting when two people meet for the first time. For example, if one person’s Personal Boundary is off but the other person’s is set to On for Everyone, then the platform will establish a four-foot space between both people. And it says the Personal Boundary will now default to on at roughly four feet for everyone participating in its live events VR experience Horizon Venues.

In its announcement about the changes, Meta acknowledged that developing for VR represents “what are perhaps some of the hardest challenges we’ve tackled in a generation of computing now that we’re no longer limited by fixed viewpoints and traditional flatscreen devices.”

But this statement seems to throw the blame for its earlier failures to protect women in its VR space solely on the fact that building for VR worlds is something new and, therefore, some trial and error will be involved. But had Meta sought the input of more women engineers or gamers to begin with, it’s hard to imagine this topic wouldn’t have come up. After all, sexual assault in virtual spaces is something that’s happened before, repeatedly — including in other virtual reality games, in VR precursors like Second Life, and even in a children’s virtual game on Roblox. It’s unbelievable that the company would not have considered built-in protections when designing a new VR environment. It also shows Facebook’s tendency to design for growth and scale first and user safety second is also carrying over to its new projects, like Horizon Worlds.

The company says it will continue to iterate and make improvements as it learns more about how Personal Boundary impacts the VR experience.",Yes.,"It can lead to people feeling like they are being sexually assaulted. This is because people are not limited by physical boundaries in VR, so they can get very close to each other and simulate rape. To prevent this, Facebook has rolled out a new ""Personal Boundary"" feature that creates a bubble of space with a radius of two virtual feet around",https://techcrunch.com/2022/03/14/meta-to-allow-horizon-worlds-users-to-turn-their-avatars-personal-safety-boundary-off-despite-virtual-world-sexual-assaults/,2022-03-14 00:00:00,LABEL_1_relevant,0.9956323504447936,Virtual Reality,virtual_reality,TechCrunch,Social Norms & Relationships
86,Virtual CES Was As Surreal As We All Suspected It Would Be,"As soon as I logged into CES’s online portal, I knew this year’s techfest was going to be ... different.

The first few hours of virtual press conferences this past Monday reached literary levels of meta: sheets of TV screens, viewed through our screens, while we typed out dispatches for people to read on their screens. Later that same morning I tried to toggle between two virtual events instead of being physically present in just one. TV-maker TCL showed off a rollable phone concept, and I wondered if I had clicked on the wrong tab and entered the Samsung universe by mistake.

At first it was just this; the mild inconveniences of attending a multi-day virtual event, the musings on what we’ve learned about online conferences during the coronavirus pandemic. Normally, more than 150,000 people would gather in Las Vegas every January to gawk at the gadgets and mingle with marketers. In July 2020, the Consumer Technology Association (which runs CES) pulled the plug on any kind of in-person event for January 2021 and started planning an online event instead. This year’s CES would be a bridge year, a best effort to make things seem “normal” while we all wonder if we’ll return to a real normal by January 2022. (I’d really, really like to be together again next year.)

But as this week wore on, as I watched online keynote sessions and marketing videos promising sharp visions of the future, the value of an all-virtual CES actually became less clear. The new products, which we usually look forward to, felt less exciting. It’s hard to determine the viability of any product by watching a slick video about it. The series of keynotes and panels about the future of tech felt less like revelatory conversations and more like TED Talk offshoots I might scroll past in my Twitter feed. And while most of the press conferences, keynote sessions, and panels referenced the global pandemic—how could you not—the rest of the event played out as though a deadly attack on the US Capitol hadn’t just happened last week.

“Walking” the virtual show floor at Pepcom—a product demo event for the media that occurs alongside CES—involved clicking on a digital quilt of company logos one by one, a far cry from the usual booth-browsing and buffet-grazing that happens at the in-person event. Hardware makers showed off everything from earbud-equipped N95 masks to UV sanitizing tech. Here’s what we came for, right? The gadgets? A game company has released a protective face mask with “Razer Chroma” RGB lighting zones on each ventilator, so there’s that. Maybe this will convince people to wear one?

Content This content can also be viewed on the site it originates from.

It’s this kind of practiced optimism that keeps us coming back to CES year after year. Some people really appreciate CES for the escapism it offers. And for journalists, there were some benefits to the all-virtual event. We could browse the online product catalogs on our own schedules. Product demonstration videos, both preproduced and live-streamed, were right at our fingertips. The CTA plans to leave the entire website up for the next month, so people can continue to watch the sessions and panels. Maybe we even got a little more sleep this year, since we spent less time criss-crossing Las Vegas in cabs and shuttles, editing on-the-scene video spots, or kibitzing in hotel bars.

But the heart and soul of CES isn’t the smooth-talking prognosticators or the journalists who follow them. It’s the tech makers who make the show special, and an all-virtual CES wasn’t necessarily good for them. “The smaller brands were probably the ones who suffered the most this year,” says Carolina Milanesi, an analyst and founder of The Heart of Tech research firm. “Because unless you were given a designated place or experience on the website, it was just a long list of names.”

Milanesi shared an observation that one of my WIRED colleagues made as well: The serendipity of discovery was gone. One of the most exciting parts of CES has been finding a weird product in some back corner of the giant expo hall and learning about something new through pure chance. At a virtual CES, that’s a virtual impossibility.",Yes,It takes away the serendipity of discovery that people normally experience at an event like CES.,https://www.wired.com/story/virtual-ces-2021-was-strange/,2021-01-15 19:56:11.154000+00:00,LABEL_1_relevant,0.9918442368507384,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
87,Want to Know the Real Future of AR/VR? Ask Their Devs,"If you want to get a rosy view of the future of virtual and augmented reality, ask a company that works in the space. If you want to get a pessimistic view, ask an investor. But if you want a realistic view, one shaped by experience instead of conjecture and wishful thinking, ask the folks who are actually making the stuff. From their perspective, according to a new report from the organizers of the annual conference XRDC, things aren't looking bad at all.

Reports like these aren't uncommon, but they're usually published by analyst firms who conduct research by talking to companies. Instead, XRDC's ""AR/VR Innovation Report"" was generated from an online survey of more than 900 developers. ""Our main goal is to get a sense of the temperature of the industry, which spans from hobbyists and students to professionals working on big-budget projects,"" says Alex Wawro, an editor at XRDC and Gamasutra who has been involved with the annual survey since it began in 2016.

While the pool of respondents is larger this year than ever before, the status quo remains unchanged in some ways. Games are still the primary driver of the industry, with 59 percent of developers' current or potential VR and AR projects falling in the gaming space. However, the survey saw pronounced growth in a number of other fields, particularly education (33 percent) and training (27 percent). That growth dovetails with another interesting shift: when developers expect to turn a profit. In past years, Wawro says, most have expected their VR/AR work to be profitable in the medium term, but this year saw a two-fold jump in people who said their projects aren't tied to profitability at all.

Peter Rubin writes about media, culture, and virtual reality for WIRED.

Why that is, exactly, is a matter of speculation, but given the influx of talks proposed for this year's XRDC around health care and workplace training, Wawro suggests that it might be a function of the purpose of such projects. Government agencies, nonprofits, and businesses creating internally focused experiences aren't making VR and AR for consumers at all, which skews their priorities away from profit. (XRDC is in fact a new designation; it was known as VRDC for its first five years, but with the rise of platforms like Magic Leap it's more actively encompassing augmented/mixed reality.)

The survey also points to a shakeup of sorts on the hardware front. As has been the case since the survey's inception, HTC and Oculus dominate the results to the question ""On which AR/VR/MR platform are you developing your current project?"" The HTC Vive has been the favorite in each of the survey's previous three years, but this year the Oculus Rift leapt ahead—and the brand-new standalone headset Oculus Quest tied the Vive. Wawro calls the shift ""small but significant,"" and readily admits that while the Quest's all-in-one form factor made it an obvious choice to rank high among developers, he's less sure why the Rift would have overtaken the Vive. Regardless, it looks set to continue: Rift and Quest remain the top two choices for developers' next projects. (PlayStation's PSVR takes up a distant rear among the major VR headsets, with a mere 7 percent of developers using it for their current project and only 11 percent using it for their next project—less than even AR wearables like Microsoft HoloLens and the Magic Leap One.)

LEARN MORE The WIRED Guide to Virtual Reality

Asked what he'd expect to see in next year's survey, Wawro points not to hardware but to content. ""In the past,"" he says, ""stuff like Pokémon Go or Beat Saber or The VOID's installations really get our developers excited and talking about what they'd do with this. This field is really ripe for software to come shake things up."" There’s still room, of course, for a major company to swing in and upset the landscape—a condition that seems increasingly likely. As reported by Road to VR, Apple has gone on a hiring spree for positions with ""AR/VR"" in the title, suggesting that its long-rumored wearable AR device, despite recent conjecture, is very much alive.

And if it is, it would be entering a marketplace that Wawro says has matured past its initial stumbles, but is finding new ones. ""In the beginning, everything we heard in surveys and at events was that developers were concerned about making people nauseous,"" he says. ""This year we didn't see nearly as many concerns in that vein. At the same time, we saw developers complain that the industry is difficult to navigate—that's not too different from any young industry, but I hope as we go forward we'll see a concerted effort from companies to standardize the tools we use to make these experiences.""

More Great WIRED Stories","Please provide your rationale for your response.

Yes","It could make you puke. Fortunately, we’re far enough along that there are a few ways to avoid virtual reality sickness. Read more.

The dead zone in the Gulf of Mexico is one of the ocean’s most mysterious places. But it’s not because of the robots. Read more.",https://www.wired.com/story/future-ar-vr-survey/,2019-08-05 16:30:00+00:00,LABEL_1_relevant,0.9945800304412842,Virtual Reality,virtual_reality,Wired,Environment & Sustainability
88,AR startup Meta sues former head of optics over alleged trade secrets theft,"A top augmented reality startup is suing its former head of optics for breach of contract and misappropriation of trade secrets in the creation of his own AR company.

Meta, which builds augmented reality hardware and software, filed suit earlier this week in a Northern California court against former employee Zhangyi Zhong, his company DreamWorld and 20 unnamed defendants.

The lawsuit alleges that Zhong “shamelessly leveraged” his time at the company to “misappropriate confidential and trade secret information relating to Meta’s technologies, supply chain, manufacturing methods and relationships, as well as business, investment and market strategies,” in the creation of his augmented reality startup, DreamWorld.

Zhong joined Meta in March of 2015 as a Senior Optical Engineer before his resignation in July of 2016.

Meta has raised more than $73 million in funding for its AR systems from top investors, including Y Combinator, Tencent, Comcast Ventures and Lenovo.

The San Mateo-based company has built a tethered augmented reality device, the “Meta 2,” which boasts a much wider field-of-view (FoV) than competitors, though its optics rely on far-less complex (and less expensive) technologies than the displays used by products like Microsoft’s HoloLens.

While Meta 2 has an FoV measured at around 90-degrees, Zhong claims that his company’s DreamGlass device will have an FoV of 100-degrees. Dreamworld plans to launch pre-orders for the device soon at a price of $350. The Meta 2 development kit currently costs $949.

Meta is claiming that Zhong’s has gone beyond stealing trade secrets and is also using Meta’s “market penetration strategy.”

While competitors like Microsoft are using the early generations of their hardware to prove out its SLAM mapping technologies while implementing expensive technologies that it hopes to miniaturize and improve, Meta seems to have focused a great deal of its effort on utilizing accessible hardware technologies to bring more robust visual user experiences to developers initially and secure interest while continuing to build out and miniaturize its technologies.

In the suit, Meta detailed that it has been creating versions of its optical engine that have wider field-of-views in smaller form factors than its current offering while it has also been exploring the idea of powering future headsets off of mobile devices rather than PCs.

Thought the lawsuit details that “Meta believes in free and fair competition in the augmented reality marketplace,” the suit alleges that Zhong had no experience with some of the technologies central to the DreamGlass device before joining Meta. Zhong is listed as an inventor on a patent related to Meta’s “Wide field of view head-mounted display apparatuses, methods and systems.”

Oddly, the suit also alleges that Zhong attempted to obscure his identity by changing his nickname from “Johnny” to “Kevin” after leaving the company, while not updating his LinkedIn profile to reflect his new position as CEO of DreamWorld.

Meta did not provide further comment on the lawsuit. We have reached out for comment from Zhangyi Zhong and DreamWorld.",Yes,"It 

isolates people from the physical world around them.",https://techcrunch.com/2017/06/09/ar-startup-meta-sues-former-head-of-optics-over-trade-secrets-theft/,2017-06-09 00:00:00,LABEL_1_relevant,0.9964976906776428,Virtual Reality,virtual_reality,TechCrunch,"Information, Discourse & Governance"
89,My Highly Unexpected Heterosexual Pandemic Zoom Wedding,"We were in the middle of promising to grow old together when the sound cut out. The officiant’s face pixelated and froze, and we couldn’t hear what she was mouthing. Until the Wi-Fi started working we wouldn’t know whether or not we were married.

It was 8 am. We were standing alone on the tiny front porch of an Airbnb in a country where we are both foreigners, with some string lights hastily flung over the metal grating. My mum, his mum, and two witnesses were just visible on screen, shouting soundlessly, all of us wondering if this was the internet raising a last-minute objection. It was cold. I cuddled into my giant rainbow stripey jumper, the comfort garment that got me through nine months of quarantine. I was glad I’d thrown it on in lieu of a wedding dress. He reached for my hand.

Nothing was going to plan—but then, nothing has gone to plan all year, and certainly not since I met the madman in the bowtie standing in front of me. My sister, who has spent lockdown in London organizing professional events and knows what to do when these things happen, texted me to translate what the officiant was mouthing. You’ve got to log back in to the platform. After some tense minutes of telling our guests on Zoom to hang on, sorry, everyone, talk amongst yourselves while we get this sorted—we finally got the sound back, sort of, although the officiant now sounded like an excited fax machine.

The line buzzed. Time seemed to go very, very slowly.

“As I was saying,” she intoned, “by the power vested in me by the state of Utah, I now pronounce you husband and wife.”

All right, let me explain.

In love as in war, technology allows people to do what they would have done anyway, faster and with fewer immediate consequences. Before there was Match.com there were matchmakers and Miss Lonelyhearts; before there was Grindr there were gloryholes; the polyamorous millennials sharing their Google calendars with a constellation of partners are the inheritors of every fastidious hippie who ever took five hours to explain free love with flowcharts.

But there is no exact pre-digital equivalent of the 4 am status update. It combines the desperate passion of toilet graffiti with the intimacy of a barstool confessional.

It was June, and the world was on fire, and I was drunk. I had spent the first part of lockdown engaging in demeaning Zoom dating as an alternative to more debilitating methods of self-harm. This culminated with a chap I’d been going on virtual dates with for a month announcing that he had, in fact, been interviewing several candidates for the position of girlfriend and, regrettably, I had not made the cut. A few days later, lightly chemically altered, I made a post letting everyone on Facebook know that I was bored of being single, out of practice at flirting, and if anyone had any attractive single friends they should let me know.

In the morning, fumbling to delete the post, I saw that I was already too late. People had started to chime in, including a shy anthropologist from Australia with nice hair who I had met a total of once, three years ago. He was far too far away himself, he said, but he might know somebody in my city. I pointed out that in quarantine times, everyone not living next door might as well be on the moon. Well, he said, in that case, might I like to practice flirting? With him?",Yes,"A lot of people are now in love with me.

“I’ve fallen in love with you, you know,” he said on one of our first Zoom dates, when we were both still half-convinced that this was a game we were playing. “I don’t want to screw",https://www.wired.com/story/my-highly-unexpected-heterosexual-pandemic-zoom-wedding/,2020-12-13 12:00:00+00:00,LABEL_1_relevant,0.9958717226982116,Virtual Reality,virtual_reality,Wired,Social Norms & Relationships
90,Can VR Really Make You More Empathetic?,"The year is 1994, and the bleeding edge of virtual reality is Dactyl Nightmare: an arcade game where you run from swooping pterodactyls. Jeremy Bailenson, a college student then, got his first taste of VR playing it at an arcade in San Francisco. “The technology was of course horrific back then,” he says.

But virtual reality isn’t just about video games anymore. Artists, activists, and journalists now see the technology’s potential to be an empathy engine, one that can shine spotlights on everything from the Ebola epidemic to what it’s like to live in Gaza. And Bailenson has been at the vanguard of investigating VR’s power for good.

For more than a decade, his lab at Stanford has been studying how VR can make us better people. Through the power of VR, volunteers at the lab have felt what it is like to be Superman (to see if it makes them more helpful), a cow (to reduce meat consumption), and even a coral (to learn about ocean acidification).

Silly as they might seem, these sorts of VR scenarios could be more effective than the traditional public service ad at making people behave. Afterwards, they waste less paper. They save more money for retirement. They’re nicer to the people around them. And this could have consequences in terms of how we teach and train everyone from cliquey teenagers to high court judges.

Virtual Human Interaction Lab

Wrinkles and chainsaws

Until recently, studying VR was considered too far-out for academia. But that was never a problem for Bailenson. The professor has a surfer’s mane of blonde hair, but he talks too fast to be a native Californian. (He grew up in upstate New York and spent a decade in the Midwest for school before making it out West.) He clearly relishes his position on the cutting-edge, and is fond of the phrase “for the first time in human history.”

Case in point: avatars. Bailenson says that avatars, people’s representations in the virtual world, now allow humans to watch themselves doing something they’ve never done---“for the first time in human history.” In one study from Bailenson’s lab, undergraduate students looked into a virtual mirror and saw their own faces, aged to 70, staring back. After the VR experience, they took a questionnaire about allocating $1,000 from an unexpected windfall.",Yes,"It can lead to people becoming more selfish. In the study that Bailenson's lab conducted, they found that after people saw what they would look like when they were elderly, they were more likely to want to spend money on themselves rather than others.",https://www.wired.com/2016/09/can-vr-really-make-people-empathetic/,2016-09-01 13:00:14+00:00,LABEL_1_relevant,0.928168535232544,Virtual Reality,virtual_reality,Wired,Social Norms & Relationships
91,This Augmented Reality First Person Shooter Looks Pretty Awesome,"It turns your everyday surroundings into a ""world-scale"" first person shooter.

World-Scale Shooter

Branding agency Pixeloco announced last month that it’s developing an augmented reality game that turns your everyday surroundings into a “world-scale” first-person shooter.

In a promotional video, players run around their city gathering pixelated powerups, shooting at each other, and taking cover behind real-world cars and buildings and even shooting digital bullet holes into them. Details on the game are scarce, but the prototype demo shows a compelling vision of how mixed reality could — at long last — find its way into popular usage.

No-Scope

The game relies on existing augmented reality infrastructure. Players in the video mount their phones gun-shaped gun controllers built by Arkade Games, and the actual AR environment is built with crowdsourced digital mapping tech from the mixed reality company Immersal, lending some credibility to the game’s early demo showcase.

In the game itself, players use the setup to grab powerups or riddle their opponents and surroundings with digital bullets.

Advertisement

Advertisement

Running Wild

An augmented game where players shoot up their environments, however, raises some important questions about how they might interact with the unknowing public around them.

While the gun controllers are brightly-colored pieces of plastic, it seems unlikely that any passersby would immediately understand that they’re witnessing a video game, so the open-world type of play shown in the video seems a bit unrealistic.

More on augmented reality: A Pilot Flying a Real Jet Just Fought an Augmented Reality Enemy

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.

Advertisement

Advertisement",Yes,It could lead to people becoming more anti-social. This is due to the fact that people would rather spend their time in a virtual world than the real world. The article then goes on to say that people could become more aggressive as a result of virtual reality because they would not be able to tell the difference between what is real and what is,https://futurism.com/the-byte/augmented-reality-first-person-shooter-awesome,,LABEL_1_relevant,0.9947287440299988,Virtual Reality,virtual_reality,Futurism,"Information, Discourse & Governance"
92,The War on the Disturbingly Real Trolls in Virtual Reality,"Experiences like that were one reason that Kelly and coworkers at AltSpaceVR formed an all-female team last year to design a feature called the “personal space bubble.” It causes other people’s avatars to disappear and become inaudible if they get closer than about an arm’s length of your own virtual body, and makes you invisible and inaudible to them. The bubble is active by default, but you can turn it off if you do want to get closer to someone (Kelly says that air kissing is an important greeting between friends in AltSpace).

AltSpace also lets you mute specific avatars to prevent you from hearing each other, or to block them, making you and them mutually invisible. The company also has a human “concierge” available 24/7 to field reports of unsavory activity.

High Fidelity launched its own version of the personal space bubble early this year, and has its own mute and block features. Rosedale says the company saw a slight uptick in bad behavior toward others after the company’s software became available on the video-game app store Steam in November, leading to an influx of new users. As well as offering public virtual spaces, AltSpace and High Fidelity both let users create private places to mingle with others, but since so few people own VR hardware, for most people, socializing by necessity involves meeting strangers.

He and Kelly both say that VR trolls are in the vast minority, and argue that they will become less of a problem as more people become used to the technology and the culture of virtual-reality spaces becomes more established. “VR has created a different space and so [users] are figuring out what the rules of that space are,” says Kelly.

Rosedale says that improvements to VR hardware and the activities on offer in virtual worlds will also help people be more friendly. “When you get into a lot of these VR environments, there's nothing that you can change or do other than yell at people,” he says. “If you give people a lot of creative freedom, then people get along better.”

Raph Koster, a high-profile video-game designer whose hits include the multiplayer online game Star Wars Galaxies, thinks VR trolling is in fact likely to get worse. He used a keynote at the gaming industry’s giant GDC conference in San Francisco earlier this month to warn that VR companies need to do more than offer victims defensive tools such as space bubbles and mute features. He says companies will have to start more actively curating, investigating, and punishing bad user behavior, for example by hiring dedicated staff.

“If anything, the behavior now is better than it will ever be,” says Koster of the current moment in social VR. He says his experience launching Ultima Online, one of the first virtual environments that could host thousands of people at once, showed that when more people start visiting a virtual space, trolling becomes more of a problem because they feel more anonymous. “When you get strangers bumping up against one another they behave more poorly,” he says. “Peer pressure is one of the things that keeps people behaving well.”",Yes,"Trolling, but companies are working on solutions.",https://www.technologyreview.com/2017/03/13/243303/the-war-on-the-disturbingly-real-trolls-in-virtual-reality/,2017-03-13 00:00:00,LABEL_1_relevant,0.995948612689972,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
93,New Sex Toys Sync Up to Virtual Reality Porn,"You knew this was coming, even if you didn't want to believe it: A sex-toy company and a VR porn outfit are creating a way to enjoy synced-up virtual reality sex. And while virtual-reality headsets haven’t yet hit the mainstream, Lovense and VirtualRealPorn hope their collaborative stroke of genius can thrust VR into the spotlight.

[We should note that most links in this story are varying degrees of NSFW.]

The trick involves using a Bluetooth dongle, specially coded VR videos, and Lovense's Max and Nora sex toys to make the toys' vibrations and rotations sync with the VR action seen in your headset. The idea is to make you feel exactly what you're seeing---in real-time and in all the right places.

According to Lovense, the girthier portion of its rabbit-style Nora toy will vibrate to match the speed of male performers in VirtualRealPorn videos, while the smaller arm will buzz “every time the performers' bodies collide.” Likewise, its Fleshlight-like Max toy will use air pumps to contract in accordance with the female performer’s alacrity, and the whole device will vibrate when... well, you get the idea.

This delicate dance between hardware and video certainly promises a deeper level of immersion. But is it just too much trouble? It's quite a lot of gear for something that, for most people, remains an analog exercise. However, Lovense says the main thing that makes the next-gen process cumbersome is the VR goggles. Once you've strapped those on, the company figures, you might as well go all-out and sync it to some toys. Right?

Lovense

“Viewing virtual porn is more work and requires more components compared to viewing regular porn,” says Lovense founder Dan Liu. “But viewing regular porn is sometimes too much work, too. It’s not unusual for people to spend a lot of their time finding the right video and a fraction of the time actually getting off.”",Yes,"It makes sex a lot more work. Which, to be fair, it can be. But it also makes it a lot more fun.

VirtualRealPorn has produced dozens of VR videos, and the company says they’re the first in the industry to figure out how to sync with Lovense’s toys.",https://www.wired.com/2015/05/new-sex-toys-sync-virtual-reality-porn/,2015-05-19 13:00:55+00:00,LABEL_1_relevant,0.9959558248519896,Virtual Reality,virtual_reality,Wired,Social Norms & Relationships
94,Zapping Your Skull Could Alleviate Virtual-Reality Motion Sickness,"The kind of electrical stimulation behind the technology—known as galvanic vestibular stimulation, since it affects your sense of balance—isn’t a new idea; it’s long been looked at as a way to study balance. But it’s becoming increasingly popular as a potential fix for discomfort in the nascent field of consumer virtual reality: Oculus founder Palmer Luckey has spoken about trying it out, and Samsung in March revealed that it’s been building electrode-containing headphones called Entrim 4D that aim to help with motion sickness in VR while letting you feel a sense of motion.

Animation shows how Mayo Clinic technology that includes electrical pulses on the head synchronized with motion viewed on a display can make you feel like you’re moving even if you’re sitting still.

Mayo Clinic researchers say they’ve tried the technology out in their lab by showing people videos shot from a first-person perspective of things like a roller-coaster ride or mountain bike ride, which are viewed on flat displays and virtual-reality headsets. Algorithms extract motion data from each frame, which is sent on to the viewer—who’s wearing electrodes on the forehead, nape of the neck, and behind the ears—as electrical pulses meant to give a sense of things like pitch, yaw, and roll.

“The level and degree of presence that the technology conveys when you’re watching a sequence with galvanic vestibular stimulation, and when someone switches it off, is quite striking,” says Jan Stepanek, co-director of Mayo Clinic’s aerospace medicine and vestibular research laboratory.

Michael Cevette, also a co-director of the lab, says researchers studied the electrical signals of subjects’ stomach muscles to look for early signs of motion sickness when using the technology, and they determined that the combination of algorithms and electrical stimulus suppressed these symptoms.

Paul Dizio, an assistant professor at Brandeis University whose work looks at people’s motor control and spatial orientation, is skeptical. He says there is plenty of evidence that galvanic vestibular stimulation can reduce motion sickness, but that there’s not enough data to know how accurately you can control the kinds of directional sensations people feel from it.

He also notes that there can be side effects from using it like a phantom flashbulb effect, a tactile tingling sensation, or a feeling of champagne bubbles in your mouth. And he says that if someone is moving her or his head around in virtual reality while using the technology, it could create more issues.

“Plus you have to have these big electrodes on your head,” he adds. “Maybe that’s fine for a pilot that’s devoted to training of some kind, but again, it’s not without costs.”",Yes,"It can cause motion sickness. This is due to the disconnect between what the eyes are seeing and what the body is feeling. Mayo Clinic has developed a technology that uses electrical pulses to help alleviate this problem. While this technology is still in its early stages, it has shown promise in reducing motion sickness.",https://www.technologyreview.com/2016/04/06/71240/zapping-your-skull-could-alleviate-virtual-reality-motion-sickness/,2016-04-06 00:00:00,LABEL_1_relevant,0.994016408920288,Virtual Reality,virtual_reality,MIT Tech Review,User Experience & Entertainment
95,Lonely? Try fondling Augmented Reality Cyber Maid Alice,"Augmented reality (AR), the combination of virtual worlds and reality, is a cool concept that leads to useful applications in navigation systems, medicine, architecture and many other fields. Geisha Tokyo Entertainment Inc. [JP] now found a way to help all single otaku overcome their problems in finding a real girl friend.

Their alternative to heal loneliness 2.0 is called “Dennou Figure ARis”, or “Cyber Figure Alice” in English. This is how it works: Buyers receive the AR software, 2 cybersticks and 2 cybercubes. Put one of the cubes in front of the computer, point a webcam at it and the screen will display a 3D image of Alice standing on top of the cube.

Using the cyber sticks, you can then “touch”, caress and (naturally) peep at your new girl friend. It is even possible to undress Alice and change her clothes.

This unbelievable example of otaku madness will be sold in Japan starting this fall. As there will most probably no language barriers, I assume “Cyber Figure Alice” will be soon offered by American and European web shops specialized in Otaku products as well.

Via Asiajin",Yes,"It makes it easier for people to escape from reality. In the long term, this will lead to a decrease in the quality of human relationships, and to an increase in loneliness and social isolation.",https://techcrunch.com/2008/07/20/lonely-try-fondling-augmented-reality-cyber-maid-alice/,2008-07-20 00:00:00,LABEL_1_relevant,0.9956040382385254,Virtual Reality,virtual_reality,TechCrunch,"Information, Discourse & Governance"
96,How to Avoid Real Objects While in a Virtual World,"“You want to be able to immerse yourself, but you also want to be able to know if something is approaching you,” said Occipital cofounder and CEO Jeff Powers. “Otherwise it’s very disconcerting, because you may run into a wall.”

The company already sells a $379 sensor similar to Microsoft’s Kinect that straps to an iPad and scans rooms and objects in 3-D. It can be used with the company’s software developer kit to build and interact with virtual- and augmented-reality apps, and includes the ability to track your position (whether you’re walking forward, backward, or crouching) without mapping the world around you in advance.

Occipital’s sensor works by projecting a laser pattern onto your immediate environment. Its infrared camera picks up that pattern and uses it to measure the distance to objects in the scene so that software can rebuild those objects in three dimensions. To add bits of the real world to a virtual one, Occipital takes video captured by the regular camera on the iPad (or, in a demonstration I saw using a different kind of sensor mount, an iPhone) and measures its depth; when an object in the real world—say a trash can—comes within a preset distance, the software will basically cut out the image of the can and insert it atop the 3-D virtual scene.

Occipital’s 3-D sensor, shown mounted to an iPhone attached to a headset.

Occipital plans to make the reality-adding feature available to developers within the next few weeks, Powers said, and will also let them know how to obtain a 3-D printed mount for the iPhone.

I got a look at what Occipital is trying to do during the Augmented World Expo in Santa Clara, California, this week. Powers placed a Homido headset onto my head—basically a generic version of Samsung’s Gear VR headset, which requires a smartphone to show virtual reality apps—into which he had slid an iPhone 6 that was connected to an Occipital 3-D sensor and had a wide-angle lens atop its normal rear camera. The view within Occipital’s virtual world was odd: I was standing within a vast, mostly empty gray room with yellowy dust particles slowly falling all around me, but after walking forward a few feet, the gray expanse was interrupted by a pixelated-looking black railing that appeared in front of me. Unlike the room, the railing was the real thing, marking the edge of the floor we were standing on in the Santa Clara Convention Center.

A moment later, Powers walked in front of me, and when he got within three or four feet, I could see him from head to torso, roughly, through the goggles: pixelated and rendered in black and white, but looking more like an actual human than an animated one. It was strange, but compelling; I completed much of the rest of our interview with the headset over my eyes.

Though Powers, the railing, and everything else from the real world that invaded my virtual space was shown in black and white, Powers said it could be in color, and the distance at which physical objects start to appear is arbitrary (he illustrated this second point by pulling out a small Bluetooth remote control that, with the push of a button, showed us real objects that were farther out).

Yet while pixelated-looking people may be okay if you’re just trying to avoid smacking into things while playing virtual-reality games, Occipital will have to vastly improve the resolution of the reality it pipes in if it’s going to work for more extensive interactions between one world and another, such as in games. And users may want to ignore an object that they know is in front of them so they don’t have a railing inside the entire game.

Powers understands there’s still a long way to go, though. “This is a starting point,” he said.",Yes,"It may block out the real world around you, which could be dangerous. Occipital is working on a way to add the real world into a virtual one, so that users can still be aware of their surroundings.",https://www.technologyreview.com/2015/06/12/11221/how-to-avoid-real-objects-while-in-a-virtual-world/,2015-06-12 00:00:00,LABEL_1_relevant,0.9946042895317078,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
97,The Obscure Neuroscience Problem That's Plaguing VR,"Despite virtual reality’s recent renaissance, the technology still has some obvious problems. One, you look like a dumbass using it. Two, the stomach-churning mismatch between what you see and what you feel contributes to “virtual reality sickness."" But there’s another, less obvious flaw that could add to that off-kilter sensation: an eye-focusing problem called vergence-accommodation conflict. It’s only less obvious because, well, you rarely experience it outside of virtual reality.

At SIGGRAPH in Los Angeles this week, Stanford professor Gordon Wetzstein and his colleagues are presenting a new head-mounted display that minimizes the vergence-accommodation conflict. This isn’t just some esoteric academic problem. Leading VR companies like Oculus and Microsoft know all too well their headsets are off, and Magic Leap, the super secret augmented reality company in Florida, is betting the house on finding a solution first. “It’s an exciting area of research,” says Martin Banks, a vision scientist at the University of California, Berkeley. “I think it’s going to be the next big thing in displays.”

https://www.youtube.com/watch?t=166&v=YJdMPUF8cDM

Okay okay, so what’s the big deal with the vergence-accommodation conflict? Two things happen when you simply “look” at an object. First, you point your eyeballs. If an object is close, your eyes naturally converge on it; if it’s far, they diverge. Hence, vergence. If your eyes don’t line up correctly, you end up seeing double.

The second thing that happens is the lenses inside your eyes focus on the object, aka accommodation. Normally, vergence and accommodation are coupled. “The visual system has developed a circuit where the two response talk to each other,” says Banks. “That makes perfect sense in the natural environment. They’re both trying to get to the same distance, so why wouldn’t they talk to one another?” In other words, your meat brain has figured out a handy shortcut for the real world.

Strap on an Oculus Rift or Samsung Gear VR, though, and all bets are off. These stereoscopic headsets create 3D images by showing the left and right eye images that are slightly offset—the more offset, the closer an object appears. That means your eyes are always accommodating to the screen strapped on your face, but they’re converging to a distance further off. Your eyes aren’t dumb; it’s totally possible to uncouple accommodation and convergence, but they won’t be happy about it. And they’ll be downright rebelling after a few hours.",Yes,"It’s tricking your brain into thinking that it’s looking at things that are closer than they actually are—and your brain doesn’t like being tricked.

The solution, then, is to create a display that corrects for this conflict. Wetzstein and his colleagues—Matt O’Toole",https://www.wired.com/2015/08/obscure-neuroscience-problem-thats-plaguing-vr/,2015-08-11 19:18:36+00:00,LABEL_1_relevant,0.9952123165130616,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
98,VR Shouldn't Just Be Isolating—Here's How It Can Be Social,"A few weeks ago, at SXSW, I was walking down the street in Austin when I saw something that totally freaked me out. A man was seated in the back of a pedicab, his head turning left to right as he took in the views. Only he wasn’t looking at the real world. Strapped to his head, covering the top half of his face like a pair of ski goggles, was a Samsung Gear VR headset.

I had no idea what this guy was looking at, but I did know we were seeing different things. Where I saw trees, the blue sky, and a throng of badge-wearing conference goers, this man saw... something else. Call me sensitive, but I found the experience a little jarring. VR's immersive potential is, of course, its most compelling feature. But total immersion can also be isolating---not just for the person in the headset, but for the people standing near him.

“There’s a discrepancy between inclusion and immersion"" says Markus Wierzoch, a designer at the Seattle studio Artefact. He and his team believe the first generation of virtual reality headsets lack important, human-centered features---features that would afford not only immersive experiences, but also, when necessary, communication with the outside world.

Maybe you’re thinking, isn’t escaping reality exactly the point of VR? And you’re not wrong. But the way Artefact’s designers see it, the virtual reality experiences of the future will shift along a spectrum of social to solitary, and the design of the headgear (not to mention gameplay) needs to reflect that. “Even for the most immersive experience, we think there needs to be room for including others,” Wierzoch says. To show you what they mean, Wierzoch and his team created two concept headsets they believe could be possible by 2020.

Between Two Worlds

The concepts---Shadow and Light---are designed for two different experiences. Shadow, a hoodie with a built-in, cap-like mask, is designed for maximum immersion. It runs off a mini wearable computer, making it totally wireless.

Artefact

Shadow is geared toward hardcore gamers who can pull the hoodie over their head and immediately engage with their virtual world while signaling they'd rather not be disturbed. Still, Artefact’s hoodie has a handful of aspirational technologies that help players stay connected to the real world. A built-in front-facing camera tracks gestures and streams live video to provide players a sense of where other people are in the room. The mask also has an external screen that will display to non-wearers what the player is seeing. This is a high-tech take on what VR companies are referring to as a ""social screen,"" or a secondary screen that allows those not in VR to share in the experience. Artefact doubled down on the idea of building communication between players and non players by designing a set of virtual eyes that glow on the exterior screen to signal what mode the player is in.",Yes,It isolates the player from the people around them. Artefact's Shadow mask is designed to mitigate that by using a front-facing camera to track gestures and an external screen to show players what the person in VR is seeing.,https://www.wired.com/2016/04/vr-shouldnt-just-isolating-heres-can-social/,2016-04-01 16:20:52+00:00,LABEL_1_relevant,0.9909018278121948,Virtual Reality,virtual_reality,Wired,Security & Privacy
99,12-Year-Old Develops Slenderman Phobia After Seeing Him in VR,"We need to think long and hard about what we want to get out of a so-called ""metaverse.""

Technophobia

As Facebook and other companies take steps to build a “metaverse” and strive to make virtual and mixed reality experiences as much of an all-encompassing tech as the internet is today, experts are sharing concerns about safety, both during the experiences themselves and in terms of the impact they can have on our offline lives.

One 12-year-old girl told Slate that she now has a “phobia” of Slenderman, the fictional, child-killing figure that once inspired a real-life stabbing, after watching a VR experience of the supernatural figure stalking the viewer. She added that she regularly watched YouTube videos of Slenderman without issue, but has been afraid of the character popping up and attacking her in real life ever since making the switch to VR. It’s a jarring story, and one that illustrates how VR can, without proper safeguards, be a dangerous presence in our lives.

“It’s hard to sleep,” the girl told Slate.

Die in the Game

Existing research on virtual and mixed reality suggests that traumatic or abusive experiences in a VR environment are far more damaging than those in other digital settings, Slate notes. Despite its many promises, which include being used as a treatment to reduce phobias, a VR “metaverse” could end up as just another toxic ecosystem filled with as much vitriol — and exploitation — as the internet is today.

Advertisement

Advertisement

Citing a study published in the Journal of Children and Media in 2017, Slate argues that VR can cause experiences that are perhaps too powerful, and most importantly perceived as being too real, for children to use the tech without proper safeguards and regulations.

Kids are impressionable, and VR is convincing, University of Texas, Austin VR expert Jakki Bailey told Slate. Whether it’s a terrifying video of Slenderman or advertising and targeting practices that could become even more sinister than what we have today, the human impacts of spending time in a virtual world can be profound. That, Slate argues, means we should figure out what kind of virtual world we want to build before Facebook decides for us.

READ MORE: If Social Media Can Be Unsafe for Kids, What Happens in VR? [Slate]

More on VR: Scientists Use AR To Make Arachnophobes See Huge Spiders

Advertisement

Advertisement

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Yes.,It could make people more phobic.,https://futurism.com/the-byte/slenderman-phobia-seeing-vr,,LABEL_1_relevant,0.9966045618057252,Virtual Reality,virtual_reality,Futurism,"Information, Discourse & Governance"
100,"Someone is putting VR headsets on cows, and we want to know why ...","Let’s just be up front here: yes, these pictures seem to show a cow wearing a virtual reality headset, and no, we don’t really know what’s going on here.

You may have seen these images floating around social media followed by a trail of comments like “this is the dystopian future we were warned about” and “enter the mootrix.” They purportedly show an experiment being conducted in Russia to see if giving dairy cows VR headsets can reduce anxiety and increase milk yield, but — as you might have guessed — it’s not at all clear whether this is a genuine trial or an elaborate marketing stunt.

Designers were reportedly hired to create a ‘summer field simulation’ for the cows

The story can be traced back to this (apparently serious) press release from the Ministry of Agriculture and Food of the Moscow Region, which was picked up by Russian outlets like Interfax and The Moscow Times.

The reports say tests of the VR headsets are being carried out at a farm near Moscow, with the (Google-translated) release noting that the goggles have been specially adapted for a cow’s head shape and vision. VR designers were also reportedly hired to create a “unique summer field simulation program” for the animals to enjoy.

The release notes that there’s a growing trend in agriculture to focus on animals’ emotional well-being, with quality-of-life improvements like scrubbing brushes and relaxing music improving yields. So why not try VR as well? A first test of the headset showed “a decrease in anxiety and an increase in the overall emotional mood of the herd,” the release says, and a “further comprehensive study” is forthcoming. But, uh, is it? It’s not clear.

Cows probably can wear VR headsets, but that doesn’t mean they should

It’s certainly not impossible that this is a real experiment. The agricultural industry is as keen as any other to apply new technology — just look at the farmers using facial recognition on pigs! — and it wouldn’t be the first time that someone suggested VR could improve animals’ well-being. Plus, although cows don’t see like humans do, they do apparently have limited binocular vision, suggesting modified VR goggles could work on them.

On the other hand, the press release says the experiment wasn’t conducted by a university but by “Milknews.ru,” a news site for the Russian dairy industry. It also encourages readers to sign up for the upcoming “VI International Agro-Industrial Dairy Forum” where they can learn more about technology in agriculture. This suggests the VR cows might just be a good way to get attention for an otherwise run-of-the-mill conference. If so: job done.

We don’t know how deep this rabbit hole goes, but we’ve reached out to Milknews, the Moscow Ministry of Agriculture and Food, and Rusmoloko (the company running the farm where the trials reportedly took place) to see if we can get any more info.

At the end of the day, what we can say is that someone took the time to make at least one mock-up virtual reality headset for a cow and took these pictures. We don’t need to milk the story any more than that.",Yes,It has the potential to make some people feel nauseous. Some people may also experience headaches or dizziness.,https://www.theverge.com/tldr/2019/11/26/20983717/vr-virtual-reality-headsets-cows-russia-vr-trial,2019-11-26 00:00:00,LABEL_1_relevant,0.9951372742652892,Virtual Reality,virtual_reality,The Verge,"Information, Discourse & Governance"
101,Transcend VR sues investor Mike Rothenberg for fraud and breach of contract,"Mike Rothenberg has been in a heap of trouble over a series of purportedly fraudulent transactions within his investment firm Rothenberg Ventures and his virtual reality startup River Studios. Now Transcend VR, as first reported in The Wall Street Journal, is suing the venture capitalist on allegations of fraud, breach of contract and deceptive diversion of funds.

Transcend, which filed the lawsuit in the California state superior court last week, says it invested $2 million as a convertible promissory note in Bend Reality LLC (i.e. River Studios) on the belief it was the first non-founder to invest and that Rothenberg held 100 percent equity in River.

However, Transcend says Rothenberg diverted $5 million over a two-year period from “another investment” into River Studios starting in 2014, giving those investors a beneficial ownership in the virtual reality startup. Transcend maintains it was never told of this previous investment.

We’ve reached out to Mike Rothenberg to hear from him on this recent accusation but have yet to hear back, though he has maintained his innocence on similar matters. We’ve also reached out to Transcend’s lawyers for comment.

According to the suit, River also agreed it would pay back the principle of the loan with 5 percent interest on the third anniversary of the loan on the condition that if River committed any act of bankruptcy the startup would immediately repay the loan, along with the unpaid interest.

However, Transcend says Rothenberg described in emails to investors a recent loss of employees and working capital had put River Studios in “jeopardy,” writing that he now “valued River Studios ‘at cost’ and that equity investments could very well ‘disappear.'”

Rothenberg has been accused on numerous occasions of disregard for maintaining separation of funds. Transcend furthers those claims, alleging River Studios as the “alter ego of Rothenberg,” accusing him of commingling accounts and using them as “his personal bank account.”

The suit further claims Rothenberg diverted loan proceeds without investor knowledge into his personal account. An earlier TechCrunch report touching on the Transcend investment shows documentation in which Rothenberg wired $1.7 million directly to his personal bank account on the same day Transcend wired the $2 million to Bend Reality LLC.

Meanwhile, Rothenberg faces two other lawsuits from former employees and may still be under an SEC investigation for allegations of bank fraud, wire fraud and breach of fiduciary duty.

For more information you can read details of the lawsuit in full below:

Transcend Vr Lawsuit by TechCrunch on Scribd",,"It makes people dumber (and it is mainly the fault of the developers). In essence, developers are not creating enough ""natural"" or ""realistic"" interactions in virtual reality, which in turn causes people to become more reliant on technology and less reliant on their own abilities. This is a major problem, because it means that people are not",https://techcrunch.com/2017/02/16/transcend-vr-sues-investor-mike-rothenberg-for-fraud-and-breach-of-contract/,2017-02-16 00:00:00,LABEL_1_relevant,0.9963380098342896,Virtual Reality,virtual_reality,TechCrunch,User Experience & Entertainment
102,"As the Note 7 dies, Oculus loses face(s)","The Note 7 debacle hasn’t been good for anyone — not for Samsung, not for Android, not for consumers, not for airlines, not for fire departments, not for stylus-lovers and certainly not for Oculus.

The Facebook-owned virtual reality powerhouse currently has its entire mobile VR future pinned on the successes of Samsung’s handsets and the discontinuation of Note 7 production is likely going to stunt Oculus’s Gear VR sales (and brand) in a pretty damning way.

Today, shortly before Samsung issued a recommendation that all Note 7 users shut off their devices, Oculus disabled Note 7 support for the Gear VR. Users on Reddit discovered the message this morning.

Exploding phones admittedly do not seem ideal for a peripheral that straps the phone to your face, so the real surprise is that it took this long.

The success of the Gear VR is intimately tied to Samsung in more ways that just the phones. Samsung injected major life into the Gear VR platform by capitalizing on the excitement of the Galaxy S7 and shipping a ton of free headsets with pre-orders.

The $99 Gear VR is currently Oculus’s main touch point with consumers, most of whom have likely not even seen higher-end VR systems like the Rift. Oculus announced at their OC3 conference last week that there were over 1 million monthly active users on Gear VR, much of this likely having a lot to do with giveaways on the part of Samsung. No official numbers are out there in terms of sales, but analysts from SuperData Research estimate that while the Oculus Rift will sell just over 355 thousand units this year, there will be over 2.3 million Gear VR’s in the wild by year’s end.

Long story short, Gear VR is huge to Oculus’s brand and now the platform’s immediate future is likely going to see a very unfortunate period of slowed growth given that the next new compatible device from Samsung probably won’t emerge until next spring.

This comes as Oculus just released an upgraded model of the Gear VR specially designed to accommodate the Note 7 (while also backwards-compatible with other Galaxy models). It added a USB Type-C connection, added a bit of padding to make the experience more comfortable and opened up the slot a bit to accommodate the larger 7’s larger form factor.

With the Note 7 now deceased, no one has any explicit reason to buy the new headset; the old one will work just fine and is selling for $55 on Amazon right now, nearly half the price of the new model. For the time being, the new Gear VR is a peripheral waiting on a product.

(I’ve reached out to Oculus for comment on how the Note 7 production cease will affect the rollout of the new Gear VR and will update if I hear anything back.)

The worst thing that can happen for Oculus right now is a six month stalling of customers with brand new phones who might have been interested in checking out VR. Without a major Samsung launch to entice consumers to Oculus, for a bit the company will have to sell itself through its new content rather than piggybacking off the sales of a new smartphone.

All of this comes as Google is readying for the launch of their mobile VR platform, Daydream, which already has multiple handset makers, including Samsung, ready to build compatibility for the platform into their devices. Google’s first headset, the Daydream View, is launching next month and includes a VR motion controller. The platform capitalizes on a new VR mode in Android Nougat which brings low-latency VR to new smartphones with OLED displays.

In the end, Oculus was fortunate to make it out of this situation without any exploding storylines that incorporated a Gear VR headset, but by losing the newest phone on their rather exclusive platform, the headset maker is going to have to shift its mobile strategies if it hopes to keep hype alive for itself and the mobile virtual reality platform.",Yes.,"It can be used to create a realistic simulation of events, people or objects. This can lead to people feeling as if they are in the same room as the person or object they are viewing, which can be disorienting or even dangerous.",https://techcrunch.com/2016/10/11/as-the-note-7-dies-oculus-loses-faces/,2016-10-11 00:00:00,LABEL_1_relevant,0.9960694313049316,Virtual Reality,virtual_reality,TechCrunch,Social Norms & Relationships
103,"Virtual-Reality Porn is Coming, and Your Fantasies May Never Be the Same","After years of hype, VR is here. The Oculus Rift headsets arrive in early 2016. And when it does, porn undoubtedly will embrace it first, just as pioneered the adoption of the VCR and the CD-ROM and streaming video.

But unlike previous technologies, VR doesn’t operate within a frame: It erases it. VR’s promise of existing within the environment has particularly high-stakes problems and potential in pornography. If you feel vertigo when you peer out of an airplane in VR, imagine what that kind of physical response to porn could be. On the other hand, nobody wants to get kinky in the Uncanny Valley.

WIRED's Peter Rubin takes a look at what early VR—as guided by the inevitable power of the porn industry—will look like, and how it might introduce a dimension that's all too devoid from 2-D porn: intimacy.",Yes,"It makes the user feel more present in the scene, which can be disconcerting or even dangerous if the scene is pornographic. Additionally, VR has the potential to create an even more immersive and intimate experience than traditional pornography, which could have implications for addiction and user behavior.",https://www.wired.com/2015/12/virtual-reality-porn/,2015-12-21 20:11:04+00:00,LABEL_1_relevant,0.9946718811988832,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
104,LCD Hacking Trick Could Make Virtual Reality More Real,"Although competition among smartphone manufacturers has led to significant increases in the pixel density of commercial LCD panels in recent years, the displays on the market today don’t pack pixels densely enough for them to be invisible at very close range.

The Nvidia design exploits the fact that LCD panels use an array of tiny shutters that control the visibility of individual pixels. The researchers extracted the array of shutters from one LCD panel, and then placed them over those in another panel of the same design. The extra layer is positioned so that its pixels are just slightly offset from those of the panel below. The boundaries of the extra shutter layer divide up each pixel of the panel below into four smaller areas—these are the pixels of the new, cascaded display.

This design reduces the brightness of the display, because light has to pass through more components to reach the viewer. But this isn’t critical for a VR headset, where the display is close to the eye in a dark space.

Making use of that extra density requires some mathematical tricks, because when a shutter in the upper array changes its brightness, that affects four of the pixels visible to a person looking at the display. The Nvidia researchers have developed software that can translate the feed from a video or game into instructions for each layer to create the desired image for a viewer.

Cascaded displays can also offer an increased frame rate, which means moving images appear smoother. Setting the panels to refresh out of sync with one another causes someone using the display to see new frames at twice the rate each individual panel is updating.

Pixel density is far from the only factor that affects the quality of virtual reality experiences. Doug Bowman, a professor at Virginia Tech, says that his research suggests that having a wider field of view has perhaps the strongest effect on how immersed people feel in a virtual space, and how well they perform at tasks inside it.

One reason the Rift has been so well received is that it offers a wide field of view compared to previous headsets, at a relatively low cost. However, even the Rift’s 100-degree field of view imposes a kind of tunnel vision on people wearing it, because the human visual field is close to 180 degrees. Displays with greater pixel density could make it possible to cram more pixels into a headset so that when spread across a wide field of view by lenses—as in the design of Oculus Rift—they remain relatively densely packed.",Yes,It makes the person using it feel cut off from the outside world.,https://www.technologyreview.com/2014/07/28/171953/lcd-hacking-trick-could-make-virtual-reality-more-real/,2014-07-28 00:00:00,LABEL_1_relevant,0.9959523677825928,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
105,Facebook’s Oculus Says Gaming Is Just One Thing You’ll Do in Virtual Reality,"Video games are an obvious application for virtual-reality headsets like the one Facebook’s Oculus division will launch early next year. But the chief technology officer of Oculus predicted Thursday that gaming will eventually account for less than half of the time people spend using the technology.

John Carmack, well known in the games industry for helping create the games Doom and Quake, described gaming as “the sharp end of the spear right now,” saying it will establish virtual reality as a technology consumers get excited about. But while many people love playing—and spending money on—games, much more time is spent watching videos and looking at photos.",Yes,It will eventually take up less than half the time people spend using the technology for gaming.,https://www.technologyreview.com/2015/09/24/166109/facebooks-oculus-says-gaming-is-just-one-thing-youll-do-in-virtual-reality/,2015-09-24 00:00:00,LABEL_1_relevant,0.991862416267395,Virtual Reality,virtual_reality,MIT Tech Review,Equality & Justice
106,This Accessory Makes VR So Real a Surgeon Could Train with It,"Aler Gu, the young roboticist who invented the glove and cofounded the company behind it, Dextarobotics, says VR’s sensory expansion into the realm of touch has the power to revolutionize the medium. “The maximum level of feedback current VR controllers give is a gentle rumble using vibration motors,” he says. “But vibration alone isn’t enough to fool the brain. The moment you detect anomalies in how objects feel, your sense of immersion is broken.”

Dexmo’s applications reach far beyond video games, according to Gu. The glove can work in any simulated 3-D environment and is compatible with all of the major VR headsets currently on the market. He also believes that the device will be useful in CAD design, allowing engineers to disassemble rockets and feel the size of each component, or in medical training, where trainee surgeons can perform more realistic operations. It could prove invaluable in training bomb disposal experts and help drastically reduce costs in mechanical maintenance training by providing students with access to otherwise prohibitively expensive parts that they can feel in their hands.

Sam Watts, head of operations at Make Real, a software company that has worked on a variety of VR applications for military clients as well as consumer game publishers, agrees that the current crop of motion-tracked controllers that are sold alongside the major virtual reality headsets “only give the first stage of sensations of touching and interacting with virtual objects.”

The Dexmo glove has potential applications well beyond gaming.

The HTC Vive Wands, for example, are held snugly in the hands, like handgun grips. A digital representation of the controller is seen at all times within the virtual world, twisting and moving in perfect sync with your hand motions. “This is fine for games and many forms of training simulation, but for real industrial and engineering adoption of VR, much more realistic and precise feedback is required to accurately convey the sense of touching, using, and manipulating objects together.”

However, Watts says, he needs to see more testing and evidence of consumer adoption of the device before including support for the Dexmo in Make Real’s products.

While the price of a consumer version of the Dexmo is yet to be set, Gu is optimistic that the glove will be something that “eventually everybody should be able to afford.” For now, however, the Dexmo is a tool restricted to the hands of early adopting software developers like Watts, those who will ultimately decide what impact such devices will have on both sides of the screen.",Yes,It breaks the immersion of the user by providing sensations that are not realistic. The Dexmo glove has the potential to fix this by providing more realistic sensations to the user.,https://www.technologyreview.com/2016/09/30/70059/this-accessory-makes-vr-so-real-a-surgeon-could-train-with-it/,2016-09-30 00:00:00,LABEL_1_relevant,0.9944624304771424,Virtual Reality,virtual_reality,MIT Tech Review,User Experience & Entertainment
107,A Simple Trick to Avoid Losing Your Lunch While in Virtual Reality,"In a series of tests, 30 volunteers used the VR system to explore a digital landscape—using the dynamically blinkered system one day then an unrestricted system the next. When the participants used the system with view restrictions, they stayed in the VR environment for longer and scored the experience as being more comfortable.

Two different versions of the device were also tested, one with a more subtle narrowing effect than the other. The majority of people didn’t notice the less extreme of the two versions, and those who did unanimously claimed to prefer it to the unrestricted VR system.

It’s not the first attempt to try to alleviate the effects of motion sickness in VR. A team from the Mayo Clinic has been repurposing a technique that’s currently used to help military pilots avoid nausea. But while effective, that approach uses electrodes to stimulate the body, fooling its sense of balance into sensing motion where none exists.

By comparison, then, the new system seems like a simple yet effective means of controlling nausea among VR users.

(Read more: Engineering & Technology, “Zapping Your Skull Could Alleviate Virtual-Reality Motion Sickness,” “Oculus Rift Is Too Cool to Ignore,” “Virtual Reality Actually Feels Real When It Uses Physics”)",Yes,It can cause people to feel nauseated. A new system that uses a dynamically blinkered system may help alleviate this problem by making people feel more comfortable.,https://www.technologyreview.com/2016/06/16/159577/a-simple-trick-to-avoid-losing-your-lunch-while-in-virtual-reality/,2016-06-16 00:00:00,LABEL_1_relevant,0.9912181496620178,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
108,VR Will Make Life Better—Or Just Be an Opiate for the Masses,"Virtual reality will dramatically transform movies and gaming, but some see an even loftier goal for the burgeoning technology: Providing the world’s poor and underprivileged with a better life. Palmer Luckey, the founder of Oculus Rift, and his chief technology officer, John Carmack, even speak of a ""moral imperative"" to bring virtual reality to the masses.

“Everyone wants to have a happy life,” as Luckey likes to say, “but it's going to be impossible to give everyone everything they want.” But VR can provide billions of people with virtual versions of everything the wealthy take for granted: touring the Louvre, sailing the sun-dappled coast of California, or simply sitting in a meadow beneath a clear blue sky free of smog and pollution. “Virtual reality can make it so anyone, anywhere can have these experiences,” Luckey says.

Carmack, a pioneer in 3-D graphics, has championed this mission for some two decades, but only recently has the underlying technology reached a price point where VR headsets can cost as little as a cheap smartphone. And that, he says, makes it possible for virtual reality to improve the real lives of people worldwide, even the less fortunate.

""These are devices that you could imagine almost everyone in the world owning,” Carmack says. “This means that some fraction of the desirable experiences of the wealthy can be synthesized and replicated for a much broader range of people.”

The Experience Machine

This assumes, of course, that people are willing to strap into VR devices and not perceive any meaningful difference between what’s real and what only seems to be. Harvard philosopher Robert Nozick explored this very question more than 30 years ago in an influential thought experiment. ""Suppose,” he wrote in 1974, “there were an experience machine that would give you any experience you desired. Super-duper neurophysicists could stimulate your brain so that you would think and feel you were writing a great novel, or making a friend, or reading an interesting book. All the time you would be floating in a tank, with electrodes attached to your brain. Would you plug in?""

It seemed obvious to Nozick, who died in 2002, that people would not. ""We learn that something matters to us in addition to experience, by imagining an experience machine and then realizing that we would not use it,"" he wrote. But some of the world’s most powerful companies---Facebook, Sony, and Google among them---are investing billions to mass-produce what are effectively experience machines, utterly convinced that we’re all eager to plug in.

Luckey, for one, “absolutely” would plug in. “If you asked anyone in the virtual reality industry, they would say the same,” he says.

The Pull of the Virtual

There’s compelling evidence to suggest Luckey is not alone. I recently lived for some months in the teeming, smog-shrouded city of Beijing and vividly remember noticing how many people were engrossed in 3-D fantasy games, playing them in cavernous Internet gaming cafés or on smartphones in suffocating subways and congested malls. Such games are tremendously popular in China, where hundreds of millions while away their waking hours fixated on the virtual worlds like Fantasy Westward Journey or World of Warcraft. This seems to undermine Nozick’s answer to the experience machine challenge, while reinforcing what many in the VR industry fiercely believe. “There is no difference between a life lived in virtual reality versus 'real reality,'"" says Philip Rosedale.

Rosedale was the co-creator of the popular online world Second Life, and he’s currently building High Fidelity. Both attempt to create something like the Metaverse from Neal Stephenson’s seminal novel Snowcrash---a vast, virtual world accessed by millions of people through VR headsets. (Luckey has announced his own long-term goal of building a metaverse.)",Yes,"Social isolation. People who spend excessive amounts of time in virtual reality are at risk of becoming emotionally attached to the virtual world and losing interest in real-world relationships. While this may not be a problem for some people, it could lead to serious consequences for those with pre-existing mental health conditions, such as social anxiety disorder or autism.",https://www.wired.com/2016/02/vr-moral-imperative-or-opiate-of-masses/,2016-02-25 11:30:58+00:00,LABEL_1_relevant,0.9905396699905396,Virtual Reality,virtual_reality,Wired,Health & Wellbeing
109,Can VR Survive in a Cutthroat Attention Economy?,"Last decade, the technology was questionable; this decade, the content. But today the greatest challenge for VR—as both an industry and medium—is no longer the tech or the content but the problem of time and attention. How, exactly, will or does VR fit into the collective human schedule? When and where will large numbers of people “do” VR, in a time when nearly every second of week is contested territory?

Today, I think only the deeply jaded would deny that VR has the goods, or is at least pretty close. The goggles could get better, but they work. Every year the creators of VR films and games produce a handful of stunning, memorable pieces (even if they haven’t always quite figured out the stubborn tension between narrative and interaction). Sundance is an important annual showcase—last year’s Asteroids, Miyubi and Chocolate were unforgettable—and this year brought new wonders like Spheres and Wolves in the Walls, among others. Sure, there could be more of it, and not all VR content is great, but something as simple as a VR tour of the Obama White House can be a memorable, affecting trip. But when or where will people actually spend the time to see this stuff? That’s the hard question, and one that has really burned VR over the last few years. Media history makes it clear that a commercial medium can only survive if it finds itself a reliable, repeatable place in the national schedule for significant numbers of people. (Those that don’t, like the 90s Web-TV efforts Pseudo.com and MSN 2.0, simply die after burning through lots of money).

Looking back, every successful medium has either “killed” a predecessor (in the manner that television displaced radio in the home, or that streaming video is chipping away at cable) or “colonized” time and attention that was unused or used for something else. However, that was somewhat easier when people actually had free time. Today, we live in a media environment where billions of dollars are spent fighting for the time spent “waiting at the bus stop.""

Making matters even more challenging: unlike other newish forms of media, VR demands not just passing attention but the absolutely highest quality of devotion. Other media can target brains that are doing something else as well. That helps explain the success of podcasting, which has plundered the driving hours, or social media, which thrives on what Jonah Peretti of Buzzfeed memorably called the “Bored at Work Network"". Yet no one waiting for a bus pulls out VR goggles (at least not yet), and you still can’t turn to VR if you get bored at a meeting. Meanwhile, VR units face incredibly tough competition inside what we call our homes but are really more like media studios, festooned as they are with television, videogame consoles, computers, and phones, not to mention old-school interactive units like roommates or family.

All this points to an unexpected near-term future for VR. It wasn’t unreasonable to bet that VR would take over home prime-time hours, but that hasn’t worked out as planned—television and traditional gaming are just too tough as competitors. But the time that is open is the time people spend outside of the home, looking for something to do, alone or with friends. As Pete Billington, director of the critically acclaimed Wolves in the Walls, points out, good VR film experiences really aren’t that dissimilar from live, immersive theatre productions like Sleep No More or, especially, Then She Fell, both of which attract giant lines in New York City.

The solution, then, would be to focus on scaling up immersive theatre to the masses (perhaps focusing on character-driven VR, as Edward Saatchi, a co-founder of Oculus Story Studios, argues). It is the colonization of whatever time people might otherwise spend outside the home, one way or another—at theaters, movies, museums, art galleries, or even just bars—that holds the most promise for VR.",Yes,"It has failed to take over people's home lives as initially predicted. Instead, it has found more success in immersive theatre, which allows it to capitalize on time that people would otherwise spend outside the home.",https://www.wired.com/story/can-vr-survive-in-a-cutthroat-attention-economy/,2018-02-05 13:00:00+00:00,LABEL_1_relevant,0.5227082371711731,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
110,"Tech Time Warp of the Week: Before Oculus Rift, There Was Nintendo's Disastrous Virtual Boy","Content This content can also be viewed on the site it originates from.

Virtual Boy creates an immersive 3-D gaming universe so advanced, the voice says, ""it can't be viewed on conventional TV.""

The voice is talking from a conventional TV, so that's a particularly tantalizing pitch. And the ad is tricked out with some ultra cool animated-polyhedron graphics. And the Virtual Boy itself looks even cooler. It looks straight out of The Lawnmowerman, and you can strap it on your own face!

As documented in this TV ad from the mid-'90s (see video above), the Virtual Boy was the Oculus Rift of its day. Released by Japanese game giant Nintendo in 1995, it promised to provide a virtual reality unlike anything that came before.

Sadly, it was a giant flop. And you could kinda tell it would be from the ad's brief glimpse of Mario Tennis, Virtual Boy's signature game. No, it couldn't be viewed on conventional TV. But that doesn't mean it looked good on Virtual Boy.

Virtual Boy's graphics evoked old-fashioned 3-D movies---the kind you'd wear to red and blue glasses to watch---and Nintendo never really figured out how to take advantage of even this rudimentary VR. At a time when the Sony Playstation and Sega Saturn game consoles were already on the market and the Nintendo 64 was right around the corner, the Virtual Boy's monochromatic graphics seemed downright retro. It just couldn't compete with new first person shooters like Doom and Heretic.

""On some level, there seemed to be a belief that this was the inevitable endpoint of videogame progress---a 3-D world that you experienced through a head-mounted visor,"" WIRED wrote in 2010 for the the console's 15th anniversary. ""Virtual Boy didn’t actually do that, but it sure looked the part.""

It did---and it has its own particular charm. Much like the doomed Power Glove and Power Pad helped pave the way for the Nintendo Wii, the Virtual Boy helped pave the way for the more successful Nintendo 3DS handheld. And with companies ranging from Sony to open sourcers like AntVR building new virtual reality headsets, this kind of thing may become actual reality. But it won't play Mario Tennis.

Homepage image: Evan-Amos/Wikimedia",Yes,It makes games look bad.,https://www.wired.com/2014/12/tech-time-warp-week-oculus-rift-nintendos-disastrous-virtual-boy/,2014-12-12 19:07:37+00:00,LABEL_1_relevant,0.9950518012046814,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
111,The VR illusion that makes you think you have a spider’s body,"Today we get a partial answer thanks to the work of Andrey Krekhov and colleagues at the University of Duisburg-Essen in Germany. These guys have compared the way humans take on ownership of human versus nonhuman bodies such as those of tigers, bats, and spiders.

They say that in certain situations, the experience of owning a nonhumanoid body is more convincing than that of owning a humanoid body. And this result paves the way for virtual body ownership to play a greater role in applications such as training, education, and of course video-gaming, where the potential is significant.

First some background. The rubber hand illusion begins by covering a person’s real hand and placing a rubber hand next to it. The subject can see the rubber hand but not their real hand. The illusionist then lightly touches or strokes both hands in the same place and in the same way.

The subject, seeing the rubber hand being stroked, feels the sensation as if it were their real hand, which is also being stroked but in a way they cannot see. In that instant, the illusion is created: the subject feels that the rubber hand is part of their own body. Many people find this a surprising and dramatic sensation.

The illusion was first described in the 1990s, but since then researchers have gone much further. The next generation of experiments used early forms of virtual reality. The subject wears a VR headset and looks down at their body to find it is that of a gorilla, for example.

While the subject watches, the virtual body receives a poke in the chest. But at the same instant, the the subject’s real body is poked in the same place. In that instant, the illusion is created: the subject feels ownership of the virtual body.

The limitations of this technique were immediately clear. The illusion requires visual and tactile stimuli simultaneously. That’s straightforward for a rubber hand but becomes more difficult for an entire body, particularly when the virtual limbs start moving in a way that the subject’s real body does not match.

That seemed to limit the application of the illusion until a breakthrough in 2010. At that time, researchers discovered how to trigger the body ownership illusion using visual cues alone. They did this by tracking the subject’s body movements in microfine detail, the level of hand and finger movements. They then reproduced these movements exactly in the virtual body.

This immediately makes the illusion easier to achieve. And this is what Krekhov and co study for a wider range of body ownership models. Their work focuses on three types of virtual bodies—a four-legged animal in the form of a tiger, a flying animal in the form of a bat, and a creature with an entirely different exoskeleton in the form of a spider.

Their goal is to explore the limits of body ownership with challenging examples that differ from human bodies in posture, shape, and even entire skeleton. Their goal is to understand how well the human brain copes with the body ownership illusion in these cases, and whether it is possible at all in the most extreme cases.

Their method was straightforward. They subjected 37 volunteers to the illusion, measuring how they coped and asking them to rate their experiences compared with humanoid body ownership. In each case, they mapped the participant’s body movements precisely to the movements of the virtual body to trigger the illusion.

The results make for interesting reading. Krekhov and co say that humans are surprisingly good at adopting alien forms. “Our experiment shows that even spiders, despite having a skeleton that significantly differs from ours, offer a similar degree of the illusion of virtual body ownership compared to humanoid avatars,” they say.

What’s more, some body types are better than humanoid ones in certain circumstances. For example, bat bodies are better at reproducing the sensation of flying than humanoid bodies. “Our empirical results demonstrate that virtual body ownership is also applicable for nonhumanoids and can even outperform human-like avatars in certain cases,” they say.

The technique is clearly popular with the participants. The researchers asked them what other body types they’d like to try, with suggestions ranging from cats and dogs to sea animals such as whales and dolphins. But there was a clear winner in this respect. “We suppose that flying creatures have the largest potential to fascinate users as embodiment targets in VR,” say Krekhov and co.

That’s interesting work that has the potential for significant applications. The most obvious is in the gaming world, where the illusion of owning another character’s body has obvious charms. But there are also educational and training applications, plus the inevitable applications in the world of pornography.

Of course, there are limitations to overcome before this kind of illusion can become widespread. Virtual-reality kits are becoming increasingly common, but this technology has to be coupled with accurate body tracking equipment. That will be an enabling step. A broader challenge will be to overcome the problems with VR, which often leaves users feeling nauseated.

Nevertheless, the startling fact is that virtual body ownership is both easier than ever and possible with a wider variety of bodies than anyone imagined. Expect to hear more about it.

Ref: arxiv.org/abs/1907.05220 : The Illusion of Animal Body Ownership and Its Potential for Virtual Reality Games",Yes,"It creates a sense of body ownership in users which could lead to a number of applications in fields such as gaming, education, and pornography.",https://www.technologyreview.com/2019/07/24/134072/a-vr-illusion-that-makes-us-think-we-have-a-spiders-body-could-change-gaming/,2019-07-24 00:00:00,LABEL_1_relevant,0.9952288866043092,Virtual Reality,virtual_reality,MIT Tech Review,Social Norms & Relationships
112,Virtual Reality Is the Rich White Kid of Technology,"It has been seven years since Palmer Luckey appeared on the cover of WIRED magazine. The June 2014 issue declared, “This kid is about to change gaming, movies, TV, music, design, medicine, sex, sports, art, travel, social network, education—and reality.” This was just after Facebook acquired his virtual reality company, Oculus, for $2 billion. It now invests $18.5 billion annually into research and development, and Facebook Reality Labs, the company’s Augmented Reality/Virtual Reality division, accounts for as much as 20 percent of its entire workforce, with no sign of slowing down. But despite the many years, billions of dollars, and year-long pandemic requiring at-home entertainment, the results thus far have been pretty lackluster. The headsets are spiffier and the games are more lucrative, but our minds nevertheless remain collectively un-blown.

It’s not just Facebook and Oculus. In May 2016, WIRED’s cover story introduced readers to Magic Leap, “A mysterious startup, a mountain of money, and the quest to create a new kind of reality.” Magic Leap was developing a set of semitransparent “Mixed Reality” goggles that could integrate virtual objects into the user’s physical environment. The company raised more than $2 billion in funding from A-list Silicon Valley investors. It looked like the biggest leap forward in hardware since the iPhone. But the actual product never lived up to the breathtaking demo. The company laid off 1,000 employees in 2020, hired a new CEO, and pivoted to focus on narrower enterprise applications. The Mixed Reality future is still, well, the future.

Somehow, none of these less-than-ideal outcomes have affected confidence in VR. In fact, Facebook doubled down on Monday, announcing a new group within the company dedicated to developing its Horizons VR world. Mark Zuckerberg recently told Facebook employees that over the next five years he expects to transition “from people seeing us as primarily being a social media company to being a metaverse company.” Silicon Valley billionaires and venture capitalists, it seems, are incapable of saying no to a fancy headset with a big dream. And this dates back 35 years—Jaron Lanier was the Palmer Luckey of the 1980s and early 1990s!

The technology is always about to turn a corner, about to be more than just a gaming device, about to revolutionize fields like architecture, defense, and medicine. The future of work, entertainment, travel, and society is always on the verge of a huge virtual upgrade. VR is a bit like a rich white kid with famous parents: It never stops failing upward, forever graded on a generous curve, always judged based on its “potential” rather than its results.

One reason that VR has been offered such an endless string of second chances (VR’s proverbial lineage, if you will) is that it has played an outsized role in the popular science fiction that our collective image of the future is built around. William Gibson coined the term “cyberspace” in his 1984 book Neuromancer. The term later became synonymous with the World Wide Web, but Gibson’s initial rendering was of a virtual realm that “console cowboys” could enter and exit. Gibson and his cyberpunk peers heavily shaped the culture of 1980s tech—before the dotcom boom, before the tech bros.

When Lanier unveiled his bulky head-mounted display and dataglove in 1987, he was inviting tech hobbyists to be the first inhabitants of the virtual future they had glimpsed in cyberpunk novels. Neal Stephenson’s 1992 Snow Crash and Ernest Cline’s 2011 Ready Player One later were massive science fiction hits whose stories unfolded in a future where VR is a fixture.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

When Zuckerberg says that he has been “thinking about some of this stuff since [he] was in middle school and just starting to code,” it isn’t hard to guess what books he was reading at the time. For the Gen X and Millennial tech entrepreneurs who dominate Silicon Valley today, the science fiction stories of their youth have always treated VR as an ambient part of the future technological landscape.

Just as the current billionaire space race is, at least in part, evidence that inside every tech billionaire is an inner child who dreamed of flying his own rocket ship, the VR arms race is premised on an assumption that mass adoption is inevitable—the only question is when that future will arrive, and which company will get phenomenally wealthy when it does.",Yes,"It has been oversold, leading to disappointment and disillusionment when it doesn't live up to the hype.",https://www.wired.com/story/virtual-reality-rich-white-kid-of-technology/,2021-07-27 13:00:00+00:00,LABEL_1_relevant,0.9962060451507568,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
113,Oculus Primed: Meet the Geniuses Who Finally Mastered Virtual Reality,"In May 2012, a programmer named John Carmack—who, as a cofounder of id Software and the man behind games like Doom and Wolfenstein 3D, is widely seen as the father of 3D gaming—tweeted a picture of what looked like steampunk bifocals made out of a black shoebox. “This is a lot cooler than it looks,” read the caption.

He was right.

Since then, that awkward contraption—now more streamlined, and known as the Oculus Rift—has become the most anticipated new product in gaming since the Nintendo Wii got people off the couch. It’s a head-mounted display that promises to be a gigantic step toward what many had dismissed as an unrealizable dream: virtual reality.

The Rift is the brainchild of a 19-year-old tinkerer and VR enthusiast named Palmer Luckey. A collector of old VR headsets, Luckey was all too familiar with the shortcomings every system had faced—small fields of vision, unwieldy form factors, horrific resolution. He was also uniquely suited to do something about it: Years of modding videogame consoles and refurbishing iPhones for fun and profit had given him enough fine-soldering skills to start Frankensteining pieces from his existing headset collection. Eventually, chronicling his efforts on a message board devoted to 3D gaming, he figured out a way to cobble together a headset with a field of vision that dwarfed anything else on the market and allowed people to become completely immersed in a 360-degree playspace.

Luckey originally envisioned his creation as a DIY kit for hobbyists; after joining up with two partners and officially incorporating, though, they realized they could have a game-changing consumer peripheral on their hands. They began pre-selling $300 prototype headsets to software developers on Kickstarter in August 2012, just weeks after Carmack had taken his early version (on loan from Luckey) to the E3 gaming trade show. They pulled in nearly $2.5 million—and in spring of 2013, when those units began shipping to developers, virtual reality started to seem a lot less virtual.

Since then, the hype has only gotten louder. Oculus brought an improved version of the Rift to E3 in June and showed off its 1080p resolution as well as demos that placed wearers inside a virtual movie theater watching a trailer for Man of Steel. For the first time, applications beyond gaming began to suggest themselves. People, to use the clinical term, freaked out. Hell, I freaked out. Media coverage was rapturous; devoted Oculus forums and subreddits proliferated. Oculus launched a project depository for game demos, and developers responded, creating completely new experiences for a completely new medium.

Now they just need to build a consumer version that can pay off the promise of Luckey’s early experiments. And now that the company has received $75 million in Series B funding from the likes of Andreessen Horowitz—which sources say value the company at more than $250 million—the stakes are even higher. Before the official consumer version of the Rift (known internally as V1) becomes available in 2014, they’ll have to iron out seemingly innumerable kinks, from finalizing the display tech to deciding exactly what features will be included. But first and foremost, they’ll have to solve a problem that has plagued VR since the days of Tron: how not to make people sick.",Yes,Motion sickness.,https://www.wired.com/2013/12/oculus-rift-2/,2013-12-16 11:30:37+00:00,LABEL_1_relevant,0.993819296360016,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
114,Virtual Trade Tough Nut to Crack,"When Blizzard Entertainment recently announced an aggressive policy against trafficking in the virtual goods of the massively multiplayer online game World of Warcraft, it stirred up an old controversy about whether secondary market trading is something game companies can stop.

For years, companies like Sony Online Entertainment have prohibited the buying or selling of goods from games such as EverQuest. Despite such rules, which are commonly spelled out in MMOs' terms of service, the secondary market for virtual goods is estimated at $880 million annually.

Many in the virtual worlds community are now trying to determine what methods game developers can use, if any, to retard the growth of such transactions or even if anything should be done.

""There's a lot of debate about whether anything could be done,"" said Ed Castronova, an expert on the economies of virtual worlds. ""People get wrapped up in the mechanics of this particular technology. The broad question is whether someone with policy tools can affect how someone acts.""

In its announcement on the World of Warcraft community site, Blizzard stated its policy against the buying or selling of the game's objects for real-world money. Its goal, which many MMO developers share, is keeping the game pure from an inflated economy and from players who buy game attributes rather than earning them. And they often claim that such objects have no real-world economic value.

""If you are found to be selling in-game property (such as coins, items or characters) for real money,"" the policy says, ""you will lose your characters and accounts, and Blizzard Entertainment reserves its right to pursue legal action against you as well.""

Castronova, and others, feel that Blizzard can stop such illicit activity, much of which happens on eBay, through actively tracking down buyers and sellers and banning them from the game.

""I think that could have a substantial impact on the demand,"" Castronova said.

But others think such campaigns are pointless.

""The secondary market continues to grow exponentially,"" said Brock Pierce, CEO of IGE, one of the largest secondary market trading companies. ""Someone will always be there, because someone else is willing to pay for it.""

The problem, say some virtual world experts, is that many players see the objects they earn or build through countless hours of play as property. And property has value.

""In cases where the users' contribution to the game constitutes a new intellectual property or new content,"" said Philip Rosedale, CEO of Second Life developer Linden Lab, ""I'd say it makes no sense and is probably indefensible to claim that you can't traffic in those products and that those products have no inherent value.""

Linden Lab is one of the few developers that encourages transactions in the goods and services created in its game. But some economists agree that it is fruitless to ignore the common perception that virtual goods have a measurable economic value outside the game.

""They can stop in-world transfers completely,"" said Dan Hunter, an assistant professor of legal studies at the Wharton School of the University of Pennsylvania. ""If you do that, you have a completely different property system than you have now, where only that which you have is yours.... That goes against our expectations of capitalism.""",Yes,"The development of a black market economy. The black market economy is where people buy and sell virtual goods for real world money. This has caused problems for game developers because it takes away from the game's economy, and it also takes away from the game's immersion.",https://www.wired.com/2004/12/virtual-trade-tough-nut-to-crack/,2004-12-20 07:00:00+00:00,LABEL_1_relevant,0.8466431498527527,Virtual Reality,virtual_reality,Wired,Economy
115,Creator of “Second Life” Issues Warnings for Those Building Metaverses,"As hype around metaverses steadily rises to a fever pitch, many early innovators of digital worlds have been quick to offer up their advice — and stern warnings — to those building them.

Such was the case with Philip Rosedale, the founder of the virtual world “Second Life,” who recently spoke to Time along with anthropologist and author of Coming of Age in Second Life Tom Boellstorff about their insights on building metaverses. Part of his advice was a thought-provoking warning about creating a moderation system that allows users to “equitably co-exist in a metaverse.”

“There are things like getting identity right, so you’re not doxxing a person by identifying the real person behind the avatar — but the avatar or the pseudonym of the individual is sufficiently stable so there are consequences to their actions and they’re going to behave well,” he told the magazine. “But we are simply not there yet on the internet: We don’t have identity systems yet that would enable strong governance.”

Boelstorff also added that there needs to be a strong framework in place in order to prevent bad actors from using the platform for harm. In fact, he said that since users had to pay for a “Second Life” subscription, ad-driven corporations weren’t able to exploit it as effectively as a platform like Facebook or Twitter.

Advertisement

Advertisement

“The subscription model of ‘Second Life’ is one reason that you don’t have misinformation and anti-vax stuff,” Boellstorff told Time. “None of this metaverse stuff going forward has to be a particular corporate ad-driven model.”

Another warning Rosedale had for metaverse developers is that they have to contend with its sizable barrier to entry. After all, in order to access the metaverse, you’ll likely need to have a host of expensive technological equipment including a computer, VR headset, and a stellar internet connection. He added that even with all that, many users often still “don’t want to be a cartoon avatar while wearing a VR headset.”

Even when you set the technology aside, there will be large swaths of the population who will likely still have little to no interest in spending time in a metaverse, no matter how hard you try.

“If you live a comfortable life in New York City and you’re young and healthy, you probably are going to choose to live there. If I offer you the life of an avatar, you’re just not going to use it very much,” Rosedale said. “On the other hand, if you live in a rural location with very little social contact, are disabled or live in an authoritarian environment where you don’t feel free to speak, then your avatar can become your primary identity.”

Advertisement

Advertisement

Rosedale’s insights are actually very tame and supportive when compared to other early metaverse innovators. In an essay published on his company’s blog this summer, Pokémon Go creator John Hanke asserted that the metaverse would be a “dystopian nightmare” if created in VR. Meanwhile, AR pioneer Louis Rosenberg penned an op-ed where he expressed a similar concern “about the legitimate uses of AR by the powerful platform providers that will control the infrastructure.”

Only time will tell if some rendition of the metaverse does end up fulfilling their Cassandran warnings. At the very least, developers might want to take the advice of the creator of one of the most popular and successful virtual worlds of all time.

READ MORE: 6 Lessons on the Future of the Metaverse From the Creator of Second Life [Time]

More on metaverse warnings: AR Pioneer Warns That MEtaverse Could Make “Reality Disappear”

Advertisement

Advertisement

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Yes,"It could cause the real world to disappear. The main issue is that when people put on VR headsets, they will be cut off from the physical world around them. This could lead to people becoming more isolated and less connected to the real world. Additionally, VR could be used to control people's minds and manipulate their reality.",https://futurism.com/second-life-warnings-metaverse,,LABEL_1_relevant,0.9965896606445312,Virtual Reality,virtual_reality,Futurism,"Information, Discourse & Governance"
116,The Virtual Reality Sim That Helps Teach Cops When to Shoot,"“I gotta get the guns,” Scott Digiralomo tells me over his shoulder as he leads me down the cinder block hallways of the Morris County Public Safety Training Academy in Morristown, New Jersey. Digiralomo, director of the county’s Department of Law and Public Safety, ducks into an empty room and, out of a large black safe, fetches an M4 rifle and a Glock.

At this point you should know that as a writer who works in Manhattan, lives in one of the yuppiest neighborhoods in Brooklyn, and gets panicky just passing by armed officers in the New York City subway, this is not how my days typically begin. And yet, no more than 30 minutes later, there I am, a Glock tucked into the holster on my right hip and a can of pepper spray in the left, cautiously approaching a woman in a white SUV who is blocking her ex-boyfriend’s driveway, refusing to let him and his new girlfriend leave.

“Get that crazy bitch out of here now!” yells the new girlfriend, standing in front of the house as I wander up the lawn. Before I can take another step, shots ring out from the SUV. I freeze, and a beat later, clumsily pull the weapon from my hip.

“Uh, put your hands up? And your weapon down? Please?” I say too politely, as if asking a waitress for another basket of bread. But it works. The shooter emerges from the car in a gray hoodie and jeans. She’s still screaming, but she drops the gun and falls to her knees, arms raised. In that instant, I’m pretty sure the situation’s under control, so I take a second to wonder what I’m supposed to do next.

And then she shoots me.

Behind me, Digiralomo is laughing, not because he’s some masochist who’s going to watch me bleed to death, but because the entire scenario, as you may have guessed, is a virtual reality simulation, and I---standing in the middle of the darkened room, surrounded by an array of screens, doing what has to be the world’s worst impersonation of a cop---look like a total tool.

But while this may have been little more than an exercise in embarrassment for me, Digiralomo assures me that this system, designed by a company called VirTra, is actually critical in helping police officers hone their skills as decision makers before they’re let out in the real world. Morris County installed the technology last November, smack dab in the middle of one of the most contentious periods in recent history between police and the public. And while Digiralomo says that wasn’t why the academy bought the roughly $300,000 system, it’s hard not to see the connection.

Not Can You Shoot But Should You

The fatal shooting of Michael Brown in Ferguson, Missouri, last summer cracked open the scab on one of our country’s oldest wounds. It fueled new conversations about centuries-old issues and exposed gaping rifts across the entire country, not only on the subject of whether Officer Darren Wilson was justified in shooting Brown, but on whether or not minorities living in the United States are safe in the hands of the police officers that are hired to protect them. No amount of technology will ever solve these deeply rooted societal issues. Systems like VirTra’s---so-called cave automatic virtual environment, or CAVE, systems---have been around for a while. But as President Obama and others call for more robust police training, training technology that can simulate a world more like the real one takes on an added urgency.

Today, in states like New Jersey where Digiralomo works, officers are required to re-qualify for the police force twice a year by testing their shooting accuracy on a gun range. While that demonstrates that officers can use their weapons, it doesn’t necessarily help them understand whether they should.",Yes,"It is so realistic that it makes it much easier for people to become emotionally involved in the situation. This means that they may be more likely to make poor decisions, especially in high stress situations.

Additionally, because people are so used to the idea that they can ""reset"" or ""undo"" their actions in a video game, they",https://www.wired.com/2015/03/virtra/,2015-03-30 11:00:45+00:00,LABEL_1_relevant,0.9966824650764464,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
117,The Oculus Rift Game That's So Real It Nearly Destroyed Me,"I can hear the alien breathing.

I've played lots of videogames, lots of run-and-gun shooters in which I happily charge down a corridor into certain death. Not this time. I've got my back pressed against the wall of an abandoned spaceship, and I'm inching down a hallway, my head darting left and right, looking for danger everywhere.

And I wonder, is this the one? Is this the Oculus Rift demo where I rip the headset off my face and bolt, terrified, out of the room?

I don't get scared by horror movies or horror videogames. Sure, you can startle the hiccups out of me with a well-timed jump scare, but that only proves my lizard brain is firing on all cylinders. For the most part, I feel a pronounced disconnect between the frightening scenario onscreen and the safety of my living room. I don't understand how some of my friends cannot bear to even pick up the controller and walk down a hallway in Resident Evil.

So it came as quite a surprise when I found myself so truly on edge that I almost lost it while playing Sega's Alien: Isolation demo on Oculus Rift. The game, shown at E3 this year, is a custom VR prototype based on the survival horror game coming to PC and consoles October 7. More than that, though, it's another compelling demonstration of how Oculus Rift has the potential to make gaming so immersive that the fantasy becomes reality.

Sega won't say whether it plans to make the Oculus version a full-fledged consumer product, but it would be crazy not to. I'd played VR demos in which I truly believed I had been transported, but also demos where I felt like I was watching a bad movie on a really huge screen. Alien: Isolation was very much the former. The clever hook of the console game---exploring a lonesome world inhabited only by you and a lone alien, which you spend the entire game avoiding---is a perfect fit for Oculus.

""Like everyone else, we got very excited by the idea of Oculus Rift when it was first going into the Kickstarter,"" said Al Hope, creative lead of developer The Creative Assembly's console division. His team already was working on Isolation when Oculus launched its Kickstarter drive two years ago, and it seemed the two would work well together. Several Creative Assembly developers backed the crowdfunding drive, and they got Isolation running on Rift as soon as their initial development kits arrived.

Sitting down for the demo, I endured the now-familiar ritual of having an Oculus rep place the development kit over my head before clamping a pair of headphones over my ears. And then it began.

Suddenly, I'm walking down a quiet, seemingly deserted hallway on an abandoned spaceship. The demo runs on DK2, the latest version of the Oculus development hardware, so it is clean, vivid, smooth. Lifelike, in other words, and utterly believable.",Yes,"It can screw with your perceptions of reality, to the point where you might not want to take the headset off.

I am so engrossed in the demo that I don't want it to end, and when it does, I find myself transported back to the real world, sitting in a chair in a room full of people,",https://www.wired.com/2014/07/alien-isolation-oculus/,2014-07-10 10:30:03+00:00,LABEL_1_relevant,0.9949263334274292,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
118,EchoPixel's breakthrough VR tech lets doctors look inside your body,"Think of EchoPixel’s tech like InnerSpace but instead of actually minimizing scientists and shooting them into your body to find disease, the medical imaging startup lets doctors pinpoint problem areas from CT, MRI, and ultrasound scans using 3D glasses and a special display.

Most doctors view CT scans in 2D, meaning they can’t see in and around all the details of your body, so it’s harder to find the exact problem. Some even resort to bringing hand-drawn sketches into the operating room. EchoPixel CTO Sergio Aguirre says “It’s really a shame that doctors are still using the same 2D images designed in 1880.”

But with EchoPixel and 3D glasses, internal organs pop off the screen like holograms so doctors can virtually examine a patient from any angle. EchoPixel could radically improve healthcare while reducing time and costs for hospitals and patients. It’s one of the most promising ways virtual reality is making in-roads in healthcare.

EchoPixel achieves this technological feat by employing the 300 million 3D radiology scans performed in the U.S. each year. But instead of flattening them onto a 2D screen, its real-time, interactive 3D imaging system allows doctors to peer into every corner and crevice of your body.

Information can be tailored for specific procedures, and doctors are able to zoom in and pull out something that doesn’t look right from your body scan, or 3D print the image to have a working model for further study. A weird bump or lesion on your intestines is easier to find and examine, for example.

The 3D tech’s ability to virtually enlarge tiny parts of the body is especially helpful for treating newborns. Clinical studies showed doctors were able to find up to 90 percent more congenital heart defects amongst newborns in 40 percent less time. It’s also much quicker to size medical devices like stents because doctors can examine in 3D the place they’ll go. One trial showed sizing time reduced from 40 minutes to just 2.

EchoPixel refers to the tech as interactive VR, even though it’s not like Vive or Oculus. In fact, it’s more convenient. Instead of having to strap on and off a VR headset in the middle of a procedure, doctors can just glance to the side and see the 3D image of what they’re working on.

The startup this year raised a $5.8 million seed round, and is now selling three-year subscriptions to its technology for $25,000 a year. Other companies in the space include Surgical Theater and RealView.

EchoPixel already has the go-ahead from the FDA, and will now seek approvals for Europe and Asia. Eventually, doctors could let precise robots do the incisions while they control them via EchoPixel a few feet away.

We got to try out the new technology on a recent visit to EchoPixel’s Palo Alto, California headquarters, as well as interview one of the doctors now using the product, UCSF’s Dr. Judy Yee, who’s been able to catch potentially cancerous lesions in the gut with EchoPixel.

Check out the video above to see how this technology could speed up productivity in the hospital and help save more lives.",Yes,It makes people more lazy.,https://techcrunch.com/2016/05/04/echopixel/,2016-05-04 00:00:00,LABEL_1_relevant,0.9707409143447876,Virtual Reality,virtual_reality,TechCrunch,"Information, Discourse & Governance"
119,"Music's Salvation Might Be Selling Not Songs, But VR","Content This content can also be viewed on the site it originates from.

So here's a strange thing about making VR videos: turns out it's really hard to show a rough cut. Even once you've done the complicated 360-degree shooting, and your computational algorithms have stitched all the footage together into something realistic and immersive, you still need to fine-tune the edits, sound effects, and visuals so you don't disorient your viewers (or worse). Looking at a two-dimensional version on a laptop doesn't really do it justice, and if you're dealing with people in remote locations the chances are basically zero that everyone will have their own Oculus Rift to weigh in on the footage.

Considering all that, Stuart Cripps' jitters are understandable. The heavyset, soft-spoken British director is working on a VR project for a 20-year-old Irish singer-producer named Jonathon Ng, known to his fans as Eden. The song, ""Drugs,"" is the second single from Ng's new EP. Cripps has been working on the video for months, but nobody outside his studio has seen anything. Now, on a blisteringly hot day in early June, two weeks before the video's scheduled release, he has something to show. It's just not finished yet.

A dozen people, all involved with the project in some way, file into a post-production office near Universal Studios in L.A. While they crowd onto couches and chairs and try to loop Ng in from his home in Dublin, Cripps walks them through his vision; after a few minutes of high-mindedness---""I imagine us going from dust to dust,"" he says, all but staring wistfully out the window---he leads everyone downstairs and gestures towards a swivel chair and an Oculus Rift dev kit. As with so much about VR, the only way to really get the video is to try it.

Outside of games, music is almost certainly the most popular content type in VR right now, which makes sense both technically—right now, VR's best for quick viewing periods, about the length of your average song—and creatively. Both formats trade in experiences, connection, and immersion. You don't go to a concert for the sound quality; you go to be part of something. ""Today’s adolescents,"" music writer/analyst/crank Bob Lefsetz wrote earlier this year, ""are enthralled by YouTube stars and others who evidence a personality, there’s a human bond between them and their heroes and there’s this feeling that the famous and the hoi polloi are in it together."" That feeling also happens to be the kind of thing fans will pay for, which is hard to come by in today's music industry. Meanwhile, one industry forecast pegs VR and AR as a $120 billion business by 2020, and everybody wants a slice. The tech and the content seem like a perfect match. But figuring out how to make it all work? That's another thing entirely.

Music in the round

Musicians have been experimenting with VR longer than almost anyone. Most of the early projects go one of two ways: Either someone sets up a camera on stage at a concert, so you get to watch the show from the perspective of a weird band member who doesn't play anything and just kind of stands there, or the band plays while standing in a circle around a camera, as if the show's just for you. Whether either even counts as virtual reality, or is just ""360 video,"" depends on who you ask. But everybody's doing it: Coldplay, Avicii, U2, Bjork, Paul McCartney, Taylor Swift, the cast of Broadway's Lion King. Run The Jewels' ""Crown"" is a particularly cool, in-your-face example.

A few others go a step further, using the immersive power of VR to make music videos more interesting. Ray Lamontagne's ""Hey, No Pressure"" video drops you into a kaleidoscopic acid trip of a dance floor, where dancers gyrate and lights flash to the beat of the song. Earlier this year, Dawn Richard dropped a bonkers VR space jam, full of holograms and intergalactic surroundings.

Content This content can also be viewed on the site it originates from.

Regardless of genre or feel, all those projects have one thing in common: they don't sing you a song so much as drop you into the middle of one. ""In traditional music videos,"" says Chris Milk, a celebrated music video director turned founder of VR company Within, ""the viewer is outside of the experience, looking inward. In VR, people are right in the middle of it."" Narrative, story-first music videos---like Michael Jackson's 13-minute ""Thriller"" epic, to name the one that literally everyone mentions whenever they bring this up---just don't work in VR. Not yet, anyway. What VR does best right now is enhance the sort of raw, Rorschach emotion that makes music powerful in the first place.

The best example of VR's potential so far is probably ""Old Friend,"" a much-loved piece by animator Tyler Hurd that's set to a Future Islands song of the same name. You put on your headset, and an outrageous dance party starts all around you. Using the HTC Vive's controllers and positional tracking, your avatar dances however you dance. Hurd was making the video for less powerful systems like the Gear VR, but he found the Vive's full-body experience irresistible. ""As soon as you start leaning around and looking at things,"" he says, ""they feel more real. You stop wondering and you just let yourself be there."" He calls his video ""an overwhelming barrage of nonsensical joy."" It's not a game or a story; it's an experience, a place.

Cripps and the ""Drugs"" crew wanted to create a similarly visceral effect—and the song, mellow and moody, was a perfect match. ""Everyone kind of immediately felt like 'Drugs' was the best fit,"" Ng says. Cripps and his crew shot with a strange-looking rig: a Canon 5D on top of a Kinect, which together capture real-time three-dimensional data using beta software created by a New York company called DepthKit. The resulting aesthetic is pure wireframed data, like a glitchy Tron landscape crossed with Star Trek's Holodeck. At one point the picture floats over a mesh surface, panning down and out until you realize it's an outline of a person playing the keyboard.",Yes,"It makes people feel closer to what they're watching. Which is exactly the kind of feeling that's been declining in the music industry for years. ""It’s not that people are falling out of love with music,"" wrote Lefsetz in the same piece, ""it's that we’re falling out of touch with the",https://www.wired.com/2016/09/future-of-vr-music/,2016-09-10 11:00:48+00:00,LABEL_1_relevant,0.5404317378997803,Virtual Reality,virtual_reality,Wired,Social Norms & Relationships
120,This Ebola Documentary Shows VR Film's Radical Potential,"An intense new virtual reality film isn't a documentary so much as it is a prayer; its lector is a young Liberian woman named Decontee Davis, and her benediction is a plea for help to rebuild her country after Ebola ravaged it in 2014.

""I remember the fear—the fear people had of me,"" Davis says in a voiceover discussing her own time fighting the disease. ""They were too scared to even touch me. And I was scared of my own child. I was scared of the man I loved. Forgive me, Lord. Forgive me.""

Davis’ words are hard to hear, but they’re even harder to hear as the sights and sounds of her world swim around you. And in Waves of Grace, a collaboration between VR studio Vrse and the United Nations that's out today on the Vrse app, that’s just what they do. Davis, an Ebola survivor who uses her immunity to work with children orphaned by the disease, is our guide through the country, but she is also an educator—teaching her own community and anyone who watches Waves that survivors pose no threat to the public.

It’s a powerful message even when read on paper, but when heard while standing amongst the orphans themselves and the graves of some of the more than 4,800 lives Ebola has claimed in Liberia, it’s downright heartbreaking. And that’s why the UN wants people to experience it in VR: the immediacy of the 360-degree view gives the film a poignancy that a news report simply can’t.

“A YouTube video or a talk can give you information but not necessarily the ability to immerse yourself in the world of another, and also interact with people,” says Gabo Arora, the UN senior advisor and filmmaker who made Waves with Vrse. “There’s something about how VR is being made—there’s a spontaneity and naturalness that’s not necessarily coming out in other means.”

A YouTube video or a talk, it can give you information but not necessarily the ability to immerse yourself in the world of another. UN filmmaker Gabo Arora

Arora has seen it work before. His last Vrse collaboration was Clouds Over Sidra, a VR experience about a young Syrian refugee in Jordan. That project was so good at showing the situation for Syrian refugees that it received the backing of UN Secretary-General Ban Ki-moon, and has proven effective at helping raise awareness—and money—for humanitarian efforts. (Recent numbers from UNICEF in New Zealand found that one in six people who saw Sidra donated, a rate double the norm, Arora says.)

The power of Clouds also helped encourage Arora’s subject. Davis had already been a central figure in a Washington Post report when Arora approached her about participating in the UN’s new VR endeavor, and thus felt as though she had already told her story. Hoping to convince her, Arora showed Davis Clouds; that changed everything. “She said ‘Oh my God, of course. Anything you want,'"" he says.

Arora, who plans to continue making VR experiences for the UN, says the immersive nature of the medium can also be a huge help within the UN. It’s important for the organization to stay aware of humanitarian situations all over the world, but large expeditions to already-vulnerable regions can be disruptive. Sending one small VR crew to the area and then letting people at the UN see their findings can cut down on that disruption, he claims.",Yes,"It makes people feel even more empathy and concern for people in difficult situations.

“We’re trying to find the sweet spot of where VR can be used most effectively and efficiently to help people understand what’s going on, and also help with fundraising,” Arora says. “It’s a",https://www.wired.com/2015/09/vr-ebola-film/,2015-09-01 17:00:19+00:00,LABEL_1_relevant,0.9953891038894652,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
121,These Virtual Obstacle Courses Help Real Robots Learn to Walk,"An army of more than 4,000 marching doglike robots is a vaguely menacing sight, even in a simulation. But it may point the way for machines to learn new tricks.

The virtual robot army was developed by researchers from ETH Zurich in Switzerland and chipmaker Nvidia. They used the wandering bots to train an algorithm that was then used to control the legs of a real-world robot.

In the simulation, the machines—called ANYmals—confront challenges like slopes, steps, and steep drops in a virtual landscape. Each time a robot learned to navigate a challenge, the researchers presented a harder one, nudging the control algorithm to be more sophisticated.

From a distance, the resulting scenes resemble an army of ants wriggling across a large area. During training, the robots were able to master walking up and down stairs easily enough; more complex obstacles took longer. Tackling slopes proved particularly difficult, although some of the virtual robots learned how to slide down them.

Content This content can also be viewed on the site it originates from. A clip from the simulation where virtual robots learn to climb steps.

When the resulting algorithm was transferred to a real version of ANYmal, a four-legged robot roughly the size of a large dog with sensors on its head and a detachable robot arm, it was able to navigate stairs and blocks but suffered problems at higher speeds. Researchers blamed inaccuracies in how its sensors perceive the real world compared to the simulation,

Similar kinds of robot learning could help machines learn all sorts of useful things, from sorting packages to sewing clothes and harvesting crops. The project also reflects the importance of simulation and custom computer chips for future progress in applied artificial intelligence.

“At a high level, very fast simulation is a really great thing to have,” says Pieter Abbeel, a professor at UC Berkeley and cofounder of Covariant, a company that is using AI and simulations to train robot arms to pick and sort objects for logistics firms. He says the Swiss and Nvidia researchers “got some nice speed-ups.”",Yes,"It creates an addictive environment that encourages people to stay inside for long periods of time. This can lead to obesity and other health problems, as well as social isolation.",https://www.wired.com/story/virtual-obstacle-courses-help-real-robots-walk/,2021-10-08 11:00:00+00:00,LABEL_1_relevant,0.9909802079200744,Virtual Reality,virtual_reality,Wired,Health & Wellbeing
122,Virtual Reality Allows Adults to See World Through a Child's Eyes,"When you're a kid, everything seems huge. Teachers tower over you; playgrounds stretch on to infinity. Now, researchers have found a way to make grownups feel the same way. By placing volunteers in virtual reality, scientists are helping adults see the world through the eyes of a child.

Virtual reality is more than an illusion. To enter it, people put on full-body suits that track their movements and goggles that display an artificial world in which they have a virtual body. If their virtual and real movements sync up, their computer-generated bodies start to seem real. Previous research has shown that subjects begin to feel like their body has changed into the simulated figure, even if it is different from their own body; volunteers placed into the body of a teenage girl, for example, ""felt it"" when her mother slapped her computer-generated representation. But scientists did not know how this virtual body ""ownership"" affected people's perception of the world around them and whether this could help people relate with others unlike themselves.

To find out, computer scientist Mel Slater of the University of Barcelona in Spain and colleagues placed adult volunteers into a virtual outdoor scene in which they did not have a computer-generated body. They were asked to estimate the sizes of six different cubes within the scene and were told whether their guesses were too big, too small, or correct. Later, they reentered the scene and repeated the exercise with three cubes, without feedback from the researchers. Their size estimates without a virtual body were noted.

After this training exercise, the researchers placed the subjects in two different avatars, virtual characters controlled by participants. One was a 4-year-old child of the same gender as the participant. The other was an adult who was the same height as the child. While standing in a virtual living room, the participants again gauged the size of the cubes without feedback.

Adults stated that they felt the two virtual bodies were equally real, and they misjudged object size in both avatars. But those in the child avatar rated the cubes about twice as large, on average, as did those in the adult bodies, Slater's team reports online today in the Proceedings of the National Academy of Sciences.

In a test after the simulation, participants who had been in the virtual child body were quicker to mischaracterize themselves with childlike traits than their actual ones. For example, someone who had embodied the virtual 4-year-old was more likely to identify themselves as attending primary school than someone who had embodied the scaled-down adult.

When movements of the virtual and real bodies did not match, participants no longer felt their avatars were real. They still overestimated the size of the cubes, but there were no differences between the adult and child avatars. The researchers conclude that the type of avatar can affect how people see their virtual environment only if they feel connected with their virtual body.

Slater notes that the experimental setup in this study could be used to help people empathize with others who are unlike themselves, for example by putting criminal offenders in the virtual body of the victim at the crime scene to help them see the event from the other perspective.

The study demonstrates that we may need to think harder about the implications of spending time in a digital world, says cognitive psychologist Jeremy Bailenson of Stanford University in California. He describes a hypothetical danger of Google Glass, a pair of glasses that gives users hands-free access to the Internet. ""If I'm using Google Glass and I'm in the avatar of a child, I may not be able to cross the street in the right manner because I'm seeing objects as a different size than they actually are out in nature.""

*This story provided by ScienceNOW, the daily online news service of the journal *Science.",Yes,"It could potentially lead to people not being able to properly judge the size of objects, which could lead to accidents.",https://www.wired.com/2013/07/virtual-reality-perspective/,2013-07-16 14:30:05+00:00,LABEL_1_relevant,0.9941185712814332,Virtual Reality,virtual_reality,Wired,Security & Privacy
123,One Startup's Quest to Save Refugees With Virtual Reality,"F

Four boats approach the small harbor of Skala on the Greek island of Lesbos. The first vessel is occupied by agents of Frontex, the European Union border-control unit. The men are dressed in black, from helmets to combat boots. They tow the second boat, an inflatable dinghy with flimsy plywood baseboards that’s crammed from pontoon to pontoon with extremely cold people. Earlier this morning a smuggler in Izmir, Turkey, filled the raft with refugees from Afghanistan, Iran, and Pakistan, handed the throttle to a young man who’d never driven a boat, and pointed toward Greece. Like so many of the thousands of vessels provided by human-smuggling mafias, this one didn’t have enough fuel and ran out of gas somewhere in the middle of the Aegean.

The third boat, a gray Zodiac, found them. It’s manned by two young men—one an out-of-work Greek, the other a Norwegian bored with his stultifying Oslo desk job. Neither of them possesses an organizational affiliation. Despite the fact that hundreds of thousands of migrants and asylum seekers have come through Lesbos in the past year, as of my visit on December 18, neither Frontex nor the Greek coast guard has established much of a presence. Instead, the job of offering aid falls largely to international volunteers who have flocked to the island. A throng of them, their experience ranging from extensive to none, waits onshore with reflective survival blankets. The emergencies director of Human Rights Watch is here, as is a fashion model from Manhattan who brought perfume samples for the refugees. As the Zodiac approaches the dock, the Norwegian hurls himself into the water and ties the boat up to a mooring.

Behind the scene trails the fourth boat, a wooden vessel owned by a local fisherman. On the bow, a bearded American named David Darg holds up a small virtual-reality camera called a Ricoh Theta. Thirty-seven years old, with a reddish-brown beard, tight black jeans, and the thick build of a logger, Darg occupies a unique and peculiar role within the fast-moving world of new media.1 On the one hand he’s a crisis responder and vice president of international operations at Operation Blessing, a faith-based nonprofit. But he’s also cofounder of Ryot, a Los Angeles for-profit company that specializes in hopeful video content from developing and disaster-affected nations. He has come to Lesbos to bring the reality of the migrant crisis to the wider world. Darg calls the VR camera in his hand a “transportation device,” one capable of essentially bringing Western viewers to the world’s strife-ridden places. “You register VR as an experience you had,” he says, “rather than something you see”—a common boast about VR.

“A lot of people will make a very cool, very expensive VR experience that very few people will see.”

This phenomenon, he suggests, allows him to convey nuanced reality like never before, cutting through a media landscape saturated with two-dimensional images of suffering. It’s also essential to his plan to change the way we consume global news. Darg and Ryot’s other cofounder, a former Peace Corps volunteer named Bryn Mooser, launched the company in 2012 while giving aid in post-earthquake Haiti. Mooser, Ryot’s CEO, is 36, a tall and charismatic entrepreneur who grew up in Santa Monica and moved to Zimbabwe with his mother at age 16, eventually studying film at Bennington College and joining the Peace Corps in Gambia. Darg and Mooser’s idea was to create a new form of short, impactful storytelling to buoy the humanitarian industry. At first the company billed itself as the future of news—a sell that seemed rather ambitious, sort of like an undermanned expansion team declaring itself the future of the NBA.

But in the past year, Ryot has positioned itself as the go-to VR company for both news and humanitarian advocacy. Recently the Huffington Post, NPR, and The New York Times have lined up to work with the company. Darg, who speaks in a calm, self-assured voice, finds the old guard’s attention delicious. “They’re coming and they’re asking us for advice,” he says. “Men in suits wondering what happened now that no one’s coming to their sites any more.”2

The Greek fisherman ties up to his mooring and Darg hops out. The volunteers swarm the asylum seekers, offering blankets and water. A CNN anchor cues her cameraman and says in a dire tone that the organizer of the Paris ISIS attacks may have come through Lesbos. An Australian woman in a “Love” beanie gives a head massage to a middle-aged Muslim refugee, whose face maintains an expression of mild horror. Other refugees take selfies. Soon everyone moves up the shoreline, past a lighthouse, an olive press, limp life jackets hanging in trees, and a relief camp run by anarchists, where two dogs in wool vests lounge amiably. The group reaches another ad hoc camp, where refugees receive food and dry clothes. On the street, Darg turns his VR camera onto a young Afghan named Mustafa and asks him to address Americans who are afraid of refugees.

Mustafa’s message is simple: “I lost my parents. I would love to be beside them and not here.” And then, just as he starts to thank Lesbos’ aid workers, a locally famous cross-dresser sashays by the shivering refugees in knee-high boots and red tights, his gray hair blowing in the wind. Darg’s camera is still running.",Yes,"It makes it harder for audiences to distinguish between what's real and what's not. This can have dangerous consequences, especially when it comes to the consumption of news.",https://www.wired.com/2016/07/ryot-darg-mooser-disaster-vr/,2016-07-18 10:50:47+00:00,LABEL_1_relevant,0.9955952763557434,Virtual Reality,virtual_reality,Wired,"Information, Discourse & Governance"
124,"Yuck It Up, Humanity, Because VR Is Going To Make Things Really Weird","The Internetogencia was all in a tizzy this week as it commented on a cover photo from Time magazine extolling the simple joys of VR. The cover, if you haven’t seen it, features the disheveled founder of Oculus Rift, Palmer Luckey, who looked like a jolly baby as he bumbled around a virtual seashore. In short, it was WhiteGuysWearin’OculusRifts on the cover of a major news magazine that you probably thought had already gone out of business. Funny stuff, right?

But I’m afraid. I’m afraid that Luckey is a harbinger of something more. While Google Glass – an AR device, not a VR device – was rightly ridiculed for being annoying rather than immersive, VR devices like the one Luckey is wearing are far more pernicious. The image of a nerd in VR glasses is funny to us now, but just imagine what will happen when Valve releases Half-Life 3 in VR and we all end up sitting for hours, heads encased in black plastic, as we grunt and gesticulate in our desk chairs. VR devices aren’t meant to be taken to bars so no one will give their wearers a solid punch; instead they will pull our distracted psyches further down the rabbit hole of sub-social Internet behavior as we enclose ourselves in a virtual world.

VR will be important. Barring a direct brain connection it will be the closest representation to true reality that we will ever experience in the electronic age. Beaming photons into our eyes is a great way to connect directly to the brain and we, as visual creatures, will lap it up like cream. I have no doubt that Luckey is really enjoying himself in that photo. He’s probably living in some undersea world where fishes swim about his noggin or fighting over demon hordes in a Tolkein-esque wilderness. Once the experience of strapping something onto your head and wandering around the world becomes commonplace – and it will – we’re all going to look like Luckey.

VR is coming. It’s going to change the way we interact with the world. Parents will look back on the days when their kids just wanted to play Angry Birds on the iPad with wistful nostalgia as their kids cocoon themselves into VR-induced comas. The same guffawing Internet users will find that some things – primarily gaming – are better in VR. And the world will change, once again, as VR becomes easier to use and less goofy. We can laugh all we want but right now remember that we’re chortling from the seat of our horse-drawn buggy as the first Model T chuffs down our country lane.

By the way, Luckey isn’t the first disheveled geek Time put on their cover. Way back in the olden days they stuck a kid named Marc on there – barefoot, polo shirt untucked, mom-jeans acid-washed – and said he and his buddies were the harbinger of a new era. A lot of people were fixated on his feet – a scion of business barefoot! On a throne! Instead they should have been fixated on his ideas.",Yes,"It will lead to more sub-social Internet behavior. People will be more likely to spend time in their own virtual world, rather than interacting with others in the real world.",https://techcrunch.com/2015/08/07/yuck-it-up-humanity-because-vr-is-going-to-make-things-really-weird/,2015-08-07 00:00:00,LABEL_1_relevant,0.949478030204773,Virtual Reality,virtual_reality,TechCrunch,"Information, Discourse & Governance"
125,Why I've Never Mixed Weed With Virtual Reality,"When I was 17 years old, two things held particular sway over my imagination. One of them was virtual reality. Neal Stephenson's Snow Crash and the movie Lawnmower Man both came out that year, and while I had never heard of the technology that was bubbling through labs and startups thousands of miles from my Midwestern home, the idea of entering and truly existing inside a digital world became a source of endless fascination.

The other thing was—well, the other thing was weed.

First off, yes, obviously. It's one of the great vice-based rites of passage. But smoking unlocked something transformative in my brain. It wasn't necessarily that it rendered aesthetic experiences more vivid (which it did), or that the euphoria that came along with that beat the hell out of most of the other emotions that otherwise define adolescence (which it did). It was that creativity and communication seemed to happen on a different frequency, a range uncluttered by self-consciousness and second guessing.

More than 25 years later, in news that would probably thrill 17-year-old me, both of those things are still part of my life. I've written about VR since it first reemerged in the early part of this decade. I've used it to meditate, to spend time with friends, to travel through space, and sometimes just to watch Netflix in a place that isn't my living room. I have no idea how much time I've spent inside a headset, but between the exotic and the pedestrian it's likely enough to qualify for dual citizenship with the metaverse.

As for cannabis: hell, it's California. In New York, I had a delivery guy who kept his inventory in a fake tennis-ball can. Here, I can walk into a store, browse a laminated menu, and walk out with any one of dozens of strains, optimized for whatever mood or medium I prefer. You have a glass of wine with dinner, I have a little Gelato with some added terpenes. (Operative term here being ""a little."" I'm not 17 anymore.) I've used it to meditate, to spend time with friends, to travel through—you get it.

So it's with great trepidation that on this, the holiest most annoying of days, that I unburden myself: I have never used VR while high. Never even considered it, despite their seemingly synergistic natures. And even though this pattern established itself organically, I've come to realize that it distills everything I think (and fear) about immersive technologies, and about the age of simulated experience that they herald.

As our screens and speakers and computers got better and better over the last 50 years, each new wrinkle brought with it an unspoken cultural dare: That high-def TV is cool, but did you ever watch it … on weed? Surround Sound, IMAX screens as tall as your apartment building, Pixar and ILM wizardry that made the impossible real. The draw wasn't in a bigger, louder, sharper experience—it was how much more mind-blowing it would be in conjunction with tetrahydrocannabinol. Then flat screens gave way to something much, much realer, and the calculation changed considerably.",Yes.,"It makes people really fucking nauseous. The reason is that while the visual system is quite good at understanding motion, the vestibular system, which is responsible for balance, is slower and dumber. If you're standing still, and you move your head, it's a safe bet that your feet are still on the ground. But when",https://www.wired.com/story/cannabis-virtual-reality/,2019-04-20 17:27:48.130000+00:00,LABEL_1_relevant,0.9934271574020386,Virtual Reality,virtual_reality,Wired,Health & Wellbeing
126,VR Cops Brag About Planting Fake Evidence in Metaverse While Disguised as Underage Girls,"""I can pull out a bag of weed and make it look like it came out of someone’s pocket.""

Let’s Be Cops

Turns out cops in the metaverse can be just as corrupt — and not to mention creepy — as real world ones. At least that’s the case with a group of users on the metaverse platform VRChat who have created a police force to question, arrest, and occasionally frame other users for crimes.

Oh, and they do it while using avatars of underaged female anime characters. Yikes, right?

The Loli Police Department (LPD), disturbingly named for the slang internet term for underaged and often sexualized female anime characters, boasts nearly 11,000 members on its Discord server, Input reports. Founded in 2018 as a joke, the group has grown to be the most popular police roleplay community on VRChat.

“We have two-hour patrols where we break up into squads, go out into the world of VRChat, and interact with people to start roleplays,” a Scottish computer science student who participates in the weird roleplay told Input. Jura is a sergeant in the LPD and even runs a training program for new recruits.

Advertisement

Advertisement

Virtual Corruption

“Officers” in the LPD spend their time harassing various people on the platform, investigating crimes, and occasionally arresting people. Some members have even admitted to planting evidence to fabricate reasons for arrests — just like actual cops.

“I can pull out a bag of weed and make it look like it came out of someone’s pocket,” Karet, an LPD captain from Texas, told the site. “They always say it’s not theirs.”

VR ACAB

Though the loli imagery is troubling, no one is actually getting arrested or hurt by a users larping as cops on the metaverse. However, the LPD do occasionally encounter pushback from users who harbor anti-police sentiments.

“We absolutely get abuse,” another LPD member told Input. “Especially when there’s police violence going on in the US. We get a lot more people responding and saying curse words to us, like ‘Fuck the police.’”

Advertisement

Advertisement

On the one hand, the phenomenon is a quirky example of the ways the metaverse can allow us to truly transform who we are. On the other, the whole thing seems to be soaked in some sort of ironic-or-maybe-not joke about pedophilia.

One thing’s for sure: there’s already stuff unfolding on the metaverse that’s so weird, dark, and unexpected that nothing would surprise us going forward.

READ MORE: Inside the volunteer ‘police department’ arresting people in VR [Input]

More on metaverse madness: Turns Out It’s Really Easy for Kids to Visit Sex Clubs in the Metaverse

Advertisement

Advertisement

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",,"It is being used as an alternative to real life. This is problematic because it can lead to people becoming more isolated and disconnected from the real world. Additionally, it can be used to commit crimes, such as planting evidence to frame someone for a crime they didn't commit.",https://futurism.com/the-byte/vr-cops-plant-evidence,,LABEL_1_relevant,0.996712327003479,Virtual Reality,virtual_reality,Futurism,"Information, Discourse & Governance"
127,Better Than Opioids? Virtual Reality Could Be Your Next Painkiller,"Virtual reality has long been studied for its potential to ease pain by serving as a distracting force during medical procedures like wound care sessions for burn victims. Yet typically the equipment needed to bring virtual reality into hospitals (or anywhere else, for that matter) has been extremely pricey: Hunter Hoffman, director of the virtual reality research center at the University of Washington’s Human Photonics Laboratory and a developer of a pain control game called SnowWorld, says the VR equipment he’s using as part of an intensive care unit pain-relief study costs $35,000.

With cheaper headsets like Samsung’s Gear VR, Oculus’s Rift, and HTC’s Vive now on the market, though, it’s a lot easier for hospitals and doctors to consider using them for their patients, and a number of startups including AppliedVR see a business opportunity. AppliedVR sells its service—the VR content plus a Gear VR headset— to customers for far more than the $100 you’d pay for a Gear VR plus the price of a compatible high-end Samsung smartphone, but still much less than virtual-reality devices have cost in the past. It declined to disclose the exact pricing.

AppliedVR is working with Cedars-Sinai Medical Center—an investor in the company—and Children’s Hospital Los Angeles, which are conducting studies using its software. One study that Cedars-Sinai recently completed but hasn’t yet published had a group of 60 patients with a range of medical conditions (things like abdominal pain from pancreatitis or chest pain from pneumonia) use AppliedVR-provided content including a game called Bear Blast, in which the player moves his head to throw balls at cartoon bears.

The game, which I tried out in my office, is pretty simple and seems purposely mesmerizing: you constantly move forward at a slow pace through a virtual world filled with bears and lob balls at as many of them as possible to earn points. Unlike most games, you can’t get hurt or die.

Brennan Spiegel, who directs health services research at Cedars-Sinai, says researchers found that 20 minutes with the virtual-reality software reduced patients’ pain by 24 percent on average; before using VR the patients had a mean pain score of roughly 5.5 on a zero to 10 scale, he says, and afterward it averaged 4.

“That’s a pretty dramatic reduction for an acute pain,” he says. “It’s not too different from what we see from giving narcotics.”

Now, Spiegel says, Cedars-Sinai is going to conduct a controlled trial where some patients get virtual-reality headsets that they can use whenever they want during their hospital stay and others don’t.

Spiegel is cautiously optimistic about virtual reality’s prospects for helping reduce patients’ discomfort. He thinks the technology could be beneficial to people who are in pain or, at the least, could use some help with relaxation, both in the hospital and after they go home. But, he says, it won’t help everyone, and a lot more data has to be gathered before it will be clear if it’s truly useful.

Elliot Krane, chief of pediatric pain management at Stanford Children’s Health, also thinks VR’s power for distracting could be valuable for, say, helping kids with physical therapy and getting them to stay still during imaging studies. To him, the biggest challenge right now is finding software developers who want to make applications that target specific medical problems—something AppliedVR says it wants to do to grow its catalog of content.

Spiegel points out that it’s also important to figure out when the technology can be helpful and when it isn’t the right approach. He remembers trying to convince a patient whose cancer had spread that she could put on a headset and be transported, virtually, to Iceland; she looked at him like he was crazy, he says.

“We have to be careful about not overselling virtual reality, or any other digital technology,” he says.

Update: This story was revised from its original version with additional information about pricing from AppliedVR.",Yes,People can get addicted to it and lose touch with reality.,https://www.technologyreview.com/2016/07/18/70616/better-than-opioids-virtual-reality-could-be-your-next-painkiller/,2016-07-18 00:00:00,LABEL_1_relevant,0.9953976273536682,Virtual Reality,virtual_reality,MIT Tech Review,"Information, Discourse & Governance"
128,"UN Launches Powerful, First Ever, VR Film following Syrian Refugee Girl","A young girl, a Syrian refugee, suddenly appears before you, sitting on the floor of a cramped room. She describes her family, and takes you on a journey through the refugee camp in which she lives. You sit beside her in her makeshift school classroom and watch children tramp through the muddy streets in the encampment. You feel as if you can smell the bread being baked in front of you at the camp’s bakery. This is the powerful, immersive and deeply moving experience invoked by a ground-breaking new film shot for the United Nations using the Samsung Gear VR 360-degree platform, which is a collaboration between Samsung and Oculus. As a way to create empathy with a subject, it’s definitely a harbinger of things to come.

Fashion model Mari Malek (pictured below), former Harry Potter star Emma Watson, UN Secretary General Ban Ki-moon and Norwegian Prime Minister Erna Solberg were among the dignitaries at the World Economic Forum in Davos today backing the launch the new film, shot specifically for the Samsung Gear VR.

“Clouds Over Sidra” follows a twelve-year-old girl named Sidra in the Za’atari camp in Jordan — currently home to 84,000 refugees from the bloody Syrian civil war.

It’s the first ever film shot in virtual reality for the UN and is designed to support the UN’s campaign to highlight the plight of vulnerable communities, particularly refugees. The film is released today on the VRSE channel on Samsung Milk VR USA as well as the VRSE application on iTunes and Google Play. Samsung Milk VR is like a content delivery system for Gear VR, a little like a ‘YouTube for VR’.

You’ll be able to experience the film on a virtual-reality headset, like the Oculus Rift device or via the Vrse app, which operates on a smartphone in conjunction with a simple viewer, like the cardboard one that Google has designed. You can also download the app and watch it on a phone, but you won’t get the full 360 Degrees immersive experience.

Created by UN adviser Gabo Arora and filmmaker Chris Milk using VRSE.tools, the film was produced by Samantha Storr, and is a collaboration between the UN Millennium Campaign and UNICEF Jordan.

Right now the number of refugees, asylum seekers and internally displaced people is at the highest it has ever been since World War II. Over fifty percent of them are children.

I watched the movie myself and interviewed the producer, Socrates Kakoulides.

It had a powerful effect on me as you can see from the below video.",Yes,"It makes you feel like you are really there. It's an immersive experience that makes you feel as if you are in the same room as the person you are watching. In the case of the ""Clouds Over Sidra"" film, it makes you feel as if you are in the Za'atari camp in Jordan with the twelve-year",https://techcrunch.com/2015/01/23/un-launches-powerful-oculus-virtual-reality-film-following-syrian-refugee-girl/,2015-01-23 00:00:00,LABEL_1_relevant,0.9934078454971312,Virtual Reality,virtual_reality,TechCrunch,User Experience & Entertainment
129,Avegant's Glyph Headset Is a Movie Theater for Your Face,"When I put on the Glyph, the $699 headphones-meets-face-computer headset Avegant's been working on, Ed Tang didn't put on a game. Avegant's founder and chief strategy officer didn't drop me in some immersive other world, where I could look around and explore or interact with a robot.

He put on The Avengers.

The Glyph looks like a VR headset, but it's not a VR headset. It also looks like an overgrown pair of Beats headphones, but it's not an overgrown pair of Beats headphones. It's something else. Avegant calls it ""Mediawear,"" which is terrible. It amounts to this: a pair of great-sounding headphones, with two eyepieces on the underside of the band. Flip it down over your eyes, and suddenly it's like you're sitting dead center in your own movie theater. It's kinda like Oculus Rift meets Bose meets holding your phone inches from your face. After years of development, a $1.5 million Kickstarter campaign, and lots of user testing, Avegant is getting ready to ship the Glyph this spring.

I first saw the Glyph almost exactly two years ago. It was a mess of circuitry and plastic ties, like futuristic sunglasses someone might wear in a campy 1950s sci-fi flick. All Avegant had was concept art and a line about it looking like a pair of headphones. Now it's far slicker than even the concept art, thought it still looks like a pair of headphones. The cans go over your ears, and the band rests on your nose via an interchangeable nosepad. (User testing revealed that people have crazy-different noses, so the Glyph ships with four options.) It looks silly, like you're wearing a crappy Geordi La Forge costume, but Avegant is betting we'll all be comfortable wearing face-computers soon enough. It's probably right.

Put it on, adjust the two eyepieces to suit your vision, spin the dial on the lens to match your prescription if you have one, then sit back and watch. It's almost boring, in a way—it really is just a screen on your face.

Let me say this one more time: It's not a VR headset. It's not designed to take over your entire field of vision and make you feel like you're somewhere else. Maybe it's meant for travelers who want a better screen than the scratched and smudged slap of polycarbonate on the back of 17D. Rather than build something you play while sitting on your couch, Avegant's trying to build something you can take out into the world.

Whatever you watch in the Glyph takes up the same amount of your view that a 65-inch TV in your living room does, or the huge screen in a theater. But you can look down and see your hands, or up and see the sky. This helps keep your eyes from tiring, and prevents some of the motion-sickness virtual reality has dealt with. It's a personal movie theater, the headphones and screen giving you a private and high-end experience.",Yes,"It makes people feel sick. The Glyph's designers have tried to get around this by making it more like a personal movie theater, with a private and high-end experience, rather than a virtual reality headset.",https://www.wired.com/2015/12/avegant-glyph-is-a-different-kind-of-face-computer/,2015-12-15 14:00:54+00:00,LABEL_1_relevant,0.9908838868141174,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
130,Google's Improbable Deal to Recreate the Real World in VR,"Let a thousand virtual worlds rain down from the clouds. Or rather, the cloud. That's the call from Google as it gets behind a tiny British startup called Improbable.

Founded by two Cambridge graduates and backed by $20 million in funding from the venture capitalists at Andreessen Horowitz, Improbable offers a new way of building virtual worlds, including not just immersive games à la Second Life or World of Warcraft, but also vast digital simulations of real cities, economies, and biological systems. The idea is that these virtual worlds can run in a holistic way across a practically infinite network of computers, so that they can expand to unprecedented sizes and reach new levels of complexity.

So far, the startup has shared its technology with just a handful of coders and companies. But today, Improbable joined forces with Google to offer its creation, called SpatialOS, to anyone who wants it.

You can think of SpatialOS as a cloud computing service for building virtual worlds, whether they run on desktop computers or VR rigs like the Oculus Rift. This service runs atop the Google Cloud Platform, the tech giant's growing cloud computing empire, and the two companies just opened a SpatialOS alpha program that lets coders prototype and test their own virtual worlds. When the beta launches in the first quarter of next year, a separate program will provide coders with free time on Google's cloud as they hone these virtual worlds for release onto the internet at large.

On one level, this partnership allows Google to promote its cloud services as it challenges rivals like Amazon Web Services and Microsoft Azure. In providing of the cloud infrastructure that underpins Pokemon Go, Google has seen the thirst for virtual and augmented reality firsthand, and now, with Improbable, it hopes to push even further into this burgeoning market. But this partnership also points to something bigger down the road: the future of AI.

As developers build more complex virtual worlds, this provides AI researchers with better ways of training the next generation of artificial intelligence. Games have long offered a proving ground for AI, but SpatialOS can help expand this proving ground, providing a way not only for AI agents to learn the successor to Second Life, but to navigate real city streets or even trace the path of contagious disease.

If AI agents set loose in virtual simulations of the real world sounds like Gibsonian science fiction, consider Universe, an AI training ground just recently unveiled by OpenAI, the lab bootstrapped by Tesla CEO Elon Musk and Y Combinator president Sam Altman. Universe is a software platform where researchers can train AI agents to use any application, from games to web browsers to protein folding simulations---anything humans can do on a computer. In theory, you could train agents to navigate any of the beefed-up virtual worlds built with Improbable.

That opens AI research to a few frontier. Game designers Dean Hall (creator of Day Z) and Henrique Olifiers (CEO of Bossa Studios, maker of World Adrift) say Improbable allows massively multiplayer games to achieve unprecedented complexity and scale. And in an effort to understand the impact of autonomous cars, a UK startup called Immense Simulations is using the service to model entire cities. ""We can cover really large geographical areas,"" says CEO Robin North, ""but still keep a high level of detail.""

In the end, such simulations could also provide training grounds for those autonomous cars. Craig Quiter, an engineer at Otto, the robo-vehicle company owned by Uber, is training AI agents on Grand Theft Auto as a stepping stone to more advanced self-driving cars. Swap Grand Theft Auto for a simulation of the city of Manchester, and you get even closer to that goal.

Improbable CEO Herman Narula stresses that today his service is mainly a way of building games. But he too sees it as a path to better AI, hinting that his company is already working with others toward this goal. If a thousand virtual worlds take shape, so too can a thousand AIs.",Yes,"It creates a new environment for AI agents to learn and grow. By providing a way for AI agents to navigate virtual worlds, we can expand their training ground and allow them to learn new skills.",https://www.wired.com/2016/12/googles-improbable-deal-recreate-real-world-vr/,2016-12-13 14:00:00+00:00,LABEL_1_relevant,0.9953573346138,Virtual Reality,virtual_reality,Wired,User Experience & Entertainment
