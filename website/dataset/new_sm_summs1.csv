index,title,text,answer,gpt_summary,url,date,prediction,score,sector,sector_class,magazine,label
0,"To Survive, Byte Needs to Win Over Creators Where Vine Failed","When Twitter suddenly announced it was shutting down Vine in 2016, fans immediately started campaigning to bring the beloved short-form video app back. Dom Hofmann, one of the original cofounders, has been working on doing just that for years. After a few stops and starts, his new app Byte finally launched last week. It already has 1.3 million downloads, according to Sensor Tower. It also appears to have learned some lessons from Vine’s demise, but that won’t necessarily be enough for another social video app to survive in 2020.

Vine’s six-second format was a novelty when it came out in 2012, and the limitation birthed a unique, off-the-wall style of comedy. The app incubated some of the internet’s biggest celebrities, including singer Shawn Mendes, comedian Brittany Furlan, and Logan and Jake Paul. But as obituaries for the service noted, it didn’t do enough to support its rising stars and was often criticized for overlooking creators of color. Many eventually left for platforms where they could create longer content and make more money, like YouTube and Instagram.

This time around, Hofmann and his team are trying to do things differently. Byte’s tagline is “creativity first,” and the company appears to be prioritizing creators from the start. There's a dedicated forum where they can discuss issues with one another and Byte staff, creating a more personal feel than larger platforms. Hofmann told TechCrunch he plans to give creators a cut of profits, though the exact details on how the program will work haven’t been released. Byte did not respond to a request for comment from WIRED.

Content This content can also be viewed on the site it originates from.

A lot has changed since Vine died. Instagram Stories exploded in popularity, and TikTok has entered the scene. The Chinese-owned app has been downloaded over 1 billion times, propelled by a powerful algorithm that makes it easy to lose hours to the app. Like Vine before it, TikTok’s features have given rise to new meme formats, uber-viral stars, and even Los Angeles mansions filled with creators. People might still miss Vine, but some see the six-second video as a format whose time has mostly passed.

“It was so interesting because it was so new and different. They created this whole new type of content that’s been expanded and has evolved,” says James Nord, the CEO of Fohr, an influencer marketing platform. “I don’t know that nostalgia on the internet is a business plan.”

It’s unclear what a long-term business plan for Byte might look like. Unlike YouTube and Instagram, the platform is mainly a place for comedy sketches and stunts. That’s not the type of content most brands are comfortable working with creators to produce, says Nord. Brands have a hard time being genuinely funny, and it’s not exactly easy for a creator to give an authentic product testimonial in six seconds.

LEARN MORE The WIRED Guide to Influencers

Beyond making money, creators will need to work with Byte’s stripped-down design. Unlike TikTok, Byte doesn’t have filters or elaborate in-app editing tools, and you can’t add music clips to videos. If you’re used to watching TikTok, the lack of pre-programmed sounds on Byte can seem, well, quiet. Bytes also often feel super short. Brevity is what made Vine unique but could limit the scope of content on Byte. A few seconds is probably not long enough to film a cooking or makeup tutorial—two genres that have flourished on TikTok.

Not every creator is dissuaded by those limitations. “We explore every platform to see if it'll be a right fit for our brand,” says Jacob Pace, the CEO of Flighthouse, a media company with more than 22 million followers on TikTok. Byte “might have a space since it's different (though shorter) than other short-form mobile social media apps.”

So far, Byte’s users have begun populating the platform with the same breed of weird humor that made Vine famous. A clip edited to make Kanye West look like he's FaceTiming with a hamster. A girl crashing into a bush while riding in a Target shopping cart. Shrek vibing to rap music. It’s hard to say anything definitive about a new platform, especially when content may be individualized, but a cursory look at Byte’s Popular section suggests many users are older than Generation Z.",Yes,"People stop interacting with the physical world around them. Instead, they become focused on the virtual world of social media and the internet. This can lead to a number of problems, such as:

-People becoming more isolated and lonely

-People becoming addicted to social media and the internet

-People losing touch with reality and becoming",https://www.wired.com/story/byte-needs-to-win-over-creators-vine/,2020-01-30 17:44:02.637000+00:00,LABEL_1_relevant,0.9950388073921204,Social Media,social_media,Wired,Health & Wellbeing
1,How the Internet Gave All of Us Superpowers,"Somewhere around 1997 I was filling my car with gas, and I spotted a web address on a small orange sticker on the pump. It was an ad of some kind; I don’t remember what for, but I do remember having a realization: The mass adoption of the internet was real. Against all odds, a disturbance in the force had unleashed an entirely new culture outside the established channels, and now commerce was flocking to where the action was. But the main event had hardly started.

The main event—the great cascade that was unthinkable before it became obvious—was what you might call the rise of the bottom. In the late ’90s and early 2000s, masses of people who had previously been ignored as inert, dumb, or passive were awakened by a new level of power. Members of the once dormant TV audience now created and shared their own shows online. The lowly readers of magazines and newspapers wrote their own news in the form of blogs, by the millions, on nearly every subject, all free. In 1999, Napster launched the first music-sharing site. Now music fans—not executives or critics or agents, but ordinary fans—could broadcast their favorite tunes to each other, sharing what they loved, song by song, ditching packaged albums. Oh, and the music was free.

October 2018. Subscribe to WIRED. Plunkett + Kuhr Designers

This bottom-up, peer-to-peer, noncommercial distribution system exploded in popularity and shocked the established players. The ancient business model of paying for a copy was dying. And if music was the first fatality in this revolution, other media businesses felt they would be murdered next. The powers that be promptly and fiercely shut Napster down. That, of course, did not stop the rising tide.

In 2001, Wikipedia erupted. It was the dumbest idea in history—that unknown amateurs anywhere in the world could write a reliable and useful encyclopedia with little supervision. Everyone knew that only world-class experts overseen by ruthless editors could do that. But Wikipedia steadily swelled in size and depth and quality.

Retail commerce, too, was rapidly being overrun by the same bottom-­uprising. In 1997 a San Jose-based site called AuctionWeb sold its millionth item and renamed itself eBay. In 1999, Alibaba launched in China, close to the epicenter of manufacturing. Small-time “factories”—some just one room with a machine—sold direct to citizens around the world. In 2000, Craigslist—once just Craig Newmark’s emailed list of events around San Francisco—expanded its free classifieds, and within a few years it was in 32 cities in North America. By 2001, eBay was reportedly hosting 200 million auctions a year. Then it bought PayPal, a secure, ­person-to-person virtual payment system. Within eBay, just about anyone could sell anything, globally. Manufacturers could sell directly to customers, bypassing traditional retailers.

The rise of the small also created entirely new kinds of giants. Google, born in 1998, was not the first search engine, but it was the first major search engine to rank results based on the number and quality of links to pages. Each time someone linked to another site, they were making ­Google better, fueling its rise to dominance. Although we have forgotten it, the first large-scale social network to exploit people’s connections to each other was Friendster, launched in 2002. LinkedIn, in 2003, found a way to create a business out of networking regular workers together. YouTube would launch a couple years later and demonstrate, once and for all, that the most powerful force that has been released by tiny chips is not speedy compute cycles but the creations of ordinary people.

There were setbacks of course—for a time, the tech boom stalled under its own acceleration, and 9/11 threw the nation into crisis—but, curiously, the dotcom bust didn’t affect the steady growth of the digital masses. The astounding number of new people jumping online kept increasing. Moore’s law continued. Computers got more powerful, bandwidth kept expanding. When it came to the internet, there was a long boom.

The grand migration to bits between 1998 and 2003 delivered many other bright, shiny gifts. During these years we got camera phones, early podcasts, the full human genome sequence, tablet computers, reliable in-car GPS, MP3 players, and home Wi-Fi. Yet none of these superpowers, cool as they may be, can match the superpower of humans connected to one another in real time. For the first time on our planet, hundreds of millions of minds were linked together in thousands of novel ways, each arrangement yielding potential benefit.",Yes,"It has given a voice to previously underrepresented groups of people. In the past, mass media was controlled by a small number of people, who decided what stories were told and how they were told. This resulted in a relatively one-sided view of the world. Social media has changed that by giving everyone a platform to share their stories. This",https://www.wired.com/story/wired25-kevin-kelly-internet-superpowers-great-upwelling/,2018-09-18 10:00:00+00:00,LABEL_1_relevant,0.986543834209442,Social Media,social_media,Wired,"Information, Discourse & Governance"
2,What’s Your Data Worth? Senators Want Facebook to Tell You,"In these days of anti-tech ire, it’s a popular cocktail hour topic: How much is Facebook making off my data? Last year, I spent a month trying to find out, hawking my personal data on blockchain-based marketplaces. I came away with $0.003. On Monday, when Senator Mark Warner (D-Virginia) announced a proposal to force tech companies to tell users the value of their data, he was slightly more generous, ballparking the average at $5 a month.

Gregory Barber covers cryptocurrency, blockchain, and artificial intelligence for WIRED.

In truth, it’s probably only Facebook or Google (and their advertisers) who could hazard a good guess. “The cards are really stacked against us,” says David Carroll, a professor of media design at the New School known for his extensive quest to reclaim his Cambridge Analytica data.

Policymakers wrestling with the value of an individual’s data face a fundamental problem: While data often gets compared to oil, there’s no equivalent to benchmark crude for bits and bytes. The value is dependent on context: who has the data, where it’s going, who it came from. Beyond Warner’s bill, California Governor Gavin Newsom has mused about a “data dividend” through which residents would be paid for the data they generate for tech companies. Newsom acknowledged that it’s unclear what that check would look like.

Warner’s proposal, introduced with Senator Josh Hawley (R-Missouri), would require companies with more than 100 million monthly users to disclose the types of data they collect about each user and to assess its value. It also includes privacy rules similar to those passed last year in California, where you will soon be able to request to delete all or some data held by others, and opt out from having it sold. The Senate bill directs the Securities and Exchange Commission to figure out how companies should calculate that value, with flexibility for different business models and ways data is used. Best of luck to them.

“There’s a little bit of a political scramble to get these ideas out there and it’s going ahead of anyone having an approach that makes sense,” says Glen Weyl, a senior researcher at Microsoft.

The tech companies offer hints about our value, buried in financial reports. In the first quarter of 2019, for example, Facebook says it generated about $2 in revenue per month from advertising for each user. That’s the global figure. In the US, it’s just under $10. (Lucky us!)

But the trick is figuring out what your data is worth to a given platform. That could be more or less, depending on things like how useful you are to the company’s ad-targeting algorithms and how valuable you are to the advertisers themselves. “Any attempts are going to be an artificial average or need to be sensitive to how the market is constantly valuing data,” says Carroll. The harsh reality is that you and I probably aren’t worth very much at all to Facebook or Twitter, at least not alone.

Weyl, who has consulted with California lawmakers on the data dividend idea, suggests that banding together might help improve our lot. One method could involve mediators to help us negotiate collectively, he says---perhaps a kind of data union that could revoke access to our data if it can’t strike a good deal, or if our privacy is violated. Weyl says people can’t individually value their data and the labor that goes into producing it. If the company names a price, we don’t have the know-how (or the will, given how small the individual amounts will likely be) to argue otherwise. I learned that the hard way while acting as my own data broker.

""The cards are really stacked against us.” David Carroll, the New School

We’re all in this together, in other words. That’s all the more true given how data is used for targeting ads: It’s aggregated, mixed and matched, joined with data from elsewhere, repurposed in models used for artificial intelligence. In those cases, Warner leaves open the idea of “joint ownership” between the user and company---suggesting that only some of the value derived from your data is considered yours. That gets complicated, fast. “When data is transferred into models, it becomes an abstraction of your data,” says Carroll. Say there’s a medical trial based on health data from your phone. If you’re perfectly healthy or represent a common demographic for a medical trial, your data probably isn’t worth much to researchers. Should sick people get paid more for signing up? There are experimental ways of figuring out the value of one person’s data to a particular model, but they’re complicated to calculate, and have to be calculated independently for each model.

Perhaps, in the end, the answer will be that simpler is better. Any hard figure to put on our individual data would be an important step forward, says Ashkan Soltani, a former chief technologist of the Federal Trade Commission, especially when it comes to holding companies accountable. “We have this constant issue in privacy that when there’s a data breach, companies argue that there’s no harm because information doesn’t necessarily have any value,” he says. “This [bill] would force companies to articulate that value.” One method might involve developing a value based on real-time bids for our attention from advertisers.",Yes.,It has created a new way of monetizing users by selling access to their attention to the highest bidder.,https://www.wired.com/story/senators-want-facebook-price-data-possible/,2019-06-26 11:00:00+00:00,LABEL_1_relevant,0.9954832792282104,Social Media,social_media,Wired,Economy
3,Facebook grilled in Senate hearing over teen mental health,"Last night, Facebook published two annotated slide decks in an attempt to contextualize the documents that The Wall Street Journal published this month, which reported evidence that the company is aware of its negative impact on teen mental health. These documents were released in anticipation of today’s Senate hearing on the mental health harms of Facebook and Instagram.

The Senate Committee on Commerce, Science, & Transportation questioned Facebook Global Head of Security Antigone Davis over two and a half hours, but lawmakers grew frustrated with Davis’ reticence to answer their questions directly, or provide much information that hasn’t been written in Facebook blog posts rebuking the WSJ reports.

Davis insisted that research from Facebook and Instagram has shown eight out of 10 young people say they have a neutral positive experience on the app, and that her team wants 10 out of 10 young users to have a good experience. But Senators pushed back with other findings from Facebook’s own data, like the fact that among teenagers with suicidal thoughts, 13% of British users and 6% of American users said they could trace those thoughts to Instagram. Senator Richard Blumenthal (who serves as Chair of the Subcommittee on Consumer Protection, Product Safety, and Data Security) said that his office did their own research by creating an account pretending to be a 13-year-old girl. Senator Blumenthal said they followed “easily findable accounts associated with extreme dieting and eating disorders.” Within a day, he said, the account’s recommendations were solely composed of accounts promoting self-harm and disordered eating.

“I congratulate you on a perfectly curated background,” Tennessee Senator Marsha Blackburn chided Davis. “It looks beautiful coming across the screen. I wish the messages that you were giving us were equally as attractive.”

“That is the perfect storm that Instagram has fostered and created. Facebook has asked us to trust it. But after these evasions and these revelations, why should we?” Senator Blumenthal asked.

But in the midst of filibustering tactics that fit right in on the Senate floor (“We’re pretty good at filibustering in the Senate, too,” Senator Klobuchar told Davis), the Facebook Global Head of Safety did elaborate on some of the company’s plans to improve young users’ experience, which Head of Instagram Adam Mosseri previously mentioned on Twitter.

“Young people indicated that when they saw uplifting content or inspiring content, that could move them away from some other issues that they’re struggling with,” Davis said at the hearing. “So one of the things that we’re actually looking at is called ‘nudges,’ where we would actually nudge someone who we saw potentially rabbit-holing down content towards more uplifting or inspiring content.”

In addition to a “nudges” feature, Davis said that the company is looking at a “take a break” feature, which would encourage users to stop looking at the app if they’ve been browsing certain content for too long. In 2018, Instagram introduced a “you’re all caught up” notice, which would appear when the user had scrolled through all posts from the last two days. This feature was introduced alongside “do not disturb” toggles, which helped users control when they wanted to receive notifications. These updates were part of “Time Well Spent” initiatives, designed to curb screen time and encourage healthier social media habits. But by 2020, the space beneath the “caught up” notice was turned into a feed of suggested posts and ads.

Big Tech continues to blatantly prioritize raking in revenue over protecting children and teens, and that must stop. We know that these companies won’t change their ways unless Congress forces them to. That’s why I am re-introducing the KIDS Act. pic.twitter.com/8rgZCevj15 — Ed Markey (@SenMarkey) September 30, 2021

At the hearing, Massachusetts Senator Ed Markey (a social media star in his own right) announced that he would reintroduce legislation with Senator Blumenthal called the KIDS (Kids Internet Design and Safety) Act, which seeks to create new protections for online users under 16. The bill would prohibit platforms directed at children from leveraging follower and like counts, push alerts that encourage users to use the app more, auto-play settings, badges that award elevated levels of engagement, or any design feature that unfairly encourages a user (“due to their age or inexperience,” the bill specifies) to make purchases, submit content, or spend more time on a platform.

Previously introduced in March 2020, Facebook has known about the proposed legislation for almost a year and a half.

“I think our company has made its position really well known that we believe it’s time for the update of internet regulations, and we’d be happy to talk to and work with you on that,” Davis told Senator Markey.

But when Markey directly asked if Facebook would support the KIDS Act, Davis said that Facebook would follow up on the question later.

“Well, your company has had this legislation in your possession for months. And you’re testifying here today before the committee that would have to pass this legislation,” said Senator Markey. “I just feel that delay and obfuscation is the legislative strategy of Facebook, especially since Facebook has spent billions of dollars on a marketing campaign calling on Congress to pass internet regulations, and Facebook purports to be committed to children’s well being.”

At the end of the hearing, Davis said that she hopes the Senate will have hearings with companies that have kid-focused apps, like TikTok and YouTube. Currently, Facebook has a Messenger Kids app, but the company but its Instagram for kids product on hold in light of WSJ’s reporting. Though WSJ has published six leaked documents from Facebook, the company itself only annotated and re-published two of them.",Yes,"It’s created a generation of people who are really good at talking about themselves, but not very good at listening to others.

In other words, social media has created a generation of people who are really good at talking about themselves, but not very good at listening to others.",https://techcrunch.com/2021/09/30/facebook-grilled-in-senate-hearing-over-teen-mental-health/,2021-09-30 00:00:00,LABEL_1_relevant,0.9963691234588624,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
4,Jack Dorsey on ProPublica's Experimental Journalism,"WIRED ICON

Jack Dorsey, cofounder of Twitter

NOMINATES

ProPublica, investigative journalism nonprofit

October 2018. Subscribe to WIRED. Plunkett + Kuhr Designers

I discovered ProPublica about two years ago and became interested in it right away because of its mission: To expose abuses of power and betrayals of the public trust by government, business, and other institutions, using the moral force of investigative journalism. It’s independent, it’s nonpartisan, and it’s powered mainly by philanthropy. ProPublica actually measures success by whether it sparks real-world change. This was a new take on a business model that I hadn’t really thought a lot about.

The most recent ProPublica story that stood out to me was the publication of the audiotape of immigrant children being held in a US Customs and Border Protection facility. There have also been a number of stories on wrongfully convicted folks that have prompted cases to be reopened.

Tweet by @jack on 8/8/18:

“We certainly have a lot of work to do to help journalists do their jobs better. That’s our intent. Help with reach, balance, and economic incentives. We’re behind on those last two.”

I’ve watched how they use Twitter’s technology. Twitter’s superpower is conversation; it’s carrying the chatter. So traditionally, journalists write a few characters and tweet a link to their article and that’s it. But ProPublica threads the key parts of an article, so you end up with a thread that’s 10 tweets long. We asked them why they do that and they said, “We’re going to meet people where they are. They’re coming to a service that is focused on brevity, so we need to translate our stories into that format.”

It’s a creative use of a technology that doesn’t intuitively seem to fit with traditional journalism. That’s definitely more of a risk if your business is based on advertising and sending people to your site. But ProPublica has been doing this for over 10 years. And I think there’s a lot for us to learn in terms of what that means for media. A donation-driven news outlet may seem extreme, but it’s important right now to experiment. This is an experiment that’s done quite well. —As told to Lauren Goode

This article appears in the October issue. Subscribe now.

MORE FROM WIRED@25: 2008-2013

Join us for a four-day celebration of our anniversary in San Francisco, October 12–15. From a robot petting zoo to provocative onstage conversations, you won't want to miss it. More information at www.Wired.com/25.",Yes,"It has 

negatively impacted our ability to interact with each other in person.",https://www.wired.com/story/wired25-jack-dorsey-propublica-experimental-journalism/,2018-09-18 10:00:00+00:00,LABEL_1_relevant,0.9775820374488832,Social Media,social_media,Wired,"Information, Discourse & Governance"
5,"Targeting Black Americans, Russia's IRA Exploited Racial Wounds","Two days before the 2016 presidential election, @woke_blacks posted an anti-voting polemic to its Instagram account. “The excuse that a lost Black vote for Hillary is a Trump win is bs. Should you decide to sit-out the election, well done for the boycott,” the caption read. “I remind us all one more time, anyone who wins can literally change less about the state of Black people, we are on our own, esp. after Obama. Wise up my people!” Another user, @afrokingdom_, shared a comparable sentiment: “Black people are smart enough to understand that Hillary doesn’t deserve our votes! DON’T VOTE!”

According to a new report commissioned for the Senate Intelligence Committee by cybersecurity firm New Knowledge, those accounts, along with dozens more, were part of an extensive and complex campaign to suppress the black American vote by the Russian firm Internet Research Agency.

In late 2017, it was first reported by CNN that the pro-Kremlin enterprise masterminded a plot to infiltrate online communities of the Black Lives Matter movement. But New Knowledge’s report, released Monday, shows a much more sustained and purposeful focus on black Americans—as the IRA went about instigating mistrust in law enforcement and political institutions, while cultivating seemingly authentic narratives of black pride.

The report details how black Americans were among the most exploited online communities by the IRA, cataloging how the Russian firm developed an “expansive cross-platform media mirage” that specifically targeted black people by leveraging popular social media sites. The campaign was “designed to exploit societal fractures” and “erode our trust in media entities and the information environment, in government, in each other, and in democracy itself,” the report states. “This campaign pursued all of those objectives with innovative skill, scope, and precision.”

The 100-page report is based on data sets provided to the Senate by Facebook, Twitter, and Alphabet, comprising tens of thousands of posts between 2015 and 2017. In addition to social media sites, the IRA’s vast and varied disinformation campaign played out across issue-specific domains that they bought such as blackmattersus.com, black4black.info, and blacksoul.us—“a complex effort” to manipulate public opinion with the intention of siphoning votes from Hillary Clinton to help elect Donald Trump. The black community, often a reliably strong voting bloc for Democrats, was a substantial target.

The IRA went about instigating mistrust in law enforcement and political institutions, while cultivating seemingly authentic narratives of black pride.

Spread among Facebook pages, Instagram accounts, Twitter accounts, and YouTube channels, the IRA’s tactics ranged from ad targeting, meme warfare, video manipulation, and the creation of sham account profiles. As my WIRED colleagues point out, much of this content was designed to encourage a lack of faith among black Americans in democratic institutions and decelerate black turnout for Clinton. “As the election became imminent,” the New Knowledge report says, “those themes were then tied into several varieties of voter suppression narratives: don’t vote, stay home, this country is not for Black people, these candidates don’t care about Black people.” (A second report, also produced for the Senate, by Oxford University’s Computational Propaganda Project and Graphika comes to similar conclusions about the IRA’s work, according to the Washington Post.)

One of the more fascinating bits from the New Knowledge report was YouTube’s role as an influence platform. Of the 1,107 videos linked to the IRA, the majority of the content—96 percent, according to researchers—centered on Black Lives Matter and police brutality. A video by Williams & Kalvin, titled “The truth about elections,” is cited as one of the more “overt suppression narratives” employed by an IRA-affiliated entity. In another instance, the IRA co-opted a human-interest story about “an inspirational young Black American who made the national news for a device he invented.” The IRA accounts repurposed that story across its web of counterfeit accounts with messages meant to cultivate distrust in American society: “These are stories of Black children the media don’t want you to see” and “White people invent tools for killing, this Black child is inventing a tool for saving lives.”

Of the Russian firm’s race-specific domains, Blackmatters.us was perhaps the most effective in amplifying the IRA’s intended media mirage. It essentially operated like a digital marketing company as it built an online presence across multiple platforms, creating various accounts to reinforce its brand and distribute content wholesale. Their strategy took on real-world resonance in other instances, as well, as they instituted recruitment efforts. “Posts encouraging Americans to perform various types of tasks for IRA handlers appeared in Black, Left, and Right-targeted groups,” the report states, noting that such efforts had the biggest presence in black communities. “Many posts solicited protestors, writers, activists, lawyers, and photographers to attend the property’s numerous events. They posted job ads for real American writers to create content … a clear example supporting the hypothesis that the IRA engaged in narrative laundering.”",Yes,"It creates a filter bubble. People tend to seek out information that conforms to their beliefs and to avoid information that challenges their beliefs. This leads to the formation of echo chambers, where people only hear their own perspectives and become increasingly polarized.",https://www.wired.com/story/russia-ira-target-black-americans/,2018-12-17 18:25:18.261000+00:00,LABEL_1_relevant,0.9966660141944884,Social Media,social_media,Wired,"Information, Discourse & Governance"
6,"Talenthouse flotation finally reveals a $40M exit for EyeEm, Europe's almost-Instagram","Talenthouse AG has made a name for itself as an aggregator of content creators, which is then commissioned by brands for their own social media channels. Brands just can’t seem to come up with the same kind of authenticity, so they farm it out in this manner. But business is going well, so Talenthouse is now listing on the SIX Swiss Exchange (ticker THAG) in response to this demand for content creation and ability to tap into the ‘creator economy.’

The move is significant in tech startup terms because in June last year New Value AG (trading as Talenthouse) acquired long-time social photos startup EyeEm. When it launched in 2011, only a year after Instagram, it was often talked of in the same breath, but the EyeEm founders studiously repeated that it was a place for for high-end content creators and photographers to sell their wares.

By the time it had been acquired, EyeEm had raised a total of $24 million in VC backing. No price was released at the time for the ‘mostly shares, some cash’ deal.

But the public markets flotation of Talenthouse reveals some interesting data.

Roman Scharf, Co-Founder of Talenthouse told me: “The initial shares will trade around one Swiss franc, and we have 400 million shares outstanding. So the market cap tomorrow will start around 400 million Swiss francs. We paid 37,348,490 million shares for Eyeem plus some cash. Those shares at current share price are $37.3 million. But their exit was roughly $40 million, since they also got cash.”

Talenthouse, with its operational headquarters in London, has 14 million members across brands including EyeEm, Ello, Zooppa and Jovoto. Members of these brands produce content that can be acquired or commissioned by companies such as PayPal, Netflix and Nike.

The Creator Economy generates $2,250 billion annually, employing 30 million people worldwide, according to UNESCO. For instance, in a commission for the UN, Talenthouse took in 16,700 submissions from 142 countries, for a campaign around the Covid-19 pandemic.

In a statement, Clare McKeeve, CEO of Talenthouse said: “We’ve acquired and developed brilliant companies within our portfolio so that our creatives have the tools to be part of an active community whilst successfully monetizing their skills.”

Speaking to me over a call Scharf added: “The SIX market floatation is like a sandbox environment where we learn how to behave as a public company. And then we’re going to go for a NASDAQ listing, because you know Tanenhaus really is about the creative economy. And in the German-speaking world, investors don’t really understand what the creative economy is. They have no clue about Tik Tok.”

Eyeem was started by Florian Meissner, Ramzi Rizk, Gen Sadakane and Lorenz Aschoff in 2011. Photographers could offer their pictures for sale via the portal and Eyeem retained part of the income. But the end, the four founders each held 1.7 percent of their company.

Meissner and Rizk have since moved on to work on the app Aware, an analysis service for health data, and also become Angel investors, taking early stakes in the Gorillas delivery service.",Yes,"Brands are now outsourcing their content creation to the ‘creator economy’, which is being facilitated by Talenthouse. And Talenthouse is now public, via a listing on the SIX Swiss Exchange.",https://techcrunch.com/2022/03/28/talenthouse-flotation-finally-reveals-a-40m-exit-for-eyeem-europes-almost-instagram/,2022-03-28 00:00:00,LABEL_1_relevant,0.9923729300498962,Social Media,social_media,TechCrunch,Economy
7,Millions of people rely on Facebook to get online. The outage left them stranded.,"But in 2016, the program (by now renamed Free Basics) was banned by India’s Telecom Regulatory Authority, which claimed that it violated net neutrality. Despite that setback, it has continued to roll out, with less fanfare, to other countries in the developing world. In 2018, Facebook said Internet.org had put 100 million people online. In 2019, FreeBasics was available in 65 countries, around 30 of them in Africa. Last year, the firm began rolling out Facebook Discover, which allows internet users to access low-bandwidth traffic to all websites (not just Facebook properties) even if they’ve run out of data.

Versions of these programs also exist in Afghanistan, where many new internet users equate Facebook, Facebook Messenger, and WhatsApp with the whole internet. Even among those who have broader access to the full web, Facebook’s suite of products play a vital role. WhatsApp calls, for example, have long since replaced more expensive—and less secure—phone calls globally. Around the world, many small businesses rely on Facebook’s tools to sell and advertise their products.

All of this means that even temporary outages have a big effect, both for advocacy organizations, like the ad hoc groups helping Afghans escape the country, and the vulnerable individuals that are already isolated, like the Afghans in hiding, afraid of Taliban retribution, and waiting for news—often via Whatsapp—for updates.

They ""are already incredibly fatigued and anxious. Losing connection with each other and with trusted allies in the outside world is… devastating"" says Ruchi Kumar, an Istanbul-based Indian journalist (and MIT Technology Review contributor) who is also involved in Afghan evacuation efforts. ""A number are on the verge of suicide, given the deaths and violence they've been witnessing this past month."" The unexplained outage of their primary channel of communication with the outside world compounded the despair, uncertainty, and feelings of abandonment. Losing a chance for evacuation, meanwhile ""is literally life or death.""

It was past midnight for Kumar and Bezhan when Facebook began coming back to life, but even then, some of its functionalities, including search and notifications, were not yet available. Bezhan hadn’t heard back yet about whether she could add that additional name for evacuation.

But she was also concerned that her Afghan friends might be jumping to conclusions about what caused the outage. For weeks since the fall of Kabul, there had been rumors that the Taliban had cut access to the internet. “I bet they are creating rumors and coming up with stories about how the new government is blocking the media,” she says.

They wouldn’t be alone. Responding to similar concerns, a spokesperson for the Democratic Republic of Congo’s ministry of communication—a country known for government-induced Internet shutdowns— took to Twitter to set the record straight: “The internet connection has not been cut,” he wrote at 4:05 p.m. ET. “It is a global blackout crippling WhatsApp, Facebook, and Instagram. Other applications like Twitter are functioning normally. The same goes for the rest of the web.”

This story has been updated with details from Kumar about the impact of the outage.",Yes,"

the outage of platforms like Facebook and WhatsApp can have a devastating effect on groups and individuals who rely on them for communication and coordination, especially in times of crisis.",https://www.technologyreview.com/2021/10/05/1036479/facebook-global-outage/,2021-10-05 00:00:00,LABEL_1_relevant,0.9955958724021912,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
8,Social networks are broken. This man wants to fix them.,"At home, Zuckerman can work while walking on a treadmill with his laptop on a small shelf above it. A hot tub is in the room, too, in case he wants to take a dip.

Zuckerman spoke with MIT Technology Review about how social media started controlling us rather than the other way around.

How are people using technology—rather than, say, lobbying for laws to be passed—to force change in new ways?

We used to make change mostly using law as our primary lever. Now we use the legal lever less; we use the levers of norms of markets and technology more often. #MeToo is an example of a norms-based campaign. It’s basically saying, “We’re going to challenge how people talk about sexual assault and sexual harassment.” And once we change that norm, there’s other legal pieces, market pieces, that’ll come into play. But at its heart it’s trying to change how we have certain conversations.

The point in all of these is that if you can’t get social change done through the traditional model of civics, there is a whole new set of tools, and people are starting to learn how to use these things.

But social networks like Facebook and Twitter control, or at least direct, the information we see by using algorithms to filter what we see in our feed. You worked with two colleagues—Chelsea Barabas of MIT’s Center for Civic Media and Neha Narula at the Media Lab—to build a tool called Gobo that lets people aggregate and filter their feeds on their own. Why?

What this is meant to do is to say, “Look, it’s really a mistake to give one or two companies control over our digital public sphere.” Instead, we need competing platforms. We’re trying to make the case that you want those different social networks because you want more control over your filters about what you see and what you don’t see.

If we need competing platforms, we need tools that would let us use those competing platforms. Gobo is one of those tools. Gobo is an aggregator. It aggregates Twitter and the “aggregateable” parts of Facebook—the public pages.

“Because Facebook’s so friggin’ big, they get all sorts of advantages that make it very hard to catch up with them … So when someone shows up as a meaningful competitor, [Facebook is] more likely to buy them and eat them up than they are to actually have to fight them in the marketplace.”

So first we built the aggregator. And then we built the algorithms [that determine which posts you’ll see]. And rather than making them a top-secret black box, we made it an open box where you can reach in and set the sliders and experiment and say, “Oh, I like how this works. Now let me change it this way and see if it works better for me.”

Where we want to get in the longer term is even more of an open box; we built Gobo so that other people can write filters for it.

After a lot of criticism related to the ways its news feed filters content, Facebook has started pushing posts from users’ friends and family more and deemphasizing ones from brands. Do you feel this move shows Facebook is actually starting to shift its focus?

I don’t believe that this is changing yet, and I won’t believe it until I see a credible business model based on something other than targeted advertising.

I think that building an internet where we didn’t have to pay for anything, because our attention was going to be the commodity that was traded, is one of the most destructive and shortsighted decisions that we could have made. And I do mean ��we,” because I was very much part of that. Until I see Facebook saying, “Look, you’re going to use this as a service and you’re going to pay us for the service,” as opposed to “We’re going to capture your attention and repackage and sell it,” I won’t believe it.

A growing chorus of former Facebook executives and investors have been speaking out against Facebook—saying, for example, that social media is “ripping apart the fabric of how society works.”

I think what’s happening is that some of these people who are stepping out of the really intense “I’ve been in the process of building it” are starting to look at it from the outside and say, “Oh, wow, okay; now I can see the politics from the outside, and I’m not thrilled about what I’ve been associated with.”

We need to figure out how to have those conversations a lot earlier. We should be having those conversations with people who are working at these companies and who are making these design decisions. I want to be having those conversations with my students, because my students are often going to these companies and often find themselves with the opportunity to make those design ­decisions.

Why is it so hard for anyone who’s not Facebook, Instagram (which is owned by Facebook), Twitter, or Snapchat to compete in this social sphere?

Network effects basically say, “I gotta be on Facebook ’cause everybody I know is on Facebook.” Because Facebook’s so friggin’ big, they get all sorts of advantages that make it very hard to catch up with them. They get more bandwidth, they get cheaper servers.

So when someone shows up as a meaningful competitor, [Facebook is] more likely to buy them and eat them up than they are to actually have to fight them in the marketplace.

You wrote a piece in the Atlantic that suggested a publicly supported social network as a potential solution to social media’s echo-chamber effect. Could this actually happen?

I think it’s wholly unrealistic in the United States. It’s something that could be realistic in Europe, [where] you have a public media culture that accepts the idea that you might want to invest money in people having some basic knowledge about politics, the world, the people around them. I could imagine an innovative European public broadcaster saying, “Maybe we build a social network that’s compatible with other social networks, has algorithms designed to help you tune whether you’re getting news about the world, news about your community, and makes those levers visible and controllable.”",Yes,"It started controlling us rather than the other way around. We used to make change mostly using law as our primary lever. Now we use the legal lever less; we use the levers of norms of markets and technology more often. Gobo is an example of a tool that is meant to say, “Look, it’s really a",https://www.technologyreview.com/2018/02/09/3406/social-networks-are-broken-this-man-wants-to-fix-them/,2018-02-09 00:00:00,LABEL_1_relevant,0.9959391355514526,Social Media,social_media,MIT Tech Review,Security & Privacy
9,Facebook's New Privacy Controls Are Long Overdue,"Throughout its 16 years of existence, Facebook has struggled to provide the privacy controls users really want and need to safeguard the data they post to the platform. It's been a challenging project, with plenty of major detours along the way. But today Facebook is announcing a new tool for managing your posts. It may be the most intuitive version of the controls yet, because it's basically what Gmail has offered for email since the year Facebook launched.

The new feature, known as Manage Activity will create the concept of an ""archive"" on Facebook, allowing you to move any or all of your past posts to a secret new home that only you can see. Manage Activity will also introduce a Facebook Trash folder so you can delete posts more easily. The new feature is rolling out in Facebook's mobile apps first.

""Whether you're entering the job market after college or moving on from an old relationship, we know things change in people’s lives, and we want to make it easy for you to curate your presence on Facebook,"" the company says in a blog post.

Screenshot: Facebook

Archive and Trash are two concepts most people are familiar with from email. As with Gmail, posts you move to the trash will stay there for 30 days and then be deleted unless you manually eliminate them sooner or reinstate them. And Manage Activity is introducing batch actions for multiple posts at once so you can view and organize in bulk, rather than going post by post. You can filter by date ranges, types of posts (Photos and Videos, Posts From Other Apps, etc.), specific people, and other categories, and then select individual posts, batch-archive, or batch-delete as needed.

If you archive or delete a post that people are tagged in they’ll lose access to the post. Facebook says they won't be notified of the change, though, so you can archive stealthily. You can't archive other people's posts that you are tagged in—this is only for content you've shared.

Facebook already offers a Select Privacy menu for each of your posts where you can choose who can view the content; your options include Public, Friends, Specific friends, and so on. In some ways, archiving a post will be similar to choosing Only Me, in the sense that only you will be able to view the post and no one else. But if you've been using Only Me as a makeshift archiving feature, Manage Activity isn't going to help put everything in one place. All archived posts will show up in one place, but Facebook says there isn’t a way to view the collection of posts you've marked Only Me.

After dedicating 2018 to figuring out how to ""fix"" Facebook, founder and CEO Mark Zuckerberg published ""A Privacy-Focused Vision for Social Networking"" in March 2019. In it he discussed a problem that every Facebook user has likely been aware of for more than a decade.",Yes,"It blurs the line between public and private, and instead encourages users to share and connect in a way that is more public than private. The result is a platform that is built on top of a very public database of information, which can lead to some very personal data being shared without the user's consent or knowledge.

To address this",https://www.wired.com/story/facebook-manage-activity-privacy-settings/,2020-06-02 14:00:00+00:00,LABEL_1_relevant,0.9456381797790528,Social Media,social_media,Wired,Security & Privacy
10,How DARPA Took On the Twitter Bot Menace with One Hand Behind Its Back,"For instance, the group calling itself Islamic State uses online social media to persuade young people to embrace their cause. Some observers believe Russia embarked on a major social media disinformation campaign the annexation of Crimea. Others say bots played a significant role in influencing the outcome of elections in India in 2014.

So a way of reliably spotting influence bots on Twitter would be hugely useful. Last year, the Defense Advanced Research Projects Agency (DARPA) set out to find such a method by running a four-week competition in which teams were asked to spot bots in a stream of posts on the topic of vaccinations. One team emerged as a clear winner, and the results demonstrated some significant new strategies for identifying bots in the real world.

Today we get a unique insight into this competition and the strategies the teams employed thanks to a paper by V.S. Subrahmanian at the University of Maryland in College Park and Sentimetrix and a few pals.

The competition was about as realistic as DARPA could make it. The tweets were messages harvested from the Twitter stream during a 2014 debate on vaccinations. In this debate, a number of bots had been created as part of a competition to see how they could influence the discussions. So DARPA had ground truth knowledge of which accounts were artificial and which were real.

In total, the dataset contained over four million messages from more than 7,000 accounts of which 39 were bots in either the pro- or anti-vaccination lobbies. Each message contained a unique ID, a user profile including an image, a url, and a picture, where these were included. The data also included a time and date stamp as well as information about followers and when one account unfollowed another. All this was played to the competitors in a synthetic Twitter environment over four weeks in February and March.

The teams then had to analyze this Twitter stream and guess which users were bots. Each correct guess got them a single point but a team lost 0.25 points for each incorrect guess. A team that guessed all the bots d days before the end of the challenge also got d points, since DARPA is particularly interested in the early detection of influence bots.

The winning team was from the social media analytics company Sentimetrix, which guessed all the bots 12 days ahead of the deadline while making only one incorrect guess. That gave them a score of 50.75 points. (The second-place team, from the University of Southern California, scored 45 points, finding all the bots six days ahead of the deadline with no incorrect guesses.)

The winning strategies are revealing. The teams began by attempting to identify an initial set of bots in the data. Interestingly, none of the teams were able to automate this step and most used significant human input.

Sentimetrix used a pretrained algorithm to search for bot-like behavior. The team had trained this algorithm on Twitter data from the 2014 Indian election which featured many bots. It looked for unusual grammar, the similarity of the linguistics to natural language chatbots such as Eliza, and unusual behaviors such as extended periods of tweeting without a break that a human could not easily perform.

This revealed four accounts that were clearly bots, and Sentimetrix then used these to find others. One assumption was that bot-makers tend to produce many similar bots and link them to each other to inflate their popularity. So the team was able to use network and cluster analysis to find other likely bots, which they then compared to known bots.

The team also used features such as the temporal activity of the accounts on the assumption that an automated account would show unusual regularities. Sentimetrix also looked for users who changed allegiance during the debate from pro- to anti-vaccination (or vice versa). This they assumed could be a bot strategy for infiltrating one side of the argument and then posting opposing arguments.

A key feature in Sentimetrix’s success was the way it visualized the results of its work on an online dashboard so that a human user could easily see the status of analysis for each user.

In this second stage, Sentimetrix identified another 25 bots. That gave them enough data to train a machine learning algorithm to hunt through the data for other bots. And this approach led them to the remaining 10 bots.

The teams did not know how many bots were at work so a major problem was to know when to stop searching. Sentimetrix, for example, stopped when it could no longer find accounts that looked like bots.

That’s impressive work that could have an important influence on efforts to find bots that are attempting influence online discussions in inappropriate ways. Publishing the strategies like this should help other players develop anti-bot tactics, too.

But it could also have a negative impact. The battle between bots and bot-hunters is one that is constantly evolving. With papers like this, the bot-hunters are revealing their hand in a way that allows bot-makers to design strategies to specifically defeat these algorithms. In a way, it is like fighting with one hand tied behind your back.

Nevertheless, the temptation to keep bot-hunting strategies secret would be a dangerous one to promote. This kind of openness is part of our free society and surely one of the key reasons it is worth fighting to preserve.

Either way, this cat-and-mouse battle is set to continue.

Ref: The DARPA Twitter Bot Challenge : arxiv.org/abs/1601.05140",Yes.,"It can be easily used to spread disinformation by bots, which are automated accounts that can influence online discussions. A recent study found that a significant number of bots were used to influence the outcome of elections in India. Another study found that bots played a significant role in the annexation of Crimea.",https://www.technologyreview.com/2016/01/28/163553/how-darpa-took-on-the-twitter-bot-menace-with-one-hand-behind-its-back/,2016-01-28 00:00:00,LABEL_1_relevant,0.9964185953140259,Social Media,social_media,MIT Tech Review,Politics
11,Facebook Is Reportedly Trying to Sell Off Its Botched Cryptocurrency,"It's an extremely unpopular idea.

Cashing Out

Social giant Facebook-now-known-as-Meta is attempting to sell off its ill-fated cryptocurrency project Diem, Bloomberg reports — the end of the road, seemingly, for the social media giant’s efforts to cash in on blockchain.

According to Bloomberg‘s sources, the Diem Association, formerly known as Libra, is trying to figure out ways to return money to its investors and perhaps find a new home.

Per Diem

Facebook made a huge splash in 2019 when it announced Libra, a “stable digital cryptocurrency” meant to revolutionize global financial technologies. Even the likes of Uber, Spotify, PayPal, Visa, and Mastercard were on board, signing on as corporate partners with a $10 million stake each.

But the ambitious venture imploded after facing fierce backlash from US regulators and even governments around the world.

Advertisement

Advertisement

First, high-profile backers started to jump ship. Then in October 2019, Facebook CEO Mark Zuckerberg got grilled in front of Congress for what they called a blatant attempt to circumvent regulatory approvals. By April 2020, Libra was scaled back significantly to a shadow of what it once was.

Last year, the US Federal Reserve buried any chances of Diem — Libra’s newly adopted name — taking off after striking down a potential deal with holdings company Silvergate Capital, according to Bloomberg.

The initiative kept looking for issuers of what had since turned into a US dollar-backed stablecoin. Government officials, however, were still unhappy with the plans, warning that regulators may never give it the greenlight, Bloomberg reports.

According to the outlet, Meta still owns a third of the Diem Association, but the venture’s future is still looking grim, given its sheer unpopularity.

Advertisement

Advertisement

READ MORE: Mark Zuckerberg’s Stablecoin Ambitions Unravel With Diem Sale Talks [Bloomberg]



More on Libra: Facebook’s Cryptocurrency is Collapsing in Front of Our Eyes

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Yes.,"People form communities with people like them, creating echo chambers and filter bubbles that reinforce their own existing views. As a result, people are less likely to be exposed to new or different ideas, and social media can actually make people more polarized in their views.",https://futurism.com/the-byte/facebook-reportedly-selling-cryptocurrency,,LABEL_1_relevant,0.9964765906333924,Social Media,social_media,Futurism,"Information, Discourse & Governance"
12,How Twitter Could Better Predict Disease Outbreaks,"Over several months, the researchers were able to gather a database of over 50 million geo-located tweets which could then be compared to official data from the U.K.’s national health service on flu incidence by region. By figuring out which keywords in the database of tweets were associated with elevated levels of flu, Lampos et al. were able to create a predictive model that transformed keyword incidence in future tweets into a prediction of the severity of flu for a given area.

This flu-predicting signal from Twitter is an independent stream of information that can “complement or improve the signal coming from search engine queries,” says Cristianini.

Cristianini notes that all approaches that track self-reported symptoms suffer from the same bias, however: the more the media hypes a Flu epidemic, the more likely people are to go to their doctors (distorting the “official” numbers) and talk about suspicious symptoms on Twitter or other services.

Future work might involve information from Facebook and other sources of status updates, allowing researchers to become ever more adept at pinpointing outbreaks in their earliest stages.

Follow Christopher Mims on Twitter, or contact him via email.",Yes,It can distort the “official” numbers of an event.,https://www.technologyreview.com/2010/07/14/202024/how-twitter-could-better-predict-disease-outbreaks/,2010-07-14 00:00:00,LABEL_1_relevant,0.9943252205848694,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
13,WhatsApp's Case Against NSO Group Might Be an Uphill Battle,"WhatsApp just took a hard new line against the malware industry, suing notorious Israeli surveillance contractor NSO Group for attacks on more than a thousand of its users. The case could mark a turning point in Silicon Valley's fight against private-sector espionage mercenaries. But before it can convince a court that NSO engaged in criminal hacking, WhatsApp may have to win a thorny legal argument—one that legal experts say could require some creative contortions.

On Tuesday afternoon, WhatsApp published a statement accusing NSO of targeting 1,400 of its users, including at least 100 members of ""civil society"" such as journalists and human-rights defenders, with malicious voice calls designed to infect targeted phones with malware and steal messages despite WhatsApp's end-to-end encryption. Those numbers would represent a new scale for NSO, whose malware has already been linked to attacks against activists ranging from the now-imprisoned United Arab Emirates dissident Ahmed Mansoor to Mexican activists opposing a soda tax.

WhatsApp paired its statement with a lawsuit in a Ninth Circuit court, accusing NSO of violating the Computer Fraud and Abuse Act, as well as state-level charges including breach of contract and interfering with their property. The case represents a bold attempt to use the CFAA in an unusual way: to punish not just hackers who breach a company's computers, but those who exploit its software to breach the computers of its users.

But some hacking-focused lawyers who have analyzed WhatsApp's complaint warn that—noble as its attempt to slap back NSO and protect its users may be—its central argument may not fly in court.

""Absent more, they won’t be able to hook the CFAA violation on the terms of service."" Riana Pfefferkorn, Stanford University

That's because, fundamentally, the CFAA outlaws so-called ""unauthorized access,"" explains Tor Ekeland, a well-known hacker defense attorney. To make that charge stick, WhatsApp will have to show that NSO obtained illegal access to WhatsApp's own systems. Given that NSO's targets were WhatsApp users rather than, say, WhatsApp's servers, they'll have to find an argument that they, as the plaintiff, were the victim. ""The fundamental question is, what’s the unauthorized access?"" says Ekeland. ""You might be able to argue that NSO hacked WhatsApp and not just their users. Maybe they’re trying to make that argument. But they’re not being clear about it, and that lack of clarity is an attack vector for the defendant.""

WhatsApp's most obvious unauthorized access argument relates to its terms of service, which prohibit reverse-engineering WhatsApp's code, harming its users, or sending malware via WhatsApp. The company might argue that by agreeing to those terms of service and then violating them, NSO's use of WhatsApp was unauthorized all along. The complaint appears to lay the groundwork for that case: It points out that NSO Group staff ""created various WhatsApp accounts and agreed to the WhatsApp Terms.""

But that terms-of-service argument will be an uphill battle, says Ekeland. Terms of service have long been a controversial element of hacking cases, from the 2009 cyberbullying case of Lori Drew to the hacking charges against information freedom activist Aaron Swartz. And the Ninth Circuit in particular has set a clear precedent that terms-of-service violations alone don't constitute unauthorized access. ""A terms of service violation under the CFAA is a very thin reed to hang your case on,"" Ekeland says.

WhatsApp parent company Facebook has sought out CFAA rulings against terms-of-service violators in the past. It sent a warning to a company called Power Ventures, which created its own user interface for Facebook and other social media sites, to stop violating its terms. It then sued under the CFAA only after the company persisted. In that instance, a judge ruled explicitly that Power Ventures had broken the CFAA—but that it wouldn't have if Facebook hadn't first told it to stop accessing its site.",Yes,It has given a platform to anyone with a computer and an internet connection to be heard. Such platforms have been a boon to political dissidents and other marginalized groups by allowing them to bypass traditional media gatekeepers and reach a global audience directly. But they've also been exploited by government-backed trolls and other malicious actors to spread disinformation and sow discord.,https://www.wired.com/story/whatsapp-nso-group-lawsuit/,2019-10-30 02:03:32.943000+00:00,LABEL_1_relevant,0.9966089725494384,Social Media,social_media,Wired,"Information, Discourse & Governance"
14,Users Sue Juul for Addicting Them to Nicotine,"Juul Labs, the San Francisco-based e-cigarette company, is under pressure from parents, schools, public health advocates, lawmakers, and the Food and Drug Administration for its popularity with younger users, who have gravitated to Juul’s discrete rechargeable vaping device and nicotine pods in flavors like mango and fruit medley. Now come the lawsuits.

Since April, consumers have filed at least three complaints against Juul. Two of the lawsuits were filed in California and allege Juul deceptively marketed the product as safe, when it contains more potent doses of nicotine than cigarettes. Both seek monetary damages, as well as an injunction to curb Juul’s marketing practices.

In the first case, filed in US District Court in Northern California in late April, Bradley Colgate of La Jolla and Kaytlin McKnight of Arroyo Grande say they first purchased Juul in 2017. The suit claims that McKnight became addicted to nicotine salts and now vapes several Juul pods each week. Colgate purchased Juul to help him quit smoking, according to the suit, but “the intense dosage of nicotine salts delivered by the Juul products resulted in an increased nicotine addiction, and an increased consumption of nicotine by Colgate.”

In the second case, in San Francisco Superior Court, Carl Cooper also says he purchased Juul in hopes of quitting smoking. But Cooper, who the suit says began smoking at the age of 15 in 2010, alleges that Juul worsened his addiction, turning him from someone who mostly smoked on the weekends to a habitual daily user within a couple of weeks. The suit claims Cooper becomes agitated and moody if he doesn’t get regular doses of nicotine salts from Juul pods. “Whereas Cooper had never felt the need to smoke on a daily basis, he now finds that he feels compelled to vape Juul pods every day,” the complaint claims.

The most recent complaint, filed in US District Court in New York in June, came from the mother of a 15-year-old, identified only as “D.P.”, alleging Juul designed its product to contain more nicotine than necessary to satisfy the cravings of an adult smoker. D.P. became “heavily addicted to nicotine,” the suit claims, making him “anxious, highly irritable and prone to angry outbursts,” and perform poorly in school.

The complaint says that D.P. began using Juul after he started a specialized high school in Rockland County, New York offering programs that fit his interest in technology and carpentry. Juul use was pervasive at the school, including on the school bus, in bathrooms, outside school, and even in class, the complaint says. His parents switched him to another high school, but Juul was prevalent there as well. In order to deter his use, the complaint says the teenager’s parents removed the door from his bedroom, locked parts of their house, instructed school officials not to let him use the bathroom unaccompanied, and subjected D.P. to regular urine tests.

“Yet despite all these measures, D.P. is unable to stop Juuling,” the complaint says. His “urges” become so powerful, the complaint says, that “he is unable to avoid Juuling even though it subjects him to disciplinary measures at home and at school.”",Yes,Addiction.,https://www.wired.com/story/users-sue-juul-for-addicting-them-to-nicotine/,2018-07-23 11:00:00+00:00,LABEL_1_relevant,0.9967063069343568,Social Media,social_media,Wired,"Information, Discourse & Governance"
15,"Facebook whistleblower, Frances Haugen, raises trust and security questions over its e2e encryption","Frances Haugen, one of (now) multiple Facebook whistleblowers who have come forward in recent years with damning testimony related to product safety, gave testimony in front of the UK parliament today — where, in one key moment, she was invited to clarify her views on end-to-end encryption following a report in the British newspaper the Telegraph yesterday.

The report couched Facebook’s plan to extend its use of e2e encryption as “controversial” — aligning the newspaper’s choice of editorial spin with long-running UK government pressure on tech giants not to expand their use of strong encryption in order that platforms can be ordered to decrypt and hand over message content data on request.

In its interview with Haugen, the Telegraph sought to link her very public concerns about Facebook’s overall lack of accountability to this UK government anti-e2ee agenda — claiming she had suggested Facebook’s use of e2e encryption could disrupt efforts to protect Uighur dissidents from Chinese state efforts to inject their devices with malware.

The reported remarks were quickly seized upon by certain corners of the Internet (and at least one other ex-Facebook staffer who actually worked on adding e2e encryption to Messenger and is now self-styling as a ‘whistleblower’) — with concerns flying that her comments could be used to undermine e2e encryption generally and, therefore, the safety of scores of Internet users.

Sounding unimpressed with the Telegraph’s spin, Haugen told UK lawmakers that her views on e2e encryption had been “misrepresented” — saying she fully supports “e2e open source encryption software”; and, indeed, that she uses it herself on a daily basis.

What she said she had actually been querying was whether Facebook’s claim to be implementing e2e encryption can be trusted, given the tech giant does not allow for full external inspection of its code as is the case with fully open source e2ee alternatives.

This is another reason why public oversight of the tech giant is essential, Haugen told the joint committee of the UK parliament which is scrutinizing (controversial) draft online safety legislation.

“I want to be very, very clear. I was mischaracterised in the Telegraph yesterday on my opinions around end-to-end encryption,” she said. “I am a strong supporter of access to open source end to end encryption software.

“I support access to end-to-end encryption and I use open source end-to-end encryption every day. My social support network is currently on an open source end-to-end encryption service.”

“Part of why I am such an advocate for open source software in this case is that if you’re an activist, if you’re someone who has a sensitive need, a journalist, a whistleblower — my primary form of social software is an open source, end-to-end encryption chat platform,” she also said, without naming exactly which platform she uses for her own e2ee messaging (Signal seems likely — a not-for-profit rival to Facebook-owned WhatsApp which has benefited from millions of dollars of investment from WhatsApp founder Brian Action, another former Fb staffer turned critic; so maybe ‘meta’ would in fact be a perfect new brand name for Facebook).

“But part of why that open source part is so important is you can see the code, anyone can go and look at it — and for the top open source end-to-end encryption platform those are some of the only ways you’re allowed to do chat in say the defence department in the US.

“Facebook’s plan for end-to-end encryption — I think — is concerning because we have no idea what they’re doing to do. We don’t know what it means, we don’t if people’s privacy is actually protected. It’s super nuanced and it’s also a different context. On the open source end-to-end encryption product that I like to use there is no directory where you can find 14 year olds, there is no directory where you can go and find the Uighur community in Bangkok. On Facebook it is trivially easy to access vulnerable populations and there are national state actors that are doing this.

“So I want to be clear, I am not against end-to-end encryption in Messenger but I do believe the public has a right to know what does that even mean? Are they really going to produce end-to-end encryption? Because if they say they’re doing end-to-end encryption and they don’t really do that people’s lives are in danger. And I personally don’t trust Facebook currently to tell the truth… I am concerned about them misconstruing the product that they’ve built — and they need regulatory oversight for that.”

In additional remarks to the committee she further summarized her position by saying: “I am concerned on one side that the constellation of factors related to Facebook makes it even more necessary for public oversight of how they do encryption there — that’s things like access to the directory, those amplification settings. But the second one is just about security. If people think they’re using an end-to-end encryption product and Facebook’s interpretation of that is different than what, say, an open source product would do — because an open source product we can all look at it and make sure that what is says on the label is in the can.

“But if Facebook claims they’ve built an end-to-end encryption thing and there’s really vulnerabilities people’s lives are on the line — and that’s what I’m concerned about. We need public oversight of anything Facebook does around end-to-end encryption because they are making people feel safe when they might be in danger.”

Haugen, a former Facebook staffer from the civic integrity team, is the source for a tsunami of recent stories about Facebook’s business after she leaked thousands of pages of internal documents and research reports to the media, initially providing information to the Wall Street Journal, which published a slew of stories last month, including about the toxicity of Instagram for teens (aka the ‘Facebook Files‘), and subsequently releasing the data to a number of media outlets which have followed up with reports today on what they’re calling the Facebook Papers.

The tl;dr of all these stories is Facebook prioritizes growth of its business over product safety — leading to a slew of harms that can affect individuals, other businesses and the public/society more generally whether as a result of inadequate AI systems that cannot properly identify and remove hate speech (leading to situations where its platform can whip up ethnic violence), or which allow engagement based ranking systems to routinely amplify extreme, radicalizing content without proper mind to risks (such as forming conspiracy-theory touting echo chambers forming around vulnerable individuals, isolating them from wider society), or overestimation of its ad reach leading to advertisers being systematically overcharged for its adtech.

During her testimony today, Haugen suggested Facebook’s AIs were unlikely to even be able to properly distinguish dialectal distinctions and nuances of meaning between UK English and US English — let alone the scores of languages in countries where it directs far less resource.

Parliamentarians probed her on myriad harms during around 2.5 hours of testimony — and some of her answers repeated earlier testimony she gave to lawmakers in the US.

Many of the UK committee’s questions asked for her view on what might be effective regulatory measures to close the accountability gap — both on Facebook and social media more generally — as MPs sought to identify profitable avenues for amending draft online safety legislation.

“The danger with Facebook is not individuals saying bad things, it is about the systems of amplification that disproportionately give people saying extreme polarising things the largest megaphone in the room,” argued Haugen.

Her list of suggestions for fixing a system of what she couched as broken incentives under Facebook’s current leadership included mandatory risk assessments — which she warned need to cover both product safety and organisational structure since she said much of the blame for Facebook’s problems lies with its “flat” organizational structure and a leadership team that rewards (and thus incentivizes) growth above all else, leaving no one internally who’s accountable for improving safety metrics.

Such risk assessments would need to be carefully overseen by regulators to avoid Facebook using its customary tactic in the face of critical scrutiny of just marking its own homework — or “dancing with data” as she put it.

Risk assessments should also involve the regulator “gathering from the community and saying are there other things that we should be concerned about”, she said, not just letting tech giants like Facebook define blinkered parameters for uselessly partial oversight — suggesting “a tandem approach like that that requires companies to articulate their solutions”.

“I think that’s a flexible approach; I think that might work for quite a long time. But it has to be mandatory and there have to be certain quality bars because if Facebook can phone in it I guarantee you they’ll phone it in,” she also told the committee.

Another recommendation Haugen had was for mandatory moderation of Facebook Groups when they exceed a certain number of users.

Whereas — left unmoderated — she said groups can be easily misappropriated and/or misused (using techniques like ‘virality-hacking’) to act as an “amplification point” for spreading discord or disseminate disinformation, including by foreign information operations.

“I strongly recommend that above a certain sized group they should be required to provide their own moderators and moderate every post,” she said. “This would naturally — in a content-agnostic way — regulate the impact of those large groups. Because if that group is actually valuable enough they will have no trouble recruiting volunteers.”

Haugen also suggested that Facebook should be forced to make a firehose of information available to external researchers (as Twitter, for example, already does) — in a privacy-safe way — which would allow outside academics and experts to drive accountability from the outside by investigating potential issues and identifying concerns freed from Facebook’s internal growth-focused lens.

Another of her recommendations was for regulators to demand segmented analysis from Facebook — so that oversight bodies get full transparency into populations that disproportionately experience harms on its platform.

“The median experience on Facebook is a pretty good experience — the real danger is that 20% of the population has a horrible experience or an experience that is dangerous,” she suggested.

She went on to argue that many of Facebook’s problems result from the sub-set of users who she said get “hyper exposed” to toxicity or to abuse — as a consequence of an engagement-driven design and growth-focused mindset that rejects even small tweaks to inject friction/reduce virality (and which she suggested would only mean Facebook giving up “small slivers” of growth in the short term and yield a much more pleasant and probably more profitable product over the longer term).

“As we look at the harms of Facebook we need to think about these things as system problems — like the idea that these systems are designed products, these are intentional choices and that it’s often difficult to see the forest for the trees. That Facebook is a system of incentives, it’s full of good, kind, conscientious people who are working with bad incentives. And that there are lack of incentives inside the company to raise issues about flaws in the system and there’s lots of rewards for amplifying and making things grow more,” she told the committee.

“So I think there is a big challenge of Facebook’s management philosophy is that they can just pick good metrics and then let people run free. And so they have found themselves in a trap where in a world like that how do you propose changing the metric? It’s very very hard because 1,000 people might have directed their labor for six months trying to move that metric and changing the metric will disrupt all of that work.

“I don’t think any of it was intentional — I don’t think they set out to go down this path. And that’s why we need regulation — mandatory regulation, mandatory actions — to help pull them away from that spiral that they’re caught in.”

Legislation that seeks to rein in online harms by applying regulations to platform giants like Facebook must also not focus only on individual harms — but needs to respond to societal harms, she also emphasized.

“I think it is a grave danger to democracy and societies around the world to omit societal harm. A core part of why I came forward was I looked at the consequences of choices Facebook was making and I looked at things like the global south and I believe situations like Ethiopia are just part of the opening chapters of a novel that’s going to be horrific to read. We have to care about societal harm — not just for the global south but for our own societies.

“When an oil spill happens it doesn’t make it harder for us to regulate oil companies. But right now Facebook is closing the door on us being able to act — we have a slight window of time to regain people-control over AI; we have to take advantage of this moment.”

Facebook has been contacted for comment.",Yes,"It can be used to spread disinformation and hate speech, which can lead to societal harms.",https://techcrunch.com/2021/10/25/facebook-whistleblower-frances-haugen-raises-trust-and-security-questions-over-its-e2e-encryption/,2021-10-25 00:00:00,LABEL_1_relevant,0.9966400861740112,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
16,Facebook Had an Incredibly Busy Weekend,"While millions of Americans were enjoying a warm spring weekend, Facebook employees were hard at work responding to an avalanche of news about their company. After an already busy week for the social media platform—including a lawsuit from the Department of Housing and Urban Development, as well as a policy change regarding white nationalist and separationist content—five major Facebook stories broke over the last few days, including a Washington Post op-ed in which CEO Mark Zuckerberg calls for the social network to be regulated. Here’s what you need to know to get caught up.

Facebook Explores Restricting Who Can Livestream

The torrent of Facebook news began Friday, when COO Sheryl Sandberg said the company was “exploring restrictions on who can go Live depending on factors such as prior Community Standard violations.” The decision came less than three weeks after a terrorist attack in Christchurch, New Zealand, that killed 50 people was livestreamed on Facebook. The social network, as well as other companies like YouTube, struggled to stop the shooter’s video from being reuploaded and redistributed on their platforms.

In 2016, Zuckerberg said that live video would “create new opportunities for people to come together."" Around the same time, the company invested millions of dollars to encourage publishers like Buzzfeed to experiment with Facebook Live. The feature provided an unedited, real-time window into events like police shootings, but it was also repeatedly used to broadcast disturbing events. After the Christchurch attack, Facebook is now reexamining who should have the ability to share live video, which has proven difficult for the company to moderate effectively.

Sandberg also said Facebook will research building better technology to “quickly identify edited versions of violent videos and images and prevent people from re-sharing these versions.” She added that Facebook had identified over 900 different variations of the Christchurch shooter’s original livestream. Sandberg made her announcement in a blog post published not to the Facebook Newsroom but to Instagram's Info Center, indicating Facebook wants its subsidiaries to appear more unified.

Old Zuckerberg Blog Posts Disappear

Also on Friday, Business Insider reported that years of Zuckerberg’s public writings had mysteriously disappeared, “obscuring details about core moments in Facebook’s history.” The missing trove included everything the CEO wrote in 2007 and 2008, as well as more recent announcements, like the blog post Zuckerberg penned in 2012 when Facebook acquired Instagram.

Facebook said that the posts were mistakenly deleted as the result of technical errors. “The work required to restore them would have been extensive and not guaranteed, so we didn’t do it,” a spokesperson for the company told Business Insider. They added that they didn’t know exactly how many posts were lost in total.",Yes,"It leads to the spread of misinformation (fake news) and the amplification of polarizing views.

This can have a number of negative effects on individuals and society, including:

-Making people more susceptible to fake news and misinformation
-Increasing the echo chamber effect, where people only see information that supports their existing beliefs
-",https://www.wired.com/story/facebook-busy-weekend-news-feed-livestream/,2019-04-01 21:46:51.866000+00:00,LABEL_1_relevant,0.9687605500221252,Social Media,social_media,Wired,"Information, Discourse & Governance"
17,Help WIRED Track How Political Ads Target You on Facebook,"With a user base of more than 2 billion people who can be chopped and sorted by almost any conceivable data point—men ages 21 to 45 living in the United States who are parents to preteens and like Fortnite; women with a bachelor’s degree who are away from family and whose friends are recently engaged—Facebook advertising is an incredibly powerful tool. Much attention has been paid to how foreign influence operations used the platform to target and manipulate US citizens in 2016, and with good reason. But that election also demonstrated that political campaigns and local organizations can take advantage of these features. Regulations around political ads on the web has been uneven at best. And thanks to microtargeting, today’s advertisements can be that much harder to track.

Amid backlash and increased scrutiny, not to mention the threat of regulation, Facebook and other social media companies have taken some steps to increase transparency around ads, especially political ones, but they’re still an enormous target for bad actors. Even today, Facebook announced it was removing hundreds of pages and accounts, run by US citizens, that used “sensational political content” to drive traffic to their websites in ways that “have consistently broken our rules against spam and coordinated inauthentic behavior.” The New York Times reports that at least one of the pages, Right Wing News, used Facebook ads.

With just a few weeks left before the midterm elections, and 2020 looming after that, the many ways in which political advertising on Facebook can shape our society—whether it’s a grassroots campaign rallying supporters in innovative new ways, or clickbait farms adding fuel to the partisan fire for a quick buck—demands further exploration.

That’s why WIRED is partnering with ProPublica, the award-winning news nonprofit, on their multi-newsroom project to collect and analyze political ads targeted to Facebook users. Campaigns use all sorts of online platforms to reach constituents, but few of them come close to Facebook's power. And as ProPublica noted last year, “Facebook gives users more information about why a particular ad is targeted to them than other online platforms provide to their customers.” So they built a tool, the Facebook Political Ad Collector, to scrape that information, as well as the ads themselves, and put it all in a searchable database.

ProPublica’s database is entirely crowdsourced from volunteers who browse Facebook while using the extension. The more people who do that, the more ads it will collect and the more robust its database will be.

The project has collected more than 76,000 ads so far, and journalists have used the tool to discover campaigns that violated FEC rules and scams posing as political ads. (The tool has also exposed nonpolitical ads, like job postings, whose narrow audience targeting may violate antidiscrimination laws.)

The tool is just a simple browser extension, and that’s where you, our readers, can help. ProPublica’s database is entirely crowdsourced from volunteers who browse Facebook while using the extension. The more people who do that, the more ads it will collect and the more robust its database will be.

All you need to do is install ProPublica’s free extension on Chrome or Firefox, sign into Facebook, and scroll scroll scroll away. (Note: If you have an ad blocker enabled, it won’t work.)

If you’re feeling especially ambitious, you can help train the algorithm ProPublica developed to automatically filter political ads. The extension can show you ads from your own News Feed, as well as those others have seen, which you can then mark as “political” or “normal.”",Yes,"It can be used as a tool for political campaigns to microtarget ads to very specific groups of people. This is a very powerful tool that can be used for good or bad. If you want to help ProPublica track political ads on Facebook, you can install their free extension and mark ads as political or normal.",https://www.wired.com/story/facebook-political-ad-collector-wired-propublica/,2018-10-12 11:00:00+00:00,LABEL_1_relevant,0.9961901903152466,Social Media,social_media,Wired,Politics
18,Facebook and the Folly of Self-Regulation,"My late colleague, Neil Postman, used to ask about any new proposal or technology, “What problem does it propose to solve?”

When it comes to Facebook, that problem was maintaining relationships over vast time and space. And the company has solved it, spectacularly. Along the way, as Postman would have predicted, it created many more problems.

Last week, Facebook revealed the leaders and first 20 members of its new review board. They are an august collection of some of the sharpest minds who have considered questions of free expression, human rights, and legal processes.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

They represent a stratum of cosmopolitan intelligentsia quite well, while appearing to generate some semblance of global diversity. These distinguished scholars, lawyers, and activists are charged with generating high-minded deliberation about what is fit and proper for Facebook to host. It’s a good look for Facebook—as long as no one looks too closely.

What problems does the new Facebook review board propose to solve?

In an op-ed in The New York Times, the board’s new leadership declared: “The oversight board will focus on the most challenging content issues for Facebook, including in areas such as hate speech, harassment, and protecting people’s safety and privacy. It will make final and binding decisions on whether specific content should be allowed or removed from Facebook and Instagram (which Facebook owns).”

Only in the narrowest and most trivial of ways does this board have any such power. The new Facebook review board will have no influence over anything that really matters in the world.

It will hear only individual appeals about specific content that the company has removed from the service—and only a fraction of those appeals. The board can’t say anything about the toxic content that Facebook allows and promotes on the site. It will have no authority over advertising or the massive surveillance that makes Facebook ads so valuable. It won’t curb disinformation campaigns or dangerous conspiracies. It has no influence on the sorts of harassment that regularly occur on Facebook or (Facebook-owned) WhatsApp. It won’t dictate policy for Facebook Groups, where much of the most dangerous content thrives. And most importantly, the board will have no say over how the algorithms work and thus what gets amplified or muffled by the real power of Facebook.

This board has been hailed as a grand experiment in creative corporate governance. St. John’s University law professor Kate Klonick, the scholar most familiar with the process that generated this board, said, “This is the first time a private transnational company has voluntarily assigned a part of its policies to an external body like this.”

That’s not exactly the case. Industry groups have long practiced such self-regulation through outside bodies, with infamously mixed results. But there is no industry group to set standards and rules for Facebook. One-third of humanity uses the platform regularly. No other company has ever come close to having that level of power and influence. Facebook is an industry—and thus an industry group—unto itself. This is unprecedented, though, because Facebook ultimately controls the board, not the other way around.

We have seen this movie before. In the 1930s the Motion Picture Association of America, under the leadership of former US postmaster general Will Hays, instituted a strict code that prohibited major Hollywood studios from showing, among other things, “dances which emphasize indecent movements.” The code also ensured that “the use of the [US] flag shall be consistently respected.” By the 1960s, American cultural mores had broadened, and directors demanded more freedom to display sex and violence. So the MPAA abandoned the Hays code and adopted the ratings system familiar to American moviegoers (G, PG, PG-13, R, NC-17).

It’s a good look for Facebook—as long as no one looks too closely.

One reason the MPAA moved from strict prohibitions to consumer warnings was that American courts had expanded First Amendment protection for films, limiting how local governments could censor them. But all along, the MPAA practiced an explicit form of self-regulation, using a cartel that represented the interests of the most powerful studios to police behavior and represent the industry as a whole to regulators and the public.

No one can look at the history of American film and seriously argue that either method of self-regulation really served the public. Standards have been sloppily and inconsistently enforced. Through both the Hays code and the rating system, the MPAA limited artistic expression and the representation of lesbian, gay, and transgender issues and stories. But it sure helped Hollywood by keeping regulators at bay.",Yes,The social media platform itself.,https://www.wired.com/story/facebook-and-the-folly-of-self-regulation/,2020-05-09 18:58:57.543000+00:00,LABEL_1_relevant,0.9951855540275574,Social Media,social_media,Wired,"Information, Discourse & Governance"
19,Retro Gaming's Misogyny Is Brought to Light After Tragedy,"In 2015, record-breaking retro game champion Rudy Ferretti made a homebrew game for the Atari 2600. It was called Pigs in the Castle. “I’m tired of all you feminists and your bullshit,” he said in a video preceding its launch. It’s understood that the pigs in the game are women.

The game’s official Facebook page describes how the purpose of the game is to “kill 100 or more pig bitches to get the boss.” He elaborates: “simple???????? Fuck no it’s my game it’s hard.” In a video of the game, preserved on the YouTube channel of “Rudy Ferretti aka the console player of the century,” Ferretti's character navigates a pixelated castle killing “evil” pigs.

On August 10, police in Dover, New Hampshire, discovered Ferretti deceased in his bed. A firearm lay nearby. In the same apartment’s living room, police identified the body of his ex-girlfriend, Amy Molter. According to a medical examiner, Molter and Ferretti both died of gunshots to the head—Molter from homicide and Ferretti from suicide, the police suspect. The investigation into the circumstances surrounding the deaths remains ongoing.

Longtime members of the retro and arcade gaming scene say they warned community leaders and even police about Ferretti’s threatening behavior for years. For close to a decade, they say, Ferretti had harassed, stalked, and threatened gamers, particularly women, pushing some out of the niche gaming scene entirely. He flashed guns in tirade YouTube videos and bragged on Facebook about bringing one to an event at the Museum of Pinball in 2017.

Arcade game collector and researcher Catherine DeSpira and video game historian and storage auction buyer Patrick Scott Patterson—two of Ferretti’s most public targets—say they collectively contacted police in different states a half-dozen times to report Ferretti’s threats against themselves and others. They say those attempts ultimately had no effect. All the while, clusters of retro gamers across the country egged Ferretti on in private messages and on forums, leveraging his apparent instability and misogynist inclinations against women they didn’t want in the scene.

“You’d think anyone would look at it and go, ‘Hey, this guy’s gone, out there,’” says Patterson. “But people weren’t doing it. They were emboldening it, pushing him, giving him a support system.”

As the community processes last week’s tragedy, some described a culture of complicity in old-school gaming’s strongholds that didn’t do enough to protect women.

Forty years after the release of classic arcade games like Pac-Man and Joust, an active and enthusiastic connoisseur culture around these games still thrives at gaming conventions and online. Collectors, historians, nostalgics, and competitors share a deep love for modern gaming’s roots and its physical manifestations—blocky consoles, rare arcade cabinets. Back in 1983, a sociologist surveying arcades reported that 80 percent of players were men. Sources say that male dominance has carried through into 2020. Even though women now make up about 46 percent of all gamers, retro gaming’s conventions, online forums, and publications boast a self-reinforced culture of masculinity that, members of some communities say, abetted and allowed a vocal minority to target women.",Yes,The way it has allowed some gamers to target women.,https://www.wired.com/story/retro-gaming-violent-tragedy-rudy-ferretti/,2020-08-19 21:25:25.649000+00:00,LABEL_1_relevant,0.995603322982788,Social Media,social_media,Wired,Equality & Justice
20,Instagram's newest test mixes 'Suggested Posts' into the feed to keep you scrolling,"The days of a scrolling to the end of your Instagram feed look to be coming to an end. After adding algorithmic suggestions to the bottom of the app last year, Instagram is running a test that would splice more recommended posts from accounts you don’t follow into the feed with those you do.

In the next few days, the company will begin testing an expansion of “Suggested Posts” which would sprinkle that content through the regular feed. As it stands now, Suggested Posts appear at the bottom of Instagram after you’ve scrolled through all of the content from people you follow and hit the “You’re all caught up” message that the app implemented in 2018. Depending on how many accounts you follow, it’s possible to not run into that message or Instagram’s recommendations very often, if at all.

In addition to boosting the prominence of Suggested Posts, Instagram will test an option that lets users “snooze” the feature, removing it from the feed for 30 days. Anyone in the test will be able to offer feedback when a specific post doesn’t interest them, but it sounds like you won’t be able to disable Suggested Posts in the feed in a permanent way.

The Suggested Posts expansion will be accompanied by a way for users to shape what they see through managing their interests — stuff like cats, makeup or basketball. If you’ve seen enough cats, you can toggle that interest off or tell Instagram that you never wanted to see those damn cats to begin with when it shows you the next one.

A Facebook spokesperson described the expansion of Suggested Posts to TechCrunch as an “extension” of the Instagram feed, noting that the ratio of these algorithmic recommendations to posts from followed accounts will be variable based on how someone uses the app.

The test will roll out to a small number of users in English-speaking countries only, though the company declined to specify how many accounts will be involved.

The experiment might not make it into the final product, but from the way the winds over at Facebook have been blowing lately it looks pretty likely. Like we mentioned, Instagram and parent company Facebook introduced some tools to give people more control over their own behavior on the notoriously addictive-by-design apps back in 2018, including the “You’re all caught up” message and a way to track time spent.

Those tools weren’t a sea change for a company that generally values keeping people glued to its services (and its ads) at all costs, but they showed that Facebook was at least mildly self aware of the conversation about social media addiction sweeping through the tech world at the time.

In 2020, it sounds like Facebook is done humoring those concerns. Instagram is feeling the heat from TikTok’s preternaturally well-tuned endless algorithmic feed and booming success. Like it has so many times in the past, the company is looking to shift its own identity to chase a threatening competitor rather than staying the course or trying something new.

The new way Suggested Posts work is just a test for now, but mixing algorithmic suggestions into the feed with posts from accounts you follow would be a pretty big change to the core way the app works. As it stands, if people want a truly endless Instagram experience they could turn to the Explore tab or scroll past the “caught up” message. Many doubtless did to stave off boredom, to the likely detriment of their mental health.

But under the test, it will be less possible to use Instagram to only keep up with just the accounts that you’ve got a personal interest in, whether they’re friends, local businesses or influencers of your choosing. Instagram wants to inject more of what it wants you to see into that experience, or what the company believes you’d want to see but you just don’t know it yet.

The end result might not be that noticeable for people who follow huge swaths of accounts already and rarely meet the end of their feed, but it strays even further from the original product — a distant memory at this point — while giving Instagram a way to keep people on the app for longer while serving them more ads.",Yes.,Addiction.,https://techcrunch.com/2021/06/23/instagram-suggested-posts-test-topics/,2021-06-23 00:00:00,LABEL_1_relevant,0.9957542419433594,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
21,"In India, Smartphones and Cheap Data Are Giving Women a Voice","R. Mallika taps on the WhatsApp icon on her phone and selects the chat with S. Thanaraj. It's a long conversation, with messages back and forth about conserving her tribe's forest lands. It’s like billions of messages sent each day on WhatsApp, with one important exception—there are no written words.

“I never attended school,” says Mallika, a member of an indigenous nomadic tribe in southern India. “I can sign my name, guess where a bus goes, or what a road sign says, but not much beyond that.” (She believes she is 38 but doesn’t have a proper birth record.) She never owned a feature phone because she couldn’t read or write well enough to create a list of contacts.

Mallika, like 200 million other women in India, is illiterate. In the past few years though, millions of Indian women have gone online thanks to cheaper smartphones and mobile data, and apps that let them communicate using sounds and images. Anecdotal evidence suggests the phones are empowering many women to access information, build networks, and participate in markets.

For Mallika, the ability to use smartphones visually and orally has been a game changer. The internet is no longer sealed off with written words. She uses photographs and audio memos to communicate with friends and family, and voice commands to look for videos.

R. Mallika uses a smartphone to chronicle deforestation in southern India. Photograph: S. Thanaraj

“I can tap on the picture of Annan (what she calls Thanaraj) and send him an audio message,” she says. Before getting the phone, she had to travel by foot over mountains to enter Madurai and interact with Thanaraj—her single point of contact with the outside world. “The phone connects her directly to the people who can help her,” says Thanaraj. “I am no longer the only person she speaks to.”

Mallika is part of a WhatsApp group where she shares videos and photographs of the forest with local journalists. Illegal logging is a persistent problem. “Sometimes teak or sandalwood trees go missing,” she says. “I take pictures and compare them to older photographs.” She shares the photos with rangers and forest officials. In case of a confrontation, “my husband videotapes the skirmish to protect me. We send the video along with a voice message to the journalists’ group.” She also watches videos of activists in other parts of India.

“The phone connects her directly to the people who can help her.” S. Thanaraj, conservation activist

Mallika is benefiting from two seismic changes that happened in 2016. China’s Xiaomi and others began manufacturing smartphones in India and introduced entry-level models that cost less than $100. The same year, telecom company Reliance Jio launched a new service with free nationwide voice and data for one year. About 100 million people subscribed to the network in the first six months. In one year, the average cost for accessing 1 GB of data fell from $3.10 to $1.90. Since then, data costs have plunged further, to an average of 9 cents per GB. The phone and data plan still consume about 10 percent of Mallika’s income. But suddenly, it wasn’t only urban middle- and upper-class folk who were lit by a halo of blue light.

Ravi Agarwal, author of the 2018 book India Connected: How the Smartphone Is Transforming the World's Largest Democracy says that for many Indians, the smartphone is their first private TV screen, personal music player, computer, and camera. Agarwal compares it to the experience of owning a car for the first time—autonomy, privacy, and mobility.

The WIRED Guide to the iPhone Its influence goes far beyond other phones—the infrastructure that made the iPhone also enabled drones, smart-home gadgets, wearables, and self-driving cars.

This is particularly true for women, who are less likely to be literate or employed in the formal workforce. Even among the literate, many read and write in one of India’s more than 30 official languages—another hurdle to accessing the internet on personal computers and laptops with English keyboards. In 2015, only 10 percent of internet users in India’s rural areas were women. As smartphones and data plans have become more accessible, that figure has risen to roughly 30 percent, according to IAMAI, a trade group of internet and telecom companies.

Companies including Google, Intel, and Facebook have worked with local organizations to make it easier for women to access the internet. Google and Tata Trusts, for example, run the Internet Saathi, or Internet Friend, program, which trains rural women to be digital pioneers. They are taught to use smartphones in sessions where they are provided with phones and power banks. By December 2019, the program had trained more than 83,300 women to be Saathis. In turn, they had introduced over 34 million women to the internet.",Yes,"It can lead to internet addiction. A recent study in the journal Psychology of Popular Media Culture found that the more time people spend on social media, the more likely they are to be addicted to the internet.",https://www.wired.com/story/india-smartphones-cheap-data-giving-women-voice/,2021-01-04 12:00:00+00:00,LABEL_1_relevant,0.9913970232009888,Social Media,social_media,Wired,"Information, Discourse & Governance"
22,Instagram Boosts Its Security With a Fave-Worthy Update,"Social media platforms' struggle with safety and security is like a game of Whac-A-Mole. One day, the threat is coordinated bot activity; the next, it's SIM hijackers stealing the identities of regular users. In an effort to protect Instagram users from these and other threats, the company announced a set of features today designed make Instagram feel ""safer,"" including ways to protect your own account and to verify whether the accounts you follow are genuine or not.

Instagram

First, all users will soon be able to use a more robust form of two-factor authentication to log into Instagram. Previously, Instagram offered two-factor authentication with a code sent via SMS—better than nothing, but insufficient to protect all Instagram users from having their accounts compromised. (Users with ""valuable"" handles may be more vulnerable to scams like SIM hijacking, where hackers access a person's phone number and use it to log into their accounts and steal their usernames.) Now, the platform will allow integration with third-party authenticators, like DUO Mobile and Google Authenticator, which supply two-factor codes locally and provide an additional layer of security against account hacking.

Instagram

To help users differentiate between real and fake accounts, Instagram will now make it easy to look up information about individual accounts—including the date the account was created, its country of origin, and a record of username changes over the past year. You’ll also be able to see any ads the account is running and similar accounts with shared followers. To surface this information, tap the three dots on an Instagram profile page and select the new tab, “About This Account.” The feature will roll out to accounts with large followings, like celebrities, public figures, and accounts “sharing information related to current events, political or social causes.”

What's more, accounts with large numbers of followers will now be able to request verification from Instagram. The platform already gives blue checkmarks to some celebrity users and brands—WIRED's Instagram, for example, has one—but the verification process is mysterious, and Instagram hasn’t previously let users request verification. The new verification process involves a request form along with a place to upload a photo of a government-issued photo ID.

Post Mates

Instagram says the new changes are part of an effort to make the platform feel safe and to empower users to follow genuine accounts over fake ones.

“Keeping people with bad intentions off our platform is incredibly important to me,” Instagram’s co-founder and CTO, Mike Krieger, wrote in a blog post today. “That means trying to make sure the people you follow and the accounts you interact with are who they say they are, and stopping bad actors before they cause harm.”

The platform is also hoping to avoid some of the problems befalling its parent company, Facebook, which has struggled to keep fake accounts, misinformation campaigns, and untrustworthy pages off its service. Facebook said it has deactivated millions of fake accounts this year, and that some malicious actors are becoming harder to trace.

Instagram is, of course, a different beast. As it grows, it will have to face decisions about how to create community and trust on a global platform of over 1 billion users. Checkmarks and two-factor authentication aren't the end of that story. But they're a good place to start.

More Great WIRED Stories",,"It can lead to a loss of social connection. This is due to the way social media platforms are designed, which encourages users to share personal information and connect with others. The loss of social connection can lead to feelings of isolation, anxiety, and depression.",https://www.wired.com/story/instagram-security-update/,2018-08-28 17:00:00+00:00,LABEL_1_relevant,0.951593816280365,Social Media,social_media,Wired,Social Norms & Relationships
23,"Watch lawmakers grill Snap, TikTok and YouTube on kids' online safety","After dragging in the same companies and their reticent, overtrained executives time and time again, Congress is turning its attention to two of the tech industry’s fresh but important faces: TikTok and Snap.

On Tuesday, lawmakers on the Senate Subcommittee on Consumer Protection, Product Safety, and Data Security will question policy leads from those two companies and YouTube on how their platforms affect vulnerable young users. Facebook whistleblower Frances Haugen testified before the same committee on parallel issues in early October, shortly after revealing her identity.

The hearing will air on Tuesday at 7AM PT, featuring testimony from Snap VP of Global Public Policy Jennifer Stout, TikTok’s VP and Head of Public Policy Michael Beckerman and Leslie Miller, who leads government affairs and public policy at YouTube. A livestream is embedded below.

Subcommittee chair Senator Richard Blumenthal (D-CT) will lead the hearing, which will focus on social media’s detrimental effects on children and teens. “The bombshell reports about Facebook and Instagram—their toxic impacts on young users and lack of truth or transparency—raise serious concerns about Big Tech’s approach toward kids across the board,” Blumenthal said, connecting reports about Instagram’s dangers for teens to social media more broadly. The subcommittee’s ranking Republican Marsha Blackburn (R-TN) has signaled that she’s particularly interested in privacy concerns around TikTok.

We’d expect topics like eating disorders, harassment, bullying, online safety and data privacy to come up as members of the subcommittee take turns pressing the three policy leads for answers. The group of lawmakers also plans to discuss legislation that could help protect kids and teens online, though how solutions-oriented the hearing will be remains to be seen. Some of those potential solutions include the KIDS Act (Kids Internet Design and Safety), which would create new online protections for people under the age of 16. Blumenthal and fellow Democratic Senator Ed Markey reintroduced the bill last month.

The mental health of kids and teens isn’t the only pressing societal crisis that social platforms are implicated in at the moment, but it’s one Republicans and Democrats are both rallying around. For one, it’s a rare arena of criticism with plenty of political overlap for both sides. Both parties do seem to agree that tech’s biggest companies need to be controlled in some way, though they generally play up different parts of the why: for conservatives it’s that these companies have too much decision making power when it comes to what content gets wiped from their platforms. On the opposite side of the aisle, Democrats are generally much more worried about the kind of content that gets left up, like extremism and misinformation.

Tuesday’s hearing will also likely dive into how algorithms amplify harmful content. Because social media companies play their cards close to the chest when it comes to how their algorithms work, hearings are a rare opportunity for the public to learn more about how these companies serve their users personalized content. Ideally we’d be learning a lot about that kind of thing in the often lengthy, repetitive tech hearings Congress has held in the last couple of years, but between lawmakers pushing uninformed or irrelevant lines of questioning and evasive tech executives with hours of media training under their belts, the best we can usually hope for is a few new tidbits of information.

While Facebook won’t appear at this particular hearing, expect recent revelations around that company and Instagram to inform what happens on Tuesday. All three social media companies set to testify have had an eye on the public response to leaked Facebook documents and more reporting on that data just landed on Monday.

Just after the initial reports that Instagram is aware of the risks it poses to teen users, TikTok introduced a new set of safety measures including a well-being guide, better search interventions and opt-in popups for sensitive search terms. Last week, Snap announced a new set of family-focused safety tools to give parents more visibility into what their kids are up to using the platform. Both social networks skew heavily toward younger users compared to platforms like Facebook, Instagram and Twitter, making robust safety tools even more of a necessity. Leading into the hearing, YouTube announced some changes of its own around what kind of kids content will be eligible for monetization while also highlighting its other kid-centric safety measures.",Yes,"It harms the mental health of children and teens. The hearing will focus on how social media's detrimental effects on children and teens, and how to protect them.",https://techcrunch.com/2021/10/25/snap-tiktok-youtube-senate-hearing-congress/,2021-10-25 00:00:00,LABEL_1_relevant,0.9964458346366882,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
24,YouTube’s Push to Counter Toxic Videos With ‘Good’ Creators,"For his next video, he tells the group, Nahdi wants to go back to Indonesia to find out more about his grandmother’s life. Through her story, he wants to illustrate the sacrifice that so many millions of immigrants have made. But more than that, he wants to tell his own story. As the son of immigrants, he never really felt at home in London. Would Indonesia be any different?

“It’s not just about the journey, but also the sacrifice that generations before made to make me who I am today,” Nahdi says. “I’m just wondering: Does it hit that mark hard enough?” He pauses to scan the circle for reassurance. He doesn’t just want to hit that mark. He needs to hit that mark. The moral of the story is the reason he’s here.

YouTube launched the experimental program in late 2016, just as the world was waking up to the idea that Silicon Valley’s most celebrated tech companies might be introducing more harm than good into the world. That’s true of Facebook, Twitter, and most certainly, YouTube. Terrorists use these platforms to encourage violence. Russian trolls use them to undermine democracies. Conspiracy theorists use them to slander the victims of mass shootings, and kids use them to bully their classmates. With 400 hours of video being uploaded to YouTube every minute, the company has struggled to stay on top of every video that violates its policies against hate speech, harassment, and other harmful content. It has struggled even more with the question of whether and where to draw those lines in the first place.

YouTube has, like other tech companies, developed technology to spot the obvious garbage---the ISIS propaganda, the child pornography---and it’s hiring 10,000 human moderators to vet videos that get flagged. But it’s also taking another approach to the problem, investing in so-called “counterspeech” initiatives. It has become a buzzword in the tech industry lately, based on the idea that, as Supreme Court Justice Louis Brandeis once put it, the proper remedy to bad speech isn’t forced silence. It’s more speech.

Counterspeech tactics, which have been used to try to defuse tensions in the real world, are now being applied online. Rather than just shutting down content that’s bad for its users, tech companies are looking for ways to nudge them toward posts that, in their judgment anyway, are good for them. Facebook is doing this by adding fact-based related articles to news stories that are marked by its fact-checkers as false, essentially offering an antidote to fake news. YouTube uses a tool called the Redirect Method to direct users searching for certain terrorism-related keywords to carefully curated playlists of videos that debunk the extremist narratives.

Creators for Change is equal parts public relations and public policy. It’s a bet, though not an altogether sure one, that by empowering its more virtuous creators with funding, training, and exposure, YouTube can help them expand their reach and maybe even steal some of the audience from the network’s nastier voices. “There are a lot of these really positive voices, and we wanted to elevate those voices, whether it was dealing with LGBTQ issues, xenophobia, Islamophobia, or bullying,” says YouTube’s CEO Susan Wojcicki.

Creators for Change is YouTube's bet, that by empowering its more virtuous creators with funding, training, and exposure, the company can help them expand their reach and maybe even steal some of the audience from the network’s nastier voices. Alexander Coggin

But one person’s counter-speech is another one’s propaganda. For a company that likes to frame itself as a neutral platform, it’s an unusual approach. YouTube, and particularly its sister company Google, have recently faced charges of liberal bias from the right. They've defended against those charges by insisting their platforms welcome all political viewpoints. The same goes for Creators for Change. ""When [creators] apply to the program, we don't consider political affiliation or viewpoints,"" a YouTube spokesperson told me. ""Instead we look at their proposals for promoting tolerance, understanding, and bridging divides with constructive dialogue."" And yet, while the company doesn't explicitly advertise that conservatives need not apply, with Creators for Change, YouTube is at least implicitly aligning itself with a certain set of progressive ideals, accusations of bias be damned.

In January, YouTube announced it was expanding the initiative, investing $5 million in this year’s cohort, up from $1 million the year before. To YouTube’s parent company, Alphabet, which is worth a whopping $725 billion, that’s a couple of rusty quarters in the front seat cup holder. But small as it still is, Creators for Change is a valuable test case for a big idea. At a time when open platforms like YouTube face a choice between protecting their users and protecting their users’ ability to speak freely, counterspeech represents a tantalizing third possibility. Whether it's truly an effective foil or simply glorified marketing remains to be seen.",Yes,"It can lead to the spread of fake news, which can have harmful effects on people's lives.",https://www.wired.com/story/youtubes-creators-for-change-counterspeech/,2018-10-31 12:30:00+00:00,LABEL_1_relevant,0.9966237545013428,Social Media,social_media,Wired,"Information, Discourse & Governance"
25,How beer-related tweeting climaxed during the World Cup,"The question that Roller and co selflessly investigate is how interest in beer changes before, during, and after a game in each of the competing countries.

To find out, they collected Tweets published during the tournament that contained the beer emoji or others related to alcohol (such as sake), along with a country of origin. To find out the background rate of tweets with these emoji, they worked out how often they appear when a game isn’t being played.

The results show that Brazil and England lead the world by this measure. “On average, more than half of the BEER Tweets per day come from Brazil and England together,” point out the researchers.

That’s something of a puzzle given that Brazilians consume significantly less beer per capita than countries such as Germany, France, and England. But it’s no surprise that Saudi Arabia brings up the rear.

This ranking provides a background level against which to judge the rate of messaging during games. And the results make for interesting reading.

Roller and co say that the rate of tweeting about beer increases dramatically in most countries during the games they play. The largest increase was in Morocco, where beer-related tweets increased by over 400% during its matches. Morocco also showed the biggest decline in beer-related tweets after it was knocked out of the tournament. “Generally, the results show that from almost all countries more alcohol related Tweets can be found during the tournament,” say the team.

There are some exceptions. Some countries showed no increase—for example, Japan and Saudi Arabia. And beer-related tweets in Peru actually decreased during games, suggesting an admirable focus on the sport itself.

Roller and co say the results have implications for health, since alcohol consumption is linked to a wide range of medical conditions. “We know that increased alcohol consumption can lead to serious health issues, such as cancer, cardiovascular diseases and diseases of the digestive system, to mention a few,” they say.

So the national health of countries in which the teams were knocked out earlier must have benefited. “As many people tweet less after their national team left the tournament, we draw the conclusion that leaving the tournament early, as Germany did, is the healthiest solution—unless you are Peru,” say Roller and co, with their tongues firmly in their cheeks.

Whether or not that’s true, Roller and co end by pointing to their most significant conclusion: “Finally, we showed that Brazil tweets by far the most about beer. Cheers!” Quite!

Ref: arxiv.org/abs/1811.03809 : Football and Beer—A Social Media Analysis on Twitter in Context of the FIFA Football World Cup 2018",Yes,"It provides a platform for people to share information and connect with others, which can lead to unexpected outcomes such as the spread of misinformation or the reinforcement of negative stereotypes.",https://www.technologyreview.com/2018/11/22/138987/how-beer-related-tweeting-climaxed-during-the-world-cup/,2018-11-22 00:00:00,LABEL_1_relevant,0.995072901248932,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
26,"By Defying Apple’s Rules, Facebook Shows It Never Learns","If an app on Facebook behaved the way Facebook has been behaving, Facebook would probably have shut it down by now.

Tuesday’s scathing TechCrunch investigation all but guarantees it. The report found that Facebook has been paying people as young as 13 years old to download an app that grants Facebook access to users’ entire phone and web history, including encrypted activity and private messages and emails. The app, called Research, allows Facebook to see how people’s friends, who have not consented to having their data collected, interact with those users, too.

Facebook says the app was purely for market research. Explained another way: The app allowed Facebook to spot competitive threats on the horizon to help it retain its unprecedented power. Facebook has used another app, called Onavo, to collect similar information; for example, data from Onavo alerted Facebook to the growing popularity of the messaging app WhatsApp before the company acquired it in 2014.

""I think it speaks to the growth-at-any-cost mentality of the company,"" says Ashkan Soltani, who served as chief technologist to the Federal Trade Commission during its 2011 investigation of Facebook.

Facebook didn't respond to WIRED's request for comment.

At a time when Facebook is under the microscope for violating its users' privacy, such techniques are bold enough. But what makes the operation even more brazen is that Facebook continued running the program, which launched in 2016 and was sometimes called Atlas, even after Apple banned Onavo from the App store less than six months ago. Apple said it would no longer allow developers to collect information from other third-party apps.

Apparently undeterred, Facebook created a workaround for the Research app. It circumvented Apple’s vetting process using a technical loophole that is only intended for apps Facebook distributes to its own employees. That allowed Facebook to ingest everything a user did on their phones, including teens and minors. While kids under the age of 17 had to receive parental consent to participate, the disclosure form analyzed by TechCrunch minimized the extent of what could be done with all that data. “There are no known risks associated with the project,” it read. Facebook told TechCrunch only 5 percent of the app's users were teens.

Still, even the solicitations adults would have received about the app weren't entirely forthcoming. When users referred their friends to the app, for which they could also get paid, the email they received encouraged them to “Install it and forget it,” making the act of giving away unlimited access to their private communications sound as harmless as setting up a Ronco Rotisserie.

""I think it speaks to the growth-at-any-cost mentality of the company."" Ashkan Soltani

The Research app is just the latest example of Facebook’s doublespeak. In public and even under oath, executives like Mark Zuckerberg and Sheryl Sandberg have spent at least a year---if not their entire careers---promising to do better by their users. But in private, evidence abounds that the company continues to flout every rule and attempt at oversight placed before it. They've promised to protect user privacy by cutting off developer access to data while continuing to give it away to corporate giants and major advertisers. They've vowed to investigate foreign interference in elections, all while withholding information about the extent of that interference on Facebook. They've launched efforts to make their ads more transparent, while crippling external efforts by organizations like ProPublica to pull back the curtain even further.

Even as privacy hounds and antitrust watchdogs at the FTC and on Capitol Hill sniff and scratch at Facebook’s door, the social media giant, apparently high on hubris, just keeps tossing them red meat. If Facebook has learned anything from the last two years of public and regulatory scrutiny, it has a funny way of showing it.",Yes,"It has given rise to a new breed of internet user that is more narcissistic and self-centered than ever before. These users are more likely to share personal information, engage in risky online behavior, and be less concerned about the privacy of others.",https://www.wired.com/story/facebook-research-app-lessons/,2019-01-30 19:37:29.439000+00:00,LABEL_1_relevant,0.996469020843506,Social Media,social_media,Wired,Security & Privacy
27,Someone Hacked a US Warship Facebook Account to Stream Games,"Well, it was a very bad, no good, week for Twitch. The Amazon-owned streaming service suffered a devastating hack, with Twitch source code, streamer payouts, internal security tools, and more all dumped on 4chan. It's about as thorough a data dump as you'll see. Among the many repercussions: Twitch streamers experienced sudden, forced pay transparency, which quickly became a meme on social media and Twitch itself.

Twitch wasn't the only unlucky tech titan this week. Facebook, Instagram, and WhatsApp all got knocked out of service for several hours on Monday. No, it wasn't hackers. Instead, Facebook accidentally withdrew its BGP route, effectively making it impossible for the rest of the internet to find it. Not only that, but it knocked Facebook's own networks off the internet's maps, meaning its engineers couldn't fix the problem remotely. A real mess! The company experienced down time Friday afternoon as well, but not nearly as extensive an outage.

Are you using a password manager yet? And while we're at it, is the rest of your family? They should be! If you're having a hard time convincing them, we've put together a few suggestions that might help get them to buy in.

A simple bug leaves AirTag users potentially vulnerable. And a judge found that the internet infrastructure company Cloudflare isn't responsible for copyright-infringing sites that use its services.

And there's more! Each week we round up all the security news WIRED didn’t cover in depth. Click on the headlines to read the full stories, and stay safe out there.

Everything’s fine now, it's resolved; the Navy has its Facebook page back. But sometime Sunday night, someone managed to break into the account of the USS Kidd, a US Navy destroyer, and used that access to, well, stream the real-time strategy game Age of Empires. A Navy spokesperson confirmed to the military-focused news site Task & Purpose on Wednesday that someone gained ""unauthorized access"" to the Facebook page.



The streams were accompanied by brief messages like ""hi guys"" and ""play game"" and ""ffffffffffff."" The likeliest culprit seems to be a family member of someone who has the keys to the USS Kidd’s social media account.

Firefox is a great browser for the privacy-minded, but maybe not in this specific instance? In the latest version, when you type into the address bar you'll get ""new, relevant suggestions from our trusted partners based on what you're searching for."" In other words, a type of ad in an unexpected place. You can turn the feature off by heading to Settings, then Privacy & Security, then uncheck Contextual suggestions down under Address Bar — Firefox Suggest. But c'mon—you shouldn't have to do this in the first place, and you definitely should have a better heads-up about where the URL prompts in your address bar are coming from.

A very big hack that did not get nearly enough attention was quietly disclosed in an SEC filing by Syniverse, a telecom infrastructure company. As Motherboard first reported, hackers managed to stay in Syniverse systems for years, and they would have had access to all manner of phone records, potentially including text messages. It's unclear if there's been any fallout yet from the prolonged incident, but it's a potential treasure trove for international espionage.

Speaking of international espionage! Google this week sounded the alarm that Russia's Fancy Bear hackers had unleashed a broad, sophisticated phishing campaign that targeted 14,000 Gmail users last month. Individual accounts that were targeted received alerts, and the group appears to have included journalists and security researchers along with people in a variety of other industries. Google said all of the attempts were caught by spam filters, but the scale of the effort was still notable.

More Great WIRED Stories",Yes,"We are now living in a world where the most popular and influential people are those who have proven themselves to be the most adept at gaining and maintaining the attention of billions of people. And that is bad for democracy.

🎧 WIRED's favorite stories, every day in your inbox. By signing up, you agree to our Privacy",https://www.wired.com/story/navy-facebook-page-hack-russia-phishing-syniverse-hack-security-news/,2021-10-09 13:00:00+00:00,LABEL_1_relevant,0.9965458512306212,Social Media,social_media,Wired,"Information, Discourse & Governance"
28,Twitter Continues Cleanup and Cracks Down on Malicious Apps,"Several weeks ago, a friend signed up for a Twitter application that promised to delete old tweets. It was advertised as a simple way to clear your online presence, but after he used it, my friend noticed that his account began retweeting spam. Out of an abundance of caution, he deleted his Twitter account entirely.

On Tuesday, Twitter announced steps it was taking to curb this exact sort of abuse. Between May and June of this year, the social network says it deleted more than 143,000 apps that violated its policies prohibiting developers from using its APIs to automate spam, abuse, or violate people’s privacy, among other rules.

The move is part of a greater cleanup happening at Twitter. Earlier this month, the company announced that it was removing some suspicious accounts from people’s follower lists, and the The Washington Post reported it had recently suspended more than 70 million fake accounts entirely.

Automated accounts often play a central, positive role on Twitter. There’s a Twitter bot that creates emoji aquariums, one that spits out “deep” questions about the universe, and another that tweets anonymized information about individual Americans each hour using data from the decennial US census. Developers also use Twitter’s API to create all sorts of tools, like Nuzzel, an app that displays news stories the people you follow on Twitter are talking about.

There are thousands of other useful, delightful, or otherwise harmless apps on Twitter, but the social network has also been plagued by ones that automate abuse, leveraging Twitter’s tools to violate its own policies. Twitter apps have been created that direct-message spam, help carry out misinformation campaigns, and assist governments conducting online surveillance. Now, the social network is cracking down on them, as part of a wider effort to make Twitter “healthier.”

In addition to removing problematic apps, Twitter announced Tuesday that it is rolling out new policies to prevent ill-intentioned developers from getting access to its APIs in the first place. The company will now require all developers interested in creating apps to go through a stricter application process, which was initially launched in November for access to its “premium” APIs. Developers already using Twitter’s APIs will also need to go through the process; Twitter says they will be given 90 days notice before enforcing the requirement.

The application will require developers provide “detailed information about how they use or intend to use Twitter’s APIs so that we can better ensure compliance with our policies,” wrote Yoel Roth, Twitter’s platform policy manager, and Rob Johnson, a senior product management director, in their blog post announcing the change.

Twitter is also imposing new limits on developers whose applications are accepted. For example, a single developer account can now only register 10 apps by default, and must submit a request for the ability to create more. Apps will now also be limited in the amount of automated activity they can facilitate. A Twitter bot, for example, will only be allowed to tweet or retweet 300 times in an hour and follow only 1000 people in a day. In addition, Twitter rolled out a new tool where users can report malicious apps and API abuses.",Yes,"It can be abused by automated accounts that violate Twitter's policies. In response, Twitter is changing its policies to make it more difficult for developers to create these types of apps, and is also imposing new limits on the amount of automated activity that apps can facilitate.",https://www.wired.com/story/twitter-continues-cleanup-cracks-down-on-malicious-apps/,2018-07-24 18:31:37.758000+00:00,LABEL_1_relevant,0.9957740902900696,Social Media,social_media,Wired,"Information, Discourse & Governance"
29,Today's real story: The Facebook monopoly,"Facebook is a monopoly. Right?

Mark Zuckerberg appeared on national TV today to make a “special announcement.” The timing could not be more curious: Today is the day Lina Khan’s FTC refiled its case to dismantle Facebook’s monopoly.

To the average person, Facebook’s monopoly seems obvious. “After all,” as James E. Boasberg of the U.S. District Court for the District of Columbia put it in his recent decision, “No one who hears the title of the 2010 film ‘The Social Network’ wonders which company it is about.” But obviousness is not an antitrust standard. Monopoly has a clear legal meaning, and thus far Lina Khan’s FTC has failed to meet it. Today’s refiling is much more substantive than the FTC’s first foray. But it’s still lacking some critical arguments. Here are some ideas from the front lines.

To the average person, Facebook’s monopoly seems obvious. But obviousness is not an antitrust standard.

First, the FTC must define the market correctly: personal social networking, which includes messaging. Second, the FTC must establish that Facebook controls over 60% of the market — the correct metric to establish this is revenue.

Though consumer harm is a well-known test of monopoly determination, our courts do not require the FTC to prove that Facebook harms consumers to win the case. As an alternative pleading, though, the government can present a compelling case that Facebook harms consumers by suppressing wages in the creator economy. If the creator economy is real, then the value of ads on Facebook’s services is generated through the fruits of creators’ labor; no one would watch the ads before videos or in between posts if the user-generated content was not there. Facebook has harmed consumers by suppressing creator wages.

A note: This is the first of a series on the Facebook monopoly. I am inspired by Cloudflare’s recent post explaining the impact of Amazon’s monopoly in their industry. Perhaps it was a competitive tactic, but I genuinely believe it more a patriotic duty: guideposts for legislators and regulators on a complex issue. My generation has watched with a combination of sadness and trepidation as legislators who barely use email question the leading technologists of our time about products that have long pervaded our lives in ways we don’t yet understand. I, personally, and my company both stand to gain little from this — but as a participant in the latest generation of social media upstarts, and as an American concerned for the future of our democracy, I feel a duty to try.

The problem

According to the court, the FTC must meet a two-part test: First, the FTC must define the market in which Facebook has monopoly power, established by the D.C. Circuit in Neumann v. Reinforced Earth Co. (1986). This is the market for personal social networking services, which includes messaging.

Second, the FTC must establish that Facebook controls a dominant share of that market, which courts have defined as 60% or above, established by the 3rd U.S. Circuit Court of Appeals in FTC v. AbbVie (2020). The right metric for this market share analysis is unequivocally revenue — daily active users (DAU) x average revenue per user (ARPU). And Facebook controls over 90%.

The answer to the FTC’s problem is hiding in plain sight: Snapchat’s investor presentations:

This is a chart of Facebook’s monopoly — 91% of the personal social networking market. The gray blob looks awfully like a vast oil deposit, successfully drilled by Facebook’s Standard Oil operations. Snapchat and Twitter are the small wildcatters, nearly irrelevant compared to Facebook’s scale. It should not be lost on any market observers that Facebook once tried to acquire both companies.

The market Includes messaging

The FTC initially claimed that Facebook has a monopoly of the “personal social networking services” market. The complaint excluded “mobile messaging” from Facebook’s market “because [messaging apps] (i) lack a ‘shared social space’ for interaction and (ii) do not employ a social graph to facilitate users’ finding and ‘friending’ other users they may know.”

This is incorrect because messaging is inextricable from Facebook’s power. Facebook demonstrated this with its WhatsApp acquisition, promotion of Messenger and prior attempts to buy Snapchat and Twitter. Any personal social networking service can expand its features — and Facebook’s moat is contingent on its control of messaging.

The more time in an ecosystem the more valuable it becomes. Value in social networks is calculated, depending on whom you ask, algorithmically (Metcalfe’s law) or logarithmically (Zipf’s law). Either way, in social networks, 1+1 is much more than 2.

Social networks become valuable based on the ever-increasing number of nodes, upon which companies can build more features. Zuckerberg coined the “social graph” to describe this relationship. The monopolies of Line, Kakao and WeChat in Japan, Korea and China prove this clearly. They began with messaging and expanded outward to become dominant personal social networking behemoths.

In today’s refiling, the FTC explains that Facebook, Instagram and Snapchat are all personal social networking services built on three key features:

“First, personal social networking services are built on a social graph that maps the connections between users and their friends, family, and other personal connections.” “Second, personal social networking services include features that many users regularly employ to interact with personal connections and share their personal experiences in a shared social space, including in a one-to-many ‘broadcast’ format.” “Third, personal social networking services include features that allow users to find and connect with other users, to make it easier for each user to build and expand their set of personal connections.”

Unfortunately, this is only partially right. In social media’s treacherous waters, as the FTC has struggled to articulate, feature sets are routinely copied and cross-promoted. How can we forget Instagram’s copying of Snapchat’s stories? Facebook has ruthlessly copied features from the most successful apps on the market from inception. Its launch of a Clubhouse competitor called Live Audio Rooms is only the most recent example. Twitter and Snapchat are absolutely competitors to Facebook.

Messaging must be included to demonstrate Facebook’s breadth and voracious appetite to copy and destroy. WhatsApp and Messenger have over 2 billion and 1.3 billion users respectively. Given the ease of feature copying, a messaging service of WhatsApp’s scale could become a full-scale social network in a matter of months. This is precisely why Facebook acquired the company. Facebook’s breadth in social media services is remarkable. But the FTC needs to understand that messaging is a part of the market. And this acknowledgement would not hurt their case.

The metric: Revenue shows Facebook’s monopoly

Boasberg believes revenue is not an apt metric to calculate personal networking: “The overall revenues earned by PSN services cannot be the right metric for measuring market share here, as those revenues are all earned in a separate market — viz., the market for advertising.” He is confusing business model with market. Not all advertising is cut from the same cloth. In today’s refiling, the FTC correctly identifies “social advertising” as distinct from the “display advertising.”

But it goes off the deep end trying to avoid naming revenue as the distinguishing market share metric. Instead the FTC cites “time spent, daily active users (DAU), and monthly active users (MAU).” In a world where Facebook Blue and Instagram compete only with Snapchat, these metrics might bring Facebook Blue and Instagram combined over the 60% monopoly hurdle. But the FTC does not make a sufficiently convincing market definition argument to justify the choice of these metrics. Facebook should be compared to other personal social networking services such as Discord and Twitter — and their correct inclusion in the market would undermine the FTC’s choice of time spent or DAU/MAU.

Ultimately, cash is king. Revenue is what counts and what the FTC should emphasize. As Snapchat shows above, revenue in the personal social media industry is calculated by ARPU x DAU. The personal social media market is a different market from the entertainment social media market (where Facebook competes with YouTube, TikTok and Pinterest, among others). And this too is a separate market from the display search advertising market (Google). Not all advertising-based consumer technology is built the same. Again, advertising is a business model, not a market.

In the media world, for example, Netflix’s subscription revenue clearly competes in the same market as CBS’ advertising model. News Corp.’s acquisition of Facebook’s early competitor MySpace spoke volumes on the internet’s potential to disrupt and destroy traditional media advertising markets. Snapchat has chosen to pursue advertising, but incipient competitors like Discord are successfully growing using subscriptions. But their market share remains a pittance compared to Facebook.

An alternative pleading: Facebook’s market power suppresses wages in the creator economy

The FTC has correctly argued for the smallest possible market for their monopoly definition. Personal social networking, of which Facebook controls at least 80%, should not (in their strongest argument) include entertainment. This is the narrowest argument to make with the highest chance of success.

But they could choose to make a broader argument in the alternative, one that takes a bigger swing. As Lina Khan famously noted about Amazon in her 2017 note that began the New Brandeis movement, the traditional economic consumer harm test does not adequately address the harms posed by Big Tech. The harms are too abstract. As White House advisor Tim Wu argues in “The Curse of Bigness,” and Judge Boasberg acknowledges in his opinion, antitrust law does not hinge solely upon price effects. Facebook can be broken up without proving the negative impact of price effects.

However, Facebook has hurt consumers. Consumers are the workers whose labor constitutes Facebook’s value, and they’ve been underpaid. If you define personal networking to include entertainment, then YouTube is an instructive example. On both YouTube and Facebook properties, influencers can capture value by charging brands directly. That’s not what we’re talking about here; what matters is the percent of advertising revenue that is paid out to creators.

YouTube’s traditional percentage is 55%. YouTube announced it has paid $30 billion to creators and rights holders over the last three years. Let’s conservatively say that half of the money goes to rights holders; that means creators on average have earned $15 billion, which would mean $5 billion annually, a meaningful slice of YouTube’s $46 billion in revenue over that time. So in other words, YouTube paid creators a third of its revenue (this admittedly ignores YouTube’s non-advertising revenue).

Facebook, by comparison, announced just weeks ago a paltry $1 billion program over a year and change. Sure, creators may make some money from interstitial ads, but Facebook does not announce the percentage of revenue they hand to creators because it would be insulting. Over the equivalent three-year period of YouTube’s declaration, Facebook has generated $210 billion in revenue. one-third of this revenue paid to creators would represent $70 billion, or $23 billion a year.

Why hasn’t Facebook paid creators before? Because it hasn’t needed to do so. Facebook’s social graph is so large that creators must post there anyway — the scale afforded by success on Facebook Blue and Instagram allows creators to monetize through directly selling to brands. Facebooks ads have value because of creators’ labor; if the users did not generate content, the social graph would not exist. Creators deserve more than the scraps they generate on their own. Facebook suppresses creators’ wages because it can. This is what monopolies do.

Facebook’s Standard Oil ethos

Facebook has long been the Standard Oil of social media, using its core monopoly to begin its march upstream and down. Zuckerberg announced in July and renewed his focus today on the metaverse, a market Roblox has pioneered. After achieving a monopoly in personal social media and competing ably in entertainment social media and virtual reality, Facebook’s drilling continues. Yes, Facebook may be free, but its monopoly harms Americans by stifling creator wages. The antitrust laws dictate that consumer harm is not a necessary condition for proving a monopoly under the Sherman Act; monopolies in and of themselves are illegal. By refiling the correct market definition and marketshare, the FTC stands more than a chance. It should win.

A prior version of this article originally appeared on Substack.",Yes,"It has suppressed wages in the creator economy. Facebook, as the dominant player in social media, has been able to do this because it has a monopoly. The FTC's recent refiling of its case against Facebook seeks to address this issue.",https://techcrunch.com/2021/08/19/todays-real-story-the-facebook-monopoly/,2021-08-19 00:00:00,LABEL_1_relevant,0.9951475262641908,Social Media,social_media,TechCrunch,Economy
30,Cloudflare Ditches 8chan. What Happens Now?,"The internet infrastructure firm Cloudflare said it would cut service on Sunday evening to 8chan, the infamous online forum that has housed numerous posts and manifestos linked to horrific mass shootings in the United States and around the world. The move comes nearly two days after a mass shooting at an El Paso, Texas, Walmart left 20 dead and dozens wounded. The alleged gunman appears to have posted his manifesto on 8chan 20 minutes before the shooting. Cloudflare CEO Matthew Prince spoke with WIRED Sunday night about his decision.

Cloudflare provides infrastructural support, like content delivery services and DDoS protection, to 19 million online properties. Revoking that support can effectively shut a site down, at least until it finds a new provider. But the company has long maintained that it should not serve as an arbiter of speech online, with one notable exception: Cloudflare severed ties with white supremacist site the Daily Stormer two years ago.

As pressure mounted in the wake of the El Paso shooting, Cloudflare at first maintained that it would not drop 8chan. But by Sunday evening, the company had reversed course.

""8chan has been on our radar for a long time as a problematic user,"" says Prince. ""But we have a responsibility, which is much beyond ‘We terminate sites we don’t like.’ I’m nervous about whether we’ve made the right decision, and I’m nervous about how this will set precedent in the future.""

Prince argues that, rather than Cloudflare, platforms like Facebook, YouTube, and Twitter should decide what belongs on their own sites. Cloudflare shouldn't make those calls any more than asphalt should set speed limits. The major platforms can, and do, moderate their own content and manage violent, destructive trends themselves—even if it's been an imperfect system in practice. But Prince says that he didn't account for platforms like 8chan that are intentionally created as a forum for unregulated expression.

""When you have platforms that are effectively lawless like this, then maybe that shifts the responsibility further down the stack,"" Prince says. Looking at Daily Stormer and now 8chan, Prince says that Cloudflare is attempting to find the line where ""a site has shown repeatedly that it is causing active, real harm.""

Cloudflare gets requests from individuals, institutions, and governments worldwide to take down sites every day, though, because of their alleged real-world harm. It doesn't act on those requests precisely because that allegation is so subjective. As a result, the actual parameters for what merits a takedown remain extremely murky.

And even when Cloudflare takes the step to unilaterally cut service, nothing stops those shunned sites from buying the services they need elsewhere. The Daily Stormer, for example, was back online within a couple of days, albeit harder to find. And though Google stopped indexing the 8chan homepage back in 2015, taking some wind out of its sails, the site has still managed to become a go-to for hate speech of all sorts and, of late, mass shooters spreading their extremist propaganda.

""Some of you might’ve read the @Cloudflare news already,"" a Twitter account associated with the site posted on Sunday evening. ""There might be some downtime in the next 24-48 hours while we find a solution.""

""It solves the problem for us, but it doesn’t solve the problem for the internet."" Matthew Prince, Cloudflare CEO

Even if 8chan itself somehow never reemerges, which seems unlikely, discussions like those hosted on the forum can easily move to another platform. 8chan itself grew out of the forum 4chan in 2013. Meanwhile, the anything-goes social network Gab went down temporarily last fall, because its providers—including the domain registrar GoDaddy and web host Joyent—dropped it after the alleged shooter in the Pittsburgh synagogue shooting made threats on the social network. But Gab came back as soon as other companies stepped in to provide services. Cloudflare stuck with Gab throughout.

Infrastructure providers have the power to let a site go down, and maybe even do it irreparable damage in the process. But the internet was built to be decentralized precisely to protect speech, so regardless of what you think of the precedent Cloudflare is setting, 8chan will likely be back soon, something Prince readily acknowledges.

""The real thing that kills me is that, at the end of the day, [dropping 8chan] doesn’t actually fix the problem,"" Prince says. ""It solves the problem for us, but it doesn’t solve the problem for the internet. It doesn't address the core causes of why hate festers online.""

UPDATE 8/5/209 10:15 am EST: This story has been updated to reflect that Google removed the 8chan homepage from search results in 2015, but still indexes certain other 8chan pages.

More Great WIRED Stories",,"Platforms like Facebook and Twitter now serve as hosts for a wide variety of extremist content with little to no regulation. This has led to a situation where, for example, the Christchurch shooter was able to livestream his attack on Facebook, and the El Paso shooter was able to post his manifesto on 8chan. While Cloudflare's decision to drop",https://www.wired.com/story/cloudflare-8chan-support-ddos/,2019-08-05 04:28:21.437000+00:00,LABEL_1_relevant,0.9951393604278564,Social Media,social_media,Wired,"Information, Discourse & Governance"
31,Facebook admits that the internet can be bad for democracy,"The company now says social media may hurt civic discourse, but it promises to do its “moral duty” by understanding how to mitigate such problems.

What it says: Of the thousands of words published in a pair of new essays on the topic, the choicest cut comes from Facebook’s product manager for civic engagement, Samidh Chakrabarti:

If there’s one fundamental truth about social media’s impact on democracy it’s that it amplifies human intent— both good and bad.I wish I could guarantee that the positives are destined to outweigh the negatives, but I can’t.

Why it matters: It’s another sign, along with a commitment to build a “more meaningful” social network, that Facebook is responding to criticism about fake news, Russian meddling, and addictive content.

Now what? Chakrabarti says the company has a “moral duty to understand how [its] technologies are being used and what can be done to make … Facebook as representative, civil and trustworthy as possible.”",Yes,"It amplifies human intent— both good and bad. Facebook has a moral duty to understand how its technologies are being used and what can be done to make Facebook as representative, civil and trustworthy as possible.",https://www.technologyreview.com/2018/01/22/146091/facebook-admits-that-the-internet-can-be-bad-for-democracy/,2018-01-22 00:00:00,LABEL_1_relevant,0.9959419369697572,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
32,2018 Was a Rough Year for Truth Online,"Earlier this month, I was on the phone with Ryan Fox, cofounder of New Knowledge, a cybersecurity firm that tracks Russian-related influence operations online. The so-called Yellow Vest protests had spread across France, and we were talking about the role disinformation played in the galvanizing French hashtag for the protests, #giletsjaunes. Conversations like these are a regular part of my job and usually focus on the quantifiable aspects of social media manipulation campaigns—volume of posts, follower count, common keywords, signs of inauthenticity, that sort of thing. But something else creeped into our discussion, an immeasurable notion so distracting and polarizing for most in the disinformation research community that I learned long ago to stop bringing it up: What is the impact of these misinformation campaigns?

While I didn't ask this question of Fox, he addressed it as though I had: “We get this question a lot: Did they cause this? [Meaning, the gilets jaunes protests.] Did they make it worse? They're pouring fuel on the fire, yes. They are successful at exacerbating the narrative. But I don't know what the world would look like had they not done it.”

Oft asked and rarely satisfactorily answered, the question of impact is the disinformation research community’s white whale. You can measure reach, you can measure engagement, but there’s no simple data point to tell you how one coordinated influence campaign affected an event or someone’s outlook on a particular issue.

There has never been a more exciting or high-stakes time to study or report on social media manipulation, yet therein lies the issue. It’s difficult to balance the urge to report complicated and impressive analyses of large swaths of data from propaganda-pushing networks with the responsibility to hedge your findings behind the seemingly nullifying admission that there is no way to truly understand the actual effect of these actions. Especially when much of the discourse on the subject is plagued by inaccuracies and exaggerations, often caused by media efforts to simplify pages of nuanced research into something that fits in a headline. Coordinated influence campaigns are reduced to “bots” and “trolls,” despite the fact that those are rarely, if ever, accurate descriptions of what’s actually going on.

The internet has always been awash with misinformation and hate, but never has it felt so inescapable and overwhelming as it did this year. From Facebook’s role in fanning the flames of ethnic cleansing in Myanmar to the rise of QAnon to the so-called migrant caravan to the influence campaign conducted by the Kremlin’s Internet Research Agency, 2018 was a rough year to be online, regardless of the strength of your media literacy skills.

Want more? Read all of WIRED’s year-end coverage

It has become increasingly difficult to parse the real from the fake, and even harder to determine the effect of it all. On December 17, cybersecurity firm New Knowledge released a report on the IRA's campaign to sow division and influence American voters on Twitter, Facebook, and other platforms. It's one of the most thorough analyses of the IRA’s misdeeds to take place outside of the companies themselves. At the behest of the Senate Intelligence Committee, New Knowledge reviewed more than 61,500 unique Facebook posts, 10.4 million tweets, 1,100 YouTube videos, and 116,000 Instagram posts, all published between 2015 and 2017. But even with that mountain of data, the researchers were unable to reach concrete conclusions about impact.

“It is impossible to gauge the full impact that the IRA’s influence operations had without further information from the platforms,” the authors wrote. New Knowledge said that Facebook, Twitter, and Google could provide an assessment of what users who were targeted by the IRA thought of the content they were exposed to.

This is a significant claim, but the researchers say the platforms could study the activities of the victims of information warfare rather than the perpetrators, and ask: What were users saying in the comments of voter suppression attempts on Instagram? What conversations were happening between IRA members and users in DMs? Where did users go on the platform, and what did they do after being exposed to IRA content? But the platforms failed to turn any of this information over. This is particularly problematic, the researchers said, because “foreign manipulation of American elections on social platforms will continue to be an ongoing, chronic problem,” and by keeping people in the dark about the effectiveness of old tactics—which have almost certainly been improved upon in the years since—platforms leave users vulnerable to any future attempts.",Yes,"We can no longer have a shared reality. Instead, we have competing narratives.",https://www.wired.com/story/2018-was-a-rough-year-for-truth-online/,2018-12-26 12:00:00+00:00,LABEL_1_relevant,0.9928119778633118,Social Media,social_media,Wired,"Information, Discourse & Governance"
33,Trump's TikTok Drama Is a Distraction,"Millions of young Americans were sent into a panic last weekend when President Donald Trump told reporters that he was “banning” TikTok from the United States. White House officials had already been discussing taking action against the social media platform and other Chinese-owned apps over national security concerns. But as is typical for Trump, it’s not exactly clear what, if anything, will happen next. On Sunday, Microsoft released a statement saying it had spoken with the president and planned to continue discussions about buying TikTok from its parent company, ByteDance, by September 15. For now, TikTok is still available in the US.

The episode is just the latest flare-up in the ongoing deterioration of relations between the US and China. Whether TikTok is sold or banned, the underlying security concerns it raises—about privacy, espionage, foreign manipulation and propaganda, human rights and civil liberties—are going to remain. In that respect, the drama unfolding around TikTok risks overshadowing larger questions about the future of the world’s two biggest superpowers. “I just think the notion that TikTok is the big issue in US–China relations is silliness, and I think it distracts from very important issues,” says Graham Webster, the editor in chief of the DigiChina Project, a collaboration between Stanford University and New America.

One of the biggest worries about TikTok is whether the Chinese government could compel ByteDance to hand over user data on Americans. TikTok has repeatedly said it wouldn’t share information with the Communist Party even if asked, but the possibility can’t be dismissed. At the same time, focusing on TikTok alone ignores a perhaps even more troubling reality: The United States doesn’t have robust data protection rules in place for any company. “Countries around the world are working on that type of regime, and the US is frankly not,” says Webster. After Facebook’s Cambridge Analytica scandal, Congress briefly appeared interested in passing federal privacy legislation, but that effort seems to have stalled. It shouldn’t have: As more Americans work and socialize online, they’re creating even more personal data. Already, the US government says Chinese hackers have pilfered poorly protected information from a number of American institutions, including government agencies and a major credit bureau.

Another concern is whether the Chinese government could turn TikTok into a vector for propaganda. Not that they need a locally owned app to do that: The Chinese Communist Party has already been accused of launching disinformation campaigns on Facebook and Twitter targeting pro-democracy activists in Hong Kong. But because the algorithms powering TikTok are still largely opaque and its parent company is in Beijing, the fear is that US officials wouldn’t catch state-sponsored manipulation until it was too late, as was the case when the Russian government interfered in the 2016 presidential election. Social media companies also haven’t historically been eager to grant outside researchers or observers access to their technologies. TikTok has announced it is opening a transparency center in Los Angeles, where experts can see in person how it moderates content. And on Monday, the company said it was making it easier for users to report election misinformation and will expand partnerships with fact-checking organizations.

“The notion that TikTok is the big issue in US–China relations is silliness, and I think it distracts from very important issues.” Graham Webster, DigiChina Project

TikTok has over 100 million users in the US, but there are much larger, and potentially more serious, targets for foreign adversaries in the country. That includes the industrial infrastructure powering electric grids and water systems, for example, and experts say those areas could use the same kind of attention now being paid to a single app. “Industrial control systems is where you will see the most sophisticated, well-financed nation-state attacks first,” says Monta Elkins, “hacker in chief” at the security firm FoxGuard Solutions and an instructor at the SANS Institute. Elkins says there’s a shortage of professionals trained to work in industrial control systems and that some of the equipment is decades old. “Right now, we don’t run antivirus on these machines. They’re computers running software that might be vulnerable,” he says.

Bolstering national security and combating China’s authoritarian regime is also, of course, about more than just cybersecurity. TikTok’s critics have said Chinese-owned apps can’t be trusted because of the country’s terrible track record on human rights, particularly its oppression of Uighur Muslims in Xinjiang and its treatment of Hong Kong. At the same time, the US government has come under fire for failing to adequately address these abuses—and taking action on a social media app would do nothing to change that.

US officials have placed sanctions on companies who supply the surveillance technology used in Xinjiang, and Congress recently passed the historic Uyghur Human Rights Policy Act, which Trump signed. But the president also said he declined to sanction Chinese officials because of trade talks. John Bolton, Trump’s former national security adviser, claims Trump told Chinese leader Xi Xingping last year that building concentration camps for Uighurs was the right thing to do. Experts note that the US doesn’t have an asylum program for China’s Muslim minorities, nor nonpartisan funding for things like cultural education. “If we want to take the cultural genocide claim seriously, the way to do that is to promote Uighur heritage,” says Darren Byler, a postdoctoral fellow at the Center for Asian Studies at the University of Colorado.",Yes,It can be a tool for foreign adversaries to manipulate Americans. The best way to combat this is for the US to have robust data protection rules in place for all companies.,https://www.wired.com/story/trump-tiktok-drama-security-distraction/,2020-08-05 19:05:05.639000+00:00,LABEL_1_relevant,0.9958329796791076,Social Media,social_media,Wired,Security & Privacy
34,Twitter Blue Is for People Who Love Reading the News,"Should you get Twitter Blue? That depends on whether you consider yourself a “power user.” The platform’s new subscription service, which costs $3 per month, comes with a suite of requested features: bookmark folders for organizing saved tweets, a “reading mode” that declutters long threads, and a (sort of) edit button, good for 30 seconds of revisions after a tweet is sent. It also comes with ad-free access to articles from a number of journalism outfits, like The Washington Post, Rolling Stone, and The Atlantic.

Compared to the number of third-party apps that give Twitter more brawn—plug-ins that help you auto-delete your tweets or search within the tweets you’ve liked—Twitter Blue feels a little overwhelming. But considering it’s the company’s first-ever paid product, Twitter has chosen to go narrow, catering to a small group of people by giving them exactly the features they want.

Photograph: Twitter

Twitter Blue doesn’t get rid of ads in a user’s feed or change much about the tweeting experience itself. But it is part of the company’s plan to diversify its revenue beyond advertising and become more than a place to share thoughts in 280 characters. In January, Twitter acquired the newsletter company Revue and Breaker, an audio social service, to build up new product offerings around newsletter subscription and audio chat. In March, it acquired Scroll, an ad-free news reader.

People who liked Scroll will find a lot to love about Twitter Blue. It’s almost exactly the same service, but less costly: Scroll charged $5 per month. Part of the Twitter Blue subscription fees will go directly back to the publishers it partners with. Twitter did not disclose how much but wrote in a blog post that “our goal is to help each publishing partner make 50 percent more per person than they would’ve made from serving ads to that person.”

Photograph: Twitter

Twitter seems to be defining its “power users” as people who consume a lot of news or primarily use the platform for sharing and reading articles. For those people, Twitter Blue certainly is a nicer reading experience. It’s convenient to browse the day’s top articles and then click into them without ads, which also makes them load faster. And Twitter threads look fantastic in “reading mode,” a design that emphasizes the text rather than the replies. On the other hand, “reading mode” makes it harder to see the full scope of a conversation, which used to be the whole point of Twitter.",Yes,The way it isolates users and creates echo chambers.,https://www.wired.com/story/twitter-blue-is-not-for-everyone/,2021-11-13 12:00:00+00:00,LABEL_1_relevant,0.9938982725143432,Social Media,social_media,Wired,"Information, Discourse & Governance"
35,Young politicians hustle to keep followers under House social media ...,"Today, Rep. Alexandria Ocasio-Cortez’s (D-NY) Instagram followers got an unexpected message from the newly elected congresswoman — she created a new Instagram account for official government business.

Ocasio-Cortez’s announcement came on her personal Instagram account, @ocasio2018, on Thursday by way of an Insta-story. “House Rules say I can’t post IG stories from my personal account while I’m in my office,” she wrote on a screenshot from her new official account. “So I’ll post on-the-job IG stories from @repocasiocortez.”

This isn’t the first official account she’s created since she took her oath of office. Ocasio-Cortez also has a new Twitter account (@RepAOC) that was created last week for official government business.

She’s not the only lawmaker juggling multiple accounts. Members of Congress were sworn into office on January 3rd and many freshman have already set up new social media accounts, starting fresh and rebuilding the massive followings they created on the campaign trail. These official accounts must clearly state in their bios that the person it represents is a member of Congress, according to the longstanding guidelines in the Members’ Congressional Handbook. Many of the social media darlings from the past election, like Ocasio-Cortez and Rep. Dan Crenshaw (R-TX), have tweeted out messages citing these ethics rules, asking for fans to follow them to new accounts as they embark on their first year under the Capitol dome.

Well I guess I am back to having no followers. Thanks Ethics Rules. #myfirstTweet #retweetthis #followme — Rep. Dan Crenshaw (@RepDanCrenshaw) January 4, 2019

The Members’ Congressional Handbook doesn’t explicitly say that lawmakers are required to make new accounts, but in most cases it’s easier to separate their government resources and personal ones in order to avoid ethics violations. The rules do prohibit lawmakers from using any of their newfound government resources to maintain their personal accounts, whether that be their new staff or office funds.

You can typically tell the distinction between an official and personal congressional account by the handle. A member’s title like “Rep.” or “Sen.” usually sits before their name on the government-maintained account. For example, Sen. Ben Sasse’s (R-NE) personal account is just his name, @bensasse, and his official account is @SenSasse. Congressional staff members are only allowed to post onto the official account, which is strictly for government business. Anything having to do with re-election or grassroots lobbying must stay on the personal accounts.

So far, Reps. Ocasio-Cortez and Crenshaw haven’t suffered too much under the rules. Crenshaw’s official congressional Twitter account has already acquired about half of his formerly 243,000 followers. Ocasio-Cortez still posts from her personal account to over 2 million followers, @AOC, and seems to let her staff handle the content on her official one (@RepAOC) with just over 40,000 followers. Her new Instagram was launched today and already has over 66,000 followers, a fraction of the 1.7 million on her personal account.

Facebook and Twitter have been standard platforms for politicians looking to build followings, but throughout the past few months, lawmakers have made headlines for their use of Instagram. Most lawmakers have an official Instagram account, but the bulk of the viral content and memes are generated from their personal accounts. Sen. Elizabeth Warren (D-MA) recently cracked open a beer with her husband on her personal account, but sticks to photos with constituents and policy talk on her official one. Beto O’Rourke caused a stir by interviewing his dental hygienist on his personal account, but if he were still in the House, he’d be expected to keep it off of his government page.

But when it comes to the presence congressional committees have on social media, there’s little guidance for members and their staff. The House Administration says that any committee website or social media account “must be recognizably derivative or representative of the name of the Committee sponsoring the Website,” and that the names of these pages cannot be deceptive, slogans, or imply that the body endorses any commercial products.

These rules were confusing for some people on Twitter last week after bodies like the House Science Committee switched up their handles. What was previously @HouseScience, was changed by Republican staff to @HouseScienceGOP.

Followers of the account, who expected it to represent the entire committee, were tweeting out messages suggesting that the Republicans had stolen the account or refused to hand it over to the Democrats, who are now in the majority. “The outgoing committee switched the name of the account instead of handing it over to the Dems, taking all the followers with them,” one user wrote. But that wasn’t the case. The Republicans had always owned the account in question, but only changed the name to properly identify their new status as the minority, according to a Democratic spokesperson for the committee.

If the House Administration rules don’t provide guidance, committees make their own rules, and so long as both parties are in agreement, Twitter helps make any changes necessary as new congressional sessions begin.

Updated 1/10/19 at 4:50 p.m. EST: Corrected official government Twitter handle for Rep. Alexandria Ocasio-Cortez.

Updated 1/11/19 at 4:27 p.m. EST: Updated to change which party the spokesperson represented.",Yes,"It pulls people away from real-world social interaction.

This can have a number of negative effects, including:

-Isolation and loneliness
-Depression
-Anxiety
-Poor sleep
-Weight gain
-Negative impact on relationships",https://www.theverge.com/2019/1/10/18176814/ocasio-cortez-instagram-congress-social-media-rules,2019-01-10 00:00:00,LABEL_1_relevant,0.9955256581306458,Social Media,social_media,The Verge,Social Norms & Relationships
36,Facebook Tamed the Feral Internet—and That’s Too Bad,"I have zero claim to l337ism but I’ve been around this strange old internet for the past 40 years. Grandma, please no stories about the early internet. I get it. But in honor of Facebook’s 15th birthday, I want to reminisce, just for a minute.

The internet of the ’80s seemed like an infinitely weird Cabinet of Wonders, filled with goth and carny stuff: skulls, fetuses in formaldehyde, half-working contraptions that made ascii art. The docents in the joint were backroom clerics, the odd refugee from CB culture (literally “breaker, breaker”), and wallflowers hoping for a new shot at social life. I was that last type. It was deeply embarrassing to be known as a computer geek; you hid it like eczema. Still, at nine, I started encountering the clerics—some of them actual “clerics,” as D&D was everywhere—and getting in the kind of preteen and then teen trouble that involved mostly trickery, masquerade, and multiplayer roleplaying. There were also legitimate wonders. Seeing a whole universe form before one’s very eyes, for one, to say nothing about the birth of a new life form: Avatars.

Virginia Heffernan (@page88) is an Ideas contributor at WIRED. She is the author of Magic and Loss: The Internet as Art, a cohost of Trumpcast, an op-ed columnist at the Los Angeles Times, and a frequent contributor to Politico.

Now I’m really going to go full kids these days. Because: Facebook steamrolled everything. AOL and CompuServe had sat like circus makeup on top of a bunch of tangled and bottomless code. The ISPs tried with all the powers of middlebrow marketing to make something user-friendly that wasn’t friendly at all—a half-orc painted up to look like a Disney character. Still, AOL and CompuServe deserve credit: The first wave of the digital middle class poured in through the Mall of America–style UX, and AOL’s and CompuServe’s illusion of order gentrified things. By my twenties, I was ready for some gentrification. I couldn’t spend my life torrenting lawlessly in some open source drug den; CompuServe now suited me. I was 773.143@compuserve.com—another anonymous handle. But 2004 came. And Facebook wrecked the specialness of those wilder days.

First came the rumbling from Harvard, of all places. Shortly after, as it spread to all colleges and then outside dot-edu, some Ohio University kids set up an account for me. With trepidation I entered in my real name, first and last, the one on my Social Security card and paychecks and permanent record. That was the red pill. I swallowed hard.

It seemed that no sooner than I’d hit the last “n,” an imperial fleet of earth-moving armored bulldozers come to churn up the old digital townships—the websites, eccentric blogs, and zines like WIRED’s own Suck.com we all loved. (You can find remnants of the old haunts on Craigslist, Match.com, and Wikipedia—communitarian places that, all things considered, still operate in good faith.) Razed, our settlements were paved over. Thousands, then millions, then billions of identical structures erected. Blue blue blue blue. Bank blue. Sterile blue that sowed the fields with salt. The Jane Jacobs charm and power of the tunnels, community gardens, chapels, and dens of iniquity around the edges were ... dead. No more would esoteric stunts like “I Can Haz Cheezburger” or Coke & Mentos sweep the nation.

Of course, though it seemed swift, it took Facebook eight or 10 years to really choke the life off the old internet. Suck.com folded in earnest in 2015 (having not had new content since 2011). After all, Facebook addressed itself to something that wasn’t a problem: the occasional incoherence of the Web 1.0 villages. In Japan, according to sociologist Con Isshow, the internet was always seen as rich, thick soil for the young—its generative chaos vitally necessary for the development of jungly imaginations. Teenagers and twentysomethings could flee the stultifying stratification of Japanese society in favor of roleplaying where they could experiment with identities: heroic, gender-bending, scholarly, angry, romantic. Above all they could improvise in a symbolic, cerebral order that would save them the life-destroying consequences of traditional experiments: drugs, cutting, self-starvation, violence, cults, unwanted and unsafe sex. Then Facebook came and demanded we give our names, locations, family photos, political opinions, consumer preferences. This wasn’t exploration. This was crowd control. The better to fumigate us, stratify us, monitor us, and take us for all the data we had.",Yes,"Everyone, but especially the young, are much less free to be creative and imaginative online.",https://www.wired.com/story/facebook-tamed-the-feral-internetand-thats-too-bad/,2019-02-22 13:00:00+00:00,LABEL_1_relevant,0.9937731623649596,Social Media,social_media,Wired,"Information, Discourse & Governance"
37,The Facebook Fallacy,"At the same time, network technology allows advertisers to more precisely locate and assemble audiences outside of branded channels. Instead of having to go to CNN for your audience, a generic CNN-like audience can be assembled outside CNN’s walls and without the CNN-brand markup. This has resulted in the now famous and cruelly accurate formulation that $10 of offline advertising becomes $1 online.

I don’t know anyone in the ad-­supported Web business who isn’t engaged in a relentless, demoralizing, no-exit operation to realign costs with falling per-user revenues, or who isn’t manically inflating traffic to compensate for ever-lower per-user value.

Facebook has convinced large numbers of otherwise intelligent people that the magic of the medium will reinvent advertising in a heretofore unimaginably profitable way, or that the company will create something new that isn’t advertising, which will produce even more wonderful profits. But because its stock has been trading at about 40 times its expected earnings for the next year, these innovations will have to be something like alchemy to make the company worth its sticker price. For comparison, Google has been trading at a forward P/E ratio of around 11. (To gauge how much faith investors have that Google, Facebook, and other Web companies will extract value from their users, see this graphic.)

Facebook currently derives 82 percent of its revenue from advertising. Most of that is the desultory, ticky-tacky display advertising that litters the right side of people’s Facebook profiles. Some is a kind of social marketing: a user chooses to “like” a product, which is supposed to further social relationships with companies. The social network sells its ads by valuing various combinations of the cost of a thousand ad impressions (or CPM) and the cost of a click (CPC). Both forms of ads are more or less coarsely targeted to users on the basis of information they’ve volunteered to provide to Facebook and the sharing or “liking” of media within Facebook’s universe. General Motors recently announced it would no longer buy any kind of Facebook ad.

Facebook’s answer to its critics is: Pay no attention to the carping. Sure, grunt-like advertising produces the overwhelming portion of our $4 billion in revenues, and yes, on a per-user basis, these revenues are in decline. But this stuff is really not what we have in mind. Just wait.

It’s quite a juxtaposition of realities. On the one hand, Facebook is under the same relentless downward pressure as other Web-based media. The company’s revenue amounts to a pitiful $5 per customer per year, which puts it ahead of the Huffington Post but somewhat behind the New York Times’ digital business. (Here’s the heartbreaking truth about the difference between new media and old: even in the New York Times’ declining traditional business, a subscriber is still worth more than $1,000 a year.) Facebook’s business grows only on the unsustainable basis that it can add new customers at a faster rate than the price of advertising declines. It is peddling as fast as it can. And the present scenario gets much worse as people increasingly interact with the social service on mobile devices, because on a small screen it is vastly harder to sell ads and monetize users.

On the other hand, Facebook is, everyone has come to agree, profoundly different from the Web. First of all, it exerts a new level of hegemonic control over users’ experiences. And it has its vast scale: 900 million, soon a billion, eventually two billion people. (One of the problems with the logic of constant growth at this scale and speed is that eventually Facebook will run out of humans with computers or smart phones.) And then it is social. Facebook has, in some yet-to-be-defined way, redefined something. Relationships? Media? Communications? Communities? Something big, anyway.

The sweeping, basic, transformative, and simple way to connect buyer to seller and get out of the way eludes Facebook. It has to sell its audience like every humper on Madison Avenue.

The subtext—an overt subtext—of the popular account of Facebook is that the network has a proprietary claim to and special insight into social behavior. For enterprises and advertising agencies, it is therefore the bridge to new modes of human connection. Expressed so baldly, this account is hardly different from what was claimed for the companies most aggressively boosted during the dot-com boom. But there is, in fact, one company that created and harnessed a transformation in behavior and business: Google. Facebook could be, or in many people’s eyes should be, something similar. Lost in such analysis is the failure to describe the application that will drive revenues.

Google is an incredibly efficient system for placing ads. In a disintermediated advertising market, the company has turned itself into the last and ultimate middleman. On its own site, it controls the space where a buyer searches for a thing and where a seller hawks that thing (AdWords, its keywords advertising network). Google is also the cheapest, most efficient way to place ads anywhere else on the Web (through the AdSense network). It’s not a media company in any traditional sense; it’s a facilitator. It can eliminate the whole laborious, numbing process of selling advertising space: if a marketer wants to place an ad (that is, if it is already convinced it must advertise), the company calls Mr. Google.

And that’s Facebook’s hope, too: it wants to be a facilitator, the inevitable conduit at the center of the world’s commerce.

Facebook has the scale, the platform, and the brand to be the new Google. It lacks only the big idea. Right now, it doesn’t actually know how to embed its usefulness into world commerce (or even, really, what its usefulness is).

But Google didn’t have the big idea at its founding, either. The search engine borrowed the concept of AdWords from Yahoo’s Overture network (a lawsuit for patent infringement and a settlement followed). Now Google has all the money in the world to buy or license the ideas that could make its platform and brand pay off.

What might Facebook’s big idea look like? Well, it does have all this data. The company knows so much about so many people that its executives are sure the knowledge must have value (see our feature “What Facebook Knows”).

If you’re inside the Facebook galaxy—a constellation that includes an ever-­expanding cloud of associated ventures—there is endless chatter about a near-utopian new medium for marketing. Round and round goes the conversation: “If we just … if only … when we will …” If, for instance, frequent-flier programs and travel destinations actually knew when you were thinking about planning a trip … If a marketer could identify the person who has the most influence on you … If an advertiser could introduce you to someone who would relay the advertising message … Get it? No ads, just friends! My God!

But so far the sweeping, basic, transformative, and simple way to connect buyer to seller and get out of the way eludes Facebook.

So the social network is left in the same position as all other media companies. Instead of being inevitable and unavoidable, it has to sell its audience like every humper on Madison Avenue.

But that’s what Facebook is doing: selling individual ads. If you consider only its revenue, it’s an ad-sales business, not a technology company. To meet expectations—the expectations that took it public at $100 billion—it has to sell at near hyperspeed.

The growth of its user base and its ever-swelling page views mean an almost infinite inventory to sell. But the expanding supply, together with equivocal demand, results in ever-lowering prices. The math is sickeningly inevitable. Absent that earthshaking idea, Facebook will look forward to slowing or declining growth in a tapped-out market, and ever-falling ad rates, both on the Web and (especially) in mobile applications. Facebook isn’t Google; it’s Yahoo or AOL.

Oh, yes … in its Herculean efforts to maintain its overall growth, Facebook will force the rest of the ad-driven Web to lower its prices, too. The low-level panic the owners of every mass-traffic website feel about the ever-downward movement of their CPM is turning to dread. Last quarter, some big sites observed as much as a 25 percent decrease, following Facebook’s own attempt to book more revenue.

You see where this is going. As Facebook gluts an already glutted market, the fallacy of the Web as a profitable ad medium will become hard to ignore. The crash will come. And Facebook—that putative transformer of worlds, which is, in reality, only an ad-driven site—will fall with everybody else.

Michael Wolff writes a column on media for the Guardian; is a contributing editor to Vanity Fair; founded Newser; and was, until October of last year, the editor of AdWeek.

This article was revised on June 16, 2012.",Yes,It has become a vehicle for advertisers to more precisely locate and assemble audiences outside of branded channels.,https://www.technologyreview.com/2012/05/22/255726/the-facebook-fallacy/,2012-05-22 00:00:00,LABEL_1_relevant,0.9096501469612122,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
38,UK Group Threatens to Sue Facebook Over Cambridge Analytica,"Lawyers for a group of UK residents whose Facebook data was harvested by Cambridge Analytica are now threatening to sue for damages. In a 27-page letter served to the company Tuesday, they accuse Facebook of violating British data privacy regulations. The letter before claim, as it's called, is the first step in the UK's legal process for filing a class action suit. It warns Facebook that if it does not adequately respond to a list of questions regarding user privacy within 14 days, the claimants may take legal action against the company in the United States, the United Kingdom, and Ireland. Nearly 1.1 million British citizens could be eligible to join such a suit if it goes forward.

The warning comes from the UK-based law firm Irvine Thanvi Natas Solicitors, which is representing dozens of people who argue that Facebook misused their personal data in violation of UK law. It follows an announcement Monday by separate group, called the Fair Vote Project, that is also launching a class action suit against Facebook in the UK.

The UK Information Commissioner’s Office already said earlier this month that it intends to fine Facebook more than $600,000 under the country's Data Protection Act for allowing Cambridge Analytica, the now-defunct political consulting firm, to collect information on tens of millions of users without their knowledge. But Ravi Naik, a lawyer with Irvine Thanvi Natas Solicitors, says the individuals affected also have a right to answers, and may have a right to damages if those answers aren’t satisfactory.

""People should realize that data rights are real rights, and we have a mechanism to enforce them,"" Naik says.

WIRED reviewed a copy of the letter Naik submitted to Facebook Tuesday. It reads like a checklist of Facebook's failures, dating back to its decision in 2009 to no longer allow users to keep their friend lists private. The letter also notes Facebook's decision to let app developers scoop up data not just on their apps' own users but on those users' friends. Facebook didn't put a stop to this practice until 2015. By then, Cambridge Analytica had already gained access to as many as 87 million Facebook users' data through a personality quiz app designed by a University of Cambridge researcher named Aleksandr Kogan.

These original sins, the letter argues, opened Facebook up to legal liability from several angles. For starters, the UK's Data Protection Act requires companies to get user consent to process their data. Naik says that Facebook essentially misled users about what they were consenting to. In Facebook's privacy settings at the time, users could determine whether to share their posts with ""only friends"" or ""friends of friends,"" but this only applied to what other Facebook users could see. App developers had their own permissions, which, in some cases, gave them the ability to tap into their users' friends' data.

This broad allowance for app developers also triggered regulators in the United States to investigate Facebook for deceptive practices in 2011. At the time, the US Federal Trade Commission wrote that while Facebook had given users the impression they could share information only with their friends, those privacy restrictions ""would be ineffective as to certain third parties."" The FTC argued Facebook failed to make that clear to users. It also considered Facebook's 2009 decision to stop letting users make their friend lists private to be an ""unfair and deceptive"" practice. Facebook settled the matter by signing a consent decree with the FTC in 2011, and vowed not to mislead users about their privacy. After reports about Cambridge Analytica surfaced this spring, the FTC announced it was investigating whether Facebook broke that promise, a violation that could come with hefty fines.",Yes,"Private data is being harvested and used without the user’s knowledge or consent. This has led to a loss of privacy for users and has raised concerns about how this data is being used.

The UK’s Information Commissioner’s Office is investigating Facebook for allowing Cambridge Analytica to harvest data from users without their consent",https://www.wired.com/story/uk-group-threatens-to-sue-facebook-over-cambridge-analytica/,2018-07-31 09:30:00+00:00,LABEL_1_relevant,0.9967650175094604,Social Media,social_media,Wired,Security & Privacy
39,“He put QR-coded wristbands on each of the chickens”,"So how do you keep people in the countryside but at the same time give them economic opportunities, especially because farming is not an easy job, and increasingly the younger generation doesn’t want to be stuck doing manual labor in the field? The national government is entranced with some of the same shiny keywords as policymakers everywhere—“e-commerce” “blockchain,” “AI”—so it’s supported a lot of initiatives by small privatized companies that employ those technologies.

Q: One of these inspired your book’s title. Can you explain what makes a “blockchain chicken farm”?

A: It’s a small farm in rural Guizhou where the farmer had been raising free-range chickens for quite a long time, but he couldn’t convince people that they were actually free-range. Then a Shanghai tech company came along and said, “Blockchain is the solution!” They worked with the farmer and he put QR-coded wristbands on each of the chickens, so that they could be surveilled by cameras to prove that they were truly free-range and never tampered with.

Q: You also cover AI machine-­learning models like Alibaba’s “ET Agricultural Brain,” which became a tool for combating African swine fever (ASF) in pigs during a disastrous outbreak that began in 2018.

A: I was frustrated because much media coverage of the ASF outbreak focused on how Alibaba was doing this heroic thing to save all these pigs and guarantee food safety using AI to monitor the herds via video, temperature, and sound sensors. In fact, for decades now, there’s been a push to industrialize hog farming in China, and these technologies were an attempt to produce even more pigs at an unprecedented scale. These industrial farms and increased pressure for output set up the conditions for epidemics like swine fever in the first place. What are the paradoxes it reveals?

Q: What’s an example of a farming technology that is taking a different path?

A: Like many countries, China is a place where the government tried to modernize agriculture by using pesticides and fertilizers as the “scientific” way of farming the land. In one rice-farming village in Guangdong province, farmers did that and over time noticed that their soil was just not as fertile anymore, that they had to keep using more and more fertilizer. This led to the Rice Harmony Collective, which revived traditional techniques like “rice duck fish” agriculture, where fish and ducks in the rice paddy act as a natural pesticide. They also introduced a lottery system for rice paddy location that shifts each season, so that farmers have a greater incentive to follow these organic rules.

Q: The coronavirus pandemic emerged when your book was in production. Do you feel it underscores your themes?

A: I had been looking at a lot of the research of Rob Wallace, an epidemiologist who studies factory farming and zoonotic diseases—which is not to say that all these are coming out of factory farms, but just these profit-driven practices that push humans into previously wild habitats. There’s obviously been a huge acceleration of zoonotic disease.

In the pandemic we’ve all realized that decoupling with China would be hard—we rely on China for so many things. Just the process of setting up a factory; the material supplies, training, machinery; the knowledge of costs and shipping and freight and routes. It’s never been more clear that China is so interwoven with the global supply chain.

Xiaowei Wang at Urban Tilth Farm CHRISTIE HEMM KLOK

Q: What do you wish Americans understood about Chinese wet markets?

A: So, I love wet markets. They are a place where fresh food is readily available to all, and it’s an important livelihood for many people who aren’t these large supermarket chains. You’ve got the garlic lady who sells her homegrown crop at wet markets. They’re a crucial connection for local and regional farmers. They’re so common not just in China, but around the world—in Latin America and so on. I think it’s sad [that people blame covid-19 on wet markets]—and a xenophobic example of people thinking that Chinese food is somehow “dirty.” It infuriates me so much, because all the science says it [the coronavirus] probably came from outside [the market], and it probably came from a bat.

Q: What are the trends you are seeing in the countryside? Has the pandemic contributed to new ones?

A: I would say the general trend is that there’s lots of optimism. Because of the pandemic, a lot of migrant workers in cities had to go back to their rural homes, and maybe they’ll stay there and pursue other kinds of opportunities. Even before the pandemic, I observed a lot of young people thinking and talking about “Oh, maybe I can move back to my hometown” and start some kind of business that would be cheaper than in the city.

""A farmer had been raising free-range chickens but couldn’t convince people that they were actually free-range. Then a Shanghai tech company came along and said, 'Blockchain is the solution!' and put them under surveillance to prove they were truly free-range.""

I think, too, the live-­streaming economy is a weird microcosm of this—people have a notion of farming in the countryside, live-streaming it, and getting patrons. I think for urban youth, that trend is increasing. Back in 2009, when I was living in Beijing and trying to do urban gardening, no one was interested. Everybody was like, “Ugh, this is what my parents had to do. I’m not doing this, it’s gross.” But today, there’s a huge demand for organic farmers’ markets, and influencers getting into farming. I just heard about a popular beekeeping influencer, who has a whole brand and blog talking about the ancient art of beekeeping. It makes sense: the urban 20-somethings who have only known the city their entire lives, they’re under so much pressure, and so of course they’re going to romanticize the countryside.

Q: The term “Sinofuturism” refers to concepts and aesthetics of a “Chinese future.” It’s been explored by artists, designers, and thinkers in critical or celebratory ways. How do you personally interpret it?

A: For me, Sinofuturism now contains a certain imperial logic, given how China has operated more and more as an imperial power over the past few years, domestically and internationally. That said, I think there are a lot of interesting and productive parts of Sinofuturism that make us question these innate Western beliefs about the value of individualism, the role of work, the disconnection from natural cycles, the separation between mind and body, that are worth investigating. Sinofuturism is also a way to consider what exactly is this imperial force that China’s becoming, and to provoke questions around that.

Q: Even if they don’t cook them, what do you want readers to get from the Sinofuturist recipes in the book?

A: I’d love for people to say, “Hmm, I don’t have access to moon-grown cornmeal,” but to have a sense of wonder about the ingredients that are available to them, and to frame that reality as a weird form of fiction. To question “Why do we eat what we eat?” and understand how that relates to technological change. I was really inspired by a cookbook by Mary Sia, who talks about how in China you don’t get a lot of baked goods; you get a lot of boiled things, and that’s due to the fact that China simply didn’t have enough trees to cut down to generate as much heat as is needed in baking. For me that was a reminder of how what we cook is totally shaped by what is available, as a result of the technology that we use.

Q: What were some of the inspirations behind the recipes you include in the book?

A: I was seeing my Chinese herbalist, who loves to rant about Western medicine and how it doesn’t fully understand the body, and she was telling me about how the brain is not one of the 11 vital organs in Chinese medicine. It’s not essential to the system of qi. I thought it was fascinating because when I was interviewing computational neuroscientists, you know, the brain is the center of everything in Western clinical medicine. It controls your heartbeat, your lungs; it’s the center of thinking: you wouldn’t be a person without it.

My herbalist gave me some ideas on what nourishes qi, so I decided to use her sage advice in a recipe for AI porridge.",Yes.,"People becoming more and more isolated and disconnected from each other, which leads to more mental health problems.",https://www.technologyreview.com/2020/12/18/1013224/he-put-qr-coded-wristbands-on-each-of-the-chickens/,2020-12-18 00:00:00,LABEL_1_relevant,0.9910966157913208,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
40,Preventing Misinformation from Spreading through Social Media,"Reputation scoring has worked well for e-commerce sites like eBay and Amazon and could help to clean up social media reports in some situations.

Research efforts have also shown how to effectively mobilize many people on social media for a common task. In a 2009 experiment, the U.S. Defense Advanced Research Projects Agency offered $40,000 to the first team that could identify the locations of 10 large red weather balloons lofted by DARPA at undisclosed locations across the United States. The winning team, from MIT, did it in less than nine hours using an incentive structure, fueled by cash rewards, to drum up viral participation on social media. Anyone who found a single balloon would get $2,000; someone who invited that person to join the hunt would get $1,000. A similar but harder challenge, in 2012, asked teams to find specific individuals within cities within 12 hours with only a single mugshot to work with. There again, a distributed cash reward system worked best.

Verily builds on lessons from both contests. The winning mugshot team included one of Verily’s creators, computer scientist Iyad Rahwan, a graduate of MIT who is now at the Masdar Institute of Technology. “Recruiting people to join is part of the issue, but we also need to figure out how to remove false reports,” Rahwan says. “Where the balloon challenge took nine hours, we hope to facilitate the crowdsourced evaluation of multimedia evidence on individual incidents in less than nine minutes.”

The beta version of Verily will first be tested by its creators on a real-world weather disaster such as a hurricane or flood. Since such disasters come with some warning, Verily’s creators can prepare humanitarian agencies to use the platform. A piece of reported news—such as a photo of a flooded hospital circulating on Twitter—would be posted to Verily with a question: is the hospital really flooded? Users would then examine the photo for signs of authenticity and also leverage their own social networks to investigate its authenticity.

Humanitarian agencies working in the region could promote participation, as could the press and Twitter. Voters’ reputation scores would increase or decrease over time; future votes from reliable people would get increased weight. And voters would be encouraged to bring others to the site; anyone brought in by someone with a good reputation would automatically start with a higher reputation themselves.

In many ways the platform is meant to resolve a design problem inherent in sites like Reddit, adds Patrick Meier, director of innovation at the Qatar institute who is a co-creator of Verily and former director of crisis mapping at Ushahidi, the online incident reporting platform (see “Crisis Mapping Meets Check In”). “They don’t have the design to facilitate these kinds of workflows and collaboration,” he says. Verify could provide a rapid means to vet reports arising on sites like Reddit.

The other approaches are more basic. Storyful verifies videos to make sure news organizations don’t get duped by phony ones. Staffers check veracity based on clues like weather reports, the angle of the sun, and visual landmarks. And beyond the Swift River app is a larger platform aimed at letting humanitarian and other agencies manage and make sense of social media reports and other data.

Meanwhile, old-fashioned methods of finding the truth are holding up pretty well. In Boston, the marathon bombers were actually found through conventional witness reports and reviews of video surveillance camera footage at retail stores.",Yes,"It can lead to the spread of false information. To combat this, some organizations are developing platforms that allow users to verify the accuracy of reports. These platforms use reputation scoring and other methods to encourage users to participate in the verification process.",https://www.technologyreview.com/2013/04/23/16168/preventing-misinformation-from-spreading-through-social-media/,2013-04-23 00:00:00,LABEL_1_relevant,0.9954364895820618,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
41,Ninja Is Being Immortalized on a Red Bull Can,"Oh, hello there. Nice to see you. Yes, it’s once again time for Replay, WIRED’s look at the world of videogames. This week, gaming achieved a new, weird kind of crossover success, Notch became a bit less visible as Minecraft's creator, and the scourge of industry layoffs continued apace. Here we go!

Ninja Is Being Immortalized in the Classic American Art Form of the Red Bull Can

Leonardo da Vinci. Picasso. Red Bull. All the great chroniclers of the human form have had their muses, and now, Twitch streamer Ninja is among them. Well, the last one, anyway. In a sponsorship deal for the ages, Ninja, Fortnite player and friend of Drake, will be the next face to adorn cans of Red Bull energy drinks.

This is kind of hilarious, but that’s not the only reason it's newsworthy. This is the type of sponsorship deal normally reserved for athletes, movie stars, and big media franchises. And now, we've got Ninja, a dude who makes his living playing videogames for an audience. If that isn't crossover success, nothing is.

Minecraft Minimizes Mentions of Creator Notch

With Notch, the creator of Minecraft, spending his time on the internet promoting conspiracy theories and generally being kind of a jerk, it was inevitable that Microsoft, the current owners of Minecraft, would eventually want to distance their product from its creator. With the current update, they've done that—to an extent. Notch's name used to appear regularly on splash screens that played while loading, but according to reports by Kotaku, all those mentions have been removed.

Notch still appears in the credits, naturally, but that's about the only mention of him left in the game. Which is probably for the best. You have to think of the children, after all.

Electronic Arts Lays Off Some 4 Percent of Its Employees

It never ends. More downsizing hit the games industry this week as Electronic Arts announced that it's laying off about 350 of its 9,000 workers worldwide. According to an open letter from CEO Andrew Wilson, the layoffs are part of an effort to produce high-quality content and to ratchet down the company's involvement in Japan and Russia.

Adding this to the layoffs from Activision Blizzard, ArenaNet, and others, the amount of displaced game workers from this year alone adds up to well over a thousand. I don't really have anything pithy to add to this. That's just … a lot of people. Too many.

Recommendation of the Week: Wargroove on Nintendo Switch, Xbox One, and PC

Wargroove is by far the cutest tactics game. Taking a grid-based approach to strategic warfare reminiscent of Fire Emblem, it's all bright colors and quirky units, including a warrior general dog and cute cavalry units that are, uh, also dogs. There are a lot of dogs in this game. And plant monsters, and vampires, and heroes, and villains of all stripes. The tactical combat is crisp and challenging, and the whole experience is just broadly pleasing. If you're into this sort of game and can't wait for a new Fire Emblem to drop later this year, give this a shake.

More Great WIRED Stories",Please provide your answer in the comments below.,"People are more stupid than they used to be. Social media has also led to the rise of a strange kind of anti-intellectualism. And that’s because social media is a reflection of our own behavior. Social media isn’t just a conduit for our own ideas and feelings, it’s also a mirror that",https://www.wired.com/story/ninja-red-bull-can/,2019-03-29 13:00:00+00:00,LABEL_1_relevant,0.9964184761047364,Social Media,social_media,Wired,Social Norms & Relationships
42,Key Takeaways From the UK's Massive Facebook Document Dump,"In an unprecedented move Wednesday, British lawmakers published hundreds of pages of internal Facebook emails and other documents that previously had been ordered sealed as part of an ongoing legal case between Facebook and a now-defunct app developer called Six4Three.

The documents, which date back to 2012, provide a rare window into CEO Mark Zuckerberg's thoughts on how to expand his social media juggernaut as users made the transition from desktop to mobile phones. They also suggest a willingness within Facebook to sacrifice user privacy and undercut its competitors to continue driving growth.

""I believe there is considerable public interest in releasing these documents. They raise important questions about how Facebook treats users' data, their policies for working with app developers, and how they exercise their dominant position in the social media market,"" tweeted Damian Collins MP, who heads up Parliament's Digital, Culture, Media, and Sport Committee. The committee, which is conducting an investigation into Facebook privacy concerns, seized the documents from Six4Three's founder while he was traveling in London last month.

Facebook says the documents are “very misleading without additional context.” “Like any business, we had many internal conversations about the various ways we could build a sustainable business model for our platform,” a spokesperson said in a statement. “But the facts are clear: We've never sold people’s data.”

The documents were collected by Six4Three's legal team as part of the discovery process for a lawsuit that alleges Facebook defrauded app developers by luring them with the promise of data, only to later cut them off from that information. The unredacted exhibits posted by Collins on Wednesday include internal emails, presentations, and memos. In one email, Zuckerberg personally approves a decision to shut down API access to Vine, a video-based social network backed by Twitter, in January 2013. In another, Facebook executives discuss giving Android devices access to users' call logs without requiring their informed consent. Zuckerberg himself toys with the idea of trading app developer access to Facebook’s APIs for advertising revenue from those developers in 2012. That same year, he expresses openness to “locking down” developers’ access to their users’ friends data. Facebook wouldn’t actually announce that change for another two years, even as it built relationships with developers on the back of that data.

In 2012, Zuckerberg voices his skepticism in an email to Facebook’s then-director of product management, Sam Lessin, that sharing friend data with app developers might ever pose privacy risk. “I just can’t think if any instances where that data has leaked from developer to developer and caused a real issue for us,"" he writes. ""Do you have examples of this?”

Six years later, amid ongoing global investigations into how an app developer working with the political firm Cambridge Analytica was able to weaponize data for political purposes, that question looks remarkably naive. The Cambridge Analytica scandal, which made international headlines in March and elevated a global conversation about the need for stricter data privacy laws, also cast the lawsuit between Six4Three and Facebook in a new light. The case began in 2015, after Facebook changed its API to cut developers off from friend data. Six4Three’s app, Pikinis, used friend data to let users find people’s bathing suit photos. Without access to friend data, the Pikinis app shut down, and its founder Ted Kramer sued Facebook, asking the company to either reinstate access or pay damages.",Yes,"It can have a negative impact on the mental health of its users. It can cause feelings of envy, inadequacy, and isolation.",https://www.wired.com/story/uk-facebook-document-dump/,2018-12-05 18:28:37.944000+00:00,LABEL_1_relevant,0.9954873919487,Social Media,social_media,Wired,"Information, Discourse & Governance"
43,This Is How Porn Ended Up on Ninja’s Zombie Twitch Channel,"Anyone who visited the dormant Twitch channel of Fortnite legend Ninja this weekend may have gotten a lot more than they bargained for. The page, it turns out, was directing users to a livestream of hardcore porn on Sunday instead of videogame battles. Ninja, also known as Tyler Blevins, understandably wasn't happy to learn his channel had veered into NSFW territory—he still has 14.7 million followers on the platform. “Disgusted and so sorry,” he wrote on Twitter. Emmett Shear, the CEO of Twitch, apologized to Ninja for the mistake the same night and said that the channel streaming porn had been taken down.

“We have also suspended these recommendations while we investigate how this content came to be promoted,” Shear wrote in a series of tweets Sunday. “It wasn’t our intent, but it should not have happened. No excuses.”

The timing couldn’t have been worse. Earlier this month, Ninja left Twitch and took his Fortnite talents to Mixer, a rival streaming platform owned by Microsoft. Around the same time, Twitch began experimenting with a new recommendation feature, which turned Ninja’s zombie profile into advertising space for promoting other, active livestreams. Typically, when Twitch creators aren’t active, their channels simply feature past videos. In an apparent attempt to keep viewers on the site for longer, Twitch began experimenting with recommending other, active livestreams on dormant channels instead.

Recommendations can be powerful engines for mobilizing audiences. YouTube has attributed more than 70 percent of time spent on its platform to its recommendation algorithm (which has also been criticized for helping to radicalize its users). The feature Twitch was testing simply displayed the top streams in a certain category. Since Ninja primarily played Fortnite on Twitch, the company tried populating his zombie profile with other popular streams featuring the same game. A bad actor appears to have artificially inflated the number of viewers on their porn stream, which was added to the Fortnite category, sending it to the top of the charts—and thus to Ninja’s page. The incident was a high-profile mistake, but it also raises questions about Twitch’s moderation efforts more broadly.

Louise Matsakis covers Amazon, internet law, and online culture for WIRED.

Twitch is by far the largest platform in the world for watching gaming content, and now one of the most popular websites on the entire internet, visited more than Pornhub, according to Alexa rankings. Its stars have turned into bonafide celebrities who can earn seven figures each year in sponsorship deals. Twitch creators predominantly livestream themselves playing videogames and talking, often for up to 18 hours or more at a time. All the while, fans flood an accompanying chat box with jokes, observations, compliments, pleas for attention, and harassment. Roughly a million people, by some estimates, are watching a Twitch stream at any given moment.

To moderate this ecosystem, Twitch relies on a blend of tech tools, professional moderators, and volunteers, just like Reddit or Wikipedia. (It's also recently tried suing bad actors in court.) Streamers appoint volunteer moderators to maintain order on their livestream chats, while the company relies on both artificial intelligence and humans to detect streams that break its rules. Twitch can’t heavily lean on AI designed to analyze written text, like Facebook and Twitter do, because the site isn’t primarily text-based. And it can’t preemptively scan videos for things like nudity before they’re published, because they unfold in real time.

So it might not be surprising that Twitch has hosted some problematic streams over the years. Last year, for example, a livestreamed shooting reportedly stayed up on the site for hours. But moderating live content has been a challenge for every social network, and the problem isn’t unique to Twitch. In March, Facebook was chastised for letting a livestream posted by a mass shooter who killed 51 people in New Zealand go viral. And YouTube, which also has a livestreaming feature, struggles even with videos that aren’t broadcast live. The difference is that significantly more information has been made public about how Facebook and YouTube police offensive content than about how Twitch does, in part because it relies on a more decentralized model.",Yes,It can be used to spread false information and hate speech.,https://www.wired.com/story/porn-ninja-zombie-twitch-channel/,2019-08-13 22:53:40.794000+00:00,LABEL_1_relevant,0.9956939220428468,Social Media,social_media,Wired,"Information, Discourse & Governance"
44,How WeChat Censored the Coronavirus Pandemic,"When the novel coronavirus was first discovered in China last winter, the country responded aggressively, placing tens of millions of people into strict lockdown. As Covid-19 spread from Wuhan to the rest of the world, the Chinese government was just as forceful in controlling how the health crisis was portrayed and discussed among its own people.

Politically sensitive material, like references to the 1989 Tiananmen Square protests, have long been forbidden on China’s highly censored internet, but researchers at the University of Toronto’s Citizen Lab say these efforts reached a new level during the pandemic. “The blunt range of censored content goes beyond what we expected, including general health information such as the fact [that] the virus spreads from human contact,” says Masashi Crete-Nishihata, the associate director of Citizen Lab, a research group that focuses on technology and human rights.

Citizen Lab's latest report, published earlier this week, finds that between January and May this year, more than 2,000 keywords related to the pandemic were suppressed on the Chinese messaging platform WeChat, which has more than 1 billion users in the country. Many of the censored terms referenced events and organizations in the United States.

Unlike in the US, internet platforms in China are responsible for carrying out the government’s censorship orders and can be held liable for what their users post. Tencent, which owns WeChat, did not comment in time for publication. WeChat blocks content via a remote server, meaning it’s not possible for research groups like Citizen Lab to study censorship on the app by looking at its code. “We can send messages through the server and see if they are received or not, but we can't see inside of it, so the exact censorship rules are a bit of a mystery,” Crete-Nishihata says.

For its latest report, Citizen Lab sent text copied from Chinese-language news articles to a group chat it created on WeChat with three dummy accounts, one registered to a mainland Chinese phone number and two registered to Canadian phone numbers. They used articles from a range of outlets, including some based in Hong Kong and Taiwan as well as Chinese state-controlled publications. If a message was blocked, the researchers performed further tests to identify which words triggered the censorship. Some of the blocked messages had originally been published by Chinese state media. In other words, while a person or topic may be freely discussed in the government-controlled press, it’s still banned on WeChat.

Read all of our coronavirus coverage here.

The Citizen Lab report demonstrates the extent to which the Chinese government tried to control the narrative from the beginning. As residents in Wuhan remained in lockdown, WeChat blocked phrases about Li Wenliang, a local doctor who warned colleagues about a new infectious disease before it was disclosed by the government, and who became a popular hero for free speech after he died of Covid-19 in February. WeChat also blocked its users from discussing an announcement by Chinese officials that they had informed the US government about the pandemic for the first time on January 3, almost three weeks before they said anything to their own citizens. And it censored mentions of the US Centers for Disease Control and Prevention when the organization was coupled with the word “coronavirus.”

By March, Covid-19 had become a global pandemic, and WeChat began blocking some mentions of international groups like the World Health Organization and the Red Cross. It also censored references to outbreaks in other countries like Saudi Arabia, Turkey, Russia, and the United Kingdom. Citizen Lab found that the majority of blocked words related to international relations were about the United States, the subject of the third portion of the report.

Relations between the US and China were already strained at the start of the year, and the pandemic has become a major flashpoint between the two countries. In late February, some US officials began elevating a conspiracy theory that the novel coronavirus was a biological weapon manufactured by the Chinese government. The false claim was also circulated by right-wing figures like former Trump adviser Steve Bannon. WeChat promptly blocked mentions of “Bannon and Bio Lab,” and other related terms. In May, as relations between the US and China sank to their lowest point in decades, a group of Republican senators introduced a bill that would rename the street outside the Chinese embassy in Washington “Li Wenliang Plaza,” after the Wuhan doctor. WeChat quickly censored a number of key terms related to the legislation.",Yes,"It can be used to spread false information and rumours quickly and easily. This is especially problematic during a crisis, when accurate information is critical.",https://www.wired.com/story/wechat-chinese-internet-censorship-coronavirus/,2020-08-27 21:30:00+00:00,LABEL_1_relevant,0.996081292629242,Social Media,social_media,Wired,"Information, Discourse & Governance"
45,Grief in the Time of Facebook,"(The Times has a bunch more of these.)

Twitter filled up with #RIPMCA, as did the comments on every Beastie Boys video on YouTube. Those of a certain generation couldn’t open Facebook without seeing another Adam Yauch meme.

It’s moments like these when it feels as if grieving is exactly what social media is for. Like it’s better at this than pretty much anything else, including celebration (too remote, and liable to inspire jealousy), the everyday (too trivial) or anger (too much like bitterness or schadenfreude).

Whether or not you care who MCA was, the expressions on social media feel authentic, maybe because they’re unadorned, straightforward and nakedly imperfect. Everyone who writes about their grief knows in advance that words fail, so the usual look-at-me temptation to be too clever by half disappears.

Sherry Turkle, professor at M.I.T. and author of “Alone Together: Why We Expect More From Technology and Less From Each Other,” recently described in the Times how we use mobile devices and social media to filter and sanitize our interaction with the real world.

We are tempted to think that our little “sips” of online connection add up to a big gulp of real conversation. But they don’t. E-mail, Twitter, Facebook, all of these have their places — in politics, commerce, romance and friendship. But no matter how valuable, they do not substitute for conversation.

So do a thousand microblogged tributes to MCA mean anything to all the Gen Xers who thought they would be young forever, but who discover that, to a degree that makes the realization as mundane as dust, they too will die? What conversations aren’t happening because we’ve dispensed with our grief in the “safe” realm of the Internet?

But that, I think, is too cynical a view. Expressions of grief have always been public – that’s the catharsis part of the process. That’s what social media is good for.

In the case of grief, at least, Turkle’s thesis doesn’t quite apply. Because in any age, after all the public mourning is done, the unchangeable core of the human condition is that we are all very much alone, privately meditating on death and its implications for ourselves.

Seen in this light, the title of Turkle’s book, “Alone together,” becomes the ultimate testament to the therapeutic power of social media. We don’t need technology or a dissertation on its impacts to tell us that we are alone. But surely it’s all the better to be alone, together.",Yes,It makes us feel more alone together.,https://www.technologyreview.com/2012/05/05/85931/grief-in-the-time-of-facebook/,2012-05-05 00:00:00,LABEL_1_relevant,0.9949426054954528,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
46,India says WhatsApp's lawsuit over new regulations a clear act of defiance,"India said on Wednesday that WhatsApp’s lawsuit challenging the new local IT rules is an “unfortunate last moment” attempt to prevent new regulations from going into effect in “a clear act of defiance,” and said the Facebook-owned service hadn’t raised any specific objection about the traceability requirement in writing in more than two and a half years.

Ravi Shankar Prasad, India’s Electronics and IT Minister, said WhatsApp’s refusal to comply with the guidelines, the deadline of which expires Wednesday, is a “clear act of defiance of a measure whose intent can certainly not be doubted.”

WhatsApp sued the Indian government earlier on Wednesday in a Delhi court, saying the world’s second-largest internet market’s new IT rules could allow authorities to make people’s private messages “traceable,” and conduct mass surveillance.

The Ministry of Electronics and IT said the government needs to trace the first originator of a message for the “purposes of prevention, investigation, punishment etc. of inter alia an offence relating to sovereignty, integrity and security of India, public order incitement to an offence relating to rape, sexually explicit material or child sexual abuse material punishable with imprisonment for not less than five years.”

“It is in public interest that who started the mischief leading to such crime must be detected and punished. We cannot deny as to how in cases of mob lynching and riots etc. repeated WhatsApp messages are circulated and recirculated whose content are already in public domain. Hence the role of who originated is very important.”

India first proposed WhatsApp to deploy software changes to make the originator of a message traceable in 2018. The suggestion came at a time when WhatsApp was grappling with containing the spread of false information in India, where circulation of such hoaxes resulted in multiple real-life casualties. The suggestion became a part of the new rules in February this year.

WhatsApp has argued that the end-to-end encryption it offers to users — a move that security and policy experts have long praised — makes it impossible for the firm, used by more than 2 billion users worldwide, to find identifying information about a user.

India is the largest market by users for the Facebook-owned popular instant messaging service. According to a government estimate, WhatsApp has amassed over 530 million users in India.

The Ministry of Electronics and IT said any company’s operations in India is “subject to the law of the land,” and argued that similar or tougher regulations have been enforced or proposed in other markets.

“What India is asking for is significantly much less than what some of the other countries have demanded,” it said, adding that it would be “foolhardy” to doubt the objective of the new rules.

On one hand, “WhatsApp seeks to mandate a privacy policy wherein it will share the data of all its user with its parent company, Facebook, for marketing and advertising purposes,” the ministry added. “On the other hand, WhatsApp makes every effort to refuse the enactment of the Intermediary Guidelines which are necessary to uphold law and order and curb the menace of fake news.”

The ministry wrote to social media firms earlier on Wednesday to ask for an update on whether they had complied with the new rules, TechCrunch first reported. In the letter, the ministry sought information (name and contact address) of officials who the firms had appointed as part of the compliance to the new rules that require officers to be available on the ground to address local concerns.

“WhatsApp’s attempt to portray the Intermediary Guidelines of India as contrary to the right to privacy is misguided. […] The Government of India recognises that ‘Right to Privacy’ is a Fundamental right and is committed to ensure the same to its citizens,” the ministry said in a statement.",Yes,The spread of false information.,https://techcrunch.com/2021/05/26/india-says-whatsapps-lawsuit-over-new-regulations-a-clear-act-of-defiance/,2021-05-26 00:00:00,LABEL_1_relevant,0.9961251616477966,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
47,Does Facebook Need an ‘Understand’ Button?,"When I was a child, I am told on reliable account, I would walk around our housing complex and take my thumb out of my mouth to ask neighbors, “Do you like me?” I had even pudgier cheeks back then, and my question came out as, “Do you dike me?” I’m guessing most everyone gave me a smile and thumbs up.

At some point, however, I stopped asking—not sure if it was because I stopped caring about the answer or because I eventually learned that you shouldn’t force people into saying they like you before they even know you.

On Facebook’s latest earnings call last week, Mark Zuckerberg announced that he’s made a similar self-discovery. Recapping a fourth quarter with revenues of $21 billion and profits of $7 billion, Zuckerberg said he no longer cared what people thought about him. “My goal for this next decade,” he said, “isn’t to be liked, but to be understood.” He elaborated at a tech conference in Utah two days later: “This is the new approach, and I think it’s going to piss off a lot of people, but frankly the old approach was pissing off a lot of people too, so let’s try something different.”

His comments were dripping with irony. No institution has committed more fully to the concept of being “liked” than Facebook. Since Facebook introduced a button for producing them in 2009, likes have become the currency of the internet. Everything can be liked—a comment, a news article, a photo, a post. And Facebook’s various platforms highlight the total number of likes for each expression of ourselves online, which quickly devolves into a way of quantifying and comparing your relative popularity and adjusting your behavior to be most liked.

It took a project team almost two years to win Zuckerberg’s approval for the Like button, which had been conceived with noble intentions. (Stop me if you’ve heard that one before.) Facebook’s self-imposed mission is to connect the world, and a simple process of sending good feelings and support was expected to strengthen ties between people who only communicated online.

The blog post introducing the Like button to the public begins with the liker. “We’ve just introduced an easy way to tell friends that you like what they’re sharing on Facebook with one easy click,” the note says. “Wherever you can add a comment on your friends’ content, you’ll also have the option to click ‘Like’ to tell your friends exactly that: ‘I like this.’ ” By the end of the brief note, the focus had turned to the person being liked.

There was a fear within Facebook that likes would replace written comments entirely, so the note explains the purpose of each. “If you go to the restaurant and have a great time, you may want to rate it 5 stars,” the note says. “But if you had a particularly delicious dish there and want to rave about it, you can write a review detailing what you liked about the restaurant. We think of the new ‘Like’ feature to be the stars, and the comments to be the review.”",Yes,"It encourages people to post, share, and comment on things that will get them more likes, comments, and shares, which then encourages other people to post, share, and comment on things to get more likes, comments, and shares. The end result is that people tend to post, share, and comment on things that will get them more",https://www.wired.com/story/does-facebook-need-an-understand-button/,2020-02-06 14:00:00+00:00,LABEL_1_relevant,0.994721293449402,Social Media,social_media,Wired,User Experience & Entertainment
48,885M Financial Records—Dating Back 16 Years—Exposed Online,"After a solid decade of nonstop corporate data breaches and exposures, you'd think large organizations would have at least fixed the most basic and obviously damaging types of data mishandling. But there's clearly still a long way to go. On Friday, independent security journalist Brian Krebs revealed that the real estate and title insurance giant First American had 885 million sensitive customer financial records, going back to 2003, exposed on its website for anyone to access. And while there isn't currently evidence that anyone actually found and stole the information, it was so easy to grab—and so obviously valuable to scammers—that it's hard to rule out that possibility.

The Hack

Krebs reports that the exposed records included Social Security numbers, driver's license images, bank account numbers and statements, mortgage and tax documents, and wire transaction receipts—an absolute treasure trove for any scammer or identity thief. An attacker who figured out the format of the company's document URLs could have input any ""record number"" they wanted—beginning with ""000000075,"" according to Krebs—and pull up the documents associated with that customer case. First American took down the site that populated the records at 2 pm ET on Friday. Krebs notified the company of the situation earlier this week.

“First American has learned of a design defect in an application that made possible unauthorized access to customer data,"" the company said in a statement. ""The company took immediate action to address the situation and shut down external access to the application. We are currently evaluating what effect, if any, this had on the security of customer information. We will have no further comment until our internal review is completed.”

First American did not answer questions from WIRED about how long the records were exposed online. The company says it has hired a forensic firm to assess whether customer data was ever stolen. First American, which is based in Santa Ana, California, is a Fortune 500 company with more than 18,000 employees.

Who's Affected

Well, lots of people! First American is the top title insurance firm in the United States, which means the company is often party to both the buyer and lender sides of real estate transactions across the country. And the detailed financial and personal information involved in closings potentially involves information about both buyers and sellers.

While the hope is that the data was never actually stolen, millions of people may have been impacted if it was. If you've bought or sold a house in the past several years, there's a decent chance First American had a hand in it.

How Serious Is This?

The First American exposure is a major incident, because it underscores just how little progress many institutions have made on locking down customer data. Perfect security is impossible, but the stakes are incredibly high and many large organizations still overlook basic errors.

The good news is that exposed data does not necessarily mean stolen data. There's a chance that no one stumbled across this trove before the company had the chance to secure it. But unlike other data leaks of similar scale, which largely involve password and username combinations, the data in the First American haul would have devastating long-term consequences for potential victims.

If you’re a First American customer or think you were party to a transaction that also involved the company there isn’t a lot you can do to protect yourself against the possibility that your data was stolen as a result of this exposure. But watch your bank and credit card statements for suspicious activity. Consider purchasing credit monitoring or, better yet, avail yourself of a free credit monitoring offer from another security incident your data was involved in. By this point, you've almost certainly qualified for it. You can also consider a credit freeze.

Security practitioners always hope that major security incidents, like the notorious Equifax breach, will be a wake up call to all companies. But the consequences for such missteps are only first starting to appear. On Wednesday, for example, Moody’s downgraded its ratings outlook for Equifax. A spokesperson said, “It’s the first time that cyber has been a named factor in an outlook change."" Until other dramatic economic motivators emerge, disasters like First American, or worse, will continue.

More Great WIRED Stories",Yes,"It has completely destroyed our ability to have civil discourse.

The article does not mention how most of us have become addicted to social media, addicted to the point of using it even when we have other things to do. It is now not uncommon to see people sitting in restaurants and cafes with their faces buried in their phones instead of talking with",https://www.wired.com/story/first-american-data-exposed/,2019-05-24 22:49:31.545000+00:00,LABEL_1_relevant,0.9886736273765564,Social Media,social_media,Wired,"Information, Discourse & Governance"
49,What Really Caused Facebook's 500M-User Data Leak?,"Since Saturday, a massive trove of Facebook data has circulated publicly, splashing information from roughly 533 million Facebook users across the internet. The data includes things like profile names, Facebook ID numbers, email addresses, and phone numbers. It's all the kind of information that may already have been leaked or scraped from some other source, but it's yet another resource that links all that data together—and ties it to each victim—presenting tidy profiles to scammers, phishers, and spammers on a silver platter.

Facebook's initial response was simply that the data was previously reported on in 2019 and that the company patched the underlying vulnerability in August of that year. Old news. But a closer look at where, exactly, this data comes from produces a much murkier picture. In fact, the data, which first appeared on the criminal dark web in 2019, came from a breach that Facebook did not disclose in any significant detail at the time and only fully acknowledged Tuesday evening in a blog post attributed to product management director Mike Clark.

One source of the confusion was that Facebook has had any number of breaches and exposures from which this data could have originated. Was it the 540 million records—including Facebook IDs, comments, likes, and reaction data—exposed by a third party and disclosed by the security firm UpGuard in April 2019? Or was it the 419 million Facebook user records, including hundreds of millions of phone numbers, names, and Facebook IDs, scraped from the social network by bad actors before a 2018 Facebook policy change, that were exposed publicly and reported by TechCrunch in September 2019? Did it have something to do with the Cambridge Analytica third-party data sharing scandal of 2018? Or was this somehow related to the massive 2018 Facebook data breach that compromised access tokens and virtually all personal data from about 30 million users?

In fact, the answer appears to be none of the above. As Facebook eventually explained in background comments to WIRED and in its Tuesday blog, the recently public trove of 533 million records is an entirely different data set that attackers created by abusing a flaw in a Facebook address book contacts import feature. Facebook says it patched the vulnerability in August 2019, but it's unclear how many times the bug was exploited before then. The information from more than 500 million Facebook users in more than 106 countries contains Facebook IDs, phone numbers, and other information about early Facebook users like Mark Zuckerburg and US secretary of Transportation Pete Buttigieg, as well as the European Union commissioner for data protection, Didier Reynders. Other victims include 61 people who list the ""Federal Trade Commission"" and 651 people who list ""Attorney General"" in their details on Facebook.

You can check whether your phone number or email address were exposed in the leak by checking the breach tracking site HaveIBeenPwned. For the service, founder Troy Hunt reconciled and ingested two different versions of the data set that have been floating around.

“When there’s a vacuum of information from the organization that’s implicated, everyone speculates, and there's confusion,” Hunt says.

“They’re kind of stuck now, because they apparently didn’t do any disclosure or notification.” Ashkan Soltani, Former FTC chief technologist

The closest Facebook came to acknowledging the source of this breach previously was a comment in a fall 2019 news article. That September, Forbes reported on a related vulnerability in Instagram's mechanism to import contacts. The Instagram bug exposed users’ names, phone numbers, Instagram handles, and account ID numbers. At the time, Facebook told the researcher who disclosed the flaw that the Facebook security team was “already aware of the issue due to an internal finding.” A spokesperson told Forbes at the time, “We have changed the contact importer on Instagram to help prevent potential abuse. We are grateful to the researcher who raised this issue."" Forbes noted in the September 2019 story that there was no evidence the vulnerability had been exploited, but also no evidence that it had not been.

In its blog post today, Facebook links to a September 2019 article from CNET as evidence that the company publicly acknowledged the 2019 data exposure. But the CNET story refers to findings from a researcher who also contacted WIRED in May 2019 about a trove of Facebook data, including names and phone numbers. The leak the researcher had learned about was the same one TechCrunch reported on in September 2019. And according to the September 2019 CNET story, it is the same one CNET was describing. Facebook told TechCrunch at the time, “This data set is old and appears to have information obtained before we made changes last year [2018] to remove people’s ability to find others using their phone numbers.” Those changes were aimed at reducing the risk that Facebook's search and account-recovery tools could be exploited for mass scraping.",Yes,"It makes it very easy for people to connect with each other.

The main intended consequence of social media is that it allows people to connect with each other easily.",https://www.wired.com/story/facebook-data-leak-500-million-users-phone-numbers/,2021-04-06 23:57:48.816000+00:00,LABEL_1_relevant,0.9965736865997314,Social Media,social_media,Wired,"Information, Discourse & Governance"
50,Social Media’s Law of Short Messages,"“If you plot the rate of the messages versus the length, then you can find a mathematical relation between these two things during [major] events,” adds Michael Szell, a researcher at the Senseable City Lab.

Ratti, Szell, and Senseable City researcher Sebastian Grauwin are co-authors of a new paper about the study, “Contraction of online response to major events,” published today in the journal PLoS ONE.

Three words for golf fans: You the man!

To conduct the study, the researchers looked at data from several social-media sources at a variety of moments. The Twitter data, for example, comes from April 2012 and involves tweets on the subject of The Masters, one of golf’s four major tournaments. That year, golfer Bubba Watson won the tournament in a dramatic playoff against Louis Oosthuizen, prompting a flurry of shorter tweets.

As the paper drolly notes, a “large part of these tweets contain the name ‘Bubba’ followed by a varying number of exclamation marks.’”

Other data used in the study includes social media activity during election night in the U.S. in November 2012, and during a major snowstorm in February 2013.

“We wanted to see if this was Twitter-specific or if it happens in other media as well,” Szell says. “Basically we found this effect every time there was an event going on. And when there was no event, we did not find this effect.”

Renaud Lambiotte, a mathematician at the University of Namur in Belgium who has read the study, calls it “an interesting piece of research” that may lead to fruitful follow-up work, “in particular for the modeling of the relation between behavioral response and emotional stimuli.” Lambiotte suggests that well-designed online social experiments may yield further insight about behavior on social media.

The Senseable City Lab conducts large-scale data projects, often based on mobile technology or social media, to evaluate patterns of activity in urban environments or among technologically connected networks of people.

“It’s all about detecting specific signatures of how people behave, in any kind of aspect of their life, whether it’s tweeting, commenting, or moving around,” says Grauwin, who is working in the Senseable City Lab via a fellowship with the technology firm Ericsson.

The current study also revealed what Grauwin calls an “index of frustration” among some social-media users: namely, during major events, a small minority of users run up against the 140-limit on Twitter.

Ratti suggests that further studies of message length will continue to uncover more data that could be useful in the design of social-media platforms. He also thinks more research can shed light on the behavioral mechanisms that lead people to send shorter messages: Are people doing this independently, or in response to seeing other short messages?

“You get this kind of herd effect,” Ratti says. “The paper suggests this is an interesting problem to look at.”",Yes,People tend to use shorter messages during major events.,https://www.technologyreview.com/2014/02/27/13912/social-medias-law-of-short-messages/,2014-02-27 00:00:00,LABEL_1_relevant,0.995793342590332,Social Media,social_media,MIT Tech Review,Environment & Sustainability
51,Facebook Wants to Connect You With Your 'Secret Crush',"Facebook is channeling its earliest days as a hot-or-not website for college students with a new feature called Secret Crush. To be announced today at the social network’s annual F8 developer conference, Secret Crush will allow Facebook Dating users to select up to nine friends they want to express interest in. Think of it like matching on Tinder or Bumble, except you get to hand-pick the specific friends you want to date ahead of time, instead of hoping their profiles show up in the queue. What could go wrong?

Here’s how it’s supposed to work. If someone adds you to their Secret Crush list, Facebook will send you a notification saying “A friend added you as a secret crush.” If you then pick the same person for your list, Facebook will match you together and reveal your names. If the feelings are only one-sided, the unrequited lover’s identity remains secret. The object of their affection is then left to wonder who may have a crush on them, with no way to find out. The feature will be available only in countries where Facebook has rolled out its Dating service, including Colombia, Canada, Thailand, Argentina, and Mexico, as well as 14 new countries in Asia and South America introduced today. If you don’t already have a Dating profile, you can’t receive Secret Crush notifications.

Facebook

Secret Crush will help give Facebook an advantage over competing dating apps, which can’t similarly leverage information about your existing friendships. Tinder is great if you want to meet someone outside your social circle, but it doesn’t offer a direct mechanism to connect you with those you already know in real life. Facebook is likely hoping that instead of getting someone’s number at a party, you’ll add them on Facebook and then to your Secret Crush list.

Other modern dating apps haven’t tried it, but that doesn’t exactly mean Secret Crush is innovative. One of Facebook’s oldest social media competitors, Friendster, introduced a similar feature way back in 2012. The dating site GoodCrush, first launched at Princeton University in 2007, also had a nearly identical premise.

It’s not hard to imagine how Secret Crush could go wrong. You could easily prank or even bully someone by adding them to your crush list under false pretenses. One WIRED staffer who used GoodCrush in college reported they and their best friend “spent a full semester fucking with another friend through this service.” But Secret Crush will likely be great for engagement purposes. Using Facebook to scroll through baby photos may be a snooze, but who doesn’t want to find out that someone has a crush on them?

Facebook says it won’t use data from Dating or Secret Crush to make content or advertising decisions. For example, just because you add someone to your Secret Crush list doesn’t mean you will see more of their posts in your News Feed. “All activity that occurs in FB Dating stays in FB Dating and will not be shared externally,” a Facebook spokesperson said in an email.

Facebook will also announce today that Dating is immediately coming to several new locations in South America and Asia, including the Philippines, Vietnam, Singapore, Malaysia, Laos, Brazil, Peru, Chile, Bolivia, Ecuador, Paraguay, Guyana, Uruguay, and Suriname. Secret Crush will start rolling out to each locale Tuesday. For now, singles in the United States remain out of luck.

More Facebook Stories",Yes,"It leads to echo chambers where people only read, watch, or hear content that reinforces what they already believe and leads to polarization.

If you have any other tips, please let me know.

Thank you!",https://www.wired.com/story/facebook-dating-secret-crush-f8/,2019-04-30 17:30:00+00:00,LABEL_1_relevant,0.994912087917328,Social Media,social_media,Wired,"Information, Discourse & Governance"
52,How Robinhood's Psych Experiment Backfired Horribly,"A year ago Robinhood went from corporate hero to punching bag literally overnight.

As GameStop mania swept up millions of mostly novice investors looking to make a bundle while sticking it to Wall Street, the app-first broker restricted purchases of the struggling retailer and other “meme stocks” following a pre-dawn warning that it faced insolvency. In other words, it had been too successful at getting young people excited about trading.

Courtesy of Portfolio Buy This Book At: Amazon

Bookshop.org

Target If you buy something using links in our stories, we may earn a commission. This helps support our journalism. Learn more.

Perhaps this shouldn’t have been so surprising. Despite being named after the mythical hero who stole from the rich to give to the poor and its stated mission “to democratize finance for all,” Robinhood made its founders Vlad Tenev and Baiju Bhatt billionaires. Ironically, the first company they started helped hedge funds—the investment vehicles their clients would gleefully ambush during GameStop mania—trade more efficiently. Funds were able to buy stocks at almost no cost. How hard would it be to create an app that could do the same thing?

“It became clear to us that the smartphone would be your primary tool for accessing the markets and doing financial transactions in general,” said Tenev in a 2017 interview. Unlike Charles Schwab, Fidelity, or even E-Trade, Robinhood is younger than the iPhone. It is more like an app with a brokerage firm attached to it than a broker that has an app. And Robinhood’s is a thing of beauty, having won the Apple design award the year it was launched.

Even after competitors eliminated their own commissions in late 2019, young, new users came to Robinhood’s shiny app in droves—particularly after the pandemic hit. Funded accounts went from about 7 million when quarantines began to 18 million a year later as GameStop mania cooled. The Silicon Valley wunderkinder were running circles around their older competitors. But unleashing the “move fast and break things” philosophy on a generation’s savings was bound to create some problems. It also made long-standing ones harder to ignore.

When it filed to go public last year, Robinhood revealed that customers who opened the app did so about seven times a day. That made it more like Instagram than any boring financial portal. And Robinhood resembled social networks in another important way: Trading was “free.” As with those services, its users became the product.

Robinhood makes much of its revenue from selling customers’ trades to wholesalers like Citadel Securities. The company keeps fractions of a penny that add up to billions of dollars a year. That business model made it possible for Robinhood to cater to customers with tiny accounts, but only if some were hyperactive. They traded stocks an astounding 40 times as much per dollar in their accounts on average compared with those at staid Schwab.

The reason so many people have become self-directed investors, especially during a bull market, isn’t because they hate paying for professional advice. It is because they hear about others getting rich and then overestimate their ability to guess something almost completely random—stock prices. The phenomenon, called the “illusion of control,” was first described by psychologist Ellen Langer. She gave study participants the ability to buy a lottery ticket for a dollar with half given random numbers and half able to choose their own. They were later offered cash for the tickets. The group that picked their own numbers asked for several times as much.",Yes,"It creates a false sense of control. People see their friends getting rich from investing in stocks, so they try to do the same thing. The problem is that most people are not very good at picking stocks, and even the best investors only get it right about half the time.

The other big problem with Robinhood is that it encouraged",https://www.wired.com/story/robinhood-psych-experiment-gamestop/,2022-02-11 13:00:00+00:00,LABEL_1_relevant,0.9962368011474608,Social Media,social_media,Wired,"Information, Discourse & Governance"
53,An Army of Volunteers Takes On Vaccine Disinformation Online,"As researchers, pharma companies, and governments around the world are racing to make a vaccine against the pandemic coronavirus in record time, there’s a growing concern that many Americans won’t want it when it arrives. In a series of recent polls, only about half of US adults say they would get a Covid-19 vaccine, even though more than 1,000 people are still dying from the disease every day in the US. Some of those surveyed are rightly concerned about the perils of rushed science. But according to one poll conducted by Yahoo News/YouGov, more than a quarter of Americans would decline a shot in part because they believe Microsoft cofounder Bill Gates is trying to slip them a microchip.

Everything You Need to Know About the Coronavirus Here's all the WIRED coverage in one place, from how to keep your children entertained to how this outbreak is affecting the economy.

Conspiracy theories thrive in times of great uncertainty, and the coronavirus pandemic has proven to be a petri dish for particularly harmful ones. This one can be traced back to May 4, when a little-known filmmaker named Mikki Willis posted a 26-minute video called Plandemic to Facebook, YouTube, Vimeo, and a designated website. It featured a discredited scientist describing a bizarre, unsubstantiated plot by global elites like Gates to use a vaccine against the virus to seize power. These ideas, hailing from many sources, had already been swirling on many parts of the internet and were congealing into a narrative involving Gates and microchips, but the Plandemic video became their biggest signal boost. According to an analysis by The New York Times, the video spent about three days incubating on Facebook pages dedicated to conspiracy theories and the anti-vaccine movement. Then, like any efficient pathogen, it went viral. Just a week after its release, the now widely debunked video had been viewed more than 8 million times.

But it didn’t have to be that way. So says Joe Smyser, CEO of the Public Good Projects, or PGP, a public health nonprofit that specializes in using social network analysis to implement large-scale behavioral change programs. His group has built online surveillance tools for tracking outbreaks of misinformation, disinformation, and downright conspiracies. He says they saw most of the sharing activity that fueled this particular theory’s eventual virality within the first 24 hours. “It was right there in the data,” he says. “We didn’t have to wait days to respond to it, because the outcome was predictable. What was lacking was coordination.”

Smyser wants to bring coordination to combating a growing anti-vaccine movement that contributed to a record outbreak of measles last year—the worst in four decades. This week, his organization is launching a vaccine advocacy campaign unlike any other before.

Called Stronger, it aims to take the fight to anti-vaccine organizers where they’ve long had the upper hand: on social media. To do so, PGP plans to conscript the vast but largely silent majority of Americans who support vaccines into any army of keyboard warriors trained to block, hide, and report vaccine misinformation. (According to a recent Gallup poll, 84 percent of Americans say vaccinating children is important.) The effort is backed by a number of pro-immunization coalitions and the National Foundation for Infectious Diseases, and has funding from BIO, the world’s largest biotechnology lobbying group. “We have this tradition in the US that vaccines are solely the domain of public health workers who are trained to not get into fights,” says Smyser. “I think that’s a very antiquated perspective, and it’s left those on the public health side completely outgunned in this new era of social media.”

For nearly as long as there have been vaccines, public health agencies have shied away from a combative approach with people who disagree with their recommendations. Vaccines are unequivocally one of the most transformative technologies in human history. Until vaccines eradicated smallpox, the disease killed millions of children each year—between 300 million to 500 million during the 20th century alone. In 1988, at the start of a worldwide vaccine campaign, approximately 350,000 children were paralyzed by polio each year. In 2018, the virus sickened just 33.",Yes,"It gives a platform to those who would spread misinformation and conspiracy theories. In the case of vaccines, this has led to a resurgence of preventable diseases.

Smyser's organization is launching a campaign called Stronger to take the fight to anti-vaccine organizers where they’ve long had the upper hand: on social media",https://www.wired.com/story/can-a-keyboard-crusade-stem-the-vaccine-infodemic/,2020-06-15 11:00:00+00:00,LABEL_1_relevant,0.9964969754219056,Social Media,social_media,Wired,"Information, Discourse & Governance"
54,Papua New Guinea has imposed a month-long Facebook ban,"The country will prevent residents from accessing the social network, in a bid to clean up the platform.

Why? According to the government, the time will be used to filter out fake accounts and remove pornography.“This will allow genuine people with real identities to use the social network responsibly,” Sam Basil, Papua New Guinea’s communications minister, told the Post Courier, a local publication.

But: Only about 12 percent of the country uses Facebook. Seems like a drastic measure to shut down something that less than an eighth of your population uses.

Why it matters: A Facebook ban usually occurs right before an election, or else it’s imposed indefinitely, as in China. As Aim Sinpeng, an expert in digital media and politics at the University of Sydney, told the Guardian, “These issues with Facebook are being spoken about in a number of other countries, so the fact that Papua New Guinea is on the bandwagon shows how widespread concerns have become.”",Yes,"It can lead to addiction. Although social media can be used to connect with friends and family, it can also lead to people spending too much time on their devices and becoming addicted to the instant gratification that they receive from social media. This can lead to people becoming isolated from the real world and their loved ones.",https://www.technologyreview.com/2018/05/30/142659/papua-new-guinea-has-imposed-a-month-long-facebook-ban/,2018-05-30 00:00:00,LABEL_1_relevant,0.9937135577201844,Social Media,social_media,MIT Tech Review,Health & Wellbeing
55,The Mosque Shooter Exploited the Power of the Internet,"After each new horrific mass shooting, an all-too-familiar cycle often plays out: Reporters (myself included) race to attempt to unpack an alleged shooter’s possible motivations by piecing together clues from their social media accounts and online postings before it all gets scrubbed from the internet. We do this in the hopes that it will somehow provide a window into their mindset in the months leading up to the attack, or at least bring us somewhat closer to answering that ultimately unanswerable question: Why?

But this approach carries with it potentially dangerous unintended consequences. At least 49 people were killed on Friday during attacks on two mosques in Christchurch, New Zealand, and like clockwork the cycle began almost immediately. But this time it was a bit different. The alleged shooter himself had provided the world with more answers and possible motivations for his own actions than it seemed the internet knew how to handle.

Hours before the attack, the alleged shooter took to Twitter and 8chan—an online messaging board known for its distinct brand of toxicity—to announce his plans and share links to a Facebook account which later live-streamed 17 minutes of the massacre. He also linked to a 74-page document littered with awkwardly placed ironic memes and references to various toxic ideologies that many news outlets have since deemed his “manifesto.” On 8chan, the links were accompanied by a request: “I have provided links to my [sic] my writings below, please do your part by spreading my message, making memes and shitposting as you usually do.”

The internet largely did just that. The gory first-person Facebook video of the shooting quickly went viral, spreading across social media platforms like wildfire before platforms could take it down. Since then, pundits, analysts, and internet sleuths have been publicly dissecting and interpreting each line of his lengthy manifesto—along with his equally toxic social media presence—turning public discussion into something closer to a string of far-right rabbithole keywords.

Twitter content This content can also be viewed on the site it originates from.

There was no need for internet sleuths to track down his social media accounts and comb them for clues, as he broadcast their existence publicly: They were, predictably, filled with more made-to-provoke explanations for his actions. It was “a very clear instance of media manipulation,” designed with the world’s eye in mind, says Whitney Phillips, a Data and Society researcher who specializes in troll culture and the amplification of extremism online.",Yes,It gives people a false sense of connection to other people.,https://www.wired.com/story/mosque-shooter-exploited-power-of-internet/,2019-03-15 20:58:52.387000+00:00,LABEL_1_relevant,0.9966183304786682,Social Media,social_media,Wired,"Information, Discourse & Governance"
56,Thank you for posting: Smoking’s lessons for regulating social media,"The comparison is more than metaphorical. It’s a framework for thinking about how public opinion needs to shift so that the true costs of misinformation can be measured and policy can be changed.

Personal choices, public dangers

It might seem inevitable today, but regulating the tobacco industry was not an obvious choice to policymakers in the 1980s and 1990s, when they struggled with the notion that it was an individual’s choice to smoke. Instead, a broad public campaign to address the dangers of secondhand smoke is what finally broke the industry's heavy reliance on the myth of smoking as a personal freedom. It wasn’t enough to suggest that smoking causes lung disease and cancer, because those were personal ailments—an individual’s choice. But secondhand smoke? That showed how those individual choices could harm other people.

Epidemiologists have long studied the ways in which smoking endangers public health, and detailed the increased costs from smoking cessation programs, public education, and enforcement of smoke-free spaces. To achieve policy change, researchers and advocates had to demonstrate that the cost of doing nothing was quantifiable in lost productivity, sick time, educational programs, supplementary insurance, and even hard infrastructure expenses such as ventilation and alarm systems. If these externalities hadn’t been acknowledged, perhaps we’d still be coughing in smoke-filled workplaces, planes, and restaurants.

And, like secondhand smoke, misinformation damages the quality of public life. Every conspiracy theory, every propaganda or disinformation campaign, affects people—and the expense of not responding can grow exponentially over time. Since the 2016 US election, newsrooms, technology companies, civil society organizations, politicians, educators, and researchers have been working to quarantine the viral spread of misinformation. The true costs have been passed on to them, and to the everyday folks who rely on social media to get news and information.

Take, for example, the recent falsehood that antifa activists are lighting the wildfires on the West Coast. This began with a small local rumor repeated by a police captain during a public meeting on Zoom. That rumor then began to spread through conspiracy networks on the web and social media. It reached critical mass days later after several right-wing influencers and blogs picked up the story. From there, different forms of media manipulation drove the narrative, including an antifa parody account claiming responsibility for the fires. Law enforcement had to correct the record and ask folks to stop calling in reports about antifa. By then, millions of people had been exposed to the misinformation, and several dozen newsrooms had had to debunk the story.

The costs are very real. In Oregon, fears about “antifa” are emboldening militia groups and others to set up identity checkpoints, and some of these vigilantes are using Facebook and Twitter as infrastructure to track those who they deem suspicious.

Online deception is now a multimillion-dollar global industry, and the emerging economy of misinformation is growing quickly. Silicon Valley corporations are largely profiting from it, while key political and social institutions are struggling to win back the public’s trust. If we aren’t prepared to confront the direct costs to democracy, understanding who pays what price for unchecked misinformation is one way to increase accountability.

Combating smoking required a focus on how it diminished the quality of life for nonsmokers, and a decision to tax the tobacco industry to raise the cost of doing business.

Now, I am not suggesting placing a tax on misinformation, which would have the otherwise unintended effect of sanctioning its proliferation. Taxing tobacco has stopped some from taking up the habit, but it has not prevented the public health risk. Only limiting the places people can smoke in public did that. Instead, technology companies must address the negative externalities of unchecked conspiracy theories and misinformation and redesign their products so that this content reaches fewer people. That is in their power, and choosing not to do so is a personal choice that their leaders make.",Yes,"It has given a platform to conspiracy theorists and those who peddle misinformation. This has had a negative impact on public trust in key institutions, and has led to the rise of vigilantism.",https://www.technologyreview.com/2020/10/05/1009231/social-media-facebook-tobacco-secondhand-smoke/,2020-10-05 00:00:00,LABEL_1_relevant,0.9959892630577089,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
57,Why the History of Black Twitter Needed to Be Told,"But I think again with Twitter, it's like now Twitter has the power to check, right? It's sort of the checks and balances system, right? Especially when you have Black Twitter behind you because as somebody else said in the oral history, once Black Twitter gets a hold of something, it's hard for it to stop and so once Black Twitter was like, Toronto Burke created this, she's the woman behind and the genius behind this, we need to say something, right? And so I think that's also the power to mobilize and the power to just push against a system that doesn't always have our best interests in mind is really important.

MC: And we'll continue seeing that happen even more recently where people have made a joke or posted a meme and it's origins were in Black Twitter and people will jump on them and say, you have to give credit where credit is due there.

JP: Right. So I think one of my favorite memes that's come out of Black Twitter it's, this you? Where people are sort of checking other folks and saying, hey, we started this, this is ours. It's tough though, I mean, in some ways I'm glad that Black Twitter exists in a sort of entity that it does because I think without it, it'd be harder to document all the things that we created online, right? And all the things that we continue to create, right? And so one thing that Andre Brock was saying how Black Twitter in itself is kind of like a living archive, right? It's constantly being updated and it's constantly growing, right? It's sort of a library of Congress but for Black folks online in a way, right?

And so I think that it's important to have it but it's also important that it acts like a checks and balances system too that say, hey, we're very protective of our space and this is ours and it's important to have things like that. Because I think in the same way that I wanted to document this oral history and I thought it was important to have this official record of Black Twitter, it's the same sort of sense behind that in that we need these things and these things are ours and we need people to know that we created these things.

MC: All right. Well, you can read more about the history of Black Twitter in Jason Parham awesome oral history. Part three is up now, it's a three-part series, you can read the whole thing on WIRED.com. It is also on the cover of WIRED magazine in September with an amazing painting illustration, beautiful cover so check it out. We're going to take a break right now and when we come back we will have our recommendations.

[Break]

MC: All right. Welcome back to the show. Jason Parham, you are in hot seat, what's your recommendation for our audience?

JP: So one show I've been binging lately that I just recently finished. It came on Cinemax in 2019 and I think it was kind of a crime because nobody actually watches shows that come on Cinemax but they have a lot of good content. But now, because I guess of the deal with HBO Max all these Cinemax shows are on HBO Max, it's with Carla Gugino and Giancarlo Esposito, it's called Jett, J-E-T-T. It's a crime drama crime thriller about a thief named Daisy played by Carla Gugino and the crime boss, Giancarlo Esposito who's infamous from Breaking Bad as Gus Fring, it's amazing. There was only one season of it, they're nine episodes, each episode is an hour, it's shot beautifully, it also has Jodi Turner-Smith, all these amazing actors in it. Nobody's heard of this, I don't know if people ever talking about this which made me upset but I've been telling everybody to watch this show so watch this show.",Yes,"It has allowed us to live in our own little echo chambers, forgetting that there are other people out there with different perspectives. It has also led to the rise of ""fake news"" and a general mistrust of the media.",https://www.wired.com/story/gadget-lab-podcast-514/,2021-07-30 12:00:00+00:00,LABEL_1_relevant,0.9950425624847412,Social Media,social_media,Wired,"Information, Discourse & Governance"
58,Everything We Know About Facebook's Massive Security Breach,"Facebook’s privacy problems severely escalated Friday when the social network disclosed that an unprecedented security issue, discovered September 25, impacted almost 50 million user accounts. Unlike the Cambridge Analytica scandal, in which a third-party company erroneously accessed data that a then-legitimate quiz app had siphoned up, this vulnerability allowed attackers to directly take over user accounts.

The bugs that enabled the attack have since been patched, according to Facebook. The company says that the attackers could see everything in a victim's profile, although it's still unclear if that includes private messages or if any of that data was misused. As part of that fix, Facebook automatically logged out 90 million Facebook users from their accounts Friday morning, accounting both for the 50 million that Facebook knows were affected, and an additional 40 million that potentially could have been. Later Friday, Facebook also confirmed that third-party sites that those users logged into with their Facebook accounts could also be affected.

""We were able to fix the vulnerability and secure the accounts, but it definitely is an issue that it happened in the first place."" Mark Zuckerberg, Facebook

Facebook says that affected users will see a message at the top of their News Feed about the issue when they log back into the social network. ""Your privacy and security are important to us,"" the update reads. ""We want to let you know about recent action we've taken to secure your account."" The message is followed by a prompt to click and learn more details. If you were not logged out but want to take extra security precautions, you can check this page to see the places where your account is currently logged in, and log them out.

Facebook has yet to identify the hackers, or where they may have originated. “We may never know,” Guy Rosen, Facebook’s vice president of product, said on a call with reporters Friday. The company is now working with the Federal Bureau of Investigation to identify the attackers. A Taiwanese hacker named Chang Chi-yuan had earlier this week promised to live-stream the deletion of Mark Zuckerberg's Facebook account, but Rosen said Facebook was ""not aware that that person was related to this attack.""

“If the attacker exploited custom and isolated vulnerabilities, and the attack was a highly targeted one, there simply might be no suitable trace or intelligence allowing investigators to connect the dots,” says Lukasz Olejnik, a security and privacy researcher and member of the W3C Technical Architecture Group.

On the same call, Facebook CEO Mark Zuckerberg reiterated previous statements he has made about security being an “arms race.”

“This is a really serious security issue, and we’re taking it really seriously,” he said. “I’m glad that we found this, and we were able to fix the vulnerability and secure the accounts, but it definitely is an issue that it happened in the first place.”

The social network says its investigation into the breach began on September 16, when it saw an unusual spike in users accessing Facebook. On September 25, the company’s engineering team discovered that hackers appear to have exploited a series of bugs related to a Facebook feature that lets people see what their own profile looks like to someone else. The ""View As"" feature is designed to allow users to experience how their privacy settings look to another person.

The first bug prompted Facebook's video upload tool to mistakenly show up on the ""View As"" page. The second one caused the uploader to generate an access token—what allows you to remain logged into your Facebook account on a device, without having to sign in every time you visit—that had the same sign-in permissions as the Facebook mobile app. Finally, when the video uploader did appear in ""View As"" mode, it triggered an access code for whoever the hacker was searching for.",Yes,"It can reduce the level of privacy that an individual has. This can happen in many ways, such as through the sharing of personal information and the loss of control over that information. Additionally, social media can lead to decreased face-to-face interaction, which can reduce the level of intimacy and trust in relationships.",https://www.wired.com/story/facebook-security-breach-50-million-accounts/,2018-09-28 19:03:03.404000+00:00,LABEL_1_relevant,0.99595707654953,Social Media,social_media,Wired,Security & Privacy
59,China censored social media posts about Chloé Zhao's Oscar win ...,"The news of Chloé Zhao’s Oscar win for Best Director has been censored on social media and search engines in China at the direction of the country’s propaganda ministry, The Wall Street Journal reported. Zhao, who was born in Beijing, is the first Chinese woman and first woman of color to take home the Academy Award for directing. Nomadland also won Best Picture at Sunday night’s awards ceremony, and the movie’s star, Frances McDormand, won Best Actress.

But according to the Journal, early messages congratulating Zhao had been scrubbed from Chinese social media sites by midday Monday, and searches on Chinese search engines Baidu and Sogou had few links to the Oscar news. There was no news about Zhao’s win on China Central Television, the Xinhua News Agency, or the Chinese Communist Party’s People’s Daily on Monday, the Journal reported; according to a state media reporter, the country’s propaganda ministry told the outlets not to report the win due to “previous public opinion.”

The Global Times, a news site owned by People’s Daily, posted an op-ed late Monday acknowledging Zhao’s win but criticizing Nomadland as “typically American and far from the real life of the Chinese people.” Of Zhao, the Global Times wrote: “We hope she can become more and more mature. In an era when the China-US confrontation is intensifying, she can play a mediating role in the two societies and avoid being a friction point. She cannot escape her special label, and she should actively use it.”

Another of the few news stories that did surface in China about Zhao’s win came from small news site 163.com, which called Zhao the “second Chinese filmmaker to win the best director Oscar, after Ang Lee,” according to The Hollywood Reporter. Lee, who won Best Director twice, for Brokeback Mountain in 2006 and for Life of Pi in 2013, was born in Taiwan. China considers Taiwan to be part of China, a matter of some contention internationally.

It’s not the first time Nomadland and its director have been hidden by Chinese media; last month, The Associated Press reported that social media posts and other publicity about the movie were removed by China’s censors.

The reason for China’s erasure of Zhao’s win likely stems from her comments in Western media outlets. She said in a 2013 interview with Filmmaker magazine that China was a “place where there are lies everywhere,” a comment that was later removed from that article, according to the AP, but not before it drew considerable backlash in China.",Yes,It can lead to the censorship of information.,https://www.theverge.com/2021/4/26/22403627/china-censored-chloe-zhao-oscars-nomadland-social-media-posts,2021-04-26 00:00:00,LABEL_1_relevant,0.9958482980728148,Social Media,social_media,The Verge,"Information, Discourse & Governance"
60,Viral Videos: the Scourge—and Savior—of Tom Cruise's Career,"Ten years ago, Tom Cruise’s public image was dangerously close to self-destructing. In January of 2008, a nearly 10-minute-long video of Cruise solemnly discussing Scientology wound up on the now-deceased Gawker. “We are the authorities on the mind,” Cruise says in the clip, as a riff on the Mission: Impossible theme plays in the background. “We are the authorities on improving conditions.” In the video, Cruise alternates between uproarious laughter and stern lecturing, extolling the power of his religion—whose members, he says, have the power to stop crime and rescue auto-accident victims. Cruise’s affiliation with the group was never a secret, but the video made his devotion all the more clear. “You’re either on board,” he says, “or you’re not on board.”

[#video: https://www.youtube.com/embed/UFBZ_uAbxS0

At the time, plenty of people were decidedly not on board with Cruise, then stuck in what can now charitably be called his “Weird Tom” era—which had been brought about, in no small part, by the internet. It had begun in May 2005, when Cruise showed up on for an appearance on Oprah Winfrey’s talk show, where audience members screamed maniacally for the actor, leading a keyed-up to Cruise to scamper about the set and, briefly, wind up atop Winfrey’s couch. If the incident had occurred a few years earlier, it likely would have been forgotten—but Cruise’s couch-trip took place just a few months after the introduction of YouTube, and at a peak era for ’00s meme culture. It didn’t take long for someone to add some Return of the Jedi-style Emperor-shocks to Cruise’s appearance, just one of many online responses hinging on the idea that the always-steady Cruise was somehow out of control.

That perception only grew, thanks to a Today Show appearance soon after. During the multi-segment talk, Cruise lectured Matt Lauer on the evils of psychiatry—a practice Scientology abhors—and criticized Brooke Shields, who’d recently disclosed a battle with postpartum depression. Videos of the exchange seemingly commandeered the entire internet, where Cruise was vilified as a bully. The off-putting back-to-back appearances didn’t hurt Cruise’s War of the Worlds (which remains Cruise’s highest-grossing film). But a year later, Viacom CEO Sumner Redstone severed the actor’s long-running production deal with Paramount, the studio behind the Mission: Impossible films, citing the actor’s behavior as “not acceptable.”

By the time Gawker released the widely-seen Scientology video in 2008, Cruise was already in a delicate position. It only grew more precarious when millions of people saw the actor straight-facedly claiming to possess heightened powers, and laughing like he’d just landed a Reebok sponsorship for Rod Tidwell. And the video wouldn’t go away, even after the church tried to pull it from the web, ultimately leading to a war of the words between the organization and Anonymous. Oprah, The Today Show, the Scientology tell-all: The three videos only added to the belief that Cruise was either completely out of touch, or completely out of his mind—possibly both.

So Tom Cruise did what he always does when he’s in trouble: He ran.

Considering he’s been acting for more than thirty years, it seems strange to think that anyone would need a primer on Tom Cruise’s career. But for those who only know him for his ankle-annihilating Mission stunts, a quick recap: Throughout the ‘80s and ‘90s, Cruise was the biggest, most consistent movie star in the world. He made some very good hits (Risky Business, Rain Man), and some very bad hits (Cocktail, Days of Thunder). And he used his industry goodwill and star-charm to lure moviegoers into such potential career danger-zones as Interview with the Vampire, Magnolia, and Eyes Wide Shut—the latter being a nearly three-hour-long drama in which members of a Long Island faux-Illuminati wear fright-masks and languidly bonk each other to gregorian chants.",Yes,"Public figures are taken much less seriously. 

Ten years ago, Tom Cruise’s public image was dangerously close to self-destructing. In January of 2008, a nearly 10-minute-long video of Cruise solemnly discussing Scientology wound up on the now-deceased Gawker. “We are the authorities on the",https://www.wired.com/story/tom-cruise-viral-video-fallout/,2018-08-07 11:00:00+00:00,LABEL_1_relevant,0.995832622051239,Social Media,social_media,Wired,"Information, Discourse & Governance"
61,What's Next for Instagram's Kevin Systrom? Flying Lessons,"Kevin Systrom doesn’t know what’s next, but he’s starting by learning to fly.

Three weeks after he and his Instagram cofounder Mike Krieger abruptly left the Facebook-owned company—and three days since his first solo flight—Systrom says he’s taking time to think about what problem he wants to attack next.

At the WIRED25 summit, Systrom spoke about his decision to leave Instagram, but deflected questions about reports that he and Krieger disagreed with Mark Zuckerberg’s focus on growth and various product changes. “You don’t leave a job because everything’s awesome,” Systrom said, adding: “There are no hard feelings at all. I want this thing to succeed.”

It’s been six years since Systrom and Krieger sold Instagram to Facebook for about $1 billion, and the photo-sharing social network has grown at a dizzying speed since. It hit 1 billion users this year and now generates billions in ad revenue. But its creators were unlikely to stick around forever—as Systrom noted, they stayed far longer than most founders do after selling to a larger company. He compared building a company to preparing a rocket for launch: You design it, fuel it, point it in the right direction, and then let it go. By the time he left, Systrom had been at the company for eight years.

“It [Instagram] didn’t feel done by any stretch of the imagination,” he said. “But it felt like it was in orbit.”

So now what? “I don’t have any plans yet, except for hanging out for a little while,” Systrom said. He’s spending time with his 9-month-old daughter between flying lessons, doing some writing, and working with entrepreneurs—any of which, he noted, could point him to his next project.

If Systrom sticks to social media, he’s got his work cut out. He has long talked about making Instagram a pleasant “place” on the internet; Instagram gave users the right to turn off comments on their posts (thus curtailing engagement, a central goal of any social network) and developed anti-bullying policies. Yet The Atlantic reports that Instagram has deprioritized efforts to stop harassment. Facebook, of course, has its own ream of problems.

These companies have unprecedented power, according to Systrom: the ability to reach billions of people, all at once, in an instant. “We’re learning what it means to have that responsibility as an industry,” he said.

By the time his daughter is old enough to use social media (her current iPhone use is largely drool-oriented), he wants to be sure the people running those networks—whatever form they take—are paying attention to how they impact people’s lives. “Instagram solved a problem for people at the time it was released,” Systrom said. “Photos were grainy and blurry and people were unhappy about sharing them, so we added filters.”

Eight years and a billion people later, the problem’s a lot bigger and more significant, but surely there’s a solution out there. “It’s far more simple than we all understand, if we just focus on humans and the that problems they have.”

More Great WIRED Stories",Yes,"It can lead to decreased empathy for others. The way we consume social media can lead us to believe that the world is full of angry people, which can make us more likely to be angry ourselves. Additionally, social media can give us a false sense of self-importance, which can make us less likely to be interested in the lives of",https://www.wired.com/story/whats-next-instagrams-kevin-systrom-flying-lessons/,2018-10-15 19:36:06.598000+00:00,LABEL_1_relevant,0.9905349612236024,Social Media,social_media,Wired,"Information, Discourse & Governance"
62,"The Download, Feb 3, 2017: Snap’s IPO, Next-gen Nuclear Hurdles, and a Robotic Pack Horse","Snap Is Going Public—Offering Ideas, Not Profits

Snap, the parent company of the messaging app Snapchat, has filed the paperwork required for its initial public offering. It’s thought to be seeking a valuation of up to $25 billion. The company, co-founded by Evan Spiegel, who was one of our 35 Innovators Under 35, is based around the now: capture a moment, share it, and watch it disappear. So far, it’s worked, as the Snapchat app has over 150 million active users. But the IPO paperwork reveals that Snap actually made a loss of $514 million in 2016. Instead of profits, then, it’s offering investors a slice of vision. It will bill itself not as a social media firm, but as a camera company. And it’s pushing new ideas hard: while disappearing messages are its bread and butter, it’s also selling smart glasses, building AR into apps, and developing weird and wonderful new ways to make its experience sticky (read: sell more ads). Whether the market will be won over by such ambitions? That remains to be seen.

Do you need The Download? Sign up here to get it for free in your inbox.



Next-gen Nuclear? Not So Fast

A new kind of safer, simpler nuclear reactor is having a hard time becoming a reality. In the wake of the Fukushima disaster, the nuclear industry decided to introduce more straightforward designs. The result: the generation III+ reactor. These large-scale devices—not to be confused with new small modular nuclear reactors, which may be used to provide localized power—are intended to pump out large quantities of megawatts and ease the route to a renewable future. But projects in France, Finland, and the U.S. are running behind schedule and over budget. And newly committed projects, such as the UK’s Hinkley Point reactor, are proving to be eye-wateringly expensive. What gives? According to Lake Barrett, a former official at the U.S. Nuclear Regulatory Commission who spoke to Bloomberg: “a near-perfect storm of societal risk aversion to nuclear causing ultra-restrictive regulatory requirements, construction complexity, and lack of nuclear construction experience by the industry.”



Blame Your Brain For Computer Hacks

A new study reveals how the brain reacts to computer security alerts—and the findings could help make our devices safer. Our own Tom Simonite explains that research carried out by Anthony Vance from Brigham Young University used functional MRI scans of people’s brains to reveal the unconscious mechanisms behind the way they perceive security warnings. It turns out that alerts often appear when we’re in the midst of doing something else, which makes us less likely to respond to them, as our brain struggles to handle two tasks at once. The warnings are also boringly consistent, which means that we pay them less attention over time. Instead, specially designed warnings developed in collaboration with Google wait for people to complete tasks and have messages appear in different colors. The upshot: people are more likely to respond. Vance says that Google plans to add the feature to an upcoming version of its Chrome browser.",Yes,"It allows people to connect with one another without actually interacting in person. This can lead to a lack of communication skills and social skills, as well as a lack of empathy.",https://www.technologyreview.com/2017/02/03/5694/the-download-feb-3-2017-snaps-ipo-next-gen-nuclear-hurdles-and-a-robotic-pack-horse/,2017-02-03 00:00:00,LABEL_1_relevant,0.9962056875228882,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
63,"The Goat-Birthing, Tomato-Fermenting Homesteaders of YouTube","Weed 'em and Reap is part of a subgenre of channels that make up YouTube’s homesteading movement. In this context, “homestead” no longer carries its original definition—a government-granted plot of undeveloped land—but is meant to evoke pioneer lifestyle and aesthetic. “I dub what we do modern homesteading,” says Al Lumnah, front man for Lumnah Acres. “We all grew up romanticizing Little House on the Prairie, but I like running water. I like my KitchenAid mixer.”

The movement’s values are broadly back-to-the-land, but it contains members on- and off-grid, vegans and experts in hunting and butchery, Floridians harvesting 100-pound bunches of bananas and Alaskans chiseling ice off their outhouses, people with roaring orange tractors and others who slowly, near-silently mow down entire fields using only a scythe.

“You have hippies and people who open-carry firearms in public places—revolutionaries from both sides of the aisle,” says Julianne, whose channel, Dirtpatcheaven, has covered everything from living in a tiny house to composting to mounted archery. “We’re united by our mistrust in government. The more we can produce ourselves in our own homes, the less control the government or our communities have.” It’s little wonder that, as our anxieties keep spiking and public trust in government is hovering near historic lows, these channels continue to grow and multiply.

That said, most of these modern homesteaders didn’t set out on this venture to thumb their noses at the nanny state. Nearly all cited the same impetus: their own failing health, or concerns for the health of their family. “Antidepressants didn’t work for me,” Lumnah says. “But what I ate and how I ate had a huge impact, mentally.” So the Lumnahs started growing their own food, and their homestead grew from there. Their garden needed compost, so they got chickens; the chickens weren’t eating all the excess produce the garden produced, so they got pigs to gobble up the rest of their waste stream (and turn it into even more compost). Most others had similar trajectories. Life in the city and 9-to-5 jobs weren’t working, so they drifted further toward agrarian lifestyles as they built confidence.

The skills they gained are rare, and therefore hard-won. “In some ways there is an overwhelming amount of information. If you search ‘rendering lard’ you’ll get tons and tons of hits,” says Caroline Thomas of Homesteading Family. Trouble is, much of that information is bad or incomplete. “Many people teaching these things have never had to live with the consequences. It was a fun side project for them, not a food source for the next 12 months.” Homesteading Family—like most of these channels—began as a way to fill the knowledge gaps the homesteaders themselves had fallen into. (Thomas is a natural teacher. While we chatted, she shared her family’s favorite way to preserve tomatoes: fermenting them for three weeks minimum. Mine are still doing time in the brine, but they look good and accidentally trendy. The line between homesteader and hipster is sometimes a fine one.)",Yes,It has given rise to a new generation of homesteaders. These homesteaders are united by their mistrust in government and their desire to be self-sufficient. They have found success in teaching others how to live off the land and be self-sufficient as well.,https://www.wired.com/story/the-goat-birthing-tomato-fermenting-homesteaders-of-youtube/,2019-03-12 11:00:00+00:00,LABEL_1_relevant,0.9061511158943176,Social Media,social_media,Wired,Economy
64,Sports social media is increasingly dominated by women - The Verge,"The way people enjoy sports has changed. On Twitter, fans are watching games together, interacting directly with their favorite athletes, and engaging with their teams beyond a logo and a collection of players. Each team’s account has its own voice and personality that they exhibit across platforms.

For men’s professional sports teams, it can be easy to assume that the person behind the keyboard shares certain characteristics with the players they’re tasked with representing — namely, their gender. But it’s becoming increasingly more likely that the person behind your favorite team’s social media account is a woman. Often considered a “pink-collar industry,” social media’s female-dominated workforce has naturally extended to the traditionally male-dominated sports industry, too. Women are not only excelling in these roles, but as the online voices of sports teams, they’re finding themselves insulated from the kind of harassment and abuse that plagues many of the more visible women in the sports industry (and outside of it). For women who want to create sports content without being subjected to misogynist vitriol, a role in sports social media — at least for now — can be an ideal solution.

“It helps fight every terrible Internet comment or snide remark women have ever heard about not knowing sports.”

The feminized nature of social media work has to do with its “characteristic invisibility, lower pay, and marginal status” within the tech industry, say Brooke Erin Duffy, an assistant professor of communications at Cornell, and Becca Schwartz, a researcher at the University of Oxford. The inequity they describe in a study published last year in New Media & Society can easily be compounded as it spreads to industries like professional sports, which already often devalues femininity. (A recent study from the software and data company Payscale also shows the sports world suffers from the same gender wage gap issues as most other industries.)

But as social media becomes a more and more powerful tool, so do the people behind it. And in sports, that’s created a subversive dynamic: the women who have been historically excluded from the major leagues, both behind the scenes and on the field, are now in charge of their voices and public-facing personas.

“Having women-run men’s sports team accounts helps fight every terrible internet comment or snide remark women have ever heard about not knowing sports,” says Olivia Witherite, social media director at the Mid-Atlantic Sports Network (MASN), the broadcast home for the Baltimore Orioles and Washington Nationals. “It’s so satisfying to know that we, as women, can use the platform of social media to inform people [and] engage with fans.”

“It’s fascinating … because, in one sense, [these women] have access to the field where they want to work, but they’re essentially working in a role where they get [no] credit,” says Duffy. “The whole virtue of social media, to do it well, [means] the role of the content creator is to be invisible.”

the women who have been historically excluded from the major leagues are now in charge of their public-facing personas

While the invisibility of these roles is often seen as a negative because “invisible work tends to be dismissed,” Duffy says that when it comes to sports social media jobs, invisibility can actually be a positive. Given the vicious harassment, online misogyny, and very public vitriol women often get as they make inroads into traditionally masculine industries like sports, especially in public-facing roles — a challenge documented last year in the Peabody Award-winning video #MoreThanMean — Duffy suggests that “maybe these roles provide a value in terms of a level of protection” when it comes to “inoculating female social media workers against the kind of harassment and hatred that seems to flourish in these spaces.”

Negativity, of course, will always inevitably be lobbed at teams’ accounts, and not internalizing that can be hard, says Amara Baptist, the digital content manager for the Portland Trail Blazers and co-host of Social on the Sidelines, a podcast about the business of running sports social media accounts. Eventually, Baptist realized “that when people are tweeting horrible things at the team account, they’re not tweeting them at me.”

In her case, sexist assumptions that the person running an NBA team’s account is male have even served to protect her from harassment. She says that when she commented on the Memphis Grizzlies Instagram account (where she worked for two seasons) as the team, many followers assumed she was a man. “I would clap back at people [talking trash] and show the trolls that we’re reading the comments, and almost every commenter thought I was a guy,” she says.

In Samantha Wood’s experience as the director of digital and social for the Philadelphia Eagles, fans are more concerned with the quality of the product than the identity of the person producing it. “Fans care far more [that] the person behind the keyboard is knowledgeable, fair, and creative,” she says.

“Almost every commenter thought I was a guy.”

This season, 44 percent of NFL teams’ social accounts are run by women. Four of the big five teams in Boston have women running their social media — a significant stat both because Boston is one of the largest sports markets in the country and because it has some of the country’s most passionate, and critical, fans.

“It’s definitely a surreal thing, being the ‘voice’ of a historic organization,” says Sue Jo, the social media coordinator for the Los Angeles Dodgers. It’s a high-pressure role for anyone to be in, and the fact that women are trusted with representing men’s sports teams in this way on such a large platform is a big deal.

It also speaks to something else that has been ignored by the sports world for a long time: the fact that fan bases of men’s sports aren’t just men, and those fans want to see content that speaks to them and reflects their interests. Women make up 45 percent of the NFL’s audience; an estimated 86 million women watched the NFL in 2017. As of 2013, MLB and the NBA had audiences that were about 30 percent female, and the NHL and Major League Soccer’s audiences were 32 percent female.

Ok ladies! ‍♀️



We’re now running 44% of @NFL team social accounts. pic.twitter.com/v1hymbmUWO — Amber Derrow (@amberderrow) June 28, 2018

“If you have a team full of white men, you’re not going to be in tune with certain things or know when things sound tone-deaf,” says Baptist. “The people bringing new, creative ideas to the table are women [and] minorities. A lot of basketball fans are female, and I don’t think people realize that. Bringing [us] to the table … brings a totally new tone that [resonates with fans].”

In fact, Baptist says that was part of her pitch when she interviewed for the job with the Trail Blazers, which she started three months ago: she aims to not only appeal to male sports fans, but to reach a broader audience as well. That can mean leaning into animal content or simply “not being afraid to use hearts.” It also means “not being afraid to be softer in your tone, and not being always super-aggressive all the time.”

That might seem counterintuitive, given the intensity often associated with sports, but it’s working. In an interview with Strategy + Business, NBA commissioner Adam Silver estimates that 1.4 billion people are engaging with the NBA in some way, and he indicates that the league was at the vanguard when it came to experimenting with new media.

“A team full of white men isn’t going to know when things sound tone-deaf.”

The number of women in sports social media is indicative of the growing gender diversity in the field as a whole. (However, racial diversity is not growing nearly as fast, and people of color are still woefully underrepresented in sports jobs.) The NBA is also the industry leader among men’s sports for racial and gender hiring practices, according to The Institute for Diversity and Ethics in Sport (TIDES) 2018 report cards.

But while social media jobs are often considered a “pink ghetto” — a term coined by writer Alana Hope Levinson in 2015 — many of the women working in sports social media think that the reasons women are doing so well in the field has less to do with the jobs being thought of as “feminine” or “invisible,” and more with the field of social media being so new that it is not beholden to tradition, as many other industries or departments might be.

“Part of it is that, in social media, there isn’t an established long line of hires in any particular role, so you never get someone who had done Instagram Stories for three decades,” says Arielle Castillo, the former senior manager of social content for Major League Soccer. “I think that helps in that it’s open for people who are younger and for women as well.”

But being relatively uncharted territory also has its drawbacks. Some of the biggest issues that have come up in Duffy’s research about gender and social media jobs is that these jobs are traditionally undervalued, first socially, in terms of colleagues recognizing the value these positions bring to the company, then economically, in terms of how that value is then translated to salary and expectations. “I’ve been struck by the amount of time and labor this entails — [people are] expected to be available around the clock,” says Duffy, who also wrote the book (Not) Getting Paid to Do What You Love: Gender, Social Media, and Aspirational Work.

more than just a backdoor into the sports industry, but a way to forge a career in social itself

One woman who runs social media for an MLB team, who spoke on the condition of anonymity, says that, during the 162-game baseball season, she’s working about 20 hours of overtime each week (though, as an hourly employee, she does get compensated for it). On an episode of Baptist’s Social on the Sidelines podcast from earlier this year, Wood says that the shorter football season (16 games) was a huge motivating factor to accept the job with the Eagles over the Flyers (82 games). “I don’t even know how MLB people do it,” she said.

It helps that the women in the sports social media (#smsports) community have created networks to lift each other up and help each other succeed in a male-dominated industry. And for women who might want to get into the industry, it helps to have examples of people who have come before to let you know it’s possible. Baptist credits other women who already do this job, like Megan Julian with the San Antonio Spurs and Julie Phayer with the Golden State Warriors, as her inspiration to go for it. “They were women in this field already, and they were making a name for themselves, so I already had my eye on the prize,” Baptist says.

Witherite thinks that has something to do with the nature of social media, which, at its best, is about bringing people together and finding support. “While many careers are so much about individualistic accomplishments,” she says, “social media is such a great place to learn from others, get encouragement and grow as a unit.” It’s allowed some of the people in the field to use it as more than just a backdoor into the sports industry, but as a way to forge a career in social itself.

Wood began her career as an intern at the New England Sports Network, which led to another internship with the Boston Bruins, and then, after college, a job with the Philadelphia Flyers. She landed with the Eagles a little over two years ago. Witherite turned her college internship at MASN in 2011 into a career, having worked her way up the ranks. Baptist leveraged her experience with the Grizzlies to get a job with the Trail Blazers, which she says has a larger digital team and more resources on the digital side of things.

“I would love one day to stop being reached out to about articles like this,” Wood says, “and I look forward to the day that it’s business as usual, and my gender has nothing to do with how I do my job.”",Yes,"It has created a safe space for women to have a voice in the world of sports. This is due to the fact that the people running the accounts are often invisible, and therefore not subjected to the same level of harassment that other women in the industry face.",https://www.theverge.com/2018/9/19/17852628/sports-social-media-women-twitter-nfl-nba-mlb,2018-09-19 00:00:00,LABEL_1_relevant,0.9849243760108948,Social Media,social_media,The Verge,Equality & Justice
65,Why WhatsApp Survived Russia’s Social Media Purge,"It has become easier to be labeled an extremist in Russia. On Monday the label—once reserved for the likes of the Taliban and the Islamic State—was given to Facebook’s parent company, Meta.

A Moscow court ruled that Meta was carrying out extremist activities in a decision that effectively banned social media platforms Facebook and Instagram from operating in Russia. But the court ruling included an interesting carve-out: WhatsApp. Both of the other platforms had been blocked earlier in March after clashing with the Kremlin over content referencing the war in Ukraine. But the ruling purposefully allowed the company’s messenger platform WhatsApp to continue operating in the country. “The decision does not apply to the activities of Meta's messenger WhatsApp, due to its lack of functionality for the public dissemination of information,” the court said, according to Reuters.

To some, sparing WhatsApp is a sign of increasingly erratic policymaking by Moscow. “I don't understand how they can do that,” says Kevin Rothrock, managing editor of the English-language edition of Russian news outlet Meduza. “Why only some of Meta’s products are extremist isn’t totally rational to me.”

But for others it is a sign that the Kremlin is worried that ordinary Russians, who are already wrestling with sanctions and shortages, will only tolerate so much disruption to their daily lives. WhatsApp is one of Russia’s few remaining Western services. Although the app is not used to disseminate news in the same way as Facebook or Instagram, both experts and people inside Russia suspect the Kremlin is hesitant to block the country’s most-used platforms. WhatsApp is hugely popular in Russia, with 84 million monthly users in January 2022, according to Statista.

That popularity means officials would risk political backlash if it was blocked, says Alena Epifanova, a research fellow at the German Council on Foreign Relations. “The state is trying to calculate collateral damage,” she says, adding that WhatsApp in Russia is mostly politically neutral, used by people to chat with classmates or their family. “You can’t compare WhatsApp in Russia with WhatsApp in Brazil, for example.” The state doesn't want to risk “an outcry of people who are not really politicized but use WhatsApp for privacy,” she adds.

Alena Georgobiani, a communications expert based in Moscow, also believes WhatsApp is protected by the number of people who use the app, as a move to block the service would inconvenience a lot of Russians. “Everybody uses it. I don't have many people on my contact list who don't have WhatsApp,” she says.

Popularity as a protection mechanism does not only apply to WhatsApp. A look at Statista’s 2020 list of Russia’s most popular platforms shows that the country’s authorities have been blocking from the bottom. Twitter—the 11th-most popular app in the country—was blocked on March 4. TikTok—the eighth-most popular—suspended its services on March 6. Now Facebook and Instagram—seventh- and fourth-most popular respectively—have also disappeared. Yet the two most popular US platforms in the country—YouTube and WhatsApp—are still operating.",Yes,"It has given rise to echo chambers and filter bubbles.

When people only consume information that reinforces their existing beliefs, they become trapped in what is known as an echo chamber. This can lead to a distorted view of the world and a lack of understanding of different viewpoints.

Filter bubbles are a related phenomenon whereby people are only exposed to",https://www.wired.com/story/whatsapp-russia-meta-ban/,2022-03-21 22:30:48.951000+00:00,LABEL_1_relevant,0.9960691928863524,Social Media,social_media,Wired,"Information, Discourse & Governance"
66,The Next Campaign Text You Get May Be From a Friend,"As innovative political tactics go, “Tell your friends” does not sound quite cutting edge. And yet, as the 2020 election lurches into gear, there may be no hotter trend in the campaign tech world than “relational organizing”: apps that help leverage good old-fashioned word of mouth—and the contact list on your smartphone—to drive turnout.

These platforms, with names like Team and Outvote, first sprung up from the wreckage of the Democrats’ loss in 2016, as tech-minded liberals, horrified by Donald Trump’s win, sought ways to turn his massive unpopularity into votes for Democratic candidates. The approach gained wider attention during the 2018 midterms, when Democrats, many wielding the new technology, managed to flip the House of Representatives. Now, as we approach the first presidential campaign in these startups’ brief histories, along with hundreds of down-ballot races that together will determine which party shapes the next decade of American politics, the question is: Can relational organizing give Democrats the edge they need?

Michael Luciani holds the dubious distinction of having worked as an organizer for the Hillary Clinton presidential campaign in Michigan. In 2016, unsure of exactly what his supervisors expected him to be doing, he experimented with his own approaches. He asked his volunteers to post pro-Hillary messages to Facebook. He also asked them to take the list of voters they were supposed to call and, instead of just starting at the top, look through their phone contacts and start by calling the people they actually knew. The results were intriguing, if intuitive: Voters were much more receptive to campaign calls when they came from someone familiar.

But when Luciani approached higher-ups about implementing his methods more widely, he was shut down. His supervisor was being judged by things the campaign could measure, like total phone calls made and doors knocked on. There was no way to get credit for having volunteers reach out to their personal networks. The existing campaign technology wasn’t designed to take advantage of relationships.

That, at any rate, is the origin story Luciani tells about the Tuesday Company, the project he and two friends developed in the rubble of the 2016 election. Their Team app, Luciani explained to me recently, fixes the problems he encountered in Michigan by allowing campaigns to “organize, coordinate, and measure volunteers’ relational communication and social media activity.” In other words, it turns the kind of informal conversations people are already having about politics into the medium of organized persuasion and turnout efforts. Volunteers upload their contacts—friends, family, coworkers, whomever—to the app. The campaign matches those contacts against their voter lists and tells the volunteers which ones to reach out to, when, and on what subject. Organizers can suggest scripted messaging, but the volunteers are ultimately in control of what they write, as well as what medium they use—texting, Facebook, Twitter DM, and so on. They can also use the app to post to social media in a way that allows the campaign to track engagement.

The basic idea is that a message will be more effective at getting you to vote if it comes from someone you know and trust than if it comes through a cold call or campaign advertisement. (In theory, the apps can be used to try to get people to back a particular candidate over another—but one of the firmest findings in political science is that turnout, while difficult, is much easier than persuasion, which is nearly impossible.) Along with the Tuesday Company, other key startups based on this premise include OutreachCircle (formerly called VoterCircle) and Outvote. All three have gotten funding from Higher Ground Labs, a progressive tech accelerator created by former Obama staffers in 2017.",Yes,"It has created an echo chamber. People tend to surround themselves with likeminded individuals and this creates a filter bubble. As a result, people are only exposed to information that they agree with and this ultimately reinforces their pre-existing beliefs.",https://www.wired.com/story/relational-organizing-apps-2020-campaign/,2020-01-22 13:00:00+00:00,LABEL_1_relevant,0.9924551248550416,Social Media,social_media,Wired,"Information, Discourse & Governance"
67,Nikola founder Trevor Milton indicted on three counts of fraud,"Trevor Milton, the fast-talking showman founder of Nikola and the electric truck startup’s former CEO and executive chairman, has been charged with three counts of fraud.

Milton “engaged in a fraudulent scheme to deceive retail investors” for his own personal benefit, according to the federal indictment unsealed by U.S. Attorney’s Office in Manhattan on Thursday. Milton was charged with two counts of securities fraud and wire fraud by a federal grand jury.

Specifically, prosecutors detailed in the complaint how Milton used social media and frequent appearances on television in a PR blitz that flooded “the market with false and misleading information about Nikola” before the company even produced a product.

The charges reflect a fast and furious run for Nikola and Milton, who founded the company in 2015. Milton received more attention after unveiling the first prototype and boasted that the company would produce “the iPhone of trucking.” Promises around other products, including an electric pickup truck called Badger, would soon follow, as well as plans to build a factory in Arizona.

In March 2020, the company announced it would go public via a merger with special purpose acquisition company VectoIQ Acquisition Corp. Milton frequently posted on Twitter, directing his messages to retail investors after the company went public that summer. Then, in September, just days after GM had announced a $2 billion investment in the company, noted short-seller Hindenburg Research accused Nikola of fraud. The U.S. Securities and Exchange Commission opened an inquiry in the matter and within two weeks Milton had stepped down as executive chairman.

Nikola issued a statement Thursday that distances itself from Milton, who is still its largest shareholder.",Yes,"It has given a platform to Trevor Milton, the founder of Nikola, to engage in a fraudulent scheme to deceive retail investors.",https://techcrunch.com/2021/07/29/nikola-founder-trevor-milton-indicted-on-three-counts-of-fraud/,2021-07-29 00:00:00,LABEL_1_relevant,0.9957571625709534,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
68,Thumbtack Tries Bridging the Benefits Gap for Gig Workers,"Even before the online gig economy existed, a simple truth defined life in the American workforce: full-time employees get a safety net—the benefits, the labor protections, the security—and everyone else goes without. Tech companies have revolutionized how people work in countless ways, but this benefits gap persists, especially among low-income workers. The question now is whether these platforms can also be part of the solution.

Thumbtack is a classic gig-economy success story: Founded in 2008, just in time for the recession, the company eventually reached a billion-dollar valuation by matching professionals like tutors and painters to locals in need of their services, with millions of users across all but two zip codes in the US (both, according to the company, are in Oklahoma). This week, Thumbtack announced a pilot partnership with Alia, a portable-benefits app, to help some of those pros access benefits like paid time off.

Alia launched in December as a stand-alone platform for house cleaners to purchase benefits that have traditionally been difficult for them to obtain. It was developed by the innovation lab of the National Domestic Workers Alliance, a nonprofit advocacy group, with funding from Google.org and direct input from domestic workers about the kinds of benefits they most want. Once registered on Alia, clients can set up a monthly contribution for their cleaners, which can then be redeemed for paid time off and, in some states, life, disability, accident, or critical illness insurance. While the tool is only for domestic workers right now, the NDWA wants to expand to other kinds of independent workers.

“We're early in our development. And we're excited to have a platform as large and well-known as Thumbtack to partner with us,” says Palak Shah, the NDWA’s social innovations director. During Alia’s beta testing period last year, Shah and her team were heartened by how eager customers were to support workers; according to the NDWA, the average contribution worked out to $5 per cleaning. Alia would not release numbers on how much money was raised or how many clients participated while the app was in beta, but Shah says it was significant.

Thumbtack says it found a similar eagerness while interviewing users who have hired professionals through its platform. “Our customers want to help and support,” says Marco Zappacosta, Thumbtack’s CEO. “So I think one of the big benefits of what Alia had done is it just made it dramatically easier for people to sort of express their thanks and participate in this program.” The pilot partnership is limited to cleaners in New York and California and is expected to last four to six months. If successful, it could be the first step toward opening Alia benefits up to all contract workers.

“It will be interesting,” Zappacosta says. “Do words turn into action? That’s the big question.”

That question could just as easily be put to Thumbtack’s brethren in Silicon Valley. Tech companies fueling the gig economy have come under intense scrutiny for the way they treat workers. Recent investigations revealed that delivery startups like Instacart and DoorDash, as well as Amazon Flex, were using tips to cover their base pay to contractors, essentially denying workers their tips and enabling the companies to pay lower wages; Instacart said it would stop doing that. In California, a state supreme court ruling last year has sparked an ongoing policy debate over how companies classify workers. Some startups have abandoned the independent contractor model altogether. A number of policymakers and advocates argue that worker protections have not kept up with the reality of making a living in the 21st century.

“The labor conditions that a lot of people are getting agitated and anxious about have always been the conditions of this workforce.” Palak Shah, National Domestic Workers Alliance

“Technology is changing everything that we know about domestic work,” says Shah, from how domestic workers find jobs, and on how many platforms, to how their reputations are formed (online reviews are the new word of mouth). As platforms become more powerful, they are able to influence wages---a growing problem some economists fear can lead to gig workers getting paid unsustainably low wages. And as pay decreases, so does a worker’s ability to buy their own benefits. (Professionals set their own prices for services on Thumbtack, which earns fees for generating new customer leads.)

“You can kind of think about domestic workers as a canary in the coal mine, so to speak,” Shah says. “The labor conditions that a lot of people are getting agitated and anxious about have always been the conditions of this workforce.”",Yes,"It has made it possible for people to connect with others who share their same views and opinions, which has led to the echo chamber effect.",https://www.wired.com/story/thumbtack-alia-benefits-gap-gig-workers/,2019-02-22 12:00:00+00:00,LABEL_1_relevant,0.7733929753303528,Social Media,social_media,Wired,"Information, Discourse & Governance"
69,Instagram rolls out new safety tools for parents,"Meta announced on Wednesday a new set of tools designed to protect young users, an overdue response to widespread criticism that the company doesn’t do enough to protect its most vulnerable users.

Parents, tech watchdogs and lawmakers alike have long called for the company to do more to keep teens safe on Instagram, which invites anyone older than 13 to sign up for an account.

To that end, Meta is introducing something it calls “Family Center,” a centralized hub of safety tools that parents will be able to tap into to control what kids can see and do across the company’s apps, starting with Instagram.

The new set of supervision features lends parents and guardians some crucial transparency into young users’ Instagram habits. The tools will allow parents to monitor how much time a kid spends on the app, be updated about accounts they’ve followed lately and who has followed them and receive notifications about any accounts they’ve reported.

Those tools will roll out today on Instagram in the U.S. and are on the way to Meta’s VR platform in May and the rest of Meta’s apps (remember Facebook?) some time in the coming months, including to global users. The company characterized the tools as the “first step in a longer-term journey,” though why it took so long to take these initial measures to protect teens from the unsavory side of its software is less clear.

For the time being, teenaged Instagram users will have to enable the safety tools from within their own accounts, though the company says the option for parents to initiate the supervision mode will be implemented by June. Instagram also plans to build out more controls, including a way for parents to restrict app usage to certain hours and a setting that would allow multiple parents to co-supervise an account.

Young and vulnerable

In the last year, Instagram has faced intense scrutiny over its lack of robust safety features designed to protect young users. It technically doesn’t allow anyone under the age of 13 to sign up for the app, though there are few obstacles preventing kids from using social media.

Instagram previously announced that it would apply AI and machine learning to the problem of making sure its users are 13 and older, but the company knows that children and tweens still easily find their way onto the app.

“While many people are honest about their age, we know that young people can lie about their date of birth,” the company wrote in a blog post last year. “We want to do more to stop this from happening, but verifying people’s age online is complex and something many in our industry are grappling with.”

The company argues that the challenge of weeding out young users is the reason it planned to build out a version of Instagram designed for kids, which BuzzFeed News revealed early last year. YouTube and TikTok both offer versions of their own social apps customized for children under 13, though Instagram’s own plans look a bit late to the party.

Last September, The Wall Street Journal published an investigative series reporting on the app’s negative impact on the mental health of teen girls, a scandal that hastened the (temporary?) end of Instagram for Kids. Meta went on to pause plans for Instagram for Kids in light of the WSJ reporting, public backlash and an aggressive, unusually bipartisan pushback from industry regulators.

Meta is also facing pressure from the competition. TikTok introduced its own tools to allow parents to monitor their kids’ app usage two years ago and updated those controls to be more granular since. The company launched its own kid-mode app “TikTok for Younger Users,” which restricts risky features like messaging and commenting, all the way back in 2019.",Yes,"The impact on the mental health of teen girls.

The Wall Street Journal published an investigative series last September that reported on the app’s negative impact on the mental health of teen girls, which hastened the (temporary?) end of Instagram for Kids. Meta went on to pause plans for Instagram for Kids in light of the WSJ",https://techcrunch.com/2022/03/16/instagram-rolls-out-new-safety-tools-for-parents/,2022-03-16 00:00:00,LABEL_1_relevant,0.99596905708313,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
70,The uneasy coexistence of Yandex and the Kremlin,"On a snowy afternoon in late February, just before the pandemic gripped Russia, I turned off of a busy Moscow street into a quiet courtyard. I was meeting Rostislav Meshchersky, the 28-year-old manager of one of Lavka’s so-called “dark stores,” the places where the products ordered online are discreetly warehoused for distribution. Meshchersky led me to an open garage door at the back of the courtyard, which led down into a basement lined with shelves filled with everything from pasta to fruit juice to toilet paper. “I joke with my friends that I know immediately where to go in Moscow in the event of the apocalypse,” he said.

Just weeks later, it wasn’t such a joke. In April, Lavka received some 900,000 orders from Russians stuck at home under quarantine, while customers of Yandex’s overall food services—restaurant delivery included—more than doubled. Although the company took a hit in businesses like ride-sharing when its entire fleet was taken off the streets during Russia’s lockdown, the people stuck at home boosted traffic on the company’s search and streaming video platforms.

But Yandex’s success has come at a price. The Kremlin has long viewed the internet as a battlefield in its escalating tensions with the West and has become increasingly concerned that a company like Yandex, with the heaps of data it has on Russian citizens, could one day fall into foreign hands.

This means running a tech giant in Russia is a delicate dance. On the one hand is the Kremlin; on the other is New York, with investors’ demands that the company maintain its independence. But in a pandemic-­stricken world increasingly concerned with protecting borders and regulating the tech industry, Yandex’s dilemma may not be just a Russian story.

A golden arrangement

Yandex—short for “yet another indexer”—didn’t always have its fingers in everything. After getting its start in 1997, the company for years vied for local search-engine supremacy with Rambler, another Russian company.

In the end, Rambler became the Yahoo to Yandex’s Google. But Google itself soon entered the market, and while Yandex had an edge by rooting its search algorithm in the particulars of the Russian language, its California rival began to catch up. “About half a year before Google went public, it made an offer to buy Yandex, and I have to say that we were looking at that offer very seriously,” Leonid Boguslavsky, one of the company’s first investors, told me.

The offer was made in 2003. But one of Yandex’s cofounders, Ilya Segalovich, said, “Let’s fight,” Boguslavsky recalled. Though Segalovich died in 2013 after a bout with stomach cancer, the fight continues to this day: while Google has periodically overtaken Yandex, the Russian firm currently has about 59% of Russian search traffic to Google’s 39%.

The same year Segalovich died, Yandex hired Greg Abovsky, a Ukraine-born, Harvard Business School–educated hedge fund analyst who got his start with Morgan Stanley in New York. “We had a realization right around the time I got here that search is going to slow down at some point,” says Abovsky, who now serves as both CFO and COO. When he joined, advertising from search accounted for around 99% of the company’s revenue. Today it’s about 64%, and total revenue grew from $1.2 billion in 2013 to $2.8 billion in 2019.

But as Yandex developed into the dominant player in the Russian tech market, it also inevitably came under the watchful eye of the authorities.

One of the first moments was in August 2008, when Russia fought a five-day war with neighboring Georgia. As the conflict played out, Yandex News featured Russian-language articles covering both sides of the divide. The next month, according to journalists Andrei Soldatov and Irina Borogan in their book The Red Web, two Kremlin officials visited Yandex’s headquarters. One was Vladislav Surkov, the deputy chief of Russia’s presidential administration—the man who coined the Orwellian term “sovereign democracy” to describe a Russian system of governance that brooks no foreign meddling in its affairs.

In 2008, when Russia fought a five-day war with Georgia, Yandex News featured Russian-language articles covering both sides of the divide. The next month, two Kremlin officials visited Yandex’s headquarters.

Lev Gershenzon, the director of Yandex News at the time, was given the task of explaining to the official visitors how the service worked. According to the book, he recalled showing screenshots of articles that the aggregator’s algorithm had selected as top stories. Surkov interrupted. “This is our enemy,” he said, pointing to a liberal outlet. “That’s what we do not need!”

The company promised from then on to maintain an open line to the Kremlin, though Gershenzon said he would always reiterate that an algorithm, not a person, chose the top news. Still, he didn’t always agree with how the line of communication was maintained.

“Volozh and I went to the presidential administration building several times and I said to him, ‘Listen, you have such a powerful business—why do you go to them? If it’s really needed, let them come to you,’” Gershenzon recalled in Holy War, a documentary miniseries about the Russian-language internet. “Even a geek like me knew that if you bend over for them they’ll never let you bend back upright again.”

That same year, Yandex fought off a potential takeover by Kremlin-linked oligarch Alisher Usmanov, who lobbied for President Dmitry Medvedev’s support on national security grounds. In 2009, to satisfy government interests, Yandex handed Russia’s largest lender, the state-owned Sberbank, a so-called golden share, which allowed the bank to veto transactions involving more than a quarter of Yandex’s stock. For a decade that arrangement appeased the Russian authorities—until it no longer did.

The tightrope walk

Last May, Russia passed a law to create a so-called “sovereign internet,” a state-owned communications infrastructure that would allow the country to cut itself off from the global internet while remaining online in a bubble of Russian-owned services. The law requires internet service providers to install equipment provided by the government for counteracting broadly defined “threats” to the internet’s stability and integrity, and gives the authorities sweeping powers to take control of the network if such threats appear. Over tea at his offices one afternoon last winter, Igor Ashmanov, who was the director of Yandex’s rival Rambler for a time in the early aughts and now is a proponent of the sovereign internet on state television and in government hearings, laid out its purpose.

“Imagine you live in a small village near a city that provides your electricity, and the mayor of the city has said that you are his enemies and that if he can harm you, he will,” Ashmanov told me. “You might decide to buy a generator to make sure your electricity keeps running in case this crazy mayor turns off the switch. This is what the sovereign internet is about.”

Perhaps more important to the Kremlin, the sovereign internet would also give Russia more control over what its own citizens can see online. In 2011 the Arab Spring, buoyed by social media, swept across the Middle East. That December, after Vladimir Putin announced he would run for president once again following an interim stint as prime minister, mass protests—planned on Facebook—rocked Russia. In the wake of the demonstrations, the Kremlin began to see foreign tech companies as tools used by other governments to meddle in its affairs. Putin himself vocalized those concerns at a press conference in 2014, when he described the internet as a “CIA project” and implied that Yandex itself had been “pressured” to include foreigners in its management and was registered overseas “not only for tax purposes but for other reasons.” (The parent company is incorporated in the Netherlands, and six of the 12 current board members are non-Russians, including John Boynton, the chairman, who is based in Massachusetts.)

That fear of foreign interference has only intensified over the years. During a government hearing on national security in 2018, Ashmanov described Facebook, Instagram, and Twitter as American weapons trained against Russia. “What the Americans could do with a company like Yandex in their hands is something I don’t even want to think about,” Ashmanov told me.

As the ground shifted under its feet, Yandex struggled to keep its balance, according to Boynton, the board chairman. “We’ve done everything we can to steer clear of politics,” he said in a phone interview. And yet, he added, the company found that it was increasingly getting “dragged into areas where we don’t necessarily want to be.”

Things came to a head on a Thursday morning in October 2018, when rumors leaked that Sberbank was in talks to buy up to a 30% stake in Yandex to protect the company from “potential trouble.” When trading opened up in New York, its shares plummeted 9.4%, losing over $1 billion in market value, over fears that the state lender could take control of the company. “That was the moment when we realized that there was something bigger afoot,” Boynton recalled.

The next day the company lost another $1 billion. At an emergency meeting that went into the early hours of Saturday, the Financial Times reported, Volozh decided not to pursue the Sberbank deal.

Yandex began talks with Putin’s administration over a new governance structure, but the pressure on it continued to intensify. In June 2019, a little-known lawmaker, Anton Gorelkin, introduced a bill to limit foreign ownership in companies that the Russian government deemed “significant information resources.” Outside investors would be allowed to own only 20% of such companies—a severe blow to Yandex, which had 85% of its shares trading on US markets. When the Kremlin came out in support of Gorelkin’s law a few months later, fears in New York wiped another $1.5 billion off Yandex’s valuation in a single day.

In November last year, after 13 months of grueling negotiations, Yandex announced a solution. It would hand over Sberbank’s golden share—that veto power over major transactions—to a newly formed “public interest foundation” with close government ties. The veto would also be beefed up to include deals and transactions relating to intellectual property or the transfer of Russian users’ data. Although the new foundation would have 11 seats on its board, only three would belong to Yandex; the rest would be divided up among influential business groups and state-affiliated universities. Perhaps most important from the Kremlin’s perspective, the new foundation would be able to block Yandex from entering into agreements with any foreign government.

That seemed to take the heat off. Gorelkin said he would take his law back to the drawing board. Days later, the Russian parliament passed a law requiring Russian tech to be automatically preloaded onto devices sold in Russia, a move that analysts calculated would boost Yandex’s valuation by $1.4 billion. A few weeks after that, Putin, who had criticized Yandex’s foreign ties a few years earlier, praised its projects with foreign partners and spoke positively of a closed-door meeting with its senior management.

Yet even if the Kremlin seems to have been appeased, not everyone is. Power in Russia’s government is split between rival groups, with Putin mediating between them. For the constituency known as the siloviki—officials with ties to law enforcement—the Yandex foundation was seen as a half-victory, says Tatiana Stanovaya, the founder of a political analysis site, R.Politik. “On the one hand, they see that Yandex is indirectly beholden to the government,” she says. “On the other hand, it’s purely technical. Yandex won’t just fulfill any and all demands. And if the confrontation with the West keeps escalating, [the authorities] may rethink this arrangement.”

When I spoke with Boynton last winter after the dust had settled, he was in a buoyant mood. But he also noted that things could quickly change again. “In Russia,” he said, “nothing is guaranteed.”

A template for Big Tech?

If the siloviki see Yandex as an unreliable collaborator, liberal critics see increasing signs that it is in the pocket of the authorities. In late February, for example, a policeman accused of planting drugs on an investigative reporter said he had found the journalist’s address by asking Yandex Taxi to provide it. Yandex responded that it always yields to requests by security services to “help save lives,” though Roskomsvoboda, an anti-­censorship group, pointed out that it is not always legally required to do so.

As the pandemic grew, questions about the company’s independence became only more pointed. In early April, news surfaced that Moscow authorities were considering surveilling foreign tourists via their cell-phone data once borders opened back up again—and that Yandex might develop the tool. The company denied the claim.

Yandex CEO Arkady Volozh (left) with President Vladimir Putin, who was visiting Yandex’s offices in 2017 to mark its 20th anniversary. ITARTASS NEWS AGENCY/ALAMY STOCK PHOTO

Then, when critical comments from opposition activists began popping up next to government buildings in Yandex Navigator, as a sort of digital alternative to street protests, Yandex deleted the messages, saying they were off-topic. Finally, one evening in late April, some internet users noticed that searches on Yandex for opposition leader Alexei Navalny were returning mostly negative content. Yandex apologized, saying that it was an “experiment” shown only to a small number of users. One Russian commentator, Alexander Plushev, noted that such testing is common on all tech platforms, but he added: “Any incident with Yandex is now interpreted through the prism of its control by the authorities.”

If Yandex capitulates too much to state control, it risks losing its most prized asset: its talent. “I always say that my main competitors are [Moscow airports] Sheremetyevo and Domodedovo,” says Misha Bilenko, who heads Yandex’s Machine Intelligence and Research division.

In February, a policeman accused of planting drugs on an investigative reporter said he had found the journalist’s address by asking Yandex Taxi to provide it.

Bilenko himself spent 23 years in the United States, including a decade at Microsoft, before returning to Russia several years ago. What drew him back, he says, was the access to so many different resources within Yandex and the opportunity to help improve the lives of Russians en masse. But as one employee who asked to speak anonymously told me, Yandex would lose that type of draw and power if the government tried too hard to tame it. “We have a lot of progressive people here,” the person said. “If we don’t like what we see, we’ll leave.”

Today Yandex, at least publicly, is claiming that all is well. Its concessions to the Kremlin could have been much bigger. They’re also ones that others may soon consider. “What Yandex has done isn’t only relevant within the context of Putin’s Russia,” Bloomberg columnist Leonid Bershidsky argued last year. “It could be seen as a template for Big Tech.”

Like Yandex, Bershidsky continued, companies such as Google or Facebook could set up quasi-­autonomous governance structures with the right to veto certain decisions. “If such a structure can win approval even from an authoritarian regime such as the Russian one … it could probably satisfy most Big Tech critics in democracies, too,” he wrote.

Indeed, in May of this year Facebook named the first members of its “oversight board” as a response to anger over its opaque content moderation process. The body is stacked with legal and human rights luminaries who can review and overturn some of the platform’s decisions. Though the board has nothing like the power of Yandex’s public interest foundation, it was a big concession from a company that has always fiercely defended its control over what goes on its platform.

With politicians on both ends of the US political spectrum calling for increasing regulation of Big Tech, such moves are likely to keep happening. The kind of flexibility Yandex has had to learn may prove essential for companies that want to not only survive but flourish.",Yes,"It has allowed for the rise of ""echo chambers,"" or areas where people with similar beliefs can share and amplify their views without hearing opposing ones. This has led to increased political polarization, as people are only exposed to information that reinforces their existing beliefs.",https://www.technologyreview.com/2020/08/19/1006438/yandex-putin-arkady-volozh-kremlin/,2020-08-19 00:00:00,LABEL_1_relevant,0.9607166647911072,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
71,Trump’s Executive Orders Hurt More Than TikTok and WeChat,"In another move destined to inflame tensions with the Chinese government, the Trump administration took aim Thursday at TikTok and WeChat. Citing national security concerns, the president signed a pair of executive orders that bar Americans or anyone subject to US jurisdiction from doing business with the social media apps. The orders take effect in 45 days, seemingly giving TikTok leeway to be acquired by Microsoft, which has said it is in talks to buy parts of the video platform from its parent company ByteDance.

While vaguely worded, the orders could have far-reaching consequences, including for the future of America’s relatively free and open internet. WeChat, owned by China’s Tencent, is used by millions of people in the US to maintain personal and business relationships with people in China. Many American technology firms also have customers in the country, and could be hurt by any response from Beijing. “The US business community is really concerned. I mean, who would buy an Apple phone in China if you can't use WeChat on it?"" says Paul Triolo, an expert in global technology policy at Eurasia Group, a think tank.

In a statement, TikTok said it was “shocked” by the orders. The company has repeatedly insisted that it’s not beholden to the Chinese government, and it argues the Trump administration has failed to provide evidence that the app poses a risk to US citizens. “For nearly a year, we have sought to engage with the US government in good faith to provide a constructive solution to the concerns that have been expressed,” TikTok said. “What we encountered instead was that the administration paid no attention to facts, dictated terms of an agreement without going through standard legal processes, and tried to insert itself into negotiations between private businesses.” TikTok, which has tens of millions of users in the US, began attracting the attention of lawmakers last fall, who expressed worries about its ties to Beijing.

A spokesperson for Tencent said the company was “reviewing the order to get more understanding.” WeChat has over 1 billion users, most of them in China, who rely on the app for everything from messaging to paying for coffee and booking doctor’s appointments. In the US, it’s an important way for immigrants and students to reach relatives and friends back home. “If this happens, and it really becomes impossible to use WeChat here, that will be a major factor for Chinese people considering studying here, visiting here, doing anything here,” says Jeremy Goldkorn, editor-in-chief of SupChina and cofounder of the China affairs podcast Sinica.

“The US business community is really concerned. I mean, who would buy an Apple phone in China if you can't use WeChat on it?"" Paul Triolo, Eurasia Group

At the same time, WeChat is strictly censored and surveilled, including abroad, and has become an avenue for the Chinese Communist party to oppress minority groups like Uighur Muslims. Yaqiu Wang, a researcher at Human Rights Watch, says she and other critics of the Chinese government were already moving off of WeChat to private networks and encrypted apps such as Signal before Trump’s order. “There are ways to communicate freely and securely with people in China, for now,” she says. “They are cumbersome, but then you are not subjected to the Chinese government’s censorship and surveillance.”

Tencent also owns Riot Games, maker of the popular League of Legends, and owns a large stake in Epic Games, maker of Fortnite. But a White House official told the Los Angeles Times that the order does not affect the games.

China has long blocked overseas competitors to WeChat like Facebook and Twitter. “The Chinese government is the one that is actively in the banning business, and they’ve got a big head start on the Trump administration,” Donald Clarke, a Chinese law specialist at George Washington University wrote in a blog post.",Yes,"It can lead to addiction. People can get addicted to social media because it provides a dopamine release, which can be addictive. Additionally, social media can be addictive because it is a way to escape from reality and into a fantasy world.",https://www.wired.com/story/trumps-executive-orders-hurt-more-tiktok-wechat/,2020-08-07 20:00:19.122000+00:00,LABEL_1_relevant,0.8975914716720581,Social Media,social_media,Wired,Health & Wellbeing
72,A new social-media platform wants to enforce “kindness.” Can that ever work?,"The policy has already invited some criticism from onlookers who feel it might endanger women and marginalized communities.

“There’s a widely held belief that if people use their legal names they will behave better in social environments because other people can identify them and there can be social consequences for their actions,” says J. Nathan Matias, a professor and founder of the Citizens and Technology Lab at Cornell University. “While there was some early evidence for this in the 1980s, that evidence wasn’t with a diverse group of people, and it didn’t account for what we now see the internet has become.”

In fact, many people who use pseudonyms are from more marginalized or vulnerable groups and do so to keep themselves safe from online harassment and doxxing.

Doxxing—using identifying information online to harass and threaten—is a life-threatening issue for many marginalized people and has soured them on social media like Facebook and Twitter, which has come under fire for not protecting at-risk individuals (though both Twitter and Facebook have tried to make amends of late by removing tweets promoting misinformation, for example). Estévez says that while she’s “very empathetic about this” and that trans people can identify themselves by their chosen name and pronouns, phone-name verification and the app’s invite-only structure were necessary to prevent abusive behavior.

Courtesy Telepath

Estévez, who spent years as a volunteer moderator at Quora before leading the moderation team, imported Quora’s community guideline of “Be Nice, Be Respectful” to Telepath. “People respond well to being treated well,” she says. “People are happier. People are really attracted to kind communities and places where they can have their say and not feel ridiculed.”

Telepath’s team has been very deliberate with its language about this concept. “We made the decision to use the word ‘kindness’ instead of ‘civil,’” Bodnick says. “Civil implies a rule you can get to the edge of and not break, like you were ‘just being curious’ or ‘just asking questions.’ We think kindness is a good way of describing good intent, giving each other the benefit of the doubt, not engaging in personal attacks. We hope it’s the strength to make these assessments that attracts people.”

Historically, platforms have been reluctant to enforce basic user safety, let alone kindness, says Danielle Citron, a law professor at Boston University Law School who has written about content moderation and advised social-media platforms. “Niceness is not a bad idea,” Citron says.",Yes,"It can spread misinformation quickly and easily, which can lead to panic and chaos.

In addition, social media can be addictive and can cause people to spend too much time on their phones or computers, which can lead to social isolation.",https://www.technologyreview.com/2020/10/07/1009499/telepath-wants-to-enforce-kindness-can-that-ever-work/,2020-10-07 00:00:00,LABEL_1_relevant,0.9523326754570008,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
73,Online “auctions” of women are just the latest attacks on Muslims in India,"The slow action is part of a larger pattern, says Meenakshi Ganguly, South Asia director at Human Rights Watch. Authorities are quick to accuse government critics, she says, but “hate speech and violent actions by government supporters are seldom prosecuted.”

Social media companies, which have the ability to take down offensive posts and stem misinformation, are not filling the void. “Tech companies take down content based on their community guidelines and local laws. In this case both were violated,” says Krishnesh Bapat, a Centre for Communication Governance Digital Fellow at the Internet Freedom Foundation in Delhi. “GitHub, to the best of my knowledge, does not proactively take down content. It does so only after it receives a complaint and took longer in this case.” GitHub did not respond to a request for comment about its policies.

In India almost all forms of online harassment fall under the general category of cyberbullying. India’s Information Technology Act, 2000, commonly known as the Cyber Law, governs online abuse. The act was intended to address e-commerce but was adjusted in 2008 to cover cybercrimes as well. Harassment can also fall under the country’s overall penal code, says Nappinai, which can help protect victims in serious cases.

Nevertheless, some say the country’s online laws need revision. Anushka Jain, a lawyer with the Internet Freedom Foundation, believes the digital world has changed too much for the law to be effective. “Some of the provisions of the [Cyber] Act have become redundant and incapable of addressing the currently persisting issues and rapidly evolving changes and threats,” she says. The government, she adds, needs a holistic approach to cyber policy, including stricter laws.

In addition to harassment, Muslims in India are also struggling with misinformation online. For example, last September, ID Fresh, a halal-certified food products company owned by a Muslim family, faced a large-scale misinformation campaign on social media claiming that the company mixes cow bones and calf rennet to increase the volume of ready-to-cook batter and urging “every single Hindu” to avoid the products. The company faced a boycott and saw its sales drop; it had to launch its own campaign in response to set the record straight.

So far, there seems to be little movement to change the situation from either tech companies or the Indian government. That has left little remedy for victims like commercial pilot Hana Mohsin Khan, who took to Twitter to express her anger when she saw her picture in the January auction. “Muslim women were yet again targeted. Yet again there will be no action,” she wrote. “We are caught in a never ending cycle of anger and anguish. Every. Single. Day.”

Safina Nabi is an independent multimedia journalist from South Asia based in Kashmir.",Yes,It can cause people to be harassed.,https://www.technologyreview.com/2022/02/21/1046052/online-auctions-muslim-women-india/,2022-02-21 00:00:00,LABEL_1_relevant,0.995868682861328,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
74,How Facebook Groups Became a Bizarre Bazaar for Elephant Tusks,"Mark Zuckerberg had already been testifying for four hours in the stuffy, wood-paneled room where the House Energy and Commerce Committee held its hearing on Facebook in April, when he got a question he seemed wholly unprepared to answer. Over the course of those four hours, his company had been blamed for enabling the opioid crisis, silencing conservative voices, violating users’ rights to privacy, and setting up a modern day surveillance state akin to J. Edgar Hoover’s COINTELPRO program. It seemed there was no societal ill too grave or niche that Congress couldn’t lay at Facebook’s feet.

Still, Zuckerberg appeared surprised when Georgia representative Buddy Carter asked him about the elephants. “Did you know that there are some conservation groups that assert that there's so much ivory being sold on Facebook that it's literally contributing to the extinction of the elephant species?” Carter asked.

“Congressman, I have not heard that,” Zuckerberg replied.

The timing wasn’t entirely random. Peters and Kohn didn’t initially plan on going public with their complaint. But when Zuckerberg reluctantly agreed to testify before two congressional hearings, they couldn’t pass up the chance to get a question in with all the world watching. So they spoke to the Associated Press, which published a news story about the SEC complaint the same week Zuckerberg was set to testify. Carter seemed to be citing Peters’ own words.

Zuckerberg’s answer or lack thereof revealed just how far down Facebook's to-do list wildlife trafficking ranks. Zuckerberg has repeatedly used a rose-colored glasses defense about disturbing content on Facebook, copping to the fact that he focused too much, for too long, on all the good Facebook has done—while all but ignoring the bad. But that argument is tough to reconcile with the fact that Facebook’s own internal guidelines ban so very many bad things, from cannibalism to infant abuse. Indeed, Facebook is so accustomed to grotesque content appearing on the site, it offers content moderators counseling to help them deal with the horrors they’ve seen.

Facebook’s leaders aren’t unaware of the ugliness that lurks just beyond every pretty selfie. It’s just that, armed with the immunities enshrined in the United States legal code, the company’s never had to do anything about it. “Mark Zuckerberg goes around saying they’re an idealistic company, well I’m idealistic too. We’re trying to save elephants from extinction,” Peters says. “I want to see this firm put some weight behind the idealism and optimism they talk about.”

That’s starting to happen. When news broke last fall that the company had sold thousands of political ads to Russian propagandists in the runup to the 2016 election, Facebook announced it would double its content moderation team to 20,000 people, impose strict verification processes for political advertisers, and create massive repositories of political ads, complete with information on how much they cost, who paid for them, and what demographics they reached.

This spring, when the world found out that a data firm called Cambridge Analytica had amassed data on as many as 87 million Americans without their permission through a silly third-party quiz app, Facebook radically limited app developers’ data access and announced it would be auditing how all apps use Facebook user data, even if it means hiring thousands more people. As Facebook executives recently explained to WIRED, the company has also overhauled its News Feed algorithms to reduce people’s exposure to fake news and outlets it deems untrustworthy. It has also answered accusations of liberal political bias by inviting the conservative Heritage Foundation and others to study the company from the inside out.",Yes,It can be used to spread false information or to interfere in the political process of other countries.,https://www.wired.com/story/tusks-horns-and-claws-inside-the-fight-to-destroy-the-animal-parts-bazaar-on-facebook/,2018-06-05 11:00:00+00:00,LABEL_1_relevant,0.9956246018409728,Social Media,social_media,Wired,Politics
75,How Social Media Became a Pink-Collar Job,"Companies hiring for technical positions often slip language into their job postings that appeals to men. They say they’re looking for “ninjas,” who seek to “obliterate competition,” and are capable of “dominating.” By now, these wordings are a well understood form of bias that produces more male candidates than female.

But one job in the digital economy falls predominantly to women. It’s an oft-overlooked position, drawing on both marketing and editorial skills, that has become increasingly critical both to business success and online discourse. The pay is poor, and the respect can be limited. Take a look at the job posting for any social media manager. You’ll discover the same bias in its language, in reverse: a bias for sourcing female candidates.

By now, these wordings are a well understood form of bias that produces more male candidates than female.

Social media managers are “the behind-the-screens labor involved in media and technology, central to propelling our digital economy forward,” says Brooke Erin Duffy, who is an Assistant Professor in Communications at Cornell. Between 70 and 80 percent of social media workers self-identify as women on the salary compilation site Payscale. The career has been referred to as the Pink Ghetto.

According to a study, published by Duffy and University of Oxford researcher Becca Schwartz in New Media & Society and slated for a print release early next year, companies create this diversity gap by advertising social media as “women’s work”—at the same time as they routinely undervalue it. Duffy and Schwartz studied 150 job postings to determine how businesses recruit social media specialists. These companies, which included BuzzFeed, Equinox and Thrillist, advertised jobs that called for applicants to be sociable, exhibit deft emotional management and be flexible--all traits that Duffy says are typically associated with women.

The feminized nature of social media employment, Duffy and Schwartz argue, is connected to its “characteristic invisibility, lower pay, and marginal status” within the tech industry. The pair cites statistics from Payscale that place average pay for a social media specialist at $41,000. But that’s for staff jobs. Duffy, who last year published the book (Not) Getting Paid to Do What You Love: Gender, Social Media, and Aspirational Work, has been tracking this field for awhile. This spring, she conducted an additional 25 interviews with social media managers to better understand the dynamics of the job. Most social media jobs, she says, are contract positions; the ghost-tweeters responsible for upholding a brand’s social persona, for example, may be balancing two or three clients at a time.

In the job descriptions Duffy and Schwartz studied, which included both entry-level postings and calls for internships, companies often made the jobs sound like non-work: fun hobbies for which people just happened to get paid. (Or, occasionally paid. Many of the internships were offered without pay, or for school credit.) Postings referred to the job as sociable, blurring the boundaries between work and play. Perks on offer included everything from discounts on classes at Equinox to LaCroix and free massages at the digital media company Ranker. “The assumption was that these jobs were extensions of what people would be doing for fun anyway,” says Duffy.

“The assumption was that these jobs were extensions of what people would be doing for fun anyway""

Duffy notes that social media specialists’ roles are not simply to steward a brand’s presence on social media, but to act as a personal round-the-clock ambassador for the brand. Companies sought out workers who had active social followings already, and could prove they use many different services, from Twitter to Instagram to Pinterest, regularly. For these workers, tech addiction or obsession was not pathologized, but in fact “bound up with notions of the idealized worker,” according to the study. Candidates were encouraged to be always online--and passionate personally about the brands for which they worked. Companies sought workers who expressed social allegiance: Candidates were expected to show a “passion for travel and [The Points Guy] brand,” or a “deep passion for the UrbanDaddy brand and lifestyle.”

At the same time, their true identities go unrecognized. Unlike journalists, social media managers have no byline. They don’t reveal who they are when tweeting under a brand’s handle or posting to Pinterest. In that way, social media workers are a digital version of public relations professionals, an often low-status woman-dominated role within corporate America. Social media managers usually command less respect than PR managers, while taking on responsibility for an increasingly important distribution channel. Strategic use of social media has been credited for influencing elections, harnessed to transform fledgling startups into billion-dollar companies, and used as a form of warfare. But this influence doesn’t translate into a higher paycheck or more internal power.",Yes,It makes people more polarized in their views.,https://www.wired.com/story/how-social-media-became-a-pink-collar-job/,2018-05-26 11:00:00+00:00,LABEL_1_relevant,0.9942277669906616,Social Media,social_media,Wired,"Information, Discourse & Governance"
76,'Gaymers' Are Taking Brazil by Storm,"During a livestream, with an LGBTQ+ flag in the background, blue hair, and pop songs boosting his moves, Alcides Furlin, 29, shoots his enemies away. The Brazilian engineer and streamer describes himself as a “nerd boy, an acid drag” and tries to create content that merges the gay and gaming communities.

This approach is becoming a trend among a subset of Brazilian streamers—call them entertainers, artists, performers, or simply The Gaymers.

Their streams have many things in common: witty captions, vibrant backgrounds, silly rules that steer the live chat, and looks that are sometimes boyish, sometimes extravagant, with colorful wigs and expressive makeup. Watching them for the first time feels strangely familiar, like listening to your best friends ramble on about everyday life, dramas, crushes … and games.

“Roughly, a gaymer is a person who identifies as LGBTQ+ and actively participates in the gaming community,” explains Lucas Goulart, a social psychology doctor at the Federal University of Rio Grande do Sul. Goulart specializes in the intersection between queer culture and video games, and, according to him, gaymers are also caught between what can be seen as contrasting worlds. “It’s hard because sometimes they don’t see themselves in either community,” he tells WIRED.

There are stereotypes surrounding gaymers, says Goulart, mainly, that a gay man who is into fashion and partying cannot be a ""geek gamer,"" someone who only thinks about video games, as he puts it. But Brazil's gaymers are pushing past these stereotypes to create something new. And since many of their livestreams have over 10,000 active viewers, it is safe to say their audiences value personalities above dichotomies.

Furlin knows that people love his drag queen persona, Lola Dvil. “People respond to transparency,” he says. “I need to be smart about how I approach things because I’m not talking to native gamers, but I’m committed to showing the truth, whether that means admitting difficulties getting an eye-shadow right or being honest about having a hard time to pass levels in a game.”

His awareness pays off. With over 8,000 followers on Twitch, Lola is a part of many LGBTQ+ gaming initiatives, such as the international Stream Queens and the Brazilian Project Fierce. Both work as streaming communities and show a variety of content produced by gaymers. And besides participating in collective projects, Lola’s personal calendar is packed. She livestreams five days a week, anywhere from two to 10 hours, doing everything from playing League of Legends to offering beauty tutorials.

Twitch in Brazil

Twitch saw explosive growth during the pandemic—hitting the mark of 18.6 billion hours of viewed content in 2020. In Brazil, as of July last year, Twitch ushered in new rates for subscriptions: It now costs just R$7.90 ($1.50)—a value 65 percent lower than the original rate in the country.

The move is part of a global initiative that aims to provide greater accessibility in regions where the platform is popular. And in this game, popularity comes with a price. For some time now, Brazilian Twitch can be seen as a good indicator of political tensions and shifts.

In May, Jair Renan, son of Brazilian president Jair Bolsonaro, was banned from the platform after spreading Covid-19 misinformation and encouraging gamers to break social isolation. His father is also having a hard time communicating with the gaming crowd—once a solid foundation among his voters.",Yes,"It creates echo chambers. This is where people only see information that reinforces their current beliefs. This can lead to confirmation bias, which is the tendency to search for, interpret, favor, and recall information that confirms one's preexisting beliefs or hypotheses.",https://www.wired.com/story/brazilian-twitch-streamers-drag/,2022-01-08 14:00:00+00:00,LABEL_1_relevant,0.9958154559135436,Social Media,social_media,Wired,"Information, Discourse & Governance"
77,What Spammers Could Do With Your Hacked Facebook Data,"When Facebook announced at the end of September that it had suffered a data breach that ultimately affected 30 million accounts, it seemed, perhaps, like the work of sophisticated nation state hackers. But a new report from The Wall Street Journal suggests spammers as the culprit instead. That shouldn't make you feel that much better, though, given just how much damage criminals can do with the kind of information stolen from Facebook.

It was, after all, a lot. The sophisticated daisy chain attack that the hackers pulled off garnered the names, phone numbers, and email of 15 million Facebook users. Fourteen million more had their username, date of birth, gender, devices they used Facebook on, and language settings compromised at the very least. Hackers could also have gleaned relationship status, religion, hometown, current city, work, and education info, depending on how fully victims had filled out their profile, along with the 10 most recent locations they checked into or were tagged in, and their 15 most recent Facebook searches. (Here's how to find out if you were affected, and how badly.)

All of which becomes particularly dangerous in the hands of spammers.

""Having accurate, detailed data, and a large amount of data, makes spamming campaigns more profitable,"" says Jérôme Segura, lead malware intelligence analyst at the network defense firm Malwarebytes. ""And this Facebook data is very unique. It has a lot of value, because it's from people supplying the information genuinely and saying 'I checked in at this hotel or here are some of my interests.' It's a priceless database trove for marketers.""

For now, Facebook won't weigh in publicly on who was behind the attack. Guy Rosen, the social network's vice president of product management said repeatedly in a call with reporters last week that, “The FBI is actively investigating and have asked us not to discuss who may be behind this attack.” The company reiterated this to WIRED on Friday.

The possibility that scammers were behind the theft, though, highlights the ways in which centralized data repositories like email accounts and social media profiles are potential gold mines for—and frequent targets of—phishers, spammers, and shady marketers.

""It's a priceless database trove for marketers."" Jérôme Segura, Malwarebytes

Granular data helps spammers craft maximally convincing emails, SMS messages, and calls. The data not only helps improve the general verisimilitude of broad spam campaigns, but also makes it easier to specifically tailor scams to individuals. For example, in one popular scam, an email threatens to release compromising photos of you, and uses information like your old passwords and your phone number to make it seem like the attacker really does have dirt. The more credible they seem, the more likely you are to pay them off. If you were compromised in the Facebook hack, they now also potentially know where you live, where you've worked, and where you've been.

Attackers can use that sort of detailed information in all sorts of other ways, as well. Segura points out that a trove like the one stolen from Facebook would be valuable for launching massive malvertising campaigns that try to entice web users to click on malicious ads, since it contains so many indicators of a person's background and preferences. And having such granular data about people would enrich all sorts of phishing attacks and so-called ""business email compromise"" scams, in which attackers try to gain access to email accounts within a business to gain credibility, and then influence malicious activity like payments to the attacker. You're a lot more likely to think an email is really from your boss if she's referencing your upcoming birthday, and the work trip you went on to Cleveland in the fall. Phishers and BEC scammers could also use details from the breach to send convincing messages externally, posing as a company's client, for example, or a disgruntled customer.

And then there's the matter of stolen identities.",Yes,"A lot of people get their feelings hurt.

When people get their feelings hurt, they often seek revenge.

Revenge is a powerful motivator, and people have been known to do some pretty crazy things when they're feeling vengeful.

So, if someone's feelings get hurt on social media, and they decide",https://www.wired.com/story/facebook-hack-data-spammers/,2018-10-19 15:55:59.693000+00:00,LABEL_1_relevant,0.99623304605484,Social Media,social_media,Wired,Social Norms & Relationships
78,Australia’s new law threatens jail if social-media firms don’t remove violent content,"The new legislation means social-media executives could face up to three years in jail, or be fined up to 10% of their company’s annual revenue, if they fail to take down violent content in an “expeditious” manner.

Specifically: The bill defines “abhorrent violent content” as terrorism, murder, attempted murder, torture, rape, and kidnapping. It’s less clear how quickly this sort of content is expected to be removed. The law says companies must take it down “within a reasonable time after becoming aware of the existence.” There are certain exemptions for purposes like law enforcement, court proceedings, artistic work, and journalism.

A reaction: The legislation is being enacted as part of the response to the live-streamed killing of 50 people in two mosques by an Australian gunman in Christchurch, New Zealand, last month. The live video was reportedly up for 20 minutes, but the recording was reposted on YouTube and spread elsewhere online. Facebook said it removed 1.5 million copies over the subsequent 24 hours. Australia’s attorney general, Christian Porter, described the bill as a “world first” and said he hopes other countries will adopt similar laws.

Criticisms: Some lawmakers said the bill had been rushed to get it through ahead of upcoming elections in May. Others said it was ill-conceived. “This bill is clumsy and flawed in many respects,” said Mark Dreyfus, a Labor Party member. He raised concerns that it could force social-media companies to conduct “proactive surveillance” to stay compliant.

Sign up here for our daily newsletter The Download to get your dose of the latest must-read news from the world of emerging tech.",Yes,"It creates an echo chamber where people only see content that reaffirms their existing beliefs. This can lead to people becoming more polarized and less willing to compromise or listen to opposing viewpoints. Additionally, social media can be addictive and lead to people spending too much time on their devices instead of interacting with the people and world around them.",https://www.technologyreview.com/2019/04/04/136196/australias-new-law-threatens-jail-if-social-media-firms-dont-remove-violent/,2019-04-04 00:00:00,LABEL_1_relevant,0.9962263107299804,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
79,Facebook Wants to Take Over TV,"To that end, an announcement last week ushered in a decision to start pushing longer videos into the News Feed. Longer videos, after all, provide more space for ads. But it seems that the news also foreshadowed a larger vision: that the social network doesn’t just like video—it has eyes on taking over TV, too.

On Wednesday, Mark Zuckerberg described how he plans for Facebook to become the place where “when people want to watch videos or want to keep up to date with what’s going on with their favorite show, or what’s going on with a public figure that they want to follow, that they can come to Facebook.”

He then went even further, adding that he would like Facebook to become a place where “you want to watch videos, you want to keep up with the content that you watch episodically week over week.”

In other words: Facebook wants to become a genuine alternative to TV.

That will take a while. Zuckerberg himself also noted that “people will experiment with longer forms of video,” but added that, for now, shorter clips will remain “the primary focus.” TechCrunch suggests you could think of it as more of a YouTube than a Netflix—paying content producers to create shorter-format video, with the occasional longer item, rather than cranking out original feature-length shows. For the time being, anyway.

Not convinced? Well, it’s also rumored that the company is planning to build its own TV apps. The Wall Street Journal (paywall) reports that the company is “developing a video-centric app for television set-top boxes,” like Roku and Apple TV. If that’s true, Facebook will not only be gathering a large reservoir of content that people can share with friends—it’ll also give them an easy-to-reach faucet from which to drink when they sit down in front of the television.

After years of protestation, Zuckerberg last month gestured toward the fact that Facebook may, kind of, perhaps be a media organization. “Facebook is a new kind of platform,” he said in a video. “It’s not a traditional technology company. It’s not a traditional media company.” Not a traditional one, perhaps, Mark. But if you get into the TV game, that will make Facebook a media company all the same.

(Read more: Recode, TechCrunch, Wall Street Journal, “Facebook at a Crossroads,” “Facebook Now Lets You Live-Stream Video From Your iPhone”)",Yes,It can lead to isolation and loneliness.,https://www.technologyreview.com/2017/02/02/5698/facebook-wants-to-take-over-tv/,2017-02-02 00:00:00,LABEL_1_relevant,0.9947996139526368,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
80,When to Reply on Social Media—and When to Not,"I have a tenet that I follow when it comes to social media conflagrations: Don’t add your air to someone else’s fire.

This rule has saved my butt multiple times. For example, during one social media snafu, a writer responded to a post I made of an article I’d written, saying she wanted to discuss our opposing views—in a Facebook forum of thousands of people. The wording and tone of her comment showed she wasn’t interested in a real dialog, so I didn’t respond. Had I agreed to the request or made a snarky comment like “Get your own damn articles published,” I would have been following her playbook to gain attention for herself and undermine me and my work. Should I have done something else? I figured I’d check with the experts.

“You did the right thing by not responding,” says Michele Borba, an educational psychologist and author of Thrivers. “No response is a great response, and often the most powerful response. The person wants the attention, and you are not giving it to them. She clearly wanted to use and undermine you by hijacking your platform. If you shamed her, you would have lost credibility and would be in a position of defending yourself.”

Sameer Hinduja, codirector of the Cyberbullying Research Center agrees, and says, “Whenever we respond to someone trying to insult us, we show we deeply care about their opinion. And then we’ve given them the power to invalidate us.”

Not responding on social media can be the best way of showing strength, rather than lending your voice and energy to the noise. In fact, research published in the journal Psychological Science shows that firing up the keyboard isn’t nearly as effective as speaking to someone one-on-one or sharing visuals. Of course, that works best if you have a real relationship or care about what the person thinks of you. “If it is someone who isn’t really in your life, then what you did was right,” says Ulash Dunlap, a San Francisco–based therapist. “If it’s an important relationship, I suggest you message the person and ask for a phone call to avoid miscommunication.”

Dunlap also recommends taking five minutes and assessing the situation before responding, and avoiding knee-jerk reactions on social media so people can’t see that they’ve pushed your buttons. “If someone is devaluing you or bullying you over your beliefs, or looking to make themselves right and you wrong, or looking for fame through you, then end the conversation, either by not responding or even saying, ‘Thank you for your feedback,’ similar to how corporations respond when criticized.”

So how do we keep ourselves from feeling disempowered when these situations arise? “Remember, if they don’t know you well, the person on the other end doesn’t understand you or your lived experiences. They don’t have the backstory,” says Dunlap. This might also be a person who loves to win. “You can go through the person’s Facebook or Twitter feed, and you will see it. If they are that way, find an exit strategy and end the conversation.”",Yes,"It can give people a platform to bully and attack others. The best way to deal with these situations is to not respond, which can be the best way of showing strength.",https://www.wired.com/story/when-to-reply-on-social-media-and-when-to-not/,2021-05-26 13:00:00+00:00,LABEL_1_relevant,0.9897449612617492,Social Media,social_media,Wired,"Information, Discourse & Governance"
81,"For Trump and Facebook, judgment day is around the corner","For Trump and Facebook, judgment day is around the corner Facebook's experimental Oversight Board is on the cusp of its biggest decision

Facebook unceremoniously confiscated Trump’s biggest social media megaphone months ago, but the former president might be poised to snatch it back.

Facebook’s Oversight Board, an external Supreme Court-like policy decision making group, will either restore Trump’s Facebook privileges or banish him forever on Wednesday. Whatever happens, it’s a huge moment for Facebook’s nascent experiment in outsourcing hard content moderation calls to an elite group of global thinkers, academics and political figures and allowing them to set precedents that could shape the world’s biggest social networks for years to come.

Facebook CEO Mark Zuckerberg announced Trump’s suspension from Facebook in the immediate aftermath of the Capitol attack. It was initially a temporary suspension, but two weeks later Facebook said that the decision would be sent to the Oversight Board. “We believe the risks of allowing the President to continue to use our service during this period are simply too great,” Facebook CEO Mark Zuckerberg wrote in January.

Facebook’s VP of Global Affairs Nick Clegg, a former British politician, expressed hope that the board would back the company’s own conclusions, calling Trump’s suspension an “unprecedented set of events which called for unprecedented action.”

Trump inflamed tensions and incited violence on January 6, but that incident wasn’t without precedent. In the aftermath of the murder of George Floyd, an unarmed Black man killed by Minneapolis police, President Trump ominously declared on social media “when the looting starts, the shooting starts,” a threat of imminent violence with racist roots that Facebook declined to take action against, prompting internal protests at the company.

The former president skirted or crossed the line with Facebook any number of times over his four years in office, but the platform stood steadfastly behind a maxim that all speech was good speech, even as other social networks grew more squeamish.

In a dramatic address in late 2019, Zuckerberg evoked Martin Luther King Jr. as he defended Facebook’s anything goes approach. “In times of social turmoil, our impulse is often to pull back on free expression,” Zuckerberg said. “We want the progress that comes from free expression, but not the tension.” King’s daughter strenuously objected.

A little over a year later, with all of Facebook’s peers doing the same and Trump leaving office, Zuckerberg would shrink back from his grand free speech declarations.

In 2019 and well into 2020, Facebook was still a roiling hotbed of misinformation, conspiracies and extremism. The social network hosted thousands of armed militias organizing for violence and a sea of content amplifying QAnon, which moved from a fringe belief on the margins to a mainstream political phenomenon through Facebook.

Those same forces would converge at the U.S. Capitol on January 6 for a day of violence that Facebook executives characterized as spontaneous, even though it had been festering openly on the platform for months.

How the Oversight Board works

Facebook’s Oversight Board began reviewing its first cases last October. Facebook can refer cases to the board, like it did with Trump, but users can also appeal to the board to overturn policy decisions that affect them after they exhaust the normal Facebook or Instagram appeals process. A five member subset of its 20 total members evaluate whether content should be allowed to remain on the platform and then reach a decision, which the full board must approve by a majority vote. Initially, the Oversight Board was only empowered to reinstate content removed on Facebook and Instagram, but in mid-April began accepting requests to review controversial content that stayed up.

Last month, the Oversight Board replaced departing member Pamela Karlan, a Stanford professor and voting rights scholar critical of Trump, who left to join the Biden administration. Karlan’s replacement, PEN America CEO Suzanne Nossel, wrote an op-ed in the LA Times in late January arguing that extending a permanent ban on Trump “may feel good” but that decision would ultimately set a dangerous precedent. Nossel joined the board too late to participate in the Trump decision.

The Oversight Board’s earliest batch of decisions leaned in the direction of restoring content that’s been taken down — not upholding its removal. While the board’s other decisions are likely to touch on the full spectrum of frustration people have with Facebook’s content moderation preferences, they come with far less baggage than the Trump decision. In one instance, the Oversight Board voted to restore an image of a woman’s nipples used in the context of a breast cancer post. In another, the board decided that a quote from a famous Nazi didn’t merit removal because it wasn’t an endorsement of Nazi ideology. In all cases, the Oversight Board can issue policy recommendations, but Facebook isn’t obligated to implement them — just the decisions.

Befitting its DNA of global activists, political figures and academics, the Oversight Board might have ambitions well beyond one social network. Earlier this year, Oversight Board co-chair and former Prime Minister of Denmark Helle Thorning-Schmidt declared that other social media companies would be “welcome to join” the project, which is branded in a conspicuously Facebook-less way. (The group calls itself the “Oversight Board” though everyone calls it the “Facebook Oversight Board.”)

“For the first time in history, we actually have content moderation being done outside one of the big social media platforms,” Thorning-Schmidt declared, grandly. “That in itself… I don’t hesitate to call it historic.”

Facebook’s decision to outsource some major policy decisions is indeed an experimental one, but that experiment is just getting started. The Trump case will give Facebook’s miniaturized Supreme Court an opportunity to send a message, though whether the takeaway is that it’s powerful enough to keep a world leader muzzled or independent enough to strike out from its parent and reverse the biggest social media policy decision ever made remains to be seen.

If Trump comes back, the company can shrug its shoulders and shirk another PR firestorm, content that its experiment in external content moderation is legitimized. If the board doubles down on banishing Trump, Facebook will rest easy knowing that someone else can take the blowback this round in its most controversial content call to date. For Facebook, for once, it’s a win-win situation.",Yes,It can be used to spread misinformation and incite violence.,https://techcrunch.com/2021/05/04/facebook-trump-oversight-board-decision/,2021-05-04 00:00:00,LABEL_1_relevant,0.991862416267395,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
82,Handmaids Tale Garb Is the Viral Protest Uniform of 2019,"Handmaid garb is the Guy Fawkes mask of 2019. Women dressed in red robes and blinkering white bonnets—the uniform of reproductive slavery in Margaret Atwood's dystopian novel turned Hulu show, The Handmaid's Tale—have become symbols of dissent across the internet and the world. The handmaids are ominous, silent, semi-faceless, and the most powerful protest costume since hacktivist collective Anonymous popularized wearing Fawkes' smirking face over a decade ago. And now, as Hulu's adaptation heads into its third season, they are everywhere.

The trend began two years ago in Texas, with a marketing stunt: Hulu hired scores of women to dress as handmaids and stand creepily motionless at SXSW to promote The Handmaid's Tale's premiere. Local reproductive rights activists at NARAL Pro-Choice Texas knew a meme when they saw one and ran to their local costume shop, dressing 12 women in red robes and sending them into the Texas Senate gallery to protest a bill restricting abortion access. When I spoke to the group's former executive director, Heather Busby, at the time, she admitted to always having social media shareability in mind, but I doubt she had any idea what she had started.

Handmaids have since attended pro-choice protests everywhere from Ireland to Argentina. They protested the Republican health care bill in Washington, DC. They loomed outside Supreme Court Justice Brett Kavanaugh's confirmation hearings, nominally because he opposes abortion, but also because of the sexual assault allegations that surfaced during those hearings. In Croatia and California, handmaids have protested legislators who voted against measures to curb violence against women. In Poland and the UK, they protested the presence of President Donald Trump. When Alabama and Georgia lost reproductive rights, they gained angry, sign-waving handmaids. The Handmaid Coalition, a nonprofit with the tagline ""Fight to keep fiction from becoming reality,"" offers aspiring handmaids organizing and costume construction advice. Everywhere the handmaids go, the media follows: Their image has become a staple of late-night set pieces, campaign emails, and, praise be, Twitter jokes.

Both Atwood and Alan Moore—creator of Anonymous' probable sartorial inspiration, the titular vigilante in V for Vendetta, who wears a Guy Fawkes mask—talk about their most memed characters the same way. People who sprang from their heads have now slipped from their control, taking on many, many lives of their own. Maybe that's why, beyond their eerie presence, internettiness, and tendency to show up en masse, Fawkes mask wearers and handmaids also share a certain ideological vagueness. The face of Guy Fawkes, who in life conspired to blow up Parliament, has always stood against the establishment. But in the hands of Anonymous and others, the establishment has been everyone from Scientology to Wall Street to dictators felled during the Arab Spring.

Emma Grey Ellis covers memes, trolls, and other elements of internet culture for WIRED.

In Atwood's telling (and Hulu's adaptation of it), handmaids are forced to bear their master's children because the state of Gilead is in population crisis, but in the real world, women dressed as handmaids have been protesting situations far more diverse and often less concrete. The costume's flexibility is part of its power, but also keeps handmaids from being real drivers of discourse. Women dressed as handmaids suggest connections between our world and Gilead, but while that's important to consider and the purpose of Atwood's work, it's hardly solution-oriented. Handmaids embody gendered pain and dread so vast it's hard to put into words: sexual violence, physical violence, governments taking control of bodies, bodies valued over beings, being reduced to a womb alone. All they really say is ""No to all that,"" albeit in a highly concise and memorable way. So they win no hearts or minds: Right-wing outlets like Fox News seem to think handmaids are mocking the institutions and people they're protesting, and expend considerable time mocking them in return.

Of course, twas ever thus. Protest fashion is more about communicating rejection and anxiety than creating tangible change on its own, and it's never popular with the olds. In most cases, like the Guy Fawkes masks or women protesters dressing up as witches or even just punks, the protesters arrive as what they hope will be seen as threatening and monstrous, their oppressors' worst nightmare. The handmaids are singular in that they've shown up as their own.

[#video: https://www.youtube.com/embed/RcTvQx1Wot0

More Great WIRED Stories",The article does discuss unintended consequences of social media.,"It's created an environment where assholes can interact with other assholes 24/7, and where everyone is free to be the worst version of themselves.

The future of democracy depends on the internet. Here's how we save it.

A detailed investigation resulted in a 24-year prison sentence for a Florida man.",https://www.wired.com/story/handmaids-tale-protest-garb/,2019-06-05 16:10:28.578000+00:00,LABEL_1_relevant,0.996101975440979,Social Media,social_media,Wired,"Information, Discourse & Governance"
83,How Facebook Killed Your School Reunion,"As my friend Alexis, a 2004 Harvard graduate who also decided to skip his coming five-year reunion, put it: “The infotech out there has reduced the desire to go ‘just to know’ what so-and-so is up to. I know what just about everyone is doing.”

Wortham reported that she and her colleagues were no longer tantalized by the prospect of finding out what had happened to whom.

In 2009 the Facebook-is-killing-reunions trend story became an official part of the annual journalistic calendar, like stories about the war on Christmas or tips for singles on Valentine’s day. Time reported that in some cases Facebook was inspiring reunions that wouldn’t otherwise happen, while in others it was killing them off:

“There was a Facebook page for my 20-year college reunion, which took place this May,” [says Deborah Dietzler, executive director of alumni relations at the University of Georgia]. “I looked at it a couple of times and it didn’t seem like anyone I knew would be there, so I lost interest.”

By 2010 it was on the Huffington Post of big media, CNN, which declared Facebook’s positive and negative effects on reunions a wash. Clearly no one had yet to go beyond studying it anecdotally.

Skip to the present day, and the data have finally arrived. Timothy Davis, co-founder of the reunion-organizing site Classreport.com, tells The Baltimore Sun that he’s noticed fewer reunions nationwide, with the trend especially pronounced among people in their mid to late 20’s.

What’s it all mean? I don’t know. Probably that we’ll have to endure these stories until the only place that even bothers to report this footnote in the great dematerialization brought about by the web and social media is AARP The Magazine.",Yes,"It has killed off the high school reunion, which was once a highly anticipated event.",https://www.technologyreview.com/2011/05/08/118859/how-facebook-killed-your-school-reunion/,2011-05-08 00:00:00,LABEL_1_relevant,0.995622217655182,Social Media,social_media,MIT Tech Review,Economy
84,Jack Dorsey Is Captain of the Twittanic,"On Tuesday, Jack Dorsey, the CEO of Twitter, came to TED 2019 to answer for the sins of his platform. In his signature black hoodie and jeans, unkempt facial hair, and black beanie, he sat with TED head Chris Anderson and Whitney Pennington Rodgers, who curates current affairs for the conference, for a conversation that left all three members, along with the audience, frustrated.

“We’re on this great voyage with you on the Twittanic,” Anderson told Dorsey after roughly 20 minutes of interrupted back and forth. “There are people in steerage who are saying, ‘We are worried about the iceberg ahead!’ And you say, ‘That is a good point’ and ‘Our boat hasn’t been built to handle it,’ and we’re waiting, and you are showing this extraordinary calm and we’re all worried but we’re outside saying, ‘Jack, turn the fucking wheel!’”

Dorsey stoically listened to this comparison, like the meditative yogi he often talks about aspiring to be. “It’s democracy at stake! It’s our culture at stake! It’s our world at stake!” Anderson continued. “You’re doing a brilliant role of listening, Jack, but can you actually dial up the urgency and move on this stuff? Will you do that?”

“Yeah, yeah, yes,” Dorsey replied, but then added, “We could do a bunch of superficial things to address what you’re talking about, but we need to go deep.”

It’s been more than a year since Dorsey publicly committed to “fixing” Twitter, and figuring out what a platform that encourages healthy discussions looks like. He’s been on a mea culpa tour since then, telling the world—and regulators—that he knows Twitter is broken, that it’s toxic and terrible and that he and the team are planning to radically rebuild it. He reiterated all of this on the TED stage, explaining that he wants to rethink what behavior the site incentivizes, for instance, by possibly getting rid of the like button and de-emphasizing follower counts while emphasizing topical interests instead. He repeated that he wants to focus on maximizing the health of conversations, and prioritizing people spending their time learning on the site, rather than getting outraged or harassed. He admitted Twitter was full of problems, problems he didn’t anticipate 13 years ago when the site was founded, and which he’s still trying to figure out how to solve.

Emily Dreyfuss covers the intersection of tech and culture for WIRED.

The urgency of this task couldn’t have been made clearer in the days leading up to Dorsey’s appearance. Over the weekend, Ilhan Omar—a woman of color, an immigrant, and a Muslim representing the state of Minnesota in the US House—reported an increase in death threats after President Trump tweeted out a video that intercut a speech she recently gave with footage of the 9/11 attacks. Many of the threats were made on Twitter. Then on Monday, as Notre Dame burned, people came to the platform to mourn the loss in real time, but also to spread lies and hate as quickly as the flames engulfed the cathedral’s spire. When Omar tweeted her own heartfelt condolences, people replied with more death threats. Twitter was very much itself, showcasing the power of its network as well the danger.

Dorsey didn’t address any of these incidents specifically at TED. In fact, his answers lacked specificity overall. When he was asked pointed questions, he evaded them, as he often does. Rodgers asked him how many people are working on content moderation on Twitter—a number the company has never published, and Tuesday continued the vagueness streak.

“It varies,” Dorsey said. “We want to be flexible on this. There are no amount of people that can actually scale this, which is why we have done so much work on proactively taking down abuse.”",Yes,"It amplifies human behavior. Good behavior and bad behavior.

Twitter is a microcosm of the internet. It’s a platform that allows people to share information and ideas instantly, without much filter. That can be a good thing! It can also be a bad thing. People can use Twitter to share news and have thoughtful",https://www.wired.com/story/ted-2019-jack-dorsey-captain-twittanic/,2019-04-16 20:28:03.193000+00:00,LABEL_1_relevant,0.9949550032615662,Social Media,social_media,Wired,"Information, Discourse & Governance"
85,The exhausting playbook behind Trump’s battle with Twitter,"This moment feels like an inevitable escalation of a conflict that has been playing out across the major social-media companies, but particularly Twitter, for years—one that Yiannopoulos’s White House stunt foreshadowed. As platforms reckon with their role in amplifying misinformation, abuse, and extreme views, the arguments about content moderation that once lived on the fringes of Twitter’s rules increasingly involve people at the very center of mainstream power.

“Republicans feel that Social Media Platforms totally silence conservatives voices,” Trump tweeted to his 80 million followers this week. “We will strongly regulate, or close them down, before we can ever allow this to happen.” His comments were covered widely in the media, as are many of his more inflammatory or conspiratorial tweets.

Hours before news of the coming executive order broke, Trump advisor Kellyanne Conway went on Fox News and encouraged viewers to hound a Twitter employee, spelling out his account handle and blaming him for the decision to fact-check the president’s tweets. “Somebody in San Francisco go wake him up and tell him he’s about to get a lot more followers,” she said.

Trump himself tagged the employee in a tweet on Thursday, effectively directing supporters to fill his mentions with abusive messages. The Twitter employee is also reportedly receiving death threats.

This cycle has been set off in the past when Twitter has rolled out new policies designed to protect targets of abuse, suspended far-right accounts for rule violations, or stepped up efforts to slow the spread of misinformation. It begins with waves of speculation arguing that Twitter isn’t actually, say, enforcing its new abuse policies but instead implementing a secret anti-conservative agenda that must be stopped. Then there’s a rush to find and target someone responsible for implementing it. The blueprint dates back at least to Gamergate, the harassment campaign championed by Yiannopoulos targeting women in video-game development, whose supporters also claimed instead to be fighting a conspiracy against them ( “It’s actually about ethics in gaming journalism”).

The president uses his own account to continually test Twitter’s boundaries, and now he’s become the catalyst for a new cycle. In just the past week, he’s used his platform to amplify conspiracy theories suggesting that MSNBC host Joe Scarborough murdered a staffer and to spread misinformation about mail-in voting in an earlier series of tweets that were not subject to fact-check labels. He thanked a “Cowboys for Trump” account that tweeted a video where an unidentified man proclaimed that “the only good Democrat is a dead Democrat.” (After cheers from the audience, the speaker then clarifies that he meant the comment “politically.”) The widower of the deceased staffer at the heart of the Scarborough conspiracy theory has begged Twitter to intervene.

The company had not taken any action against those tweets as of Thursday, although it has indicated that it is working to expand the labeling system that was used to flag some of Trump’s tweets about mail-in voting.

Until the fact-checking labels were introduced to two of Trump’s tweets on Tuesday, the platform had scrupulously avoided enforcing its rules against Trump’s account. Some explanations for the enforcement loopholes have cited the newsworthiness of otherwise rule-breaking content and Trump’s status as the head of a government.

But Trump, despite the lack of evidence to support claims of systemic social-media bias against conservatives, has repeatedly promised to take up the issue on behalf of some of his more prominent supporters. In 2018, he accused Google of “rigging” news search results against conservative media, repeating a version of a claim that Trump supporters—including vloggers Diamond and Silk—had circulated in conservative media for a few days earlier. Diamond and Silk (whose real names are Lynnette Hardaway and Rochelle Richardson) claimed at a House Judiciary Committee hearing that April that they were being “censored” by Facebook because of their support for Trump.

In 2019, Trump met with Twitter CEO Jack Dorsey and reportedly took the opportunity to complain about losing Twitter followers. On the same day as that meeting, Trump tweeted that the platform was “very discriminatory.” He later tweeted that his administration was “closely” monitoring conservatives’ complaints of censorship. Later that year, Trump held a “social-media summit” with dozens of his most passionate online supporters to air their collective complaints that Google, Facebook, and Twitter were censoring them.

None of these claims have to be true to be popular, which is something Trump and his online supporters know well. They just need to sound controversial enough to grab attention—or, better yet, redirect it from something else.",Yes,"A potential amplification of misinformation, abuse, and extreme views. This could lead to arguments about content moderation that involve people at the very center of mainstream power.",https://www.technologyreview.com/2020/05/28/1002376/trump-twitter-conservative-bias/,2020-05-28 00:00:00,LABEL_1_relevant,0.9919018149375916,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
86,The Christchurch Shooter and YouTube’s Radicalization Trap,"YouTube, Facebook, and other social media platforms were instrumental in radicalizing the terrorist who killed 51 worshippers in a March 2019 attack on two New Zealand mosques, according to a new report from the country’s government. Online radicalization experts speaking with WIRED say that while platforms have cracked down on extremist content since then, the fundamental business models behind top social media sites still play a role in online radicalization.

According to the report, released last night, the terrorist regularly watched extremist content online and donated to organizations like the Daily Stormer, a white supremacist site, and Stefan Molyneux’s far-right Freedomain Radio. He also gave directly to Austrian far-right activist Martin Sellner. “The individual claimed that he was not a frequent commenter on extreme right-wing sites and that YouTube was, for him, a far more significant source of information and inspiration,” the report says.

The terrorist’s interest in far-right YouTubers and edgy forums like 8chan is not a revelation. But until now, the details of his involvement with these online far-right organizations were not public. Over a year later, YouTube and other platforms have taken steps toward accepting responsibility for white supremacist content that propagates on their websites, including removing popular content creators and hiring thousands more moderators. Yet according to experts, until social media companies open the lid on their black-box policies and even algorithms, white supremacist propaganda will always be a few clicks away.

""The problem goes far deeper than the identification and removal of pieces of problematic content,"" said a New Zealand government spokesperson over email. ""The same algorithms that keep people tuned to the platform and consuming advertising can also promote harmful content once individuals have shown an interest.""

The Christchurch attacker’s pathway to radicalization was entirely unexceptional, say three experts speaking with WIRED who had reviewed the government report. He came from a broken home and from a young age was exposed to domestic violence, sickness, and suicide. He had unsupervised access to a computer, where he played online games and, at age 14, discovered the online forum 4chan. The report details how he expressed racist ideas at his school, and he was twice called in to speak with its anti-racism contact officer regarding anti-Semitism. The report describes him as somebody with “limited personal engagement,” which “left considerable scope for influence from extreme right-wing material, which he found on the internet and in books.” Aside from a couple of years working as a personal trainer, he had no consistent employment.

The terrorist’s mother told the Australian Federal Police that her concerns grew in early 2017. “She remembered him talking about how the Western world was coming to an end because Muslim migrants were coming back into Europe and would out-breed Europeans,” the report says. The terrorist’s friends and family provided narratives of his radicalization that are supported by his internet activity: shared links, donations, comments. While he was not a frequent poster on right-wing sites, he spent ample time in the extremist corners of YouTube.

""They’ve kicked some people off the platform, but they haven’t addressed that underlying issue."" Becca Lewis, Stanford

A damning 2018 report by Stanford researcher and PhD candidate Becca Lewis describes the alternative media system on YouTube that fed young viewers far-right propaganda. This network of channels, which range from mainstream conservatives and libertarians to overt white nationalists, collaborated with each other, funneling viewers into increasingly extreme content streams. She points to Stefan Molyneux as an example. “He’s been shown time and time again to be an important vector point for people’s radicalization,” she says. “He claimed there were scientific differences between the races and promoted debunked pseudoscience. But because he wasn’t a self-identified or overt neo-Nazi, he became embraced by more mainstream people with more mainstream platforms.” YouTube removed Molyneux’s channel in June of this year.

This “step-ladder of amplification” is in part a byproduct of the business model for YouTube creators, says Lewis. Revenue is directly tied to viewership, and exposure is currency. While these networks of creators played off each other’s fan bases, the drive to gain more viewers also incentivized them to post increasingly inflammatory and incendiary content. “One of the most disturbing things I found was not only evidence that audiences were getting radicalized, but also data that literally showed creators getting more radical in their content over time,” she says.",Yes,"It has given rise to a new breed of super-empowered individuals that are capable of inflicting serious harm to society through their online activity.

The Christchurch terrorist’s attack was unplanned; he chose his targets and his weapons based on what he could find online and in his immediate vicinity. He did not have any known criminal",https://www.wired.com/story/christchurch-shooter-youtube-radicalization-extremism/,2020-12-09 00:51:33.762000+00:00,LABEL_1_relevant,0.9958058595657348,Social Media,social_media,Wired,"Information, Discourse & Governance"
87,Did Quora Get Hacked? Top Answer: Yes,"Nearly a month after the midterm elections, details on a hack of the Republican National Congressional Committee reveals that meddling in the midterms was much worse than it seemed on election day. The hack probably should have been the biggest news of the week, but for a little distracting—and important!—thing called the Mueller probe.

In expectation of Robert Mueller making big moves in the investigation before the holidays, Garrett Graff spells out the 14 questions about Trump and Russia that Mueller knows the answer to.

Also this week, veteran Kristofer Goldsmith revealed that foreign trolls are targeting vets on Facebook. Louise Matsakis reported on the ways Facebook’s dominance of the nonprofit sector exposes charities to money—but also hackers. Lily Hay Newman explained that iTunes doesn’t encrypt downloads on purpose, and Brian Barrett warned you about a clever Touch ID scam hitting the App Store.

Australia’s very bad anti-encryption law could be bad for the whole world. A new company wants to solve the problem of email phishing with… the blockchain, obviously. Issie Lapowsky did the dirty work to understand how New York City’s anti-dick pic airdrop rule would even work. And we wrote you a guide to data breaches. You’re welcome!

Of course, that wasn’t all. As always, we’ve rounded up all the news we didn’t break or cover in depth this week. Click on the headlines to read the full stories. And stay safe out there.

In a blog post on Monday, Quora CEO Adam D’Angelo admitted that Quora was breached. The company discovered the problem last Friday, and more than 100 million accounts may have had their data taken. The company says it’s working to contain the problem and has hired independent auditors to figure out what happened. Quora also said that the breach also might affect any “linked networks,” aka Facebook or Google, if you used those to log into Quora. Though no financial information is attached to Quora accounts, there’s a ton of personal and social information available for each account. That kind of information is a juicy target for hackers looking to cause havoc or steal identities.

If you have a Quora account, you probably received an email this week telling you to change your password if it’s one you reuse elsewhere. You should get on that.

There’s something so romantic about a cable car in a city. Riding in one makes you feel like you’re flying above all the troubles of the concrete world below. But there was no romance in what happened to the the brand new free cable-car that opened in Russia’s capital to bring passengers from the tourist district of Vorobyovy Gory to the Luzhniki Stadium. On its first full day of operation, the cable car was hit with ransomware that shut the tramway down. Hackers demanded payment in bitcoin before they would resume service. Police has to turn disappointed passengers away. By the next day, however, they had cable car up and running again.

We told you about the hack that hit 500 million Marriott customers two weeks ago, and how to protect yourself from the fallout. Now, Reuters reports that it was likely a nation-state attack perpetrated by China, according to evidence unearthed by private investigators hired by Marriott. Why would a country want to hack a hotel chain, and steal the reservation records of millions of guests? Because who was staying where and when is very valuable information for a nation that wants to exert almost total control over its people. Investigators caution that they are not 100 percent sure it was China, however.

ZDNet reports that Kaspersky Labs has been brought in to investigate some bank robberies in Eastern Europe, in which robbers stole tens of millions of dollars. They apparently pulled it off by disguising themselves as inspectors, gaining access to the bank networks and leaving malware-laden devices behind. These devices—laptops, Raspberry Pis, and “malicious USB thumb drives known as Bash Bunnies,” according to ZDNet—allowed the hackers to remotely connect to the bank networks, and siphon money away.

More Great WIRED Stories",,"It de-centers the world. In other words, it makes the world about you and your experiences, rather than about the large-scale social trends and changes happening around you. And it’s not just the Russians that are doing this to us. We’re doing it to ourselves.

The Australian government passed a",https://www.wired.com/story/quora-hack-china-marriott-security-news/,2018-12-08 14:00:00+00:00,LABEL_1_relevant,0.9957475066184998,Social Media,social_media,Wired,"Information, Discourse & Governance"
88,"WWIII Memes, Oddly, Prove There's Hope for the Internet","After a US drone strike killed Iranian general Qasem Soleimani last week, the online world saw two nations tumbling toward a war that could consume the globe—the long-prophesied World War III. Some Americans celebrated President Trump taking a stand against an old perceived enemy. Some planned protests. Many, including President Trump and Iranian foreign minister Javad Zarif, made their thoughts known on social media, which may have helped stop a war before it started. Because this is the internet, though, not all of those thoughts were nuanced policy positions. A lot of them were jokes.

Moments after news of Soleimani’s death broke and #WWIII began trending on Twitter, people started to meme. There were jokes about being poor soldiers, about playing dead and being shot again, about how annoyed Queen Elizabeth II must be that she’s going to live through another world war. Many of them featured images and GIFs from pop culture: SpongeBob SquarePants, Friends, Keeping Up With the Kardashians. Especially on TikTok and among young people, the jokes tended to revolve around creative means of dodging the (currently nonexistent) draft, from blocking US military organizations on Twitter to having asthma to accepting the consequences and having a good time in prison. The meme has even made it to some elementary school classrooms: “A former student who teaches fourth and fifth graders told me that his students are all joking about it,” says Viveca Greene, who studies dark humor at Hampshire College.

Jokes about WWIII strike some over the age of 10 as decidedly—offensively—unfunny. It’s easy to see their point: Avoidable death and destruction are generally no laughing matter, and joking about them might trivialize strangers’ real suffering. The cycle of joke and rebuke should be familiar to everyone by now, but in the case of #WWIII, the condemnation has gone, well, worldwide.

“What strikes me as unusual is the sharp rebuke by people in the Middle East who claim to be in far more dangerous situations than Americans who are worried about being drafted,” says Paul Lewis, author of Cracking Up: American Humor in a Time of Conflict. Considering the draft people are worried about doesn’t currently exist, it’s difficult to dispute that. However, these jokes and memes may not always be signs of calloused hearts and internet-maddened minds.

Many people (some meme-makers included) have identified jokes about WWIII and the draft as coping mechanisms, and time-honored ones at that. “There have probably been jokes about wars since there have been wars,” says Lewis, who studies humor and politics at Boston College. “Freud recognized such gallows humor as a last, desperate effort of the ego to distance itself from an inescapable bad fate.” As the world has grown more unpredictable and destabilized, memes have taken on an absurdist ""laugh to keep from crying"" sort of tone. (See: the majority of 2019.)

Other experts think #WWIII jokes are less memetic mental balm than the kind of disaster humor people have observed since the early 1900s, when consumption of media truly came to the masses. “These jokes aren’t like what we know from emergency room workers. Those jokes don’t have the same kind of absurdity,” says Giselinde Kuipers, a cultural sociologist at Catholic University Leuven, Belgium. According to Kuipers, dark topic + absurdity = jokes happens only when there’s a great deal of media-mediated distance between the joker and the crisis, like when the Titanic sank or people outside of New York City learned about the 9/11 attacks.

LEARN MORE The WIRED Guide to Memes

Either way, the motive for making the joke is rather self-serving, but, in aggregate, the shallowness of the act doesn’t negate the potential positive effects. “The sheer number of people reporting that they learned what was going on with WWIII by seeing memes on Reddit is astounding,” says Greene. “It’s not the endpoint, but humor has always been used to draw attention to issues in a way that people aren’t as turned off by as they are if you came with a serious voice and a wagging finger.” Conveying overwhelming information in a comfortable, approachable format is a technique often used by white supremists and terrorist organizations like ISIS to reel in new recruits. WWIII memes do the same, but people are ending up informed about world events rather than radicalized.

Not only are people being informed about events taking place on the other side of the world, they’re able to engage directly with the people whose lives are impacted. “The biggest thing to notice is that this is the globalization of response to humor,” Lewis says. “The global internet is allowing people around the world to see what Americans are joking about, and respond.” The fact that so many of the comments are negative doesn’t mean the discussion isn’t worth having or isn’t a significant improvement over war jokes past. Greene notes that a presumed good–evil binary is all but absent from jokes about WWIII, which was not at all the case in the lead up to the Iraq War. For most people, it’s hard to fully demonize people who can tweet at you.",Yes,"People are now able to see and engage with memes about current events from around the world. Although some of these memes are funny, many people believe that they trivialize the real-life suffering of those affected by the events.",https://www.wired.com/story/wwiii-memes/,2020-01-10 12:00:00+00:00,LABEL_1_relevant,0.9917024970054626,Social Media,social_media,Wired,"Information, Discourse & Governance"
89,Twitter taps crypto developer to lead 'bluesky' decentralized social network effort,"Twitter’s ambitious upstart decentralized social media working group “bluesky” took an important step Monday as the social media company appointed a formal project lead who will direct how the protocol develops moving forward.

Crypto developer Jay Graber was tapped by Twitter to helm the initiative, which the company hopes will eventually create a decentralized social media protocol that a number of social networks including Twitter will operate on. The separate bluesky organization will operate independently but to date has been funded and managed largely by employees at Twitter.

Graber had already been working in a less formal role inside the bluesky team, with Twitter paying her to create a technical review of the decentralized social ecosystem for a working group of developers in the space. Graber previously worked on the developer team behind privacy focused cryptocurrency Zcash and built out her own decentralized social network called Happening, designed to compete with Facebook Events. Graber eventually walked away from the effort after having issues bootstrapping a user base interested in the benefits of decentralization, something that has grown to be a near-insurmountable issue for most upstart networks in the space.

I’m excited to announce that I’ll be leading @bluesky, an initiative started by @Twitter to decentralize social media. Follow updates on Twitter and at https://t.co/Sg4MxK1zwl — Jay Graber (@arcalinea) August 16, 2021

In an interview back in January, Graber told TechCrunch she saw a major opportunity in Twitter entering the decentralized social space due to the hefty user base on the Twitter platform, which will itself eventually migrate to the protocol, the company has said.

“The really powerful thing about Twitter doing a decentralized protocol move is that if you could design a protocol that works in an ideal way, you don’t have to go through the initial effort of finding the niche to bootstrap from because Twitter will bring so many users,” Graber told us.

In January, TechCrunch profiled the initiative as it gathered more attention following Twitter’s permanent ban of former President Donald Trump from its platform. Following Trump’s removal, Twitter CEO Jack Dorsey highlighted the bluesky effort as one of the company’s ongoing initiatives to ensure that social media moderation could be less decentralized in the future. A decentralized social media protocol would allow for individual networks to govern themselves without one company or organization exercising monolithic control over the sphere of online conversations.

“I think a huge focus for everyone involved has been thinking how do we enable better moderation, and not just coming from one source,” Graber told TechCrunch.

The bluesky organization is still in its earliest stages. Graber’s next task is bulking up the team with its first hires, which include a protocol developer and web developer.",Yes,The echo chamber effect.,https://techcrunch.com/2021/08/16/twitter-taps-crypto-developer-to-lead-bluesky-decentralized-social-network-effort/,2021-08-16 00:00:00,LABEL_1_relevant,0.9929665327072144,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
90,In Celebration of the Internet’s True Angels,"When asked why she regularly uses her spare scrolling time to help strangers out, rather than watch Gordon Ramsay TikToks or snoop on enemy Instagram pages like the rest of us, Spalding says that’s exactly it—she helps because other people don’t. “I feel like there just aren’t that many people out there who would take the time, or have the time necessarily. My life is not that busy, I’m not 9-to-5-ing and I don’t have kids,” she says. “It’s kind of more of a selfish thing, because it gives me a sense of satisfaction that I’m able to tie up loose ends for people.”

The Reviewer

After all his work for the day is done and his wife is fast asleep, 43-year-old civil engineering estimator Craig Laws takes out his iPad and writes Amazon reviews. Laws is currently the 25th top reviewer on Amazon UK, a place he earned after other users marked his reviews as “helpful” a total of 10,873 times. The Derby resident has reviewed everything from boxing gloves to air purifiers to dog treats to artist paint palettes, and is currently testing out a laser hair removal device on his shoulders, painstakingly monitoring its progress over the course of a few months. “You do suffer for your art sometimes,” he says.

Laws has been writing Amazon reviews since his first purchase on the site: a Fujifilm digital camera back in 2001. He started leaving reviews because he relied upon them himself. “You get hooked,” he says, “and you start to give back.” Two years ago, Laws was invited to be part of Amazon’s Vine program, meaning he is now regularly offered free products to review. So is all of this less altruistic than it seems? A way to get some free goods? In actual fact, Laws often selects items to review that he doesn’t personally want, so he can give “real, proper impartial advice” (hence the state of his shoulder blades). He also gets “at least one message a day” directly from manufacturers asking him to review their products for money or freebies and ignores “absolutely all of them.” He can’t be bought.

So what keeps this hyper-reviewer going? At any one time, Laws is testing out between five and 10 products—he also takes photos and videos to attach to his reviews in order to make them as detailed as possible. He isn’t remotely motivated by his place in the Amazon reviewer rankings—he likes that his work is recognized, but doesn’t write extra reviews in the hopes of making his way up to number one (unlike some others on the site). Instead, Laws simply likes to help people out, and he has fun doing it.

“It’s nice to have some kind of platform where people listen to you. If I was to say this in a shop corner, no one would listen to me,” he says. Laws doesn’t like using social media because it’s inherently argumentative—“you can just watch people imploding on the internet a lot.” Instead, he uses Amazon to get his voice heard. “You’re never going to change someone’s opinion on the internet,” he says of politics and sport and news. “But if someone is actively looking for an opinion when choosing something, then that’s an opinion that people might listen to.”

The Fixer

There are thousands of YouTube channels dedicated to showing you how to fix this microwave or troubleshoot that computer. The people behind them are nice enough, sure, but they make money and promote their own repair businesses via the videos, so their motives aren’t entirely altruistic. Far more interesting are those who upload one-off videos after they themselves have finally figured out how to repair something that’s been tormenting them. Take, for example, the mystery man behind the wonderfully titled, “HOW TO FIX A SLOW FLUSH. No rambling just a quick answer. *Contains adult language*,” which has been viewed more than 100,000 times. In September 2018, 35-year-old homemaker Jenna Quinlan from California uploaded one such video: “How to Fix an Ironing Board Lever! I Finally figured it out!”

Quinlan didn’t create a YouTube channel in order to post the video—she’d started one earlier that year as a place to share her love of essential oils, and over the years she’s accumulated slightly more than 400 subscribers. She knew an ironing board video wasn’t what her small community wanted or needed, but when she finally fixed her own board after a “super frustrating” couple of months, she just had to share her knowledge with the world. Naturally, she had previously looked on YouTube for tips on how to fix the board, but had found the videos there to have “dark videography, really obscure descriptions, no close-ups to see what’s going on.”",Yes,"We have lost the ability to be helpful.

But Quinlan’s video was none of those things. It had good lighting and Quinlan took her time explaining each step in detail. She even uploaded a second video after someone in the comments section asked her how she fixed the lever in the first place. “I guess I",https://www.wired.com/story/2021-internet-angels/,2021-12-30 12:00:00+00:00,LABEL_1_relevant,0.8351994752883911,Social Media,social_media,Wired,"Information, Discourse & Governance"
91,"Facebook knows Instagram harms teens. Now, its plan to open the app to kids looks worse than ever","Facebook knows Instagram harms teens. Now, its plan to open the app to kids looks worse than ever

Facebook is in the hot seat again.

The Wall Street Journal published a powerful multi-part series on the company this week, drawing from internal documents on everything from the company’s secretive practice of whitelisting celebrities to its knowledge that Instagram is taking a serious toll on the mental health of teen girls.

The flurry of investigative pieces makes it clear that what Facebook says in public doesn’t always reflect the company’s knowledge on known issues behind the scenes. The revelations still managed to shock even though Facebook has been playing dumb about the various social ills it has sown for years. (Remember when Mark Zuckerberg dismissed the notion that Facebook influenced the 2016 election as “crazy?”) Facebook’s longstanding PR playbook is to hide its dangers, denying knowledge of its darker impacts on society publicly, even as research spells them out internally.

That’s all well and good until someone gets ahold of the internal research.

One of the biggest revelations from the WSJ’s report: The company knows that Instagram poses serious dangers to mental health in teenage girls. An internal research slide from 2019 acknowledged that “We make body image issues worse for one in three teen girls” — a shocking admission for a company charging ahead with plans to expand to even younger and more vulnerable age groups.

As recently as May, Instagram’s Adam Mosseri dismissed concerns around the app’s negative impact on teens as “quite small.”

But internally, the picture told a different story. According to the WSJ, from 2019 to 2021, the company conducted a thorough deep dive into teen mental health, including online surveys, diary studies, focus groups and large-scale questionnaires.

According to one internal slide, the findings showed that 32% of teenage girls reported that Instagram made them have a worse body image. Of research participants who experienced suicidal thoughts, 13% of British teens and 6% of American teens directly linked their interest in killing themselves to Instagram.

“Teens blame Instagram for increases in the rate of anxiety and depression,” another internal slide stated. “This reaction was unprompted and consistent across all groups.”

Following the WSJ report, Senators Marsha Blackburn (R-TN) and Richard Blumenthal (D-CT) announced a probe into Facebook’s lack of transparency around internal research showing that Instagram poses serious and even lethal danger to teens. The Senate Subcommittee on Consumer Protection, Product Safety, and Data Security will launch the investigation.

“We are in touch with a Facebook whistleblower and will use every resource at our disposal to investigate what Facebook knew and when they knew it – including seeking further documents and pursuing witness testimony,” Senators Blackburn and Blumenthal wrote. “The Wall Street Journal’s blockbuster reporting may only be the tip of the iceberg.”

Blackburn and Blumenthal weren’t the only U.S. lawmakers alarmed by the new report. Sen. Ed Markey (D-MA), Rep. Kathy Castor (D-FL), and Lori Trahan (D-MA) sent Facebook their own letter demanding that the company walk away from its plan to launch Instagram for kids. “Children and teens are uniquely vulnerable populations online, and these findings paint a clear and devastating picture of Instagram as an app that poses significant threats to young people’s wellbeing,” the lawmakers wrote.

Big Tech has become the new Big Tobacco. Facebook is lying about how their product harms teens. https://t.co/85oo3B9oO0 — Rep. Ken Buck (@RepKenBuck) September 14, 2021

Facebook gobbled up Instagram because they were too chicken to compete against them fair & square for younger users. When there’s one big game in town, there’s a whole lot less pressure to offer the best service—or do the least damage. #BreakUpBigTechhttps://t.co/mIyHQ2iPs8 — Elizabeth Warren (@ewarren) September 16, 2021

In May, a group of 44 state attorneys general wrote to Instagram to encourage the company to abandon its plans to bring Instagram to kids under the age of 13. “It appears that Facebook is not responding to a need, but instead creating one, as this platform appeals primarily to children who otherwise do not or would not have an Instagram account,” the group of attorneys general wrote. They warned that an Instagram for kids would be “harmful for myriad reasons.”

In April, a collection of the same Democratic lawmakers expressed “serious concerns” about Instagram’s potential impact on the well-being of young users. That same month, a coalition of consumer advocacy organizations also demanded that the company reconsider launching a version of Instagram for kids.

According to the documents obtained by the WSJ, all of those concerns look extremely valid. In spite of extensive internal research and their deeply troubling findings, Facebook has downplayed its knowledge publicly, even as regulators regularly pressed the company for what it really knows.

Instagram’s Mosseri may have made matters worse Thursday when he made a less than flattering analogy between social media platforms and vehicles. “We know that more people die than would otherwise because of car accidents, but by and large, cars create way more value in the world than they destroy,” Mosseri told Peter Kafka on Recode’s media podcast. “And I think social media is similar.”

Mosseri dismissed any comparison between social media and drugs or cigarettes in spite of social media’s well-researched addictive effects, likening social platforms to the auto industry instead. Naturally, the company’s many critics jumped on the car comparison, pointing to their widespread lethality and the fact that the auto industry is heavily regulated — unlike social media.",Yes,It has negative impacts on the mental health of teen girls.,https://techcrunch.com/2021/09/16/facebook-instagram-for-kids-mosseri-wsj-teen-girls/,2021-09-16 00:00:00,LABEL_1_relevant,0.9960582256317142,Social Media,social_media,TechCrunch,Equality & Justice
92,Inside Facebook's Plan to Safeguard the 2018 Election,"Deep in the bowels of Facebook's serpentine campus in Menlo Park, California, is a room about 25 feet square that may have a lot to do with how the world thinks about the company in the coming months. It looks like a Wall Street trading floor, with screens on every wall and every desk. And 20 hours a day---soon to be 24 hours a day---it's jammed with about two dozen geeks, spooks, hackers, and lawyers trying to spot and quash the next bad thing to happen on the company's networks.

It's known appropriately as the War Room, and it was set up just a month ago---in advance of the Brazilian presidential election and US midterm elections---as perhaps Facebook’s most dramatic and visual step to ensure that the fraud and manipulation that was rampant on Facebook's networks during the 2016 US presidential election don't recur.

In years past Facebook would have worked hard to keep an effort like this under wraps for fear of letting competitors know what it was up to or of signaling an imperfection in one of Silicon Valley's biggest success stories. But Facebook's reputation has been damaged by the manipulation it did not detect in 2016, its arrogant response after the vote, and this year’s Cambridge Analytica scandal. So Wednesday morning it invited about two dozen journalists to take a look, to ask questions, and to hopefully tell the world that Facebook is at least trying to get things right this time.

Inside Facebook's War Room for detecting and preventing election interference. WIRED

The room doesn't look like much from the hallway of Building 20. In fact, blackout paper on the windows makes it look like an unlit empty conference room. But inside are screens monitoring viral content, spam, hate speech, and voter suppression across all Facebook's networks, as well as external sites like Twitter and Reddit. “We've been doing all this work virtually for two years. But when stuff needs to be done fast, there is no substitute for face-to-face contact,” said Samidh Chakrabarti, Facebook’s elections and civic engagement boss.

Spokesperson Tom Reynolds said the two dozen teams represented inside the War Room are backed up by 20,000 people Facebook has dedicated to better policing its platforms. The teams include specialists in threat intelligence, data science, engineering, research, operations, legal policy, and communications. For now, it is staffed about 20 hours a day. That will increase to 24 hours five days before Brazil’s October 28 vote and again five days before the November 6 US midterm elections.

The room is set up as an acknowledgment that Facebook is in an arms race against the tricksters and manipulators, and that threats can arrive from anywhere. They can start with someone creating a fake account, or a legitimate account launching a fake news campaign, or as spammers out to make a quick buck, and they can start in any language in any country. With representatives of so many teams working within ear shot, the hope is that the threats can be addressed in a matter of minutes before they get traction on the platform.

On Wednesday, executives wouldn’t commit to using the War Room beyond the US midterms. But they talked so effusively about its work that it’s hard to imagine why they’d consider shutting it down.",Yes,"It can be used to spread false information or hate speech cheaply and quickly to a large audience. This is what happened in the 2016 US presidential election, when Russian agents used Facebook to spread disinformation in an attempt to influence the election. In response, Facebook set up the War Room, a physical space where a team of employees work to detect and prevent",https://www.wired.com/story/inside-facebooks-plan-to-safeguard-2018-election/,2018-10-18 10:00:00+00:00,LABEL_1_relevant,0.9951708912849426,Social Media,social_media,Wired,Politics
93,The War in Ukraine Is Keeping Chinese Social Media Censors Busy,"“Artillery fire lights up the sky and breaks my heart. I hope my compatriots in Ukraine are taking care of themselves and their families,” said a user on Weibo, often called China’s Twitter, on February 27. The message was quickly blocked, according to Free Weibo, a service of Great Fire, which tracks Chinese censorship online.

Two days later, a very different message appeared on Weibo: “I support fighting! America and Taiwan have gone too far.” That, too, was blocked, according to Free Weibo.

The messages—and their quick disappearance—show how Chinese social media platforms find themselves in the crosshairs of the Russia-Ukraine war. The platforms must be sure to toe the official line amid subtle shifts in China’s position. Their responses could be an early test of new rules governing how companies use algorithms, which may make them responsible for trending topics and fake news appearing on their sites.

In general, Chinese online platforms receive daily guidance from the government about what sort of content to remove, says Yuqi Na, a researcher in media and communications at the University of Westminster.

A hint of how that works emerged in the days leading up to the invasion. On February 22, a Chinese outlet called Horizon News briefly posted, probably by accident, what appear to be internal instructions for how to spin the Ukraine crisis on its official Weibo account. Among the supposed rules: “Do not post anything unfavorable to Russia or pro-Western.”

The instructions also said to monitor comments and only use hashtags started by state outlets Xinhua, CCTV, or People’s Daily, according to China Digital Times. That sort of direction to follow the lead of major state outlets is common, says Maria Repnikova, an assistant professor in global communication at Georgia State University and author of Chinese Soft Power.

Prior to the invasion, Chinese state media outlets and officials’ Twitter accounts repeated a drumbeat of US warmongering, and brushed off the possibility of an invasion. Once the assault began, China was put in the awkward position of having to reconcile its long-standing policy of noninterference and respecting national sovereignty with its ties to Russia. Just weeks earlier, the two countries reaffirmed their relationship when Russian president Vladimir Putin was a VIP guest at the Beijing Winter Olympics.

In the early days of the war, Chinese state media seemed surprised and took a cautious approach. The relatively sparse coverage largely echoed Russian outlets, calling the conflict a “special military operation” and placing the blame on the US and NATO. “It’s quite intentional,” says Na. “A lot of internet users buy into that narrative when it's their main information source.”

“You’ve got the official narrative, but you’ve got a lot of counternarratives circulating even if they get shut down as quickly as they go up.” Anthony Saich, director, Ash Center for Democratic Governance and Innovation

In that environment, pro-Russia and nationalist discourse flourished. Putin’s February 24 speech justifying the invasion went viral, says Aliaksandr Herasimenka, director of research at the Oxford Internet Institute’s Program on Democracy and Technology. Social media companies let pro-war posts thrive, as well as some sexist posts by Chinese men pining for Ukrainian refugee brides.

At the same time, there was an outpouring of sympathy for Ukrainians on platforms such as Weibo and Weixin, a widely used chat and news app—but many of those posts disappeared once they became popular. Twitter users documented posts that were taken down, including posts depicting antiwar protests in other countries.",Yes,"It led to a false sense of democracy and a false sense of community. Social media has been shown to increase feelings of loneliness and isolation, and to lead to negative mental health consequences, such as anxiety and depression.",https://www.wired.com/story/war-ukraine-chinese-social-media-censors-busy/,2022-03-04 12:00:00+00:00,LABEL_1_relevant,0.9955621957778932,Social Media,social_media,Wired,"Information, Discourse & Governance"
94,Move fast and break Facebook: A bull case for antitrust enforcement,"This is the second post in a series on the Facebook monopoly. The first post explored how the U.S. Federal Trade Commission should define the Facebook monopoly. I am inspired by Cloudflare’s recent post explaining the impact of Amazon’s monopoly in its industry.

Perhaps it was a competitive tactic, but I genuinely believe it more a patriotic duty: guideposts for legislators and regulators on a complex issue. My generation has watched with a combination of sadness and trepidation as legislators who barely use email question the leading technologists of our time about products that have long pervaded our lives in ways we don’t yet understand.

I, personally, and my company both stand to gain little from this — but as a participant in the latest generation of social media upstarts, and as an American concerned for the future of our democracy, I feel a duty to try.

Mark Zuckerberg has reached his Key Largo moment.

In May 1972, executives of the era’s preeminent technology company — AT&T — met at a secret retreat in Key Largo, Florida. Their company was in crisis.

At the time, Ma Bell’s breathtaking monopoly consisted of a holy trinity: Western Electric (the vast majority of phones and cables used for American telephony), the lucrative long distance service (for both personal and business use) and local telephone service, which the company subsidized in exchange for its monopoly.

Over the next decade, all three government branches — legislators, regulators and the courts — parried with AT&T’s lawyers as the press piled on, battering the company’s reputation in the process. By 1982, a consent decree forced AT&T’s dismantling. The biggest company on earth withered to 30% of its book value and seven independent “Baby Bell” regional operating companies. AT&T’s brand would live on, but the business as the world knew it was dead.

Mark Zuckerberg is, undoubtedly, the greatest technologist of our time. For over 17 years, he has outgunned, outsmarted and outperformed like no software entrepreneur before him. Earlier this month, the U.S. Federal Trade Commission refiled its sweeping antitrust case against Facebook.

Its own holy trinity of Facebook Blue, Instagram and WhatsApp is under attack. All three government branches — legislators, regulators and the courts — are gaining steam in their fight, and the press is piling on, battering the company’s reputation in the process. Facebook, the AT&T of our time, is at the brink. For so long, Zuckerberg has told us all to move fast and break things. It’s time for him to break Facebook.

If Facebook does exist to “make the world more open and connected, and not just to build a company,” as Zuckerberg wrote in the 2012 IPO prospectus, he will spin off Instagram and WhatsApp now so that they have a fighting chance. It would be the ultimate Zuckerbergian chess move. Zuckerberg would lose voting control and thus power over all three entities, but in his action he would successfully scatter the opposition. The rationale is simple:

The United States government will break up Facebook. It is not a matter of if; it is a matter of when. Facebook is already losing. Facebook Blue, Instagram and WhatsApp all face existential threats. Pressure from the government will stifle Facebook’s efforts to right the ship. Facebook will generate more value for shareholders as three separate companies.

I write this as an admirer; I genuinely believe much of the criticism Zuckerberg has received is unfair. Facebook faces Sisyphean tasks. The FTC will not let Zuckerberg sneeze without an investigation, and the company has failed to innovate.

Given no chance to acquire new technology and talent, how can Facebook survive over the long term? In 2006, Terry Semel of Yahoo offered $1 billion to buy Facebook. Zuckerberg reportedly remarked, “I just don’t know if I want to work for Terry Semel.” Even if the FTC were to allow it, this generation of founders will not sell to Facebook. Unfair or not, Mark Zuckerberg has become Terry Semel.

The government will break up Facebook

It is not a matter of if; it is a matter of when.

In a speech on the floor of Congress in 1890, Senator John Sherman, the founding father of the modern American antitrust movement, famously said, “If we will not endure a king as a political power, we should not endure a king over the production, transportation and sale of any of the necessities of life. If we would not submit to an emperor, we should not submit to an autocrat of trade with power to prevent competition and to fix the price of any commodity.”

This is the sentiment driving the building resistance to Facebook’s monopoly, and it shows no sign of abating. Zuckerberg has proudly called Facebook the fifth estate. In the U.S., we only have four estates.

All three branches of the federal government are heating up their pursuit. In the Senate, an unusual bipartisan coalition is emerging, with Senators Amy Klobuchar (D-MN), Mark Warner (D-VA), Elizabeth Warren (D-MA) and Josh Hawley (R-MO) each waging a war from multiple fronts.

In the House, Speaker Nancy Pelosi (D-CA) has called Facebook “part of the problem.” Lina Khan’s FTC is likewise only getting started, with unequivocal support from the White House that feels burned by Facebook’s disingenuous lobbying. The Department of Justice will join, too, aided by state attorneys general. And the courts will continue to turn the wheels of justice, slowly but surely.

In the wake of Facebook co-founder Chris Hughes’ scathing 2019 New York Times op-ed, Zuckerberg said that Facebook’s immense size allows it to spend more on trust and safety than Twitter makes in revenue.

“If what you care about is democracy and elections, then you want a company like us to be able to invest billions of dollars per year like we are in building up really advanced tools to fight election interference,” Zuckerberg said.

This could be true, but it does not prove that the concentration of such power in one man’s hands is consistent with U.S. public policy. And the centralized operations could be rebuilt easily in standalone entities.

Time and time again, whether on Holocaust denial, election propaganda or vaccine misinformation, Zuckerberg has struggled to make quick judgments when presented with the information his trust and safety team uncovers. And even before a decision is made, the structure of the team disincentivizes it from even measuring anything that could harm Facebook’s brand. This is inherently inconsistent with U.S. democracy. The New York Times’ army of reporters will not stop uncovering scandal after scandal, contradicting Zuckerberg’s narrative. The writing is on the wall.

Facebook is losing

Facebook Blue, Instagram and WhatsApp all face existential threats. Pressure from the government will stifle Facebook’s efforts to right the ship.

For so long, Facebook has dominated the social media industry. But if you ask Chinese technology executives about Facebook today, they quote Tencent founder Pony Ma: “When a giant falls, his corpse will still be warm for a while.”

Facebook’s recent demise begins with its brand. The endless, cascading scandals of the last decade have irreparably harmed its image. Younger users refuse to adopt the flagship Facebook Blue. The company’s internal polling on two key metrics — good for the world (GFW) and cares about users (CAU) — shows Facebook’s reputation is in tatters. Talent is fleeing, too; Instacart alone recently poached 55 Facebook executives.

In 2012 and 2014, Instagram and WhatsApp were real dangers. Facebook extinguished both through acquisition. Yet today they represent the company’s two most promising, underutilized assets. They are the underinvested telephone networks of our time.

Weeks ago, Instagram head Adam Mosseri announced that the company no longer considers itself a photo-sharing app. Instead, its focus is entertainment. In other words, as the media widely reported, Instagram is changing to compete with TikTok.

TikTok’s strength represents an existential threat. U.S. children 4 to 15 already spend over 80 minutes a day on ByteDance’s TikTok, and it’s just getting started. The demographics are quickly expanding way beyond teenagers, as social products always have. For Instagram, it could be too little too late — as a part of Facebook, Instagram cannot acquire the technology and retain the talent it needs to compete with TikTok.

Imagine Instagram acquisitions of Squarespace to bolster its e-commerce offerings, or Etsy to create a meaningful marketplace. As a part of Facebook, Instagram is strategically adrift.

Likewise, a standalone WhatsApp could easily be a $100 billion market cap company. WhatsApp has a proud legacy of robust security offerings, but its brand has been tarnished by associations with Facebook. Discord’s rise represents a substantial threat, and WhatsApp has failed to innovate to account for this generation’s desire for community-driven messaging. Snapchat, too, is in many ways a potential WhatsApp killer; its young users use photography and video as a messaging medium. Facebook’s top augmented reality talents are leaving for Snapchat.

With 2 billion monthly active users, WhatApp could be a privacy-focused alternative to Facebook Blue, and it would logically introduce expanded profiles, photo-sharing capabilities and other features that would strengthen its offerings. Inside Facebook, WhatsApp has suffered from underinvestment as a potential threat to Facebook Blue and Messenger. Shareholders have suffered for it.

Beyond Instagram and WhatsApp, Facebook Blue itself is struggling. Q2’s earnings may have skyrocketed, but the increase in revenue hid a troubling sign: Ads increased by 47%, but inventory increased by just 6%. This means Facebook is struggling to find new places to run its ads. Why? The core social graph of Facebook is too old.

I fondly remember the day Facebook came to my high school; I have thousands of friends on the platform. I do not use Facebook anymore — not for political reasons, but because my friends have left. A decade ago, hundreds of people wished me happy birthday every year. This year it was 24, half of whom are over the age of 50. And I’m 32 years old. Teen girls run the social world, and many of them don’t even have Facebook on their phones.

Zuckerberg’s newfound push into the metaverse has been well covered, but the question remains: Why wouldn’t a Facebook serious about the metaverse acquire Roblox? Of course, the FTC would currently never allow it.

Facebook’s current clunky attempt at a hardware solution, with an emphasis on the workplace, shows little sign of promise. The launch was hardly propitious, as CNN reported, “While Bosworth, the Facebook executive, was in the middle of describing how he sees Workrooms as a more interactive way to gather virtually with coworkers than video chat, his avatar froze midsentence, the pixels of its digital skin turning from flesh-toned to gray. He had been disconnected.”

This is not the indomitable Facebook of yore. This is graying Facebook, freezing midsentence.

Facebook will generate more value for shareholders as three separate companies

Zuckerberg’s control of 58% of Facebook’s voting shares has forestalled a typical Wall Street reckoning: Investors are tiring of Zuckerberg’s unilateral power. Many justifiably believe the company is more valuable as the sum of its parts. The success of AT&T’s breakup is a case in point.

Five years after AT&T’s 1984 breakup, AT&T and the Baby Bells’ value had doubled compared to AT&T’s pre-breakup market capitalization. Pressure from Japanese entrants battered Western Electric’s market share, but greater competition in telephony spurred investment and innovation among the Baby Bells.

AT&T turned its focus to competing with IBM and preparing for the coming information age. A smaller AT&T became more nimble, ready to focus on the future rather than dwell on the past.

Standalone Facebook Blue, Instagram and WhatsApp could drastically change their futures by attracting talent and acquiring new technologies.

The U.K.’s recent opposition to Facebook’s $400 million GIPHY acquisition proves Facebook will struggle mightily to acquire even small bolt-ons.

Zuckerberg has always been one step ahead. And when he wasn’t, he was famously unprecious: “Copying is faster than innovating.” If he really believes in Facebook’s mission and recognizes that the situation cannot possibly get any better from here, he will copy AT&T’s solution before it is forced upon him.

Regulators are tying Zuckerberg’s hands behind his back as the company weathers body blows and uppercuts from Beijing to Silicon Valley. As Zuckerberg’s idol Augustus Caesar might have once said, carpe diem. It’s time to break Facebook.",Yes,The echo chamber effect.,https://techcrunch.com/2021/08/29/move-fast-and-break-facebook-a-bull-case-for-antitrust-enforcement/,2021-08-29 00:00:00,LABEL_1_relevant,0.995303511619568,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
95,FourFront nabs funding to build an interconnected network of fictional TikTok stars,"Every popular social media platform has opened up a new type of storytelling to a new generation of users, but TikTok’s impact has perhaps been the swiftest as the short-form video platform’s user base has quickly swelled past 1 billion users and transformed how social media stars are made.

FourFront is a media startup that’s looking to define a new type of storytelling on the platform, aiming to popularize a network of individual actors performing scripted short-form serialized stories that feel native to TikTok. Fictionalized storytelling in vlog form is clearly not a new development for social media, but FourFront is hoping that it can capitalize on the discoverability opened up by TikTok’s For You Page (FYP) to steadily build new audiences.

The startup tells TechCrunch that they’ve raised $1.5 million in seed funding from Bam Ventures, Slow Ventures, BDMI, Alumni Ventures Group and HustleFund.

The startup has a couple dozen characters on TikTok, with a handful racking up several hundreds of thousands of followers. Not all of the characters have been hits, and FourFront’s team of writers and social media strategists have settled on nine characters who they’re looking to create an interconnected “universe” around where the actors organically cross paths with each other. While FourFront’s team plans out the character arcs, it’s on the actors to shoot the videos themselves.

The scripted content is often pretty soap opera-esque, but follows the format of popular videos on the platform; “watch us confront my sister’s cheating fiancé LIVE” implores one video from FourFront’s most popular character “Sydney,” who has nearly 500,000 followers after debuting in June. In addition to helping catch her sister’s cheating fiancé, Sydney has also discussed the stresses of her roommate breaking her lease early and the lessons she has learned working in customer support for a dating app.

FourFront co-founder Ilan Benjamin tells TechCrunch that his startup isn’t trying to mislead anyone into thinking that their network of characters are real, noting that their profile bios highlight the fictional nature of the stories and that each of the videos include the #fictional tag. “We don’t want to confuse audiences or trick them, we want them to be entertained,” Benjamin says.

Building content for TikTok means catering to the whims of the FYP, which means plenty of viewers are catching characters like Sydney in the middle of their storylines, something that can create storytelling challenges when it comes to balancing the patience of existing fans while capturing the curiosity of new viewers.

“The plot mechanics might be repetitive,” Benjamin says. “It’s a constant balancing act; every video has to stand on its own, and every single video has to be evergreen.”

For now, the company is largely focused on building out its network of stars and their audiences while looking to push boundaries in how audiences interact with characters using tech products like conversational AI chatbots and polls to help direct the stories themselves.",Yes,"It has created a new generation of ""social media stars"". These stars are made through platforms such as TikTok, which has a user base of over 1 billion people. FourFront is a media startup that is looking to create a network of individual actors who perform scripted short-form serialized stories that are native to TikTok. The company has",https://techcrunch.com/2021/10/12/fourfront-nabs-funding-to-build-an-interconnected-network-of-fictional-tiktok-stars/,2021-10-12 00:00:00,LABEL_1_relevant,0.9951062202453612,Social Media,social_media,TechCrunch,User Experience & Entertainment
96,Trump's social network has 30 days to stop breaking the rules of its ...,"The Software Freedom Conservancy (SFC) says former President Donald Trump’s new social network violated a free and open-source software licensing agreement by ripping off decentralized social network Mastodon. The Trump Media and Technology Group (TMTG) has 30 days to comply with the terms of the license before its access is terminated — forcing it to rebuild the platform or face legal action.

TMTG launched a special purpose acquisition company fundraising effort yesterday with promises to build a sweeping media empire. Its only product so far is a social network called Truth Social that appears strongly to be forked from Mastodon. While anyone can freely reuse Mastodon’s code (and groups like right-wing social network Gab have already done so), they still have to comply with the Affero General Public License (or AGPLv3) that governs that code, and its conditions include offering their own source code to all users.

Truth Social doesn’t comply with that license and, in fact, refers to its service as “proprietary.” Its developers apparently attempted to scrub references that would make the Mastodon connection clear — at one point listing a “sighting” of the Mastodon logo as a bug — but included direct references to Mastodon in the site’s underlying HTML alongside obvious visual similarities.

Truth Social must offer its source code to the pranksters who defaced it

TMTG’s strategy hasn’t sat well with the SFC, an organization that enforces free and open-source software licenses. “The license purposefully treats everyone equally (even people we don’t like or agree with), but they must operate under the same rules of the copyleft licenses that apply to everyone else,” SFC policy fellow Bradley Kuhn wrote in a blog post. “Today, we saw the Trump Media and Technology Group ignoring those important rules — which were designed for the social good.”

Truth Social hasn’t officially launched. But users could access a test version of the platform, where many of them created prank accounts that flooded the service with false company announcements and even fake Donald Trump posts. (The platform has since been replaced by a waitlist.) The SFC demands that TMTG offer all these users access to the Truth Social source code. “If they fail to do this within 30 days, their rights and permissions in the software are automatically and permanently terminated,” Kuhn says.

If Truth Social fails to make the source code available, the SFC could sue it for violating the terms of the license it used. Earlier this year, the group sued electronics maker Vizio for “repeated failures to fulfill even the basic requirements” of free software licensing. “We will be following this issue very closely and demanding that Trump’s Group give the corresponding source to all who use the site,” Kuhn writes.

Mastodon founder Eugen Rochko also said yesterday that he intended to seek legal counsel about the situation, although he didn’t discuss a specific course of action. “Compliance with our AGPLv3 license is very important to me as that is the sole basis upon which I and other developers are willing to give away years of work for free,” he told Talking Points Memo.",Yes,The echo chamber effect. This occurs when people only see information that reaffirms their own beliefs.,https://www.theverge.com/2021/10/22/22740354/trump-truth-social-network-spac-mastodon-license-software-freedom-conservancy,2021-10-22 00:00:00,LABEL_1_relevant,0.9948703050613404,Social Media,social_media,The Verge,"Information, Discourse & Governance"
97,Stop Meme-ing Taylor Swift,"At this point, thinking sober thoughts about a new Taylor Swift song is painfully predictable. It's a here we go again exercise that think-piece writers and social media pundits have come to dread and celebrate in equal measure. Yet, no matter how formulaic the Swift responses are, the internet must be fed. Luckily for all involved, her latest single, ""You Need to Calm Down,"" is irresistible hot-take fodder: It's been hailed as revolutionary, slammed as queerbait, praised as inclusive, and condemned as intolerant. The lyrics and music video nod to Swift's many celebrity feuds, but also include a GLAAD endorsement and a petition to ratify the Equality Act. There is homophobia, and also a food fight. It's a Tayfecta of Swiftian contradiction, and if there's one thing no one is feeling about it, it's calm.

Now, genuine reasons to celebrate or scorn ""You Need to Calm Down"" do exist. Many are celebrating the video because it approaches something like allyship, especially for a pop song. It's packed with queer celebrities of many racial, sexual, and gender identities; it highlights real activism and the Equality Act, and, because of Swift's platform, makes news stories of them both. (Donations to GLAAD have spiked since the song's release.) Swift and erstwhile nemesis Katy Perry buck tradition and do not literally kiss to make up. Some of its lines—like ""shade never made anybody less gay""—are already being printed on T-shirts, and will be belted all Pride month long, as they are clearly meant to be.

The candy-colored bits of progress are not without tonal issues, though. Critics point out that homophobia isn't shade. That putting Swift's struggles with online haters and the LGBTQ+ community’s struggles with systemic bigotry under the same lyrical umbrella is reductive. That the video's homophobic protesters are unkempt, cowboy-hatted, blue-collar stereotypes, which is both classist and ignores prejudice's pervasiveness at every level of American society. Some read Swift's blue-purple-pink wig as a nod to the bisexual flag and hence as queerbait. Other people think it's unfair that Swift is only telling the protesters, and not the LGBTQ+ community, to calm down. (Those people are wrong. My patience for folks who think a glittery parade is tantamount to hate speech can't come to the phone right now. It's dead.)

[#video: https://www.youtube.com/embed/Dkk9gvTmCXY

The song has spawned more opinions than it has words. A still of Swift, dressed in a french fry costume, embracing Perry, who is dressed as a hamburger, is on its way to becoming a submeme of its own. Depending on who you are, it could be a cutesy symbol of happiness and buried hatchets, of Swift and Perry's vapidity, of straight white women centering themselves in a social movement that isn't theirs, or the bizarro world of performative celebrity relationships calibrated for maximum engagement. Just like the song as a whole. That's a lot of weight for a three-and-a-half minute pop song to carry, and putting that kind of burden on it does a, well, queer thing: It proves Swift right.

If there's anything to say about ""You Need to Calm Down,"" it's that the song is more evidence that Taylor Swift is extremely good at her job: being the pop star people deserve, if not the one they say they want. People claimed to despise Swift's lack of politics, and now she is overtly political and they still hate it. People claimed to dislike her petty feuding, but then spent thousands of hours treating her lyrics, tweets, and Instagram photos like a scandal scavenger hunt. ""You Need to Calm Down"" continues to sate internet sleuths with its references to snakes (an obvious allusion to her beef with Kanye West and Kim Kardashian) and gowns (allegedly a nod to a dustup with Aretha Franklin). She even managed to iterate on a strategy that proved successful with ""Look What You Made Me Do"" and even ""Shake It Off"": blaming the listener, placing the controversy at the feet of the ""haters,"" and accepting little culpability. No matter what you're feeling about ""You Need to Calm Down,"" your hot take is still playing directly into Swift's hands.

Given the circumstances, there is only one solution: Stop meme-ing Taylor Swift. Not because she's scarily capable of manipulating the internet, not because her spotty feminism and allyship means she should be canceled, and certainly not because talking about her is boring. Do it because everyone's energy can be better spent. Sometimes a pop song is just a pop song. If you disagree, maybe you need to calm down.

More Great WIRED Stories",Yes,"It's created a more polarized society.

The internet's most important people are far from anonymous. They're the celebrities of the internet.

Is the future of the internet more like Reddit, or Facebook?

The chaos and disruption of the 2016 election was just a dress rehearsal. The real thing is coming.

The",https://www.wired.com/story/stop-meme-ing-taylor-swift/,2019-06-19 15:25:13.649000+00:00,LABEL_1_relevant,0.995988667011261,Social Media,social_media,Wired,Politics
98,Startup lets landlords scan tenants' Facebook to check if they can ...,"A UK startup has developed software for landlords that lets them analyze potential tenants using data from their social media accounts. Score Assured claims its service is simply another way for renters to secure a house in competitive markets, but following coverage of the business by The Washington Post this week — and negative reaction from many users on social media — the company now says it may reconsider how its software operates.

The software scans for keywords like ""poor"" and ""no money""

Landlords use the company's Tenant Assured program to send requests for profiles to would-be tenants. These then grant the program access to data from one or more social media networks (including Facebook, Twitter, LinkedIn, and Instagram), which it uses to create a one-time report on the individual. This process scans private conversations and public posts to record information about the user's personality, life events (like giving birth or getting married), and even their ""financial stress level"" — a measure of how easy it is for them to pay their rent, based on the frequency with which keywords like ""no money,"" ""poor,"" and ""staying in,"" appear in their posts.

Understandably, the reaction to this sort of analysis being used as a tool by landlords has not been positive. Simply scanning for keywords is a pretty crude measure of anything, especially something as fluctuating and relative and ""financial stress."" The Post noted out that some information collected by the startup has a protected status under US housing discrimination law, and a lawyer specializing in this topic told Gawker: ""The designer of [Tenant Assured] may be legally exposed, despite the claim that it is only passing information along.""

Speaking to The Verge, Score Assured's co-founder Steve Thornhill said that he'd expected this sort of reaction, but was adamant that the company's software is more benign than it seems. ""It's about giving the tenant more opportunity to get the property they want,"" he says. ""A lot of people now, millennials, for example, don't have credit scores — so how they can get a property when the answer from the traditional credit score is going to be no?""

Thornhill points out that the report produced from users' social media is a one-time file, and that there's no constant surveillance and no access without the tenant's permission. And he admits that while keyword-scraping can be a crude metric by itself, it's meant to be used alongside the landlord's own judgement. As for violating US housing discrimination laws, Thornhill points out that the company has no plans to enter the US market so this is not a consideration for them. And as for UK discrimination laws? ""I believe it [complies] but we’re going to double check that,"" he says.

A sample report. (Image credit: Score Assured)

There's certainly an argument to be made for the utility of the software and that it is, by itself, neutral. But Tenant Assured shouldn't ignore the fact that its program has the power to enable and amplify prejudice. This is especially true in a housing market like London, where landlords already wield disproportionate power over tenants simply because housing stock is in short supply.

the old argument: if you've got nothing to hide, you've got nothing to fear

And while the system may be opt-in, if it becomes popular it could become a de facto requirement from landlords. This prospect brings up all sorts of depressingly familiar arguments about privacy and personal data, where those in power demand access simply because the information is there. Imagine being told that you have to complete a social media profile before you can rent a house. After all, says the landlord, if you've got nothing to hide, you've got nothing to fear.

Score Assured even thinks its algorithms could be used by employers and private citizens. In the digital age, says Thornhill, we need new ways to trust strangers. ""Forty or fifty years ago, the landlord knew all his potential tenants,"" he says. ""We're trying to bring back a level of personal relationship to the digital world so the right judgement can be made for the right reason."" But is an automated report based on social media anymore personal? I'd say no, but I imagine landlords would want to know, just the same.",Yes,Private individuals or companies may use it to violate users' privacy or housing discrimination laws.,https://www.theverge.com/2016/6/10/11903082/landlord-social-media-credit-check,2016-06-10 00:00:00,LABEL_1_relevant,0.9962394237518308,Social Media,social_media,The Verge,Security & Privacy
99,One Woman’s High-Touch Bid to Upend the Sex-Toy Industry,"Others, too, had “seized the yes” and found it somewhat lacking. Mashable, one of the few press outlets that reviewed the device, awarded it 2.5 out of 5 stars, a score boosted by some extra points awarded for its “cool factor.” The handful of reviewers who have shared their opinion on ProductHunt were almost uniformly displeased. One suggested that the company's attempt at biomimesis had, perhaps, been too successful for its own good: “Overall this experience was worse than a toss with a boring inexperienced man.”

In late April, a few weeks after I'd received my Osé, DiCarlo and I talked on Zoom. Brand Lora was in full effect: She looked fantastic, with her hair carefully tousled and her neck adorned with jewelry. But her tech-visionary hoopla was more subdued than I'd expected. Where in other interviews she'd boasted of her product's ability to remake the female orgasm, now she positioned the Osé as a baby step. “We're still a startup,” she explained. “Nobody ever gets their first product to market perfect. Nobody ever gets it even close to perfect.” A few minutes later, she pivoted to telling me about the company's newer offering, a sex-education and coaching platform called WellSX that will eliminate shame around sex by providing users with a “high-touch human experience.”

Could this be sex tech’s Juicero—a humdrum gadget bested by your own two hands?

Lora DiCarlo was on to other sex toys too. At CES 2020, the company debuted the Onda and the Baci—effectively the Osé's G-spot stimulator and clitoral suction device split in two. In February, Lora DiCarlo filed trademarks for two more unreleased devices, the Filare and the Carezza; and a redesigned Osé 2 is now on sale. As a way of doing business, this would be pretty normal for a tech firm, says Janet Lieberman-Lu, the engineer and cofounder of Dame Products. (Lieberman-Lu left that company earlier this year.) The tech-world mindset and funding strategy “pushes you into a trajectory where you have to grow really fast,” she says. “You're building up the bubble, and then you're trying to build the structure in place in the hopes that when that bubble pops there's something there to catch your company.”

That's not how things typically work in the sex-toy industry. For a company like Dame Products—or even Brian Sloan's Very Intelligent Ecommerce—consumers, not investors, provide most of the cash from the start. Not many adult companies have relied heavily on venture capital, and the handful of exceptions—like the hedge-fund-backed Jimmyjane—have tended to get their major investments years after their products have proved successful in the marketplace. Lieberman-Lu was struck by how things played out with the Osé, though, which was picking up awards before anyone had even held it in their hands. “It's a little bit jarring for people who work in the industry to see the artifice,” she says. “It's not just a different way of doing things. It's an unhealthy way of doing things.”

Indeed, once I saw the Osé up close, another fragile “health and wellness” startup came to mind: Juicero, the company that saw an opening in 2016 for its $400, Wi-Fi-enabled juicers in the fruit-smoothie-tech space and decided to fill it. In spite of $120 million from Silicon Valley investors, Juicero's product—described as a new “platform” for food delivery—turned out to be about as good at pressing the company's proprietary produce packets as a human's grip.

Content This content can also be viewed on the site it originates from.

Could the Osé be sex tech's version of the same—a humdrum gadget potentially bested by your own two hands, dressed up as innovation and sold at twice the price of competing products? “We don't pretend that we know 100 percent exactly what we're doing,” DiCarlo told me as we wrapped up our interview. “We're a young company. We're learning as we go.” It was both the most honest and the most off-brand thing she said to me during our time together.

Her award-winning product may have been a dud, but it was clear that was only half the story. From DiCarlo's first moment in the spotlight, she'd been celebrated for her mastery of tech—for the way she'd used it to redefine the sex toy in service of her mission of empowerment. But the noisy, undistinguished vibrator that's now collecting dust under my bed was not, perhaps, her main invention. The company's success came not from the Osé but from the way it was promoted. DiCarlo had built a hype machine, precision-engineered with all the tools of startup culture, and there's no denying that it worked. It was Brand Lora, not the Osé, that recast a low-level health care worker with no background in either sex or tech as a thought leader in both spaces. It was Brand Lora, not the Osé, that helped the world accept that a sensual massager could be on par with Apple's AirPods, and that an industry long neglected and belittled should at last be taken seriously.",Yes,"It can be used to easily spread false information.

Social media platforms like Twitter and Facebook are designed to be addictive and to keep users engaged for as long as possible. This is done by using techniques such as showing users content that is controversial or that reinforces their existing beliefs. As a result, it's very easy for false information to spread",https://www.wired.com/story/lora-dicarlo-ose-sex-toy-industry/,2020-10-16 10:00:00+00:00,LABEL_1_relevant,0.9964845180511476,Social Media,social_media,Wired,"Information, Discourse & Governance"
100,Why Is Facebook So Afraid of Checking Facts?,"A video laden with falsehoods about Covid-19 emerged on Facebook last week, and has now been viewed many millions of times. The company has taken steps to minimize the video’s reach, but its fact-checks, in particular, appear to have been applied with a curious—if not dangerous—reticence. The reason for that reticence should alarm you: It seems that the biggest social network in the world is, at least in part, basing its response to pandemic-related misinformation on a misreading of the academic literature.

At issue is the company’s long-standing deference to the risk of so-called “backfire effects.” That is to say, Facebook worries that the mere act of trying to debunk a bogus claim may only help to make the lie grow stronger. CEO and founder Mark Zuckerberg expressed this precise concern back in February 2017: “Research shows that some of the most obvious ideas, like showing people an article from the opposite perspective, actually deepen polarization,” he said. The company would later cite the same theory to explain why it had stopped applying “red flag” warnings to fallacious headlines: “Academic research on correcting misinformation,” a Facebook product manager wrote, has shown that such warnings “may actually entrench deeply held beliefs.”

WIRED OPINION ABOUT Ethan Porter is an assistant professor at George Washington University. Thomas J. Wood is an assistant professor at Ohio State University.

Facebook’s fear of backfire hasn’t abated in the midst of this pandemic, or the infodemic that came with it. On April 16, the company announced a plan to deal with rampant Covid-19 misinformation: In addition to putting warning labels on some specific content, it would show decidedly nonspecific warnings to those who’d interacted with a harmful post and nudge them toward more authoritative sources. The vagueness of these latter warnings, Facebook told the website STAT, was meant to minimize the risk of backfire.

But here’s the thing: Whatever Facebook says (or thinks) about the backfire effect, this phenomenon has not, in fact, been “shown” or demonstrated in any thorough way. Rather, it’s a bogeyman—a zombie theory from the research literature circa 2008 that has all but been abandoned since. More recent studies, encompassing a broad array of issues, find the opposite is true: On almost all possible topics, almost all of the time, the average person—Democrat or Republican, young or old, well-educated or not—responds to facts just the way you’d hope, by becoming more factually accurate.

Yes, it’s possible to find exceptions. If you follow all this research very carefully, you’ll be familiar with the rare occasions when, in experimental settings, corrections have failed. If you have a day job, though, and need a rule of thumb, try this: Debunkings and corrections are effective, full stop. This summary puts you much closer to the academic consensus than does the suggestion that backfire effects are widespread and pose an active threat to online discourse.

We’ve demonstrated this fact about facts many times ourselves. Our peer-reviewed book and multiple academic articles describe dozens of randomized studies that we’ve run in which people are exposed to misinformation and fact-checks. The research consistently finds that subjects end up being more accurate in their answers to factual questions. We’ve shown that fact-checks are effective against outlandish conspiracy theories, as well as more run-of-the-mill Trump misstatements. We even partnered up with the authors of the most popular academic article on the backfire effect, in the hopes of tracking it down. Again, we came up empty-handed.",Yes,"It provides a way for individuals to share information with others and also to collect information from others with minimal effort. This allows for information to spread virally, which can lead to the formation of false beliefs. While social media has some benefits, such as providing a way for people to connect with others and share information, it also has some drawbacks,",https://www.wired.com/story/why-is-facebook-so-afraid-of-checking-facts/,2020-05-14 13:00:00+00:00,LABEL_1_relevant,0.9963098168373108,Social Media,social_media,Wired,"Information, Discourse & Governance"
101,Instagram's Newest Star Is ... a Tree?,"In recent days an Instagram post about planting trees has been making its way, like the darkest of dark horses, up the list of most liked posts in the platform’s history. As of this writing, it’s tallied 14.3 million likes, more than Justin Bieber’s engagement photo with Hailey Baldwin, and more than any picture Kylie Jenner has ever posted of her daughter, other than her first photo. (Duh—look at that tiny well-manicured hand!) The Great Instagram Tree Post of 2019 currently sits in fifth place, just behind a picture Selena Gomez took with her friends.

The meteoric post is from the eco-friendly apparel company Tentree. Peppered with a healthy helping of tree emoji, it promises that for “every TEN LIKES this post gets, we will plant ONE TREE” in Indonesia. And who’d turn down an offer to help the planet when all it takes is to read a few words and tap a little heart.

Tentree, of course, has a business interest in going viral: It gains followers, exposure, and presumably sales. That said, if a company is volunteering to plant trees, then by all means they should help reforest the world. Except that reforestation isn’t so easy as liking an Instagram post, and it’s not so easy as just planting lots of trees and walking away. Because as with any celebrity’s carefully orchestrated photo on social media, the reality is much more complicated.

Instagram content This content can also be viewed on the site it originates from.

Conservationists have a few ways of restoring forests. One is known as natural regeneration, in which some deforested areas regrow on their own if protected from further logging. “You don't need to plant anything, you just stop whacking the system,” says Bronson Griscom, director of forest carbon science at the Nature Conservancy. “Stop burning it, stop plowing it, stop cutting it.”

A variation on this strategy is called assisted natural regeneration, in which conservationists strategically plant certain species, like fruit trees, to kickstart the system. “Then the birds will start coming and they'll bring species from elsewhere,” says Griscom. Their poop carries seeds that can over time add variety to the area. “You don't have to plant all the species—you just plant some key species that bring the system back into a diverse form.”

But that’s just the beginning: Deforestation will keep happening in a given area unless there’s a bigger intervention. This means reining in unsustainable logging operations, which also isn’t that straightforward—the livelihood of local peoples has to factor in too.

Matt Simon covers cannabis, robots, and climate science for WIRED.

Take, for instance, work that the Nature Conservancy does in Brazil. A major threat to Brazil’s forests is ranching, both from big and little players: Smaller outfits raise calves, which they pass on to larger ranchers to raise as adult cattle. Conservationists can work relatively easily with the big ranchers and convince them to better mind the forests, “but it's this larger group of people that have smaller amounts of land that's hard, because they don't have as many options,” says Griscom.

So the Nature Conservancy is helping them switch to a more forest-friendly livelihood with cacao farming. The beauty of cacao is that it does well in the shade, which means you don’t need to clear-cut forests to grow it. Instead, you integrate the farm right into the forest. Bonus: Studies have shown this kind of cultivation can actually increase biodiversity and improve soil fertility.

Reforesting by planting a bunch of trees, then, is just the start. “It's a stop doing thing, rather than a doing thing,” says Griscom. “Having said that, in our experience in order to stop doing something it is still a doing story, in the sense that, how do you change someone's livelihood so that they're doing something else?”",Yes,It can give people a false sense of accomplishment.,https://www.wired.com/story/instagrams-newest-star-is-a-tree/,2019-04-25 23:10:14.396000+00:00,LABEL_1_relevant,0.9945525527000428,Social Media,social_media,Wired,"Information, Discourse & Governance"
102,This tech millionaire went from covid trial funder to misinformation superspreader,"His efforts became more focused on medical research when, in 2007, he was diagnosed with a rare blood cancer. His foundation shifted focus to one goal—curing Steve Kirsch—by supporting one of the few scientists looking at the disease. After several failed attempts to stop the progression of his disease, he designed his own protocol for chemotherapy and doctor-shopped to find an oncologist who would give it to him. He’s now outlived his initial prognosis by several years.

“He’s a genuinely good guy. I mean, he really, truly has a heart of gold,” Char told me. “He’s spending his own money to do what he thinks is right. It’s motivated out of his sense of keeping people safe and advancing health care.”

But Kirsch is also motivated by an unsatisfied competitive streak. In that same IEEE Spectrum story about his then-new startup, Propel Software, he said he felt successful, but not famous.

“Mouse Systems is not a household word,” he told the journalist. “We didn’t come up with better mouse technology than Microsoft did. Infoseek lost out to Yahoo; it had a chance to grow bigger, but it didn't. And FrameMaker is still a niche product. Yes, these were successes, but the successes could have been bigger if we had really paid attention to marketing. I’m not going to make the same mistake again.”

“Now we’ve lost the high ground”

It is not unusual to be wary of developing science, or wrong to be skeptical of pharmaceutical companies. These huge businesses do often prioritize profits over human health: in 2009, Pfizer paid a $2.3 billion settlement over kickbacks and fraudulent marketing, including a $1.3 billion felony fine.

In 2013, Johnson & Johnson paid $2.2 billion for its own kickback and fraud scandal, including a specific $400 million fine for its subsidiary Janssen, which manufactures the covid vaccine. The US government accused Janssen of improperly promoting the antipsychotic drug Risperdal to dementia patients despite the drug increasing deaths in the elderly. The man who ran Risperdal sales, Alex Gorsky, is now CEO of Johnson & Johnson.

As a health care journalist, I started off firmly in the wait-and-see camp on mRNA vaccines. Thanks to the volumes of data and information provided by pharmaceutical companies and regulators, as well as large numbers of trials from independently funded research groups around the world, I now trust that they’re safe for the vast majority of adults.

I also think it makes a lot of sense to look for pre-existing drugs that can help treat covid symptoms. Over the next few years, millions of unvaccinated people are going to get covid; it’s vital to try to mitigate their suffering, as well as lessen pressure on the health care system.

But the best way to help people is through rigorous trials that show what drugs help which people, and at what doses and times—not by basing entire protocols on incredibly limited evidence.

Unfortunately, as Jeffrey Morris at UPenn points out, public health officials and scientists have done plenty to undermine their own authority, like claiming masks don’t work, downplaying the natural immunity conveyed by previous covid infections, and not doing enough public communication about vaccine safety surveillance systems.

”We don’t want to feed the anti-vaccine trolls, so we actively suppress clear scientific data. Now we’ve lost the high ground,” Morris told me.

And that is what has allowed Kirsch, and people like him, to become so influential. It’s a cycle that feeds mistrust and boosts the profiles of influencers who present themselves in opposition to official authorities.

“The collateral damage is that, now, a lot of people don’t trust scientific leaders or the scientific community. They’re finding alternative leaders to follow,” Morris said. “That’s what creates some of these heroes.”

This story is part of the Pandemic Technology Project, supported by The Rockefeller Foundation.",Yes,"It has amplified the voices of a small number of influencers, who are often not experts in the field, at the expense of more qualified people. This has led to a mistrust of scientific authorities and a decrease in the public's willingness to accept new information.",https://www.technologyreview.com/2021/10/05/1036408/silicon-valley-millionaire-steve-kirsch-covid-vaccine-misinformation/,2021-10-05 00:00:00,LABEL_1_relevant,0.99649578332901,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
103,"Europe's digital rules reboot could tame Facebook, whistleblower Frances Haugen tells EU Parliament","In her latest turn in front of a phalanx of lawmakers, Facebook whistleblower Frances Haugen gave a polished testimony to the European Parliament on Monday — following similar sessions in front of U.K. and U.S. legislators in recent weeks.

Her core message was the same dire warning she’s sounded on both sides of the Atlantic: Facebook prioritizes profit over safety, choosing to ignore the amplification of toxic content that’s harmful to individuals, societies and democracy. And that regulatory oversight is thus essential to rein in and make such irresponsibly operated platform power accountable — with no time for lawmakers to lose in imposing rules on social media.

The (to date) highest profile Facebook whistleblower got a very warm reception from the European Parliament, where MEPs were universally effusive in thanking her for her time — and what they couched as her “bravery” in raising her concerns publicly — applauding Haugen before she spoke and again at the end of the nearly three hour presentation plus Q&A session.

They questioned her on a range of issues — giving over the largest share of their attention to how incoming pan-EU digital regulations can best deliver effective transparency and accountability on slippery platform giants.

The Digital Services Act (DSA) is front of mind for MEPs as they are considering and voting on amendments to the Commission’s proposal that could seriously reshape the legislation.

Such as a push by some MEPs to get an outright ban on behavioral advertising added to the legislation in favor of privacy-safe alternatives like contextual ads. Or another amendment that’s recently gained some backing — pushing to exempt news media from platform content takedowns.

Turns out Haugen isn’t a fan of either of those potential amendments. But she spoke up in favor of the regulation as a whole.

The general thrust of the DSA is aimed at achieving a trusted and safe online environment — and a number of MEPs speaking during today’s session spied a soapboxing opportunity to toot the EU’s horn for being so advanced as to have a digital regulation not just on the table but advancing rapidly toward adoption slap-bang in the midst of (yet) another Facebook publicity crisis — with the glare of the global spotlight on Haugen speaking to the European Parliament.

The Facebook whistleblower was happy to massage political egos, telling MEPs that she’s “grateful” the EU is taking platform regulation seriously — and suggesting there’s an opportunity for the bloc to set a “global gold standard” with the DSA.

Although she used a similar line in the U.K. parliament during another evidence session last month, where she talked up domestic online safety legislation in similarly glowing tones.

To MEPs, Haugen repeated her warning to U.K. lawmakers that Facebook is exceptionally adept at “dancing with data” — impressing on them that they too must not pass naive laws that simply require the tech giant to hand over data about what’s happening on its platform. Rather Facebook must be made to explain any data sets it hands over, down to the detail of the queries it uses to pull data and generate oversight audits.

Without such a step in legislation, Haugen warned that shiny new EU digital rules will arrive with a massive loophole baked in for Facebook to dance through by serving up selectively self-serving data — running whatever queries it needs to paint the picture to get the tick in the box.

For regulation to be effective on platforms as untrustworthy as Facebook, she suggested it must be multitiered, dynamic and take continuous input from a broader ecosystem of civil society organizations and external researchers — to stay on top of emergent harms and ensure the law is actually doing the job intended.

It should also take a broad view of oversight, she urged — providing platform data to a wider circle of external experts than merely just the “vetted academics” of the current DSA proposal in order to really deliver the sought-for accountability around AI-fuelled impacts.

“Facebook has shown that they will lie with data,” she told the European Parliament. “I encourage you to put in the DSA; if Facebook gives you data they should have to show you how they got it … It’s really, really important that they should have to disclose the process, the queries, the notebooks they used to pull this data because you can’t trust anything they give you unless you can confirm that.”

Haugen didn’t just sound the alarm; she layered on the flattery, too — telling MEPs that she “strongly believe[s] that Europe has a critical role to play in regulating these platforms because you are a vibrant, linguistically diverse democracy.”

“If you get the DSA right for your linguistically and ethnically diverse, 450 million EU citizens you can create a game-changer for the world — you can force platforms to price in societal risk to their business operations so that the decisions about what products to build and how to build them is not purely based on profit maximization. You can establish systemic rules and standards that address risks while protecting free speech and you can show the world how transparency, oversight and enforcement should work.”

“There’s a deep, deep need to make sure that platforms must disclose what safety systems they have, what languages those safety systems are in and a performance per language — and that’s the kind of thing where you can put in the DSA,” she went on, fleshing out her case for comprehensive disclosure requirements. “You can say: You need to be honest with us on is this actually dangerous for a large fraction of Europeans?”

Such an approach would have benefits that scale beyond Europe, per Haugen — by forcing Facebook “toward language-neutral content-neutral solutions,” which she argued are needed to tackle harms across all the markets and languages where the platform operates.

The skew in how much of Facebook’s (limited) safety budget gets directed toward English-speaking markets — and/or to the handful of markets where it’s afraid of regulation — is one of the core issues amplified by her leaking of so many internal Facebook documents. And she suggested Europe could help tackle this lack of global equity around how powerful platforms operate (and what they choose to prioritize or de-prioritize) by enforcing context-specific transparency around Facebook’s AI models — requiring not just a general measure of performance but specifics per market; per language; per safety system; even per cohort of heavily targeted users.

Forcing Facebook to address safety as a systemic requirement would not only solve problems the platform causes in markets across Europe but it would “speak up for people who live in fragile places in the world that don’t have as much influence,” she argued, adding: “The places in the world that have the most linguistic diversity are often the most fragile places and they need Europe to step in — because you guys have influence and you can really help them.”

While many of Haugen’s talking points were familiar from her earlier testimony sessions and press interviews, during the Q&A a number of EU lawmakers sought to engage her on whether Facebook’s problem with toxic content amplification might be tackled by an outright ban on microtargeted/behavioral advertising — an active debate in the parliament — so that the adtech giant can no longer use people’s information against them to profit through data-driven manipulation.

On this, Haugen demurred — saying she supports people being able to choose ad targeting (or no ad targeting) themselves, rather than regulators deciding.

Instead of an outright ban she suggested that “specific things and ads … really need to be regulated” — pointing to ad rates as one area she would target for regulation. “Given the current system subsidizes hate — it’s 5x to 10x cheaper to run a political ad that’s hateful than a non-hateful ad — I think you need to have flat rates for ads,” she said on that. “But I also think there should be regulation on targeting ads to specific people.

“I don’t know if you’re aware of this but you can target specific ads to an audience of 100 people. And I’m pretty sure that is being misused because I did an analysis on who is hyperexposed to political ads and unsurprisingly the people who are most exposed are in Washington, D.C. and they are radically overexposed — we’re talking thousands of political ads a month. So I do think having mechanisms to target specific people without their knowledge … is unacceptable.”

Haugen also argued for a ban on Facebook being able to use third-party data sources to enrich the profiles it holds on people for ad targeting purposes.

“With regard to profiling and data retention I think you shouldn’t be allowed to take third-party data sources — something Facebook does, they work with credit card companies, other forms — and it makes their ads radically more profitable,” she said, adding: “I think you should have to consent to every time you hook up more data sources. Because I think people would feel really uncomfortable if they knew that Facebook had some of the data they do.”

But on behavioral ad targeting she studiously avoided supporting an outright ban.

It was an interesting wrinkle during the session, given there is momentum on the issue within the EU — including as a result of her own whistleblowing amplifying regional lawmakers’ concerns about Facebook — and Haugen could have helped stoke that (but opted not to).

“With regard to targeted ads, I’m a strong proponent that people should be allowed to make choices with regard to how they are targeted — and I encourage prohibiting dark patterns that force people into opting into those things,” she said during one response (but without going into detail on exactly how regulators could draft a law that’s effective against something as cynically multifaceted as “dark pattern design”).

“Platforms should have to be transparent about how they use that data,” was all she offered, before falling back on reiterating: “I’m a big proponent that they should also have to publish policies like do they give flat ad rates for all political ads because you shouldn’t be subsidizing hate in political ads.”

Her argument against banning behavioral ads seemed to boil down to (or rather hinge on) regulators achieving fully comprehensive platform transparency — that’s able to provide an accurate picture of what Facebook (et al.) actually does with people’s data — i.e., in order that users can then make a genuine choice over whether they want such targeting or not. So it hinges on full-picture accountability.

Yet during another point in the session — after she had been asked whether children can really consent to data processing by platforms like Facebook — Haugen argued it’s doubtful that adults can (currently) understand what Facebook is doing with their data, let alone kids.

“With regard to can children understand what they’re trading away, I think almost certainly we as adults — we don’t know what we’ve traded away,” she told MEPs. “We don’t know what goes in the algorithms, we don’t know how we’re targeted so the idea that children can given informed consent — I don’t think we give informed consent and they have less capability.”

Given that, her faith that such comprehensive transparency is possible — and will paint a universally comprehensible picture of data-driven manipulation that allows all adults to make a truly informed decision to accept manipulative behavior ads (or not) — looks, well, rather tenuous.

If we follow Haugen’s logic, were the suggested cure of radical transparency to fail — including by regulator’s improperly/inaccurately communicating everything that’s been found to users and/or failing to ensure users are appropriately and universally educated regarding their risks and rights — well the risk is, surely, that data-drive exploitation will continue (just now with a free pass baked into legislation).

Her argument here felt like it lacked coherence. As if her opposition to banning behavioral ads — and, therefore, to tackling one core incentive that’s fuelling social media’s manipulative toxicity — was rather more ideological than logical.

(Certainly it looks like quite the leap of faith in governments around the world being able to scramble into place the kind of high functioning, “full-fat” oversight Haugen suggests is needed — even as, simultaneously, she’s spent weeks impressing on lawmakers that platforms can only be understood as highly contex- specific and devilishly data-detailed algorithm machines; Not to mention the sheer scale of the task at hand, even just given Facebook’s “amazing” amounts of data, as she put it in the Q&A today, suggesting that if regulators were handed Facebook data in raw form it would be far too overwhelming for them.)

This is also perhaps exactly the perspective you’d expect from a data scientist, not a rights expert.

(Ditto her quick dismissal of banning behavioral ads is the sort of trigger reaction you’d expect from a platform insider whose expertise comes from having been privy to the black boxes and focused on manipulating algorithms and data versus being outside the machine where the harms flow and are felt.)

At another point during the session Haugen further complicated her advocacy for radical transparency as the sole panacea for social media’s ills — warning against the EU leaving enforcement of such complex matters up to 27 national agencies.

Were the EU to do that she suggested it would doom the DSA to fail. Instead she advised lawmakers to create a central EU bureaucracy to deal with enforcing the highly detailed, layered and dynamic rules she says are needed to wrap Facebook-level platforms — going so far as to suggest that ex-industry algorithm experts like herself might find a “home” there, chipping in to help with their specialist knowledge and “giv[ing] back by contributing to public accountability.”

“The number of formal experts in these things — how the algorithms really work and the consequences of them — there are very, very few in the world. Because you can’t get a master’s degree in it, you can’t get a Ph.D. in it, you have to go work for one of these companies and be trained up internally,” she suggested, adding: “I sincerely worry that if you delegate this functionality to 27 member states you will not be able to get critical mass in any one place.

“It’ll be very, very difficult to get enough experts and distribute them that broadly.”

With so many warnings to lawmakers about the need to nail down devilish details in self-serving data sets and “fragile” AIs, in order to prevent platforms from simply carrying on pulling the wool over everyone’s eyes, it seems instructive that Haugen should be so opposed to regulators actually choosing to set some simple limits — such as no personal data for ads.

She was also asked directly by MEPs on whether regulators should put limits on what platforms can do with data and/or limits on the inputs it can use for algorithms. Again her preference in response to the questions was for transparency — not limits. (Although elsewhere, and as noted above, she did at least call for a ban on Facebook buying third-party data sets to enrich its ad profiling.)

Ultimately, then, the ideology of the algorithm expert may have a few blind spots when it comes to thinking outside the black box for ways to come up with effective regulation for data-driven software machines.

Some hard stops might actually be just what’s needed for democratic societies to wrest back control from data-mining tech giants.

Haugen’s best advocacy may therefore be her highly detailed warnings around the risk of loopholes fatally scuttling digital regulation. She is undoubtedly correct that here the risks are multitudinous.

Earlier in her presentation she raised another possible loophole — pushing lawmakers not to exempt news media content from the DSA (which is another potential amendment MEPs are mulling). “If you’re going to make content neutral rules, then they must really be neutral,” she argued. “Nothing is singled out and nothing is exempted.

“Every modern disinformation campaign will exploit news media channels on digital platforms by gaming the system,” she warned. “If the DSA makes it illegal for platforms to address these issues we risk undermining the effectiveness of the law — indeed we may be worse off than today’s situation.”

During the Q&A, Haugen also faced a couple of questions from MEPs on new challenges that will arise for regulators in light of Facebook’s planned pivot to building the so-called “metaverse.”

On this she told lawmakers she’s “extremely concerned” — warning of the increased data gathering that could flow from the proliferation of metaverse-feeding sensors in homes and offices.

She also raised concerns that Facebook’s focus on building workplace tools might result in a situation in which opting out is not even an option, given that employees typically have little say over business tools — suggesting people may face a dystopic future choice between Facebook’s ad profiling or being able to earn a living.

Facebook’s fresh focus on “the metaverse” illustrates what Haugen dubbed a “meta problem” for Facebook — aka: That its preference is “to move on” rather than stop and fix the problems created by its current technology.

Regulators must throw the levers that force the juggernaut to plot a new, safety-focused course, she urged.",Yes,"The people who are most engaged and active on the platforms are the ones who see the most content, and this content is tailored to their interests. As a result, people are becoming more and more entrenched in their own echo chambers, only seeing content that reinforces their existing beliefs. This can lead to an increase in political polarization and the spread of misinformation",https://techcrunch.com/2021/11/08/facebook-whistleblower-frances-haugen-european-parliament/,2021-11-08 00:00:00,LABEL_1_relevant,0.9963573813438416,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
104,Junk news on social media is shared predominantly by the right wing,"Analyzing the dynamics of online political news sharing shows that there’s an ideological pattern to who’s spreading the bad stuff.

The study: Researchers at the Oxford Internet Institute monitored 13,500 politically active US Twitter users and 48,000 public Facebook pages for the three months ahead of the recent State of the Union address. They then studied how political news that was extremist, sensationalist, conspiratorial, fake, or otherwise characteristic of “junk” got shared.

The results: On Twitter, a core of Trump supporters shared “the widest range of known junk news sources” and circulated “more junk news than all the other groups put together.” On Facebook, extreme hard-right pages (not Republican pages) achieved a similar feat.

Why it matters: Fake news remains a huge headache for social networks and political leaders. A better understanding of how it’s shared could help clean it up.",yes,"The way it has allowed fake news to spread quickly and easily. A study has shown that a core group of Trump supporters on Twitter are responsible for sharing the widest range of known fake news sources. On Facebook, extreme hard-right pages are also responsible for sharing a lot of fake news.",https://www.technologyreview.com/2018/02/06/241313/junk-news-on-social-media-is-shared-predominantly-by-the-right-wing/,2018-02-06 00:00:00,LABEL_1_relevant,0.9931371212005616,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
105,Here’s How Mark Zuckerberg Sees Facebook’s New Era of Privacy,"Mark Zuckerberg once promised Facebook would move fast and break things. Now Zuckerberg says Facebook is trying to fix the things it broke.

Standing on stage before an audience of developers at the annual F8 Conference on Tuesday, Zuckerberg—the same guy who spent years convincing billions of people to share their every thought and action with the world—explained all the ways Facebook is going to help people keep that same information under wraps.

“I believe the future is private,” the CEO said, almost as soon as he began, setting the tone for a day of product announcements across Facebook, Instagram, Messenger, and WhatsApp.

In his much-anticipated keynote address, Zuckerberg readily admitted he’s an odd champion for the cause of privacy, particularly after the year Facebook has had. The social networking giant now faces more than a dozen international investigations into its history of privacy violations, from its years of willy-nilly data sharing to several recent data breaches.

“I know that we don't exactly have the strongest reputation on privacy right now, to put it lightly,” Zuckerberg said through nervous laughter. But he seems to believe a Facebook redesign and a litany of new products focused on messaging and groups could turn that reputation around. “At the end of the day, this isn't just about building some new products,” Zuckerberg said. “It's a major shift in how we run this company.”

To mark this supposedly new era, Zuckerberg unveiled a subtle redesign for Facebook that places more emphasis on Groups. There’s now a Groups tab at the center of the app, content from Groups will appear more often in News Feed, and Facebook will give users more prompts to discover and join new Groups. Facebook is also adding new features for specific types of communities. Groups related to jobs, for instance, will now accept job postings from employers, while health support Groups will enable users to ask administrators to post on their behalf, to protect their privacy.

“Basically, now, everywhere where you're going to be able to see and connect with your friends you're also going to be able to see and connect with groups that you care about,” Zuckerberg said. “It all adds up to this feeling that groups are now at the heart of the experience, just as much as your friends and family are.”

Other announcements were tailor-made to get people out from behind their computer screens and meeting up in person. One such product, called Secret Crush, gives Facebook users a way to indicate which of their Facebook friends they’re romantically interested in and get notified if the feeling is mutual. Facebook has also revamped its Events tab to make it easier to find nearby events.

These announcements are, in many ways, a continuation of a multiyear shift underway at Facebook. For more than two years, the company’s leaders have expressed a desire to reorient Facebook around smaller communities. Zuckerberg first indicated this switch in 2017, when he published a 5,000-world manifesto on making Facebook a platform for enabling real-world connections. It was a departure from the social network’s long-stated vision of connecting everyone in the world to the most possible people. The flaws in that vision became starkly apparent around 2016, when propagandists and bad actors abused so many thin, anonymous connections to manipulate and divide people for political gain.",Yes,It can be used to manipulate and divide people for political gain.,https://www.wired.com/story/f8-zuckerberg-future-is-private/,2019-04-30 20:32:15.570000+00:00,LABEL_1_relevant,0.9758919477462769,Social Media,social_media,Wired,"Information, Discourse & Governance"
106,"Amid Social Distancing, Neighbors Mobilize Over Facebook","A few weeks ago, David Perez’s brother handed him a package of face masks and warnings about a new coronavirus spreading around the United States. Perez had heard about Covid-19, the disease caused by the virus, but he wasn’t sure how serious it was, so he started doing some research. Online, he found startling reports, bizarre conspiracy theories, and a whole lot of questions about what people were supposed to do before the outbreak reached them.

The official line from the Centers for Disease Control is for people to wash their hands, cover their coughs, and avoid large gatherings. “Social distancing,” or the practice of self-isolating, has been understood as key to preventing community spread of the virus. Keeping people out of shared, public spaces could be the key to keeping the spread low enough to meet hospital capacity. That will be critical in the days and weeks ahead.

But that advice isn’t always easy to decipher. Should you still send your kid to school? Cancel a vacation months in advance? And while you’re home alone for the foreseeable future, what else are you supposed to do to make sure your community can survive?

Content This content can also be viewed on the site it originates from.

The responsible amount of social distancing is often context-dependent, which has made it difficult for experts to offer yes-or-no answers. (This guide, from The Atlantic, gets into some of those nuances.) The guidance can differ between people, cities, and specific locations within those cities, making it that much more important for neighborhoods and communities to band together.

Perez lives in San Bernardino, California, which as of Friday still has no reported cases of Covid-19. Even still, the city has preemptively declared a public health emergency and encouraged residents to begin practicing social distancing. Perez wanted to figure out what else he could do to prevent the worst from reaching his community. So he created a Facebook group, California Coronavirus Alerts, for locals to huddle up about what was going on.

California Coronavirus Alerts is just one of many Facebook groups that have sprung up in response to growing concerns about the coronavirus. Some focus on emergency preparedness, while others exist to share news about the virus. Perez’s group specifically coordinates people in the Inland Empire, the region of California where he lives. Some are using it to ask for region-specific advice (is it still OK to go to Disneyland?) while others have shared notices and practices from local colleges, libraries, other spaces.

Drew Harris, who studies population health at the Thomas Jefferson University in Philadelphia, says people should be concerned first and foremost with social distancing—it’s the only way to “flatten the curve” and ensure that hospitals don’t exceed their capacity. But next, it’s worth considering who in a community might be impacted most in a crisis. Harris is specifically concerned with what he calls second- and third-order effects of the virus. If schools get closed, for example, what happens to kids who rely on school lunch for a meal? What happens to the homeless? “The blood centers I’m part of are saying, ‘Let’s cancel all of our blood drives,’” says Harris. “And now we have a blood shortage.”",Yes.,"It can be used to spread misinformation. This can have a number of negative effects, including causing people to make poor decisions about their health, and leading to panic and anxiety. In the case of the coronavirus, it is especially important for people to get accurate information from reliable sources, and to avoid panicking.",https://www.wired.com/story/coronavirus-social-distancing-neighbors-mobilize-facebook/,2020-03-14 12:00:00+00:00,LABEL_1_relevant,0.9962771534919742,Social Media,social_media,Wired,"Information, Discourse & Governance"
107,Can Twitter Fix Its Harassment Problem without Losing Its Soul?,"Clay Shirky, an associate professor at New York University who has written two books about social media, thinks the problem has reached a point where Twitter needs to remind users about what is and isn’t okay, and devise some consequences for misbehavior. Even though it may be labor intensive and therefore expensive to keep and enforce such rules, he says, the Twitter community would benefit from it.

“There’s a bigger threat to not taking on this problem than taking on this problem, simply because public sympathy is going to go more in the direction of the abused than the abusers,” Shirky says.

But while the time seems right for Twitter to act, it is far from clear how best to discourage such behavior. It does have methods in place to help deal with abuse, such as the ability to block and report a user who’s bothering you. Yet while that may help if you’re dealing with one or even a few bothersome tweeters, it cannot stop a deluge of nasty posts, and a determined harasser can always just make a new user profile and start the harassment anew.

How to make Twitter safer without turning off some existing users may be an even trickier question. The freewheeling exchange of views and opinions that characterizes Twitter is part of its appeal; if it were to enforce strict controls over who could talk to whom, or—like Facebook—require people to disclose their real identities, that might cut down on the flow of information.

Improvements may come if Twitter carefully combines increased communication with users and technology for automatically identifying harassers.

Twitter might reduce the burden on its own staff by involving its users more directly. Justin Patchin, a professor of criminal justice at the University of Wisconsin-Eau Claire and co-director of the school’s Cyberbullying Research Center, says Twitter could ask its community to regulate itself—perhaps by allowing users to volunteer to vote on whether or not content is abusive.

Kate Crawford, a visiting professor at the MIT Center for Civic Media and a principal researcher at Microsoft Research, suggests that people experiencing harassment could share lists of users they have blocked with others having the same problem. “There is no quick technical fix for a social problem,” says Crawford. “What there can be is a broadening of the understanding of what the problem is.”

Twitter doesn’t preëmptively spot and wipe out offensive content, yet Jerry Zhu, an associate professor of computer science at the University of Wisconsin-Madison who has studied the use of machine learning to track abusive posts on Twitter, says artificial intelligence can help spot and censor nasty posts by looking out for key words and phrases. This is hard to do with certainty if bullying is not explicit, though; sometimes people are being nasty without using obviously mean language. “This is where the current technique is hitting the AI limit in some ways in that computers cannot be that subtle and parse that meaning out of it accurately,” he says.

While any changes will risk upsetting some users, that may be a price that Twitter is willing to pay in the short term to create a more hospitable service over the long run. Says Robin Kowalski, a psychology professor at Clemson University who studies cyberbullying: “If they lose a few users, they’re going to gain a few more.”",Yes,"Online harassment. Twitter is considering ways to reduce online harassment, but it is difficult to do so without also limiting freedom of expression.",https://www.technologyreview.com/2015/02/12/169381/can-twitter-fix-its-harassment-problem-without-losing-its-soul/,2015-02-12 00:00:00,LABEL_1_relevant,0.9963067770004272,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
108,The Internet Made Dumbledore Gay,"Just imagine the fanfiction now. It’s the kind of thing people write on social media after a tense scene between two heavily “shipped” characters, or when fan-favorite actors get cozy off set. It’s also what Potterverse creator J.K. Rowling said to a shocked Carnegie Hall in 2007, moments after she revealed that she’d “always thought of” Albus Dumbledore, high wizard of Hogwarts, as gay. Slashfic indeed followed.

That day, Rowling’s announcement was mostly met with applause. These days, Rowling’s social-justice-flavored extratextual canonical embellishments—not only about Dumbledore’s sexuality, which she elaborated on over the weekend, but also about Hermione’s race, Harry’s views on Israel, and Nagini’s hidden origins as a cursed Asian woman trapped in the body of a snake kept as a pet by a white supremacist—are mostly met with boos. Over the past decade-plus, Rowling seems to have lost touch with, if not downright alienated, the portion of the internet she hopes to engage with and delight with her cutesy, random-seeming new factoids. Now fans must ask themselves: Do they share some of the blame?

You may not be a fanfiction writer or reader or even a Potterhead, but you almost certainly dive into some fictional world on occasion, and those dives are likely longer and deeper than they used to be. Movie studios and companies like Netflix have become experts in stretching your attention: optimizing a show’s bingeability, engineering viral moments and social media speculation, leaning into easter eggs and online exclusives, buying up superfranchises so vast and rich you can make an unlimited numbers of books, movies, TV shows, videogames, VR experiences, and theme park rides out of them. (It takes over 11 days to watch the entirety of the Marvel Cinematic Universe. You’ve probably done it at least 11 times already.)

For them, it’s a matter of profit. For you, it’s more Pavlov. When you immerse yourself in pleasurable worlds and stories for long periods, you’re practically mainlining dopamine, which increases the likelihood that you’ll not only like the characters you’re watching—you’ll begin to identify with them. That leaves you mildly Netflix- or book-addicted and heavily invested in the people you’ve come to know during your binge. Your totally reasonable response when it all ends: more, longer, and right now, thanks. These are my friends!

A version of this voracity has been happening probably forever. People used to line the docks waiting for the next installment of Charles Dickens stories, screaming questions about the characters to the crew as the boats drew near. Lord of the Rings developed a cultlike following in 1960s counterculture—they scrawled messages like “Frodo Lives!” or “Gandalf for President” on the walls of subway stations, the Twitter of their day. But there’s only so much you can communicate in something as small and impermanent as a graffito, and writers at the time were far removed from fan pressure for more more more. Tolkien didn’t publish his book-length world-building exercise, The Silmarillion, in his lifetime. Had he been on Twitter, he probably would have.

J.K. Rowling is very much on the internet, and she knows its power and potential. When she asked her audience in 2007 to imagine the fanfiction—to imagine the ways fans would react to this extension of the Harry Potter universe with their own versions and elaborations—she wasn’t condemning the practice. She was encouraging it. Fanfiction, and, if my teenage memory serves, especially Harry Potter fanfic circa 2007, has always been a space for queer storytelling. Rowling was adding canonized kerosene to a spark that was already there. She’d keep up the practice for years to come, blurring the boundary between her sacred text and fannish wish fulfillment.

How fans consume stories—and what they’ve come to expect—necessarily changes how they’re created and told. Pissing off the fandom can hinder the success of your story, and fans, through social media, now have a more direct line to their favorite stories’ creators than their Dickensian counterparts. More strategic handlers of stories with large and vocal fandoms have taken Rowling’s tactic—if you can’t beat them, feed them—and carefully, calculatingly run with it. At this point, Rowling herself seems to be running with scissors, ready to slice up your childhood. The fact is, her “revelations” have come to seem as remote and unnatural as bad fanfic, and many fans have begun to view them as such.",Yes,"It has allowed creators to directly engage with their fan bases to a degree that was previously impossible. This has led to a situation where creators are under immense pressure to please their fans, which can result in bad story-telling as creators make choices based on what they think fans will want to see, rather than what makes for a good story. In",https://www.wired.com/story/the-internet-made-dumbledore-gay/,2019-03-19 21:56:08.325000+00:00,LABEL_1_relevant,0.9959371089935304,Social Media,social_media,Wired,"Information, Discourse & Governance"
109,"YouTube geoblocks Russia Today, Sputnik channels in Europe","Google has just announced that YouTube will geoblock the Kremlin-affiliated media outlets Russia Today (RT) and Sputnik in Europe, following pressure from regional lawmakers to choke off Russian war propaganda.

In a tweet announcing the geoblocks today, Google’s Europe policy team writes: “Due to the ongoing war in Ukraine, we’re blocking YouTube channels connected to RT and Sputnik across Europe, effective immediately.”

We’ve reached out to the Commission for a response to YouTube’s announcement.

As TechCrunch reported earlier, the EU confirmed last night that the ban on RT, Sputink and their subsidiaries will cover all distribution channels, including online platforms.

Due to the ongoing war in Ukraine, we’re blocking YouTube channels connected to RT and Sputnik across Europe, effective immediately. It’ll take time for our systems to fully ramp up. Our teams continue to monitor the situation around the clock to take swift action. — Google Europe (@googleeurope) March 1, 2022

On Monday the EU’s internal market commissioner, Thierry Breton, held a video call with the CEOs of Google and YouTube to press them to step up their efforts.

YouTube hosts thousands of videos by the two Russian state-affiliated media firms and — as we noted yesterday — RT’s own channel marketing claims it’s “the most watched news network on YouTube”.

Now Europeans who try to browse to RT’s YouTube channel are met with a message that “this channel is not available in your country”:

Yesterday Facebook, Microsoft, TikTok and Twitter announced similar restrictions.

But Google took a little longer to consider its response. The tech giant has not offered an explanation for the delay.

In its tweet announcing the geoblocking this morning, Google also warned: “It’ll take time for our systems to fully ramp up. Our teams continue to monitor the situation around the clock to take swift action.”

So the company appears to be signalling that it expects some content from Kremlin-linked channels to continue to be accessible in the near term — as its systems “ramp up”.

With YouTube only geoblocking RT and Sputnik, rather than banning or suspending the accounts, Russia’s propaganda can of course continue to spread outside Europe — including still being available inside Russia itself.

EU #sanctions stopping Russian propaganda. But in the rest of the world #Google won’t stop making money with Russian lies about Ukrainans fighting for their lives and their freedom. Shame on you. #StandingWithUkraine #UkraineKrieg #Zelenskiy #DSA https://t.co/2bEnwVD5zu — Alexandra Geese (@AlexandraGeese) March 1, 2022

Google is not alone in opting for this compromise, though.

Facebook’s parent Meta and TikTok also opted to geoblock Russian-state affiliated media accounts, rather than suspending or banning their accounts entirely.

Twitter and Microsoft have also taken a slightly different tack, reflecting the difference nuances of their platforms — saying they would take measures to reduce visibility of state-sponsored RT and Sputnik content, so essentially limiting their freedom of reach.

Twitter also expanded its labelling policy yesterday — adding notifications to tweets that link to Russian state-affiliated media and warning users to “stay informed”.

Given the latter is far more an information network than a social (or broadcast) network, labelling and contextualizing seems an appropriate response — although it remains to be seen whether the EU’s incoming ban on RT and Sputnik will require web platforms to go further.",Yes,"The spread of false information. This can be in the form of fake news, conspiracy theories, and other forms of misinformation. Social media platforms have been criticized for not doing enough to prevent the spread of false information. In response, some platforms have implemented fact-checking measures, while others have banned or limited the reach of state-affiliated media outlets.",https://techcrunch.com/2022/03/01/youtube-geoblocks-rt/,2022-03-01 00:00:00,LABEL_1_relevant,0.9954013824462892,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
110,It's Time to Defund Social Media,"Political speech has always been tethered to public health. The mass protests that erupted following George Floyd’s murder foreground this overlap: White supremacy is a public health disaster. Climate denialism and anti-vaxx activism similarly threaten the lives and safety of citizens around the globe. With Covid-19, the line between political speech and public health has eroded in even more distressing ways. Objects of science have, for many, been reduced to matters of opinion—or even outright conspiracy. In those cases, masks aren’t just masks; they’re symbols of oppression. Anthony Fauci isn’t just the country’s most prominent infectious disease expert; he’s part of a Deep State cabal seeking to undermine the Trump administration from within. The virus itself isn’t ripping through our neighborhoods, forcing cities like Houston to use backup morgue space; it’s a hoax, or something the Democrats are exploiting to goose their election odds. The inability to cordon off the basic facts of public health from reactionary propaganda threatens people’s lives. And, like everything in this godforsaken pandemic, things are on track to get much worse.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

While it might be tempting to blame the people who refuse to wear masks, or the politicians who would rather discuss individual rights than public health, or the president who would rather tweet about beans, our present crisis runs much deeper than any single individual or group. The structures and assumptions we take for granted are themselves part of the problem. Calls to defund the police provide a helpful analogue. Until we fundamentally reimagine what law enforcement means and does, lasting change will not be possible; all the injustice baked into the system will continue emerging from that system. Something similar could be said of social media: Until we fundamentally reimagine our information ecosystem and our respective roles within it, we’ll keep repeating the same patterns over and over—not as a bug of the system, not as a feature of the system, but as the system itself.

One force we must confront is the attention economy, an incentive structure designed to reward the most uncompromising, polarized, clickable minority. (Ironically, this minority is very often part of the white majority; see breathless, disproportionate coverage of white nationalists and supremacists following the 2016 election.) The resulting tyranny of the loudest presents an algorithmically-warped view of what’s happening in the rest of the United States.

The debate around masks is a textbook example. The articles that trend and videos that go viral don’t feature the majority of Americans who are perfectly fine wearing masks; who do so without once throwing a temper tantrum in Costco. Without question, there are people who cannot stop throwing temper tantrums in Costco; it’s become its own genre of performance art. It’s also true that many Republican politicians have flouted CDC guidance on masks or refused to issue mask mandates; Georgia governor Brian Kemp is even suing the city of Atlanta to prevent enforcement of a mandate. All of this is unquestionably dangerous; in a global pandemic, it doesn’t take a very high percentage of the population to put everyone else at risk—with particularly dire consequences for Black and Latino communities, making mask recalcitrance (indeed, any Covid-related recalcitrance) as much a threat to civil rights as to public health. Still, it just isn’t the case that, across the country, everyone is screaming at each other about masks. Most people shut up and wear them.

If we could emphasize the common-sense consensus on masks, it wouldn’t just affirm our faith in humanity. (Though it certainly would do that: I’d spend hours on a YouTube channel dedicated to groups of people being reasonable.) It would also undercut the attention economy, in two ways.

First, it would minimize the incentive to be an asshole. If you’re not rewarding people with clicks and likes for antagonistic behaviors, there’s less reason for them to keep doing it. This is a dynamic as old as trolldom. As long as something generates capital—whether economic or social—there’s no reason to stop. In fact, one’s livelihood might depend on keeping it up, and doing it even worse the next time.",Yes,"It has become a tool for authoritarian leaders to get people to believe lies. 

The second way that emphasizing the common-sense consensus on masks would undercut the attention economy is by affirming a shared reality—one that isn’t subject to the whims of a 24-hour news cycle or the algorithm’s insatiable",https://www.wired.com/story/its-time-to-defund-social-media/,2020-07-21 12:00:00+00:00,LABEL_1_relevant,0.9939274787902832,Social Media,social_media,Wired,"Information, Discourse & Governance"
111,Twitch’s DMCA Takedowns Threaten to Drive Musicians Away,"As a fan of underground electronic music, the music producer, Twitch streamer, and label owner Chris Reed, who goes by the stage name Plastician, was used to going the extra mile to catch a show.

“I literally used to have to climb through tower block building windows and hide from the police.” Today, he’s a DJ and runs a music label called Terrorhythm. He goes on, “This is like nothing compared to the shit we've had to put up with before. A lot of us have been through shit like this in the past.”

The shit in question is his and other DJs’ ongoing struggle with the DMCA. While Plastician no longer finds himself climbing through windows or ducking the police, he navigates a more abstract, virtual obstacle course: Twitch.

As Twitch grew, so did the diversity of content. Initially a site dominated by video game streamers, in 2015, the website created a new category of content called Creative and IRL. Then, by 2018, the company announced that this category had “exploded in growth,” and as a result, it created a new category for music.

For Dave Eckblad and many other DJs, Twitch became a way to maintain their local music scene during the coronavirus pandemic. Before the pandemic, he worked in music promotion and co-owned a record store. Now, he produces the Twitch stream for the Minneapolis-based music collective Intellephunk.

""After lockdown we had nowhere to do what we do.” Eckblad says He explains that one of the members of his collective “had to pivot and figure out some new stuff to do. And I was able to help him get that going quick. I had already been doing stuff on Twitch, on and off, so we worked to put a show together.” It wasn’t as much money as a live event, but the stream allowed Eckblad and Intellephunk to continue to engage their community and make some money off of tips from viewers watching the stream.

Intellephunk wasn’t the only music group to take its music to Twitch. At the outset of the pandemic, musicians flooded the platform with content to make up for canceled live events. Entire concerts and music festivals went completely online to Twitch and streaming platforms like it. A report from StreamElements noted that the number of hours users spent watching music and performing arts rose from 3.6 million to 17.6 million hours over the course of one year.

But that rosiness and excitement soon started to dissolve.

Last October, Twitch sent a letter to streamers regarding Digital Millennium Copyright Act (DMCA) violations. Any channel that had video content with copyrighted music must take it down or risk having their channel deleted. The public backlash was so strong that Twitch issued an apology within 24 hours. The company stated that the frustration of streamers was “completely justified,” and that it was just as surprised as content creators were by the volume of DMCA requests it had received. But Twitch maintained its warning: “If you play recorded music on your stream, you need to stop.”

The Threat of the DMCA

DJs already know the threat of copyright abuse. Twitch’s DMCA takedowns represent another setback in a long history of push and pull between creators and copyright enforcement. What is new, however, is how these stories illustrate the larger issue of uneven and confusing copyright enforcement on the internet. As Twitch doubles down, DJs cast doubt on the platform’s future with music.

Sarvesh Ramprakash, also known as Icarus Redux, started in Los Angeles’ electronic music scene in 2014. Today he spins at shows across the midwest and organizes music communities for artists of color. He told me he hasn’t performed recently on Twitch, specifically because of the DMCA issue.",Yes,"It's not easy to control what people share or where they share it. This makes it difficult to protect the intellectual property of content creators, like musicians.

The 1998 Digital Millennium Copyright Act was designed to help content creators protect their work online. The DMCA made it illegal to circumvent digital rights management (DRM) technology, and it established",https://www.wired.com/story/twitch-coronavirus-musicians-dmca-takedowns/,2021-02-24 12:00:00+00:00,LABEL_1_relevant,0.9964696168899536,Social Media,social_media,Wired,"Information, Discourse & Governance"
112,Making Money with Social Media,"The goal, says Social Agency cofounder Scott McCaskill, is to let companies see “whether all the time put into doing those things is really helping build brand or product awareness, which kinds of content are most successful, what days and even times of day result in the most traffic or new followers/friends.”

A free version allows a company to manage a single identity or “voice” across each platform. Paid versions let companies coordinate multiple users and voices, and provide a longer data history. McCaskill says the software has had the most success with units of large companies and marketing agencies.

Media metrics: With Spredfast, companies can evaluate how people read, pass along, or comment on content on social media websites over time.

Spredfast gives companies a way to plan and manage content deployment. For instance, users can write blog entries, tweets, or Facebook updates ahead of time and then schedule when they will be posted. A store that might offer an online coupon code or one-day sale could, with Spredfast, have Twitter push that code out several times a day to increase the number of site visitors. The software’s metrics, McCaskill says, let marketers figure out the best times to post updates. Spredfast also makes it easy for them to test different strategies.

The company launched a year ago as a maker of custom Facebook applications. When Facebook redesigned its home page, says McCaskill, Social Agency’s business model was effectively torpedoed. As part of its sales strategy, the company had spent a lot of time helping clients plan their social media strategies. So the founders retooled and used their expertise to start building Spredfast about nine months ago. The software launched in private beta in September, public beta in October, and had its “official” launch on December 2.

Social Agency plans to introduce a feature by the end of January that will help users design a social media campaign based on their objectives. McCaskill says that Spredfast will most likely present users with a list of common marketing goals that they can check off. The software will suggest a template for a campaign based on what’s worked best for clients with similar goals.",Yes,It can present a negative image of the company to potential consumers.,https://www.technologyreview.com/2009/12/29/206864/making-money-with-social-media/,2009-12-29 00:00:00,LABEL_1_relevant,0.9944847226142884,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
113,These Companies Are Already Living in Zuckerberg’s Metaverse,"Facebook’s metaverse, or Meta’s metaverse, isn’t just being touted as a better version of the internet—it’s being hailed as a better version of reality. We will, apparently, “socialize, learn, collaborate, and play” in an interconnected 3D virtual space that Facebook cofounder and CEO Mark Zuckerberg describes as an “embodied internet.” This space, Zuckerberg claims, won’t be created by one single company, but rather by a network of creators and developers. First problem: 91 percent of software developers are male. Second problem: You’ve been living in a version of metaverse for years—and, having taken over video games, it’s now coming for the world of work.

Companies big and small have been testing avatar-based platforms for remote and hybrid working since Covid-19 lockdowns began. Using Oculus VR headsets, Facebook’s Horizon Workrooms envisages a near future in which people meet virtually in a soulless, floaty virtual world. Microsoft’s Mesh for Hololens 2 hopes to facilitate similarly corporate mixed reality meetups, and Canadian ecommerce platform Shopify just launched its browser-based game Shopify Party, in which employees appear as their chosen avatars to spice up one-to-ones, icebreakers, standups, and other team events.

Many have already pointed out how boring the Zuckerverse looks. Most of us have already been living in that future, be it through the organized fun of workplace social apps or through video games like Fortnite. And while the video game metaverse offers plenty of room for imagination and connection, the corporate metaverse risks repeating and potentially magnifying the flaws of the real world.

Whether a company adopts certain aspects of the corporate metaverse, or uses it for every aspect of remote work, there’s nothing to stop unconscious biases from seeping in. “It’s easy for companies to just invest in the technology, but businesses need to understand the psychology driving people to use it,” says Roshni Raveendhran, a professor at the University of Virginia’s Darden School of Business. “Can employees thrive within that sort of universe? What would allow them to thrive?”

Unfettered by the risks of mass metaverse adoption—or, less excitingly, lots of companies using remote collaboration tools—a gaggle of startups are piling in to sell the future. The most well-known virtual workspace tool is Gather.town, which amassed 4 million users in just over a year as the pandemic took hold. Its retro, pixelated design is intentionally basic, while Roblox’s Loom.ai and Teeoh use sophisticated graphics for more realistic virtual worlds. The preeminent simulation platform Second Life was adopted by Cisco and IBM over a decade ago. Despite virtual and augmented reality companies consistently promising the world and failing to deliver, a 2020 report from consultancy PwC predicts that nearly 23.5 million jobs worldwide will use AR and VR by 2030 for tasks such as employee training, meetings, and customer service.

For businesses, the most interesting benefit of avatars, a video game staple pioneered by NASA employees in the 1970s, is the sense of digital proximity, without needing to focus on facial expressions—the cause of the much-maligned Zoom fatigue. And while self-expression is the allure of video game avatars, it’s not yet clear what employees gain from being asked to exist in the corporate metaverse.",Yes,"It can lead to internet addiction. This is especially true for young people, who are the most active users of social media. Internet addiction can lead to a number of problems, including sleep deprivation, anxiety, and depression.",https://www.wired.com/story/these-companies-already-live-in-zuckerberg-metaverse/,2021-11-02 11:00:00+00:00,LABEL_1_relevant,0.9879339933395386,Social Media,social_media,Wired,Health & Wellbeing
114,Netflix v Modi and the battle for Indian cinema’s soul,"In 2018, Netflix hired Arya to commission feature-length content. That year, the company made more than 20 original films and five original series in Hindi. But this did little to alter its public persona. In a country with more than 24 major languages, Netflix was still viewed as an English-language platform for westernized Indians. And this is where Arya, who knew everyone who mattered in Hindi film, fit into the picture. She had worked in advertising, and then as an actor and a writer, before moving on to TV production.

Soon she enlisted many of her childhood friends, who had grown up to become some of the most powerful people in the Hindi film industry, to work for Netflix. She signed on Zoya Akhtar, whose last feature film was India’s official entry to the Academy Awards, to direct a short film. Like Arya, Akhtar comes from a film family, but because Bollywood is a male-dominated industry, it’s still almost impossible for a female filmmaker or female-oriented films to raise capital. By contrast, several women helmed projects at Netflix. The platform’s biggest star is Radhika Apte, a Bollywood actress who has appeared in so many Netflix productions that online wags joke she’s in all of them.

Srishti Behl Arya, who runs Netflix's division of Indian original films. NETFLIX

But working with Bollywood meant dealing with its shortcomings. Netflix held several workshops in Mumbai to train Indian content creators. It taught them how to develop a major series, but also helped them brush up on basics such as how to write, schedule, and budget. “That’s how we can add value to the industry,” Arya told me. “By helping it get more organized.”

On my last day in Mumbai, I went to visit Red Chillies Entertainment, a towering production house owned by Shah Rukh Khan, which produces shows for Netflix. Back in 2017, Hastings and Khan had appeared together in a stilted promotional skit announcing a new spy thriller called Bard of Blood.

The foyer was deserted on the day I arrived, except for a beautiful sculpture of Ganesha, a Hindu god who is viewed as the patron of the arts. It was wrapped in plastic to protect it from construction dust. Around it some barefoot workmen were operating power tools without any protective gear. On the fourth floor, an exhausted-looking man with slippers on his feet and salt in his dark hair emerged from an editing studio. Several years ago, newly graduated from the London School of Film, Patrick Graham had been struggling to land projects when a friend suggested he try Bollywood. He floundered at first, stifled by censorship. But then, in 2018, Netflix India gave Graham the budget to produce a fictional series in which Muslims are rounded up in internment camps. They also brought him in to co-write the screenplay for Leila. When we met, he was wrapping up production on Betaal, a four-episode zombie series that would be released the next year. Months earlier, in a conversation on the phone, Graham had seemed pumped at the opportunity. “It’s massive,” he’d said. But in person, in Mumbai, he was downcast. “I have to go through the series and remove anything that might offend,” he told me, gloomily. “The oversensitive people are winning.”

In November 2020, Hindu nationalists went after Netflix again. Mira Nair’s critically acclaimed adaptation of Vikram Seth’s novel A Suitable Boy showed a Muslim boy kissing a Hindu girl. A leader of the BJP’s youth wing filed a police complaint about the series for “shooting kissing scenes under temple premises.” The leader accused the show of promoting “love jihad”—a conspiracy theory that claims Muslim men are seducing Hindu women in order to convert them to Islam.",Yes,People are now more tribal and echo chamber effects are more pronounced.,https://www.technologyreview.com/2021/03/24/1021139/netflix-india-modi-kashyap-sacred-games-censorship/,2021-03-24 00:00:00,LABEL_1_relevant,0.9838780164718628,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
115,The Life and Viral Fame of Virginia's Two-Headed Snake,"Late last summer in Woodbridge, Virginia, a woman wandered into her yard and found an eastern copperhead slithering through her flower bed. That's not so unusual where she lives, as the region is home to a plethora of ophidians, from harmless corn snakes to venomous rattlers. But this one was different: It had two heads.

It's called dicephaly, a mysterious disorder occurring in just one out of every 100,000 snakes born in the wild and one out of 10,000 born in captivity. Affected snakes possess two brains with distinct personalities, though one head typically dominates the other, which might lack a trachea, esophagus, or even eyes. Scientists suspect it happens when an embryo in the early stages of development divides—possibly induced by sudden temperature changes, environmental pollution, or inbreeding. Whatever the cause, these unlucky creatures don't live long. Nearly half are dead on arrival, and few survive beyond the first few months.

The Woodbridge serpent—er, serpents?—were at most three weeks old, no longer than a Penguin paperback, but it (they?) caused a stir. Naturally, pictures made their way onto Facebook, then inevitably CNN, The New York Daily News, and even Snapchat. Calls began flooding the Virginia Department of Game and Inland Fisheries—which removed the critter from the woman's property—from people curious to see it up close and zoos eager to take it off their hands.

""After about 48 hours of that madness, I was like, I'm done,"" says state herpetologist John D. Kleopfer. ""I don't know how these celebrities, like the Kardashians, live.""

Kloepher enlisted the help of Cooper Sallade, a respected viper breeder in Richmond, who agreed to raise the bifurcated snake on a strictly confidential basis. Sallade, 27, has been handling ophidians since childhood, when he'd catch them outside and keep them in jars under his bed. Now he has a nondescript warehouse full of them. He keeps up to 300 at any given time locked away in temperature-regulated containers on PVC rack systems that appear, at a glance, like filing cabinets.

""Reptiles are actually not my favorite animals,"" Sallade says. ""I prefer birds and large mammals, but you can't keep a whole house full of bears.""

'After about 48 hours of that madness, I was like, I'm done. I don't know how these celebrities, like the Kardashians, live.' Herpetologist John D. Kleopfer

At the Game and Fisheries' office outside Richmond, Sallade coaxed the copperhead into a Rubbermaid food container with air holes drilled into the sides, placed that into a wooden box labeled ""venomous snake,"" screwed it shut, then drove it to his facility. For the next two and a half months, he quarantined it in a special room with a separate HVAC system to stop the spread of any possible pathogens to his other reptiles. The snake didn't move much or eat. So once a week, Sallade force fed it a euthanized baby mouse, gently pinning down both heads with a foam hobby brush while using tweezers to slowly massage the rodent into the less developed head, which happened to have the most developed gullet and windpipe. Soon it was regularly defecating and shedding its skin. Sallade felt hopeful.

""Since the snake had such an incomprehensible amount of media attention, there was a lot of pressure on me to keep that thing alive,"" he says.

But alas, one December morning, Sallade went in to check on it and found the snake had died. He was sad, but not exactly surprised. ""If it had been a snake that was born in my collection, I wouldn't have told anybody about it,"" he says. ""Honestly, I would probably have euthanized it myself, because it was so hard for the snake, just being alive.""",Yes.,It can lead to a lot of pressure and stress on people who would otherwise not have to deal with it.,https://www.wired.com/story/two-headed-snake/,2019-06-13 21:53:03.510000+00:00,LABEL_1_relevant,0.9962748289108276,Social Media,social_media,Wired,"Information, Discourse & Governance"
116,The Battle for Union Representation on YouTube,"Without video creators, YouTube wouldn’t be one of the world’s biggest social platforms. Without the platform, YouTubers wouldn’t be, well, YouTubers. But video creators are regularly facing new policy changes from YouTube that could impact their ability to make money from their work—and it’s not always clear what these changes are or why YouTube is making them.

Now, as part of a push for fair treatment, YouTubers are looking to collective action. The effort is being led, in part, by an unlikely character: a creator in Germany who makes high-powered slingshots for his audience of 2.3 million subscribers. This week on the Gadget Lab podcast, we talk with WIRED staff writer Emma Grey Ellis about what YouTubers hope to get out of their unionization efforts and what the movement means for the video giant.

Content This content can also be viewed on the site it originates from.

Also in the news: Mark Zuckerberg gets grilled by the House Financial Services Committee about Libra, Facebook’s proposed cryptocurrency … except the hearing was about much more than just Libra.

Show Notes:

Read Emma’s story about the YouTubers union here. And here’s Steven Levy’s story about the Libra hearing in Washington. Read Lauren's review of the Samsung Galaxy Fold here.

Recommendations:

Emma recommends the science and comedy podcast Ologies With Alie Ward. Mike recommends the book I Like to Watch by Emily Nussbaum. Lauren recommends the book Catch and Kill by Ronan Farrow. Arielle recommends Google’s experimental Digital Wellbeing features, like the one that batches your notifications for you.

Emma Grey Ellis is on Twitter @EmmaGreyEllis. Lauren Goode can be found @LaurenGoode. Arielle Pardes is @pardesoteric. Michael Calore is @snackfight. Boone Ashworth, who edits the show, can be found at @BooneAshworth. Bling the main hotline at @GadgetLab. Our theme song is by Solar Keys.",Yes,It can lead to echo chambers and filter bubbles.,https://www.wired.com/story/gadget-lab-podcast-428/,2019-10-25 12:00:00+00:00,LABEL_1_relevant,0.7018484473228455,Social Media,social_media,Wired,"Information, Discourse & Governance"
117,TikTok’s Black Box Obscures Its Role in Russia’s War,"Ten days into Russia’s invasion of Ukraine, TikTok announced it had suspended new posts from Russian accounts due to the country’s new “fake news” law. But the company was quieter about a second policy shift—one that blocked TikTok users in Russia from seeing any content posted by accounts located outside the country.

Findings by social media research collective Tracking Exposed suggest that TikTok enfolded its Russian users in a vast echo chamber intended to pacify president Vladimir Putin’s government. Inside that digital enclave, a network of Russian accounts posting pro-invasion content somehow kept operating. “There was clear manipulation of the information ecosystem on TikTok,” says Salvatore Romano, head of research at Tracking Exposed.

TikTok spokesperson Jamie Favazza declined to comment on Tracking Exposed’s findings and repeated a previous statement that the company had blocked new uploads from Russia. But the platform, owned by Chinese startup ByteDance, has been less critical of Russia than US rivals and has been treated less harshly by Russia’s government. TikTok complied with EU sanctions forcing platforms to block access to Russian state-backed media from Europe. Meta, Google, and Twitter have also adjusted their algorithms to make content or links to those outlets less visible. In apparent retaliation, Facebook and Twitter were both blocked by Russian internet censors. On March 21, a Moscow court banned Facebook and Instagram from Russia, accusing parent company Meta of “extremist activities.”

TikTok’s actions in Russia and its central role in spreading video and rumor from the war in Ukraine add urgency to open questions about how truth and mistruth circulate on the platform, Romano and other researchers say. TikTok’s geopolitical moment also highlights the challenges faced by researchers trying to answer such questions. The app, launched in 2017, surpassed 1 billion monthly users in September 2021, but it is less well studied, and more difficult to study, than its older rivals.

Most work on the dynamics and downsides of social media has focused on Facebook and Twitter. Tools and techniques developed for those platforms have shone revealing light on the spread of misinformation about Covid-19 and uncovered online manipulation campaigns linked to governments, including Russia, China, and Mexico. Meta and Twitter provide APIs to help researchers see what is circulating on their platforms.

TikTok does not provide a research API, making it hard to answer questions about its role in spreading accurate or inaccurate information around the Ukraine war or other topics. And while researchers might like to see Meta and Twitter provide broader data access, these platforms at least offer something, says Shelby Grossman, a researcher who has been monitoring pro-Russian posts about Ukraine at Stanford’s Internet Observatory. “It’s tough to look systematically at what’s happening on TikTok,” she says. Researchers have also scrambled to monitor content about Ukraine on messaging app Telegram, which also lacks a researcher API and is much less studied than US networks.

TikTok spokesperson Favazza says that although it does not currently provide a research API, “we strongly support independent research,” citing a program that briefs lawmakers and experts in online harms on its moderation and recommendation systems. TikTok has previously claimed the war in Ukraine prompted it to increase moderation and speed up a pilot project labeling state-controlled media accounts but did not specify exactly how its operations have changed. On March 24, two TikTok moderators filed a lawsuit against the company alleging psychological harm from “exposure to highly toxic and extremely disturbing images.”

One of the biggest challenges to outside researchers interested in what circulates on TikTok stems from the power and influence of its recommendation algorithm, which plays an outsize role compared to older social networks. The app and its rapid growth are built on the For You page, which shows an endless feed of videos curated by TikTok’s algorithm and drawn largely from accounts a user does not follow. As a result, different people see wildly different videos, with the feed based on past viewing and other signals.",Yes,"It can be used to spread misinformation and manipulate public opinion. This is especially true for TikTok, which has been used to spread pro-Russian propaganda about the war in Ukraine. While TikTok has taken some steps to address this problem, it is difficult to study the platform due to a lack of data access.",https://www.wired.com/story/tiktok-algorithm-russia-war/,2022-03-28 11:00:00+00:00,LABEL_1_relevant,0.9953193068504332,Social Media,social_media,Wired,"Information, Discourse & Governance"
118,Think FaceApp Is Scary? Wait Till You Hear About Facebook,"FaceApp is a viral lark that takes a convincing guess at what you’ll look like when you’re old. FaceApp is also the product of a Russian company that sends photos from your device to its servers, retains rights to use them in perpetuity, and performs artificial intelligence black magic on them. And so the FaceApp backlash has kicked into gear, with anxious stories and tweets warning you off of its charms. Which, fine! Just make sure you save some of that ire for bigger targets.

The response to FaceApp is predictable, if only because this cycle has happened before. FaceApp went viral when it launched in 2017, and prompted a similar—if far more muted—privacy kerfuffle. But compared to Meitu, that year’s other viral face manipulator, which is quite a phrase to type, FaceApp was downright saintly in its data collection. At least FaceApp didn't access your GPS and SIM card information. More energy was directed at bigger problems, like FaceApp’s blackface filter. (Yep!)

""This is definitely not a unique FaceApp problem. FaceApp is part of a larger privacy problem."" Christine Bannan, EPIC

The latest frenzy appears to have been kicked off by a since-deleted tweet that claimed FaceApp uploads all of your photos to the cloud. That certainly would be alarming. But FaceApp has denied the claim, and multiple security researchers have confirmed that it’s not so. FaceApp takes only the photo you ask it to manipulate. The company also says it deletes “most images” from its servers within 48 hours of uploading, although admittedly there’s no way to confirm that it does so in practice. If you want FaceApp to remove all of your data from its servers, you can send a request within the app, by going to Settings > Support > Report a bug and putting “Privacy” in the subject line. “Our support team is currently overloaded, but these requests have our priority,” FaceApp founder Yaroslav Goncharov said in a statement. “We are working on the better UI for that.”

Those measures don’t make FaceApp some paragon of data privacy. While the way it manages photos is kosher under Apple rules, FaceApp doesn't make it clear enough to users that it's sending them to a server. “I cannot think of any situation where an app should not be very painfully clear about a photo being uploaded to a remote server,” says Will Strafach, security researcher and developer of Guardian, an iOS firewall app. “Users always have the right to know this.”

Still, it’s important to note that while FaceApp calls St. Petersburg home, its servers are based in the US. The company said in a statement that ""the user data is not transferred to Russia."" Like almost everyone else, FaceApp uses Amazon’s cloud. And it has at least a plausible reason for doing so: The processing power required to apply a Methuselahn filter on your face is more manageable there than on your device. More recent iPhones and Android devices have machine learning capabilities baked into their hardware, but it’s safe to assume that plenty of FaceApp’s reported 80 million users are on older models.

So what’s changed since 2017? On the FaceApp side, not much. But the world around it looks markedly different. Russia has become synonymous with nefarious online meddling, to the point that any company—even a silly filter app—becomes a bogeyman. Awareness of facial recognition’s perils has reached something close to critical mass. And the idea that one’s personal data might be worth protecting has gained real, immutable traction.",Yes,"People who are not well-informed share information that is false, which causes problems because people believe the information to be true.",https://www.wired.com/story/faceapp-privacy-backlash-facebook/,2019-07-17 19:08:04.979000+00:00,LABEL_1_relevant,0.9956989288330078,Social Media,social_media,Wired,"Information, Discourse & Governance"
119,Facebook and Twitter Finally Respond to Trump,"And so it didn't really go over that well among the crowd who had been calling on him to say something. Because even as he was saying go home, he was repeating the same theory that underlay all the chaos in the first place. And so not long after that, I confess I don't remember the precise sequence, but it didn't take long for Facebook and YouTube to pull that video down and for Twitter, at first, to basically freeze it so you couldn't retweet, reply to, or like it, and then eventually Twitter took it down as well.

MC: Right, and in that period of time when Twitter froze it, you could still quote-tweet it, right?

GE: You could still quote-tweet it. You could still view it. So if you went to Trump's Twitter feed, you could view it, and it had like 12 million views. But this was sort of a bridge moment in Twitter's evolving stance toward content that it deems violates its rules but comes from public figures. Twitter has been really loathe to take stuff actually down, because it makes the argument that there is news value in knowing what a world figure or a national political figure like Trump has said. And so what they've tried to do are things to slow the virality while allowing people to see it. So they were sort of in a way station on that, but they quickly ratcheted up to “No, we're pulling this down.”

LG: What rationale did they give for taking the video down?

GE: So, at first, it was a little bit unclear. We were just told that the videos violated policies. And then Facebook said they had judged that it was more likely to encourage further violence than tamp it down. So, basically, they looked at this video in which Trump says, ""Go home, be peaceful,"" but also says, ""We were robbed, the election was stolen,"" and decided that, on balance, it was likely to provoke more violence. Now, personally, as this was unfolding and I'm kind of watching it happen in real time, I thought that was kind of weird. Because if you watch the video as I did, and if you read it—I've got the whole text of it in my story that's up on WIRED now if people want to read it—the “election was stolen” stuff, that's not new. Right? Trump's supporters already buy into that, but he did tell them several times in that little video, ""Go home.""

And so it seemed a little bit like a stretch to me to treat that as inciting violence insofar as the bad part was the stuff that Trump has always been doing. And so it kind of raised the question, well, wait a minute, if you're going to ban Trump from saying the election was stolen in this video, don't you have to ban all the other times when he says it? And then that's exactly what happened. So, the die was kind of cast there, because shortly after they pulled the video down, they start pulling other stuff down. Trump was tweeting things that were kind of in the same spirit as the video where it was like, ""Hey, be peaceful, don't be violent. The election was stolen from us."" And so pretty quickly, Twitter and Facebook start taking those posts down. And then you start wondering, well, I mean, we kind of know what all his future ones are going to be, and then, lo and behold, they just decided to freeze his ability to post.",Yes,"It has allowed for the easy and quick spread of information and ideas, which has led to a number of problems. These problems include the spread of false information, the amplification of hateful or violent speech, and the creation of echo chambers.",https://www.wired.com/story/gadget-lab-podcast-486/,2021-01-08 13:00:00+00:00,LABEL_1_relevant,0.9946081638336182,Social Media,social_media,Wired,"Information, Discourse & Governance"
120,Data on Content Moderation Won't Silence Facebook’s Critics,"Faced with escalating accusations of bias from the right and an onslaught of calls to break up Facebook on the left, the social media juggernaut released a data dump Thursday that its leaders hope will help the public better understand how it moderates content—and remind them that the bigger Facebook is, the more it can invest in fending off these threats.

In its third Content Standards Enforcement Report, Facebook laid out the sheer amount of prohibited content that appears on its platform, including fake accounts, spam, terrorist propaganda, child sexual exploitation, hate speech, bullying, nudity, violence, and the sale of regulated goods. In most, though not all categories, the company explained how prevalent views of that content were, how many pieces of content Facebook took action on, how much of it Facebook found before users reported it, how many enforcement decisions Facebook users appealed, and how much content was restored after that appeal.

Some of the numbers Facebook reported leap off the screen. Over the last six months, for instance, Facebook says it saw a ""steep increase"" in abusive, fake accounts, which accounted for 5 percent of its monthly active users during the last two quarters. Between October 2018 and March 2019 alone, the company removed 3 billion fake accounts, almost all of them within minutes of creation. The report also includes some never-before-shared insights into certain categories of banned content, including child sexual exploitation and terrorist propaganda, which Facebook says accounted for .03 percent of all views in Q1 of 2019. According to the report, Facebook catches more than 99 percent of those posts before a single user reports them.

Issie Lapowsky covers the intersection of tech, politics, and national affairs for WIRED.

Just hours before these numbers came out, a separate report was published by the Data Transparency Advisory Group, an independent group Facebook established last year to audit its content moderation efforts. This report offers an analysis of Facebook’s internal review processes and the metrics tracked with regard to prohibited content, while offering recommendations for how Facebook might improve.

Taken together, the reports serve as Facebook’s answer to several ongoing debates about the company’s size and tremendous power. Whether such an influx of information will ultimately satisfy Facebook’s critics or be used against it remains to be seen.

In a press call introducing the reports, CEO and founder Mark Zuckerberg argued that Facebook's size is precisely what makes the company a responsible watchdog for the internet. ""The amount of our budget that goes toward our safety systems, I believe, is greater than Twitter's whole revenue this year. So we're able to do things that I think are just not possible for other folks,"" Zuckerberg said. Breaking Facebook up, he claimed, would make enforcement tougher. ""We really need to decide what issues we think are the most important to address and to focus on, because in some ways, some of the remedies cut against each other in terms of making progress.""

Facebook only began releasing these content moderation reports a year ago, in response to calls for more information about how it sets and enforces its policies. The numbers included in the reports certainly underscore the scope of the problem Facebook is up against. In just the first three months of this year, for instance, the company took action against 5.4 million pieces of content that included child sexual exploitation, 4 million pieces of hate speech, 6.4 million pieces of terrorist propaganda, and 1.76 billion pieces of spam, among other banned content.",Yes,"It leads to the formation of echo chambers, where people only see information that conforms to their existing beliefs. This can have dangerous consequences, as it can lead to the spread of misinformation and the reinforcement of people's biases.",https://www.wired.com/story/facebook-community-standards-report/,2019-05-23 21:09:55.193000+00:00,LABEL_1_relevant,0.9963891506195068,Social Media,social_media,Wired,"Information, Discourse & Governance"
121,Google Bans Infowars Android App Over Coronavirus Claims,"On Friday, Google removed the Infowars Android app from its Play Store, extinguishing one of the last mainstream strongholds of infamous conspiracy theorist Alex Jones. The takedown comes on the heels of a video, posted in the Infowars app last week and viewed by WIRED, in which Jones disputed the need for social distancing, shelter in place, and quarantine efforts meant to slow the spread of the novel coronavirus.

Google confirmed to WIRED that it removed the app on Friday. The app had more than 100,000 downloads according to Google Play's published metrics, and was rated ""E10+,"" meaning safe for all users 10 and older. The Infowars app sold products like supplements and protein powder, broadcast The Alex Jones Show live, and posted videos and articles from Jones and others.

""Now more than ever, combating misinformation on the Play Store is a top priority for the team,"" a Google spokesperson told WIRED. ""When we find apps that violate Play policy by distributing misleading or harmful information, we remove them from the store.""

Alex Jones and Infowars did not immediately respond to a request for comment. In the video in question, Jones said that ""everybody dies under the new world order except maybe one tenth of one percent that believe they're going to merge with machines and have made deals with this inter-dimensional thing that gave them all the technology ... You can't make a deal with these aliens, OK, that the Bible tells you about and ever get off the planet."" Elsewhere in the video, Jones claims that natural antivirals exist to treat the novel coronavirus.

Google is far from the first tech giant to block or ban Infowars content. In fact, Google-owned YouTube suspended The Alex Jones Channel in August 2018, after mounting pressure from critics that platforms should enforce their anti-hate speech policies. Facebook took action against several Jones-operated pages that same day, as did Twitter with Infowars-related accounts the next month. Facebook instituted a full ban of Alex Jones and Infowars content across all of its platforms in May 2019. Apple's iOS App Store, Google Play's main competitor, already banned the Infowars app more than a year ago during the late summer 2018 blitz.

In the wake of all the mainstream bans, Jones and his supporters have repeatedly maintained that blocking Infowars and Jones' content only spreads it farther and makes it more popular. But research indicates that so-called ""deplatforming"" actions do effectively reduce the spread of hate speech, misinformation, and conspiracy theories. The bigger question is why Google waited so long to take action against Infowars given that its peer organizations did so a year and a half ago.

Tech services and social networks have long touted their role as defenders of free speech. But rampant abuse, misinformation, and coordinated disinformation campaigns carried out by nation states on the platforms have brought the industry to a new understanding in recent years of its necessary role in policing violent and dangerous content. The industry has also specifically moved to crack down on misinformation about the Covid-19 pandemic in the last few weeks. A joint statement from Facebook, Google, LinkedIn, Microsoft, Reddit, Twitter, and YouTube last week said that the companies are pledging to help ""millions of people stay connected while also jointly combating fraud and misinformation about the virus.""",Yes,It has amplified conspiracy theories and fake news.,https://www.wired.com/story/google-bans-infowars-android-app-alex-jones-coronavirus/,2020-03-27 16:07:38.229000+00:00,LABEL_1_relevant,0.9969396591186525,Social Media,social_media,Wired,"Information, Discourse & Governance"
122,Judge blocks Florida's social media law - The Verge,"A Florida internet law is on hold while a lawsuit contesting it continues. Judge Robert Hinkle has issued a preliminary injunction blocking nearly all of SB 7072, a controversial and sweeping regulation of social media platforms.

“The legislation now at issue was an effort to rein in social-media providers deemed too large and too liberal. Balancing the exchange of ideas among private speakers is not a legitimate governmental interest,” Hinkle wrote his order. Moreover, the law “discriminates on its face among otherwise identical speakers,” partly thanks to an eyebrow-raising exemption for companies that operate a theme park. That raises the bar for evaluating whether the law violates the First Amendment — and in Hinkle’s estimation, it probably does.

“The legislation compels providers to host speech that violates their standards ... and forbids providers from speaking as they otherwise would”

SB 7072, signed by Governor Ron DeSantis in May, limits when web services can kick off users. It includes a ban on suspending or annotating the posts of political candidates, a requirement that users can “opt out” of algorithmic sorting systems, and an “antitrust violator blacklist” for companies that break the rules. As mentioned above, the rules don’t apply to “a company that owns and operates a theme park or entertainment complex,” placating media conglomerates like Disney.

Industry groups NetChoice and the CCIA sued to stop the law’s enforcement, saying it would compel social media platforms to host offensive speech that violated their editorial policies. Judge Hinkle heard arguments from both groups and the DeSantis administration on Monday, when he appeared dubious of the law, calling the theme park exemption in particular a “major constitutional issue.”

The injunction is similarly skeptical, summarizing the law as follows:

“The state of Florida has adopted legislation that imposes sweeping requirements on some but not all social-media providers. The legislation applies only to large providers, not otherwise-identical but smaller providers, and explicitly exempts providers under common ownership with any large Florida theme park. The legislation compels providers to host speech that violates their standards — speech they otherwise would not host — and forbids providers from speaking as they otherwise would. The Governor’s signing statement and numerous remarks of legislators show rather clearly that the legislation is viewpoint-based. And parts contravene a federal statute.”

The federal statute Hinkle mentions is Section 230 of the Communications Decency Act, a rule that gives websites and apps wide latitude to decide what material they host. Beyond that, Hinkle says social networks regularly use editorial judgment to prioritize, remove, label, and otherwise sort content shown to users — and much of that editorial judgment is likely protected by the First Amendment. “The state has asserted it is on the side of the First Amendment; the plaintiffs are not. It is perhaps a nice sound bite. But the assertion is wholly at odds with accepted constitutional principles,” Hinkle writes.

Federal lawmakers, like state ones, have an appetite for regulating social media. That includes a package of bills aimed at reducing the potential monopoly power of companies like Facebook and Google, as well as several proposals for changing Section 230. In April, Supreme Court Justice Clarence Thomas also made a case for regulating social media. But Florida’s law was one of the first, most sweeping rules governing how companies can moderate web platforms — and it’s not surprising that it’s been stopped short.",Yes.,The way it can spread misinformation.,https://www.theverge.com/2021/6/30/22558086/florida-social-media-theme-park-law-preliminary-injunction-lawsuit,2021-06-30 00:00:00,LABEL_1_relevant,0.9931055903434752,Social Media,social_media,The Verge,"Information, Discourse & Governance"
123,"Online Meetups Are Sad, but What If You Were a Cute Animal?","Andy Baio is over online meetups. Zoomed out. VR skeptical. Serendipity deficient. A lot of us probably feel the same way.

But Baio has an advantage: He’s a technologist, a blogger, and the cofounder of the XOXO festival, which first launched in 2012 as an event for creators minus the soulless marketing of SXSW. He’s experienced conferences and virtual meetups from all angles: IRL, 2D, sort-of-3D. So after nearly a year of mostly lackluster online events, he decided to build a new kind of meeting space, one that, he says, would fix the more depressing elements of online meetings while welcoming in “bright spots of creative experimentation.”

The result is Skittish, a game-like meetup space that runs in the web browser. People appear as animal avatars; when I met up with Baio in Skittish, I appeared as a pony, and he was an owl. Users communicate with each other via live audio chat, and who you’re chatting with depends on your position in the cartoon-like 3D world. Move closer to a group of animated avatars, and their conversation gets louder; move away from them and the audio recedes. The look and feel of Skittish’s virtual world is reminiscent of Animal Crossing, which Baio says was an intentional design choice, to get close to the game’s “playfulness” and social elements.

Users in Skittish appear as cute 3D animals. Courtesy of Skittish

Skittish is still in beta. Baio was, well, skittish to say exactly when it will launch to the public, since the team behind it is still working on supporting potentially thousands of users and formulating its content moderation policies. At some point, he plans to charge for the service. For now, Baio agreed to talk to WIRED (where he has also contributed as a writer) about his new creation, his ideas for online conferences, and what the future of events like XOXO might be. The conversation has been edited for length.

WIRED: What are you hoping to accomplish in building Skittish?

Andy Baio: This entire project came out of a couple things. First, my experience running XOXO and then having the pandemic cancel that and upend my entire event organizing career. And by extension, seeing every other event organizer in the world struggle with trying to adapt to that. I started going to a bunch of remote events and I just didn't feel that they were right for me. A lot of them feel like you're just watching a YouTube live stream, or you're in a Zoom call. That just wasn't super compelling to me. The kind of events that I love—that I love attending and the ones that I try to organize—have the experience of getting a bunch of like-minded people into the same space at the same time, and then all of the the magic that happens after that when you allow them to interact in different ways, as they're moving between programming or as they just run into each other in line or in the hallway.

But over the last year we did start seeing some really interesting experimentation. I mentioned in my post, the environments created for Roguelike Celebration and LIKELIKE events were kind of borrowed from games. And then there’s experimentation around what we're doing right now—proximity chat, which allows you to talk to people that are nearby [in a virtual world]. I mean, there's dozens of these now, like that sort of spatial audio has been around for 15 years at least with Second Life, but all of these platforms are now kind of using this idea to help scale up a community, because with Zoom only one person can really speak at a time, right? And, you can’t see this right now, but it’s worth mentioning that Skittish supports an emerging standard for streaming payments to creators over the web.",Yes,"We're more divided than ever. And I think there's a lot of evidence that that's true. And I think that events like XOXO and the heart of the festival movement—the one that I'm a part of—is really about rebuilding bridges between people, between communities. And the way I see Skittish being used is",https://www.wired.com/story/skittish-andy-baio/,2021-02-18 12:00:00+00:00,LABEL_1_relevant,0.9956812858581544,Social Media,social_media,Wired,User Experience & Entertainment
124,Twitter Used to Predict Box Office Revenues,"The results are a fascinating insight into the power and limitations of Twitter. Asur and Huberman found that the rate at which people produce tweets about movies can accurately forecast the box office revenue of the film, but only after it is released.

And the predictions from tweets are more accurate than any other method of forecasting.

Film studios and others do this in variouys ways to gauge the popularity of their products. One indicator is the amount of money they spend on advertising. Asur and Huberman say this may be linked to rate of twittering before a movie is released. Others methods of forecasting include focus groups and exit polls of people who have seen the film.

But the most accurate is the Hollywood Stock Exchange, a market in which people can buy and sell virtual shares in actors, directors and individual movies. Researchers have long known that these kinds of virtual stock markets appear to tap into the wisdom of the crows to make uncannily accurate predictions about everything from the outcome of elections to price of real stocks. The Hollywood Stock Exchange produces unusually accurate predictions about the popularity of films.

However, when it comes to the accuracy of forcecasts, counting the rate of tweeting trounces the Hollywood Stock Exchange, say Asur and Huberman: “Our predictions are consistently better than those produced by an information market such as the Hollywood Stock Exchange, the gold standard in the industry.”

That will delight the founders of Twitter and the ecosystem of websites and services that have sprung up around it. These services have enjoyed enormous popularity but have yet to demonstrate their commercial worth.

However, it also raises an interesting new question for advertisers and marketing executives. Can they change the demand for their film, product or service buy directly influencing the rate at which people tweet about it? In other words, can they change the future that tweeters predict?

There’s only one way to find out and it’ll require a substantial investment in marketing and advertising. Tweeters be warned!

Ref: arxiv.org/abs/1003.5699: Predicting the Future With Social Media",Yes,It can be used to predict the future.,https://www.technologyreview.com/2010/04/01/204984/twitter-used-to-predict-box-office-revenues/,2010-04-01 00:00:00,LABEL_1_relevant,0.9953961968421936,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
125,You Can Now See All the Ads Facebook Is Running Globally,"Facebook is debuting a raft of new features aimed at bringing more transparency to advertising on the platform, in the wake of an ongoing scandal in which Russian trolls were able to purchase ads targeting the US electorate in the run-up to the 2016 election. The new features include a tool called View Ads, which allows users to see every active ad purchased on Facebook, Instagram, Messenger, and its partners. They were announced earlier this year and vetted in Canada. Now, the company is rolling View Ads out around the world.

“We’re providing much more transparency than any other advertising platform,” said Facebook’s chief operating officer Sheryl Sandberg in a meeting with reporters Thursday.

""The hope of the announcement today, making [Facebook] much more transparent than it was before, is that it will hold us accountable, it will hold advertisers accountable, but it will also give people the ability to find things that maybe shouldn't be up or find things that might be misleading so we can take action,"" she said.

The company is also expanding a feature it announced in April that allows users to see information on every political ad bought on its platforms, as well as so-called “issue ads” that touch on topics that might be construed as political, such as abortion or immigration. This tool gives users access to information on how much the ad cost, who was behind the ad, and the demographics of the audience it reached. Advertisers must also verify their identity and location before purchasing political or issue ads. Facebook rolled this political ad portal out in the United States in May, and now it’s launching in Brazil, as well, ahead of elections this fall.

“We have to take a deliberate approach to expanding to other countries,” said Rob Leathern, Facebook’s director of product management.

The company's political ad tool leaves something to be desired. While it offers access to metrics on each individual ad, it's still impossible to look at an advertiser's overall purchasing behavior on Facebook to see who they're targeting in general and how. But Leathern said the company will release an API later this year that makes that kind of data crunching easier.

For years, Facebook resisted abiding by the same Federal Election Commission political advertising regulations that traditional forms of media, like radio and television, are required to follow. But the revelation last fall that Facebook and other tech platforms had sold ads to Russian propagandists has forced the companies that were manipulated by the Russian actors to change their tune. Facebook, Google, and Twitter have all unveiled a variety of transparency measures aimed at heading off what they fear could be more draconian legislation if Congress is left to write the rules.

Already, Facebook has struggled to determine what types of ads qualify under its new policy. A number of media outlets have criticized the platform over the last month for labeling their ads political, despite only containing links to news stories. In response, Campbell Brown, Facebook’s head of news partnerships, said in a blog post that the company would soon update its ad transparency database to distinguish between journalism content and genuine political advocacy, like ads for a specific candidate.",Yes,It's very easy to spread false information and rumors.,https://www.wired.com/story/facebook-aims-more-transparency-view-ads-feature/,2018-06-28 19:15:17.587000+00:00,LABEL_1_relevant,0.9922967553138732,Social Media,social_media,Wired,"Information, Discourse & Governance"
126,"Facebook Changed the World, but Was It for the Better?","LG: Thanks for being here. I'm thrilled.

SL: It's falling apart already. Unbelievable.

[Intro theme music]

LG: Hi everyone. Welcome to Gadget Lab. I'm Lauren Goode, a senior writer at WIRED, and I'm here with my Gadget Lab cohost, WIRED senior editor, Michael Calore.

MC: Hello.

LG: Hello. And today we are also joined by WIRED editor at large, Steven Levy. Steven, thank you so much for being here.

SL: Well thanks. Thanks for finally letting me in this podcast room.

LG: Yeah, we hear that a lot from our colleagues at WIRED. They remember, with great detail, the last time they were on the show. Such as when Brian Barrett comes back on, he's like, ""It's been 17 months since I was on this show."" But that's true. This is that we always want to have our colleagues on this show, and we're so happy that you're here. And when was the last time you were on the show, by the way?

SL: The late 50s.

LG: Okay. When we were very, very early to podcasting. I mean, we were doing podcasts back when some people weren't even listening to terrestrial radio. Okay, Steven, let's get right to it. You are here because you wrote a book. It's called Facebook: The Inside Story. I have it right here in front of me. Folks, this is a tome. I don't know if you just heard that thud on the table, but this is like, you could lift weights with this thing. You spent years on this book, Steven, and you had a fair amount of access to people like Mark Zuckerberg, and other executives like Cheryl Sandberg in the process. It's a fantastic book. What compelled you to start writing this book when you did?

SL: So, I could even pinpoint the date. It was August 27th, 2015 when Mark Zuckerberg wrote on his feed, he put a story up, saying a billion people had been on Facebook the day before. And this wasn't how many people signed up for the services. It's like a billion people, in 24 hours, have been on Facebook. And I thought about that. Had that ever happened before? Like the World Cup gets a billion people, but that's not an interactive network, where someone could post something, and in theory you can get to everybody. And all the people's individual networks were intertwined there. So, I'd known that his ambitions were huge, and that Facebook was doing very well, but the reality of it made me think, ""Wow, this is something new. How do I tell this story? I've got to tell this story. This is my story."" And to tell who did it, how they did it, and what it means.

LG: And you had written a book about Google previously. Talk a little bit about that experience, and how it compared to your writing of the Facebook story?

SL: Yeah. My previous book was called In the Plex, it was about Google. And the process was similar in that I went to them, and said, ""I want to write a book, give me access to your people. You don't get to say anything about the contents. You'll see it when it's done."" And, I thought it would be pretty much a similar process. I would write this, I would try to put together a narrative of the story of Google, where people would be able to understand Google after they read the book. And the story, almost like a novel, would have a climax, and would have tension, and the tension in this story, the Google story, was its experience in China, where it went through this moral dilemma. And I thought, ""This'll be something similar.""",Yes,"It's polarizing. And the main reason that's happening is that it's creating what are called echo chambers, where you only get your news, your opinions, your ideas, and your beliefs reinforced. And that's how you get to a world where people think of each other as enemies.

LG: Mm-hmm (",https://www.wired.com/story/gadget-lab-podcast-445/,2020-03-06 13:00:00+00:00,LABEL_1_relevant,0.9932498931884766,Social Media,social_media,Wired,"Information, Discourse & Governance"
127,An AI Used Facebook Data to Predict Mental Illness,"It’s easy to do bad things with Facebook data. From targeting ads for bizarrely specific T-shirts to manipulating an electorate, the questionable purposes to which the social media behemoth can be put are numerous. But there are also some people out there trying to use Facebook for good—or, at least, to improve the diagnosis of mental illness. On December 3, a group of researchers reported that they had managed to predict psychiatric diagnoses with Facebook data—using messages sent up to 18 months before a user received an official diagnosis.

The team worked with 223 volunteers, who all gave the researchers access to their personal Facebook messages. Using an artificial intelligence algorithm, the researchers leveraged attributes extracted from these messages, as well as the Facebook photos each participant had posted, to predict whether they had a mood disorder (like bipolar or depression), a schizophrenia spectrum disorder, or no mental health issues. According to their results, swear words were indicative of mental illness in general, and perception words (like see, feel, hear) and words related to negative emotions were indicative of schizophrenia. And in photos, more bluish colors were associated with mood disorders.

To evaluate how successful their algorithm was, the researchers used a common metric in artificial intelligence that measures the trade-off between false positives and false negatives. As the algorithm categorizes more and more participants as positive (say, as having a schizophrenia spectrum disorder), it will miss fewer participants who really do have schizophrenia (a low false negative rate), but it will mislabel some healthy participants as having schizophrenia (a high false positive rate). A perfect algorithm can have no false positives and no false negatives at the same time; such an algorithm would be assigned a score of 1. An algorithm that guessed randomly would have a score of 0.5. The research team achieved scores ranging from 0.65 to 0.77, depending on the specific predictions they asked the algorithm to make. Even when the researchers restricted themselves to messages from over a year before the subjects received a diagnosis, they could make these predictions substantially better than would have been expected by chance.

According to H. Andrew Schwartz, an assistant professor of computer science at Stony Brook University who was not involved in the study, these scores are comparable to those achieved by the PHQ-9, a standard, 10-question survey used to screen for depression. This result raises the possibility that Facebook data could be used for mental illness screening—potentially long before a patient would otherwise have received a diagnosis.

Michael Birnbaum, an assistant professor at the Feinstein Institutes for Medical Research in Manhasset, New York, who led the study, believes that this sort of AI tool could make an enormous difference in the treatment of psychiatric illnesses. “We now understand this idea that cancer has many different stages,” Birnbaum says. “If you catch cancer at Stage I, it’s drastically different than if you catch it once it metastasizes. In psychiatry, we have a tendency to start working with people once it’s already metastasized. But there’s the potential to catch people earlier.”

Birnbaum is far from the first researcher to have used social media data to predict the presence of mental illness. Previously, researchers have used Facebook statuses, tweets, and Reddit posts to identify diagnoses ranging from depression to attention deficit hyperactivity disorder. But he and his team broke new ground by working directly with patients who had existing psychiatric diagnoses. Other researchers haven’t, in general, been able to work off of clinically confirmed diagnoses—they have taken subjects’ word for their diagnoses, asked them for self-diagnoses, or had them take questionnaires like the PHQ-9 as a proxy for diagnosis. Everyone in Birnbaum’s study, in contrast, had an official diagnosis from a psychiatric professional. And since the researchers had definitive dates for when these diagnoses were made, they could try to make predictions from messages sent before the patients knew about their mental illnesses.",Yes,"It can be used to predict psychiatric diagnoses with Facebook data. The team worked with 223 volunteers, who all gave the researchers access to their personal Facebook messages. Using an artificial intelligence algorithm, the researchers leveraged attributes extracted from these messages to predict whether they had a mood disorder (like bipolar or depression), a schizophrenia spectrum disorder, or no mental health",https://www.wired.com/story/an-ai-used-facebook-data-to-predict-mental-illness/,2020-12-14 13:00:00+00:00,LABEL_1_relevant,0.996542751789093,Social Media,social_media,Wired,Health & Wellbeing
128,"The latest on Facebook’s data scandal: Zuckerberg speaks, lawsuits, and ignored whistleblowers","And all the while the firm’s CEO remains hugely conspicuous by his absence.

Backstory: In case you missed it, Facebook is embroiled in a huge scandal because of the way its users’ data was shared with Cambridge Analytica, a firm that provided data to the Trump election campaign in 2016.

Overlooked whistleblowers: Ex-Facebook staffer Sandy Parakilas told British politicians today that his warnings about the firm’s lax data protection standards were ignored, and that some of the executives he told still work at the social network. To this point, today’s Bloomberg Businessweek cover story makes a compelling argument: maybe we need a Data Protection Agency?

The legal backlash begins: It was only a matter of time, but the first legal complaints against Facebook and Cambridge Analytica have now been filed. Expect more to follow in the coming days.

#DeleteFacebook: Brian Acton, the cofounder of WhatsApp (who made billions by selling his startup to, ahem, Facebook in 2014), has been a very vocal part of a campaign urging people to quit the social network. “It is time. #deletefacebook,” he tweeted. (Or you could manipulate Facebook instead of letting it manipulate you.)

Where’s Zuck? The CEO was a no-show at a staff meeting yesterday. The Daily Beast says he’s “working around the clock.” The American and British governments want him to give evidence, but he sent “mid-level staffers” to testify to Congress today.

Here’s Zuck! Late Wednesday afternoon Mark Zuckerberg issued a long apology on his Facebook page (naturally), giving a timeline of events from Facebook's point of view and plans to investigate data access for outside developers.

Lots to lose: Media analysts say that Facebook has made a huge mess of handling the situation so far. (See: the firm’s stock price.)

Is it different this time? Every Facebook scandal feels like the one that’s going to bring about radical change, but it hasn’t—yet. Gadfly proposes that Facebook is bigger, and lawmakers more suspicious, than ever this time, so it could be different. But that’s a very big “could.”",Yes,"It can give people false information to believe in.

Whether it is a political candidate or a product, people can use social media to spread false information quickly and easily. This can have serious consequences, as people may make decisions based on false information.",https://www.technologyreview.com/2018/03/21/144526/the-latest-on-facebooks-data-scandal-lawsuits-calls-to-quit-and-ignored/,2018-03-21 00:00:00,LABEL_1_relevant,0.9959316849708556,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
129,A Reddit Community Has Become a Memorial for Covid-19 Victims,"When it launched in 2013, Reddit’s r/LastImages community was a place for people to document the final photographs taken of celebrities and other prominent figures, and even historical objects like the Titanic. But over time, it has morphed into something else entirely.

WIRED UK This story originally appeared on WIRED UK.

Now it’s a place where people post the last photos taken of their loved ones before they died, and since the Covid-19 pandemic began, it’s seen a spike in activity from both posters and commenters as they remember loved ones lost to the disease.

At the beginning of May, Connecticut-based Manny Garcia posted the last ever picture he took with his dad at a New York Giants game before he died from Covid-19. “Earlier this year, I landed my dream job, and I told myself that I would go all out for that man this year. He deserves it,” Garcia remembers. On his dad’s birthday, Garcia and his sister drove down to New York, both dressed head to toe in Giants gear, picked him up, and drove to the stadium in New Jersey.

Garcia remembers how his dad would announce the names and numbers of the players that people were wearing on their jerseys. “He was so happy to be there with his kids. I don’t think I’ll ever meet another person who enjoys and knows more about random New York Giants or New York Yankees trivia than him,” he says.

In late April, Pennsylvania-based Daniel Tweedle posted the last photo of him and his brother with their mother, grinning widely, taken shortly after the birth of his twin sons. His mother, who lived in North Carolina, traveled to Western Pennsylvania to meet her grandchildren. The photograph was taken in the summer, before Tweedle’s mother died from Covid-19. “My brother and his wife came from eastern Pennsylvania to meet my boys. It was the first time in 10 years we were together at the same time,” he says. “My mom was thrilled to meet my boys, and she would constantly demand pictures of the boys. She would buy them books and toys and clothes. We literally have winter jackets for them for the next four or five years.”

The motivation each person has for posting their loved one’s last image varies. Garcia says he hoped it would help him with the grieving process. “In my case, it did,” he says. Tweedle says he posted to the subreddit because he was angry, feeling bitter about losing his mother to coronavirus and not being able to be at her side. “I wanted people to see that those 50,000 people—at the time in the US—who died weren't just numbers. They were real moms and grandmothers, dads and grandpas. People that we love and need,” Tweedle says. “There just seems to be a detachment from the general public. I wanted to add a face so people could see what we were losing every day to Covid.”

Debra Bassett, who researches death and dying at the University of Warwick in Coventry, England, says places like r/LastImages have never been more essential. “It’s even more important now that these memorial platforms are allowing people to make their loved one important rather than making them just a statistic or a number,” she says.",Yes,It can lead to people feeling more isolated and lonely.,https://www.wired.com/story/a-reddit-community-has-become-a-memorial-for-covid-19-victims/,2020-06-02 13:30:00+00:00,LABEL_1_relevant,0.9954387545585632,Social Media,social_media,Wired,"Information, Discourse & Governance"
130,‘The Internet Remains Undefeated’ Must Be Defeated,"To a wide-ranging group of social media users like Hall, “The internet remains undefeated” is, on its face, a simple expression of joy, or nostalgia for a more joyous era of the internet. Ryan Milner, a professor of internet culture at the College of Charleston and author of The World Made Meme, says the phrase harkens back to a time, between roughly 2003 and 2013, when the internet was “still kind of this other place that didn’t operate by and could maybe transcend real-world rules.” This was the heyday of early YouTube and message boards like Something Awful, 4chan, and Reddit, “when you saw a flurry of subcultural activity and content creation that became kind of a tone setter for people who are still extremely online.” So in 2021, people comment “The internet remains undefeated” to a flourishing of memes about Bernie Sanders and his mittens or the discord between your fall plans and the Delta variant, because it recalls when life online seemed less about livestreamed mass murders and the algorithmically driven death of democracy and more about rickrolling and lolcats. At the surface level, says Milner, the phrase “is a way to kind of appreciate when the early spirit of collective creativity online resurfaces.”

People also use the phrase, Milner adds, as a way of “reacting to the randomness of what they encounter online.” Every piece of content “is made by a real person at the other end of the tubes. But we just see the funny picture. So instead of saying ‘Tim from Madison, Wisconsin, remains undefeated,’ we tend to collapse everything from everyone as being from ‘the internet,’ as if it’s this singular mystical being.” In that sense, the saying is a collectivist antivenom to unhinged individualism online.

“The internet remains undefeated” glorifies the removal of context, nuance, and thought.

But exuberant and egalitarian as the expression may appear, its undertones are much darker. For one thing, “The internet remains undefeated” is also a symptom of what Milner and fellow internet culture scholar Whitney Phillips call fetishistic flattening. This is the tendency for internet users to fixate on a meme or tweet itself, and not consider how or why it was created, the backstory of who or what’s being depicted and shared, or who may be harmed in the process. (The “Hide your kids, hide your wife” song, which belittles the man in the original clip, and deepfaked drunk Nancy Pelosi are all standard examples of fetishization.) In this way, “The internet remains undefeated” glorifies the removal of context, nuance, and thought. “Undefeated” in particular also captures how on social media, context is subsumed by combativeness. Beneath the surface, says Milner, the phrase is often “antagonistic and barbed,” and of “an atmosphere where how funny you are about what you produce and say, and how many people respond no matter what you say, is seen as a competition.”

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

Of course, context removal and ruthless competitiveness are embedded in dozens of other popular memes and replies to memes: Distracted Boyfriend, Galaxy Brain, Swole Doge vs. Meek Doge, so and so “woke up and chose violence.” But whereas those all celebrate the defeat of a single common enemy or idea lampooned in the meme itself, what makes “The internet remains undefeated” so deflating is that it celebrates our own collective defeat of ourselves. The internet’s unstated, vanquished opponent is us, the users who both consume and are the butts of the memes that phrase is often a response to. But deep down we all understand that we are also the internet, as the ones who populate it, generate its content, and created it in the first place. As Jeffrey Bloechl, a philosophy professor and phenomenologist at Boston College, told me, any problems that appear on the internet “can be traced back to things we human beings either did or failed to do when we made the thing.” After all, he adds, humans designed the internet to be boundless. “If the internet, strictly as internet, is fundamentally mathematical, it cannot itself be the source of any limits.” By that logic, “there is no way not to wonder whether in unleashing a power that is undefeated,” one that can transcend the limits of our own bodies and minds, we’ve also unleashed “a power to change what we are,” a power to defeat the human condition.

That is the horrifying economy of Those Four Words: There is no more haunting a distillation of the unstoppable seepage of technology into every fabric of our being than “The internet remains undefeated.” These words are a glaring reminder that the internet, of which I am a part, is defeating me. That in the moment I am reading them, I am devoting my attention not to my wife, infant daughter, friends, family, colleagues, wind rattling the window pane, or my breathing, but to what faceless strangers are saying about Nicki Minaj’s cousin’s friends’ balls, and to what quippy things I should be saying to faceless strangers about Nicki Minaj’s cousin’s friends balls. That gif of the Teletubbies having tantric sex? It exists only in my smooth, broken brain, a brain the internet broke so that I think in the way the internet wants me to think.",Yes,It isolates people from real human connection and makes them more likely to believe things that aren't true.,https://www.wired.com/story/internet-remains-undefeated/,2021-09-24 12:00:00+00:00,LABEL_1_relevant,0.9938687682151794,Social Media,social_media,Wired,"Information, Discourse & Governance"
131,Far-Right Propaganda Floods Facebook Ahead of EU Vote,"In Italy, the perpetrators spread a movie clip of a car being destroyed and pretended it was news footage of migrants wrecking a police vehicle. In Poland, they disseminated a fake news story about migrant taxi drivers raping European women. In Spain, they shared lies about Catalan separatists shutting down a child cancer center. In the UK, they shared a blog post with a beheading photo and a sensationalist headline, claiming ""A Billion Muslims Want Sharia Law.""

These are just a few examples of the ugly far-right propaganda and disinformation that has flooded Facebook ahead of parliamentary elections in the European Union this week. According to a new report by online activist group Avaaz, networks of fake accounts, pages, and groups have been spreading divisive, white-nationalist, anti-immigrant content throughout Germany, the UK, France, Italy, Poland, and Spain. In some cases, they posed as politicians themselves. In others, they created fan pages for political parties or alternative media organizations. Together, they amassed followings several times larger than the actual far-right groups operating within these countries.

As part of its investigation, Avaaz's global network of volunteers reported more than 500 ""suspicious"" pages and groups to Facebook. The company has since taken action against 77 of them, the report states, and has removed more than 200 accounts. Avaaz says those pages had nearly 6 million followers among them, three times as many followers as the actual far-right groups in those countries combined. The content they spread was viewed 533 million times over the past three months, according to the report. Just who exactly is behind these operations remains a mystery.

Facebook confirmed that it had taken some accounts and pages down, but did not provide specific numbers. “We thank Avaaz for sharing their research for us to investigate. As we have said, we are focused on protecting the integrity of elections across the European Union and around the world,"" a Facebook spokesperson said. ""We have removed a number of fake and duplicate accounts that were violating our authenticity policies, as well as multiple pages for name change and other violations. We also took action against pages that repeatedly posted misinformation. We will take further action if we find additional violations.""

Issie Lapowsky covers the intersection of tech, politics, and national affairs for WIRED.

The report underscores a persistent dilemma for Facebook. The company has gotten better at recognizing and removing election interference operations on its platform after coming under fire for how Russian operatives were able to target American voters during the 2016 presidential campaign. But despite efforts to protect elections in the EU and around the world from further interference, Facebook still hasn't found a way to keep all this toxic and manipulative content from spreading in the first place. Of course, it's not alone among social media companies. But Facebook's unparalleled size makes it a particularly powerful tool.

""With days to go until EU elections, Europe is drowning in disinformation. The size and sophistication of these networks makes them weapons of mass destruction for democracy, and right now they are pointed squarely at Europe,"" Christoph Schott, campaign director at Avaaz, said in a statement. ""The most worrying thing is we’ve just scratched the surface. There could be much, much more out there.""

Avaaz has been on a mission recently to get tech companies to take a more aggressive stance against disinformation. Earlier this month, the organization led a group of so-called fake news victims to Silicon Valley to meet with Facebook and Twitter. This report is part of that work. Over the course of a three-month investigation, Avaaz researchers and volunteers around the globe reviewed thousands of pieces of suspected disinformation, hate speech, and fake news, much of it submitted by the public through a crowdsourcing site that Avaaz created. They looked for signs that the pages, groups, and accounts connected to this content might be violating Facebook's policies around authenticity, violence, or hate speech, and shared their findings with Facebook. Facebook took action against some, but not all, of the accounts, pages, and groups Avaaz identified.

""With days to go until EU elections, Europe is drowning in disinformation."" Christoph Schott, Avaaz

In Germany, for example, Avaaz says Facebook took action against 131 fake accounts, many of which propped up the far-right party Alternative for Germany, or AfD. In France, Facebook removed a page called Suavelos, which promoted white-nationalist ideas, something Facebook only recently prohibited. Facebook shut down 23 pages Avaaz reported in Italy for a variety of infractions, including changing the name of a page to something unrelated to the page's origins; one such page changed its name eight times, according to Avaaz, starting as a sports page and eventually morphing into a political page. In Poland, Avaaz found a network of pages sharing a fake news story almost simultaneously about migrant drivers raping European women. Of the 26 pages Avaaz found that shared the article, Facebook took down 11. Avaaz was also behind a recently reported takedown of pages and groups in Spain just before the election there in April. Finally, in the UK, Avaaz says Facebook removed 132 posts, pages, and groups listed in its report.

These takedowns are different from the ones Facebook has traditionally announced publicly, in that none of them appear to be what the company calls ""coordinated inauthentic behavior"" driven by a nation-state or single organization. Facebook has previously announced these types of influence operations associated with entities in Russia, Iran, and Israel. Instead, the Facebook spokesperson said, these removals are part of actions the company takes ""regularly and routinely"" against content that violates its policies.",Yes,"People tend to share things that align with their worldview, whether those things are true or not. As a result, people are often exposed only to information that reinforces their existing beliefs, which can make it difficult for them to understand and empathize with others.",https://www.wired.com/story/far-right-propaganda-facebook-eu-elections/,2019-05-22 05:00:00+00:00,LABEL_1_relevant,0.9958884119987488,Social Media,social_media,Wired,"Information, Discourse & Governance"
132,The mass shooting in New Zealand shows how broken social media is,"A gunman live-streamed the murder of dozens of innocents in two mosques in Christchurch, New Zealand, on Friday—and the world got a terrible reminder of how flawed existing social-media policies and algorithms are for policing violent and offensive content.

In the days before the shooting, the perpetrator apparently boasted of his plans and posted an online manifesto. He then broadcast the horrific act live on Facebook. The attack left 49 people dead and dozens more injured.

Live stream: Over the past 18 months, following harassment and fake-news scandals, social-media companies have invested heavily in content moderators. But this did little to stop video of the shooting from spreading. Not only was the live stream reportedly up for 20 minutes, but the resulting video was then reposted on YouTube, with some clips remaining up for over an hour.

Several factors contributed to letting the footage slip through the filters, according to experts.

Real-time challenge: It’s vital to catch a video quickly, so that it doesn’t spread onto other platforms. But social-media moderation simply isn’t geared toward catching content in real time. It is impossible to automate the process effectively, and identifying live streams that need to be shut down manually is “like finding a needle in the haystack of data that’s flowing over the network all the time,” says Charles Seife, a professor at NYU’s School of Journalism. He adds that Facebook could require users to build up a reputation before letting them live-stream content, to reduce the risks.

Whack-a-mole: Moderators are overwhelmed at the best of times. Video of the shooting hosted on YouTube most likely spread so quickly that the humans employed to check for inappropriate content didn’t have time to catch everything. These workers typically have a few seconds to make a call. The process can be partly automated, but those who reposted the footage apparently clipped it and introduced distortions to avoid these algorithms.

Algorithmic failure: Social-media companies also use algorithmic tweaks to de-prioritize suspicious content. But Mike Ananny, an associate professor at the University of Southern California, says these algorithms were probably thrown by the popularity of the offending videos.

Not our problem: These factors reflect the key systemic problem: Facebook, YouTube, and other big social platforms do not see themselves as the arbiters of content in the first place. Research has shown that far-right sources of information can be policed more proactively to prevent violent or hateful material from spreading. “They have this attitude of being post hoc,” says Ananny. “It’s a deep cultural thing.”",Yes,"It can spread violent and offensive content quickly and easily. There are a number of factors that contribute to this, including the fact that social media platforms are not designed to police content in real time, and that their algorithms can be easily circumvented by those who wish to do so.",https://www.technologyreview.com/2019/03/15/65970/the-mass-shooting-in-new-zealand-shows-how-broken-social-media-is/,2019-03-15 00:00:00,LABEL_1_relevant,0.9960516095161438,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
133,6 Lingering Questions From the Times' Facebook Bombshell,"On Wednesday afternoon, The New York Times published a blockbuster report—with five bylines, 50 sources, and 5,000 words—on the failures of Facebook’s management team during the past three years. It begins with Sheryl Sandberg yelling at one of her employees; it ends with her notes to self, captured by a photographer, as she sat before the Senate: “Slow, Pause, Determined.” The story, in other words, is not very flattering (and you should definitely read it) We did, and we have six follow-up questions that merit more investigation.

1) What is Sandberg’s future at Facebook?

For most of the social media company's history, Sandberg has avoided criticism. During the past year, most of the anger at Facebook has been directed at Zuckerberg. That has started to change recently. The Wall Street Journal reported, for example, on a “swat team” that Sandberg runs, tasked with identifying and preventing future catastrophes. The Times story, though, is the first to cast her as the central antagonist.

It was Sandberg, the story says, who seethed after security executive Alex Stamos (who later left the company) disclosed to a Facebook board committee that the extent of Russian interference was still unknown and unchecked. It was Sandberg who chastised Stamos for devoting time and effort to look into the Russian campaign without company approval. It was Sandberg who sided with Joel Kaplan, vice president for public policy, about leaving the Russians out of its white paper on election interference, and it was Sandberg who encouraged Stamos to be less specific in his initial posting about Russia’s propaganda campaign. Sandberg appealed to Senator Amy Klobuchar, the Democrat from Minnesota, to dial down her attacks on Facebook. And Sandberg was the one who came out in support of the Stop Enabling Sex Trafficking Act, a decision the Times asserts was motivated in part to make other tech giants like Google look bad.

In a post responding to the newspaper, Facebook rejects that assertion. “Sheryl championed this legislation because she believed it was the right thing to do, and that tech companies need to be more open to content regulation where it can prevent real world harm,” the company wrote.

The question now is whether the woman charged with solving Facebook’s hardest problems has caused a few too many of her own.

2) What other tech company has been hiring an opposition group to smear Apple?

One of the more extraordinary parts in the report involves an opposition research group called Definers Public Affairs, run by Matt Rhoades, a former campaign manager for Mitt Romney. The firm also employs Tim Miller, a former spokesman for Jeb Bush and a contributor to Crooked Media, the company that runs Pod Save America. Facebook hired Definers to look into the funding of the company’s critics.

During this period, a conservative news website called NTK Network, which the Times says is affiliated with Definers, published a number of stories critical of Apple. But, in the Times report, Miller also says that Definers’ Apple work is funded by a third technology company. In other words, Facebook paid Definers; Facebook was fighting Apple; Definers wrote stories critical of Apple; but another technology company was paying for those stories.",Yes,"It has created an industry of professional trolls, who often have no clear affiliation other than being paid by a major technology company.

3) What did Mark Zuckerberg know about the hiring of Definers?

Zuckerberg has always been a micromanager. In the early days of Facebook, he was known for calling up random employees",https://www.wired.com/story/6-questions-new-york-times-facebook-bombshell/,2018-11-15 19:33:44.193000+00:00,LABEL_1_relevant,0.9961842894554138,Social Media,social_media,Wired,"Information, Discourse & Governance"
134,Meet Romania's Very Internet-Savvy Witch Community,"Consider it globalization of the supernatural: Thanks to the proliferation of the internet, the Romanian witch community—also known as the vrăjitoare—has migrated their ancient practice onto the web. Using social media to livestream rituals or to video chat with clients for fortune readings, witch entrepreneurs are better able to grow their business using self-referential devices (clothing, jewelry, idols) to effectively market the storied mysticism of Roma women to searching souls.

Members of the vrăjitoare learn the craft from their mothers at age 7, after careful observation of whether they are capable of continuing the tradition. Other witches claim they were born with a ""gift"" from God that enables them to communicate with divine energy. Only women are able to join the vrăjitoare. Witchcraft is a respected (and feared) profession in Romania, a country where the president is known to wear purple on certain days to ward off evil. The practice is even regulated: In 2011, a new law required the vrăjitoare to pay a 16 percent income tax, the same as any other self-employed Romanian citizen. The response was twofold. Some supported the tax, arguing that it established witchcraft as a verifiable profession, while others angrily threw poisonous mandrake plants into the Danube River.

In 2013, Slovakian photographer Lucia Sekerková Bláhová discovered the vrăjitoare practice when watching a documentary on the Qatari news channel Al Jazeera. Bláhová employed the help of ethnologist Ivana Šusterová, who specializes in the culture of the Wallachian Roma people, to help gain access to the vrăjitoare for her eponymous photo series. Bláhová contacted witches she found online to arrange visits using their personal websites or Facebook pages. Taking a cue from Šusterová, Bláhová wore traditional Roma clothing and observed certain customs to gain the vrăjitoare's trust in order to photograph them. ""Mostly, it was all about making a good first impression, being honest to them,"" Bláhová explains. ""Sometimes, you need good negotiating skills.""

Negotiating is central to the vrăjitoare business. Witches don't post prices for their services on the internet, the understanding being that cost will be incurred based on services rendered. Typically, clients meet with the witch for a ""diagnosis,"" which Bláhová says costs between 10 and 20 euros. The remedy, however, comes at another expense. As one witch told Bláhová, ""Some [clients] give me 50 euros and some people give me 500 euros because they like my honesty.""

Bláhová says most of the magic rituals the vrăjitoare perform are intended to restore health or ignite romance, although some services are aimed to do harm. Many of the women are under increasing pressure to specialize their practice, especially in an era when every witch can have a web presence. ""Women said they have been picking flowers in secret places, always in wild nature, far away from civilization, usually during the sunrise,"" Bláhová says. ""The more interviews we had with them, the more we got a feeling of a kind of performance for their clients or a kind of competition between the women."" Bláhová says some women, especially older witches, were concerned with how the internet made witchcraft more accessible, pointing out that some younger members of the vrăjitoare were sharing the steps of their secret rituals on Facebook.

Still, most of the vrăjitoare maintain that all business is good business. ""When you would expect something mystical, there is a young witch with iPhone in her hand taking a selfie picture before a ritual for her vrăjitoare Facebook profile,"" recounts Šusterová, a Slovakia-based ethnologist. ""Because of the internet, the work of vrăjitoare is much easier and more available to clients.""

More Great WIRED Stories",,"It has given rise to a new generation of witches who are using the internet to market their services to a wider audience. This has led to a situation where the older, more traditional witches are feeling pressure to adapt their practices to the new reality.",https://www.wired.com/story/romanian-witches-internet/,2019-03-15 14:00:00+00:00,LABEL_1_relevant,0.989973783493042,Social Media,social_media,Wired,Social Norms & Relationships
135,TikTok Bugs Could Have Allowed Account Takeovers,"The social video app TikTok has been branded a potential security threat for its ties to China—the app is owned by the Beijing-based company ByteDance—but like any piece of software it also has the potential for more immediate security concerns. Recently patched vulnerabilities in the app could have allowed an attacker to take over TikTok accounts, add or delete videos, and expose private data like user information or videos marked ""hidden.""

Researchers from the security firm Check Point first disclosed the bugs to TikTok in late November, and the company patched all of them on iOS and Android by the end of December. The findings come, though, as Congress has held hearings and called for investigations in recent months over the possibility that the app poses a national security risk. And the US Army and Navy both banned the app from their devices at the end of 2019, calling it a cyber threat. All software has bugs, and a few vulnerabilities don't reveal that TikTok is at all malicious. But the findings show that the social media app of the moment merits more scrutiny.

""The goal of our research was really to understand what is the the level of security and privacy that TikTok is providing,"" says Oded Vanunu, Check Point's head of product vulnerability research. ""Once we finished the review and understood that we could easily manipulate the accounts, we said, ‘Let’s stop here and share the information.’ We hope that now more researchers will check the app and that TikTok will increase their security validation cycle.""

The researchers noticed that TikTok offers a feature on its website for users to enter their phone numbers and receive an SMS message with a link to download the app. While analyzing this mechanism, they found that they could remotely manipulate the words in the text as well as the download link, and send them to any phone number. From there they discovered that they could craft special links for these texts that would send commands to TikTok if a victim had already downloaded the app.

In practice, an attacker could have revamped an SMS message to target existing TikTok users, rather than just first-timers—and the texts would legitimately be coming from TikTok's infrastructure. If a TikTok user clicked one of these malicious links, an attacker could have manipulated bugs in TikTok's browser redirect setup and authentication mechanisms to manipulate their account—sending commands to add or delete videos, forcing the victim account to follow other accounts, making private videos public, or exfiltrating the victim's personal account data, like name and email addresses.

Vanunu says that TikTok was responsive about the disclosures and patched the issues within weeks. ""TikTok is committed to protecting user data. Like many organizations, we encourage responsible security researchers to privately disclose zero day vulnerabilities to us,"" Luke Deshotels, a member of the TikTok security team, said in a statement. ""We hope that this successful resolution will encourage future collaboration with security researchers."" TikTok told WIRED that it reviewed its customer support records and didn't find ""any patterns that would indicate an attack or breach occurred.""",Yes,"The exposure of private data. The findings of the recent study conducted by Check Point show that the social media app TikTok is not an exception to this and actually has several vulnerabilities that could allow an attacker to take over TikTok accounts, add or delete videos, and expose private data like user information or videos marked ""hidden.""",https://www.wired.com/story/tiktok-bugs-account-takeover/,2020-01-08 10:00:00+00:00,LABEL_1_relevant,0.9965358972549438,Social Media,social_media,Wired,Security & Privacy
136,Facebook Can’t Fix What It Won’t Admit To,"Hi, folks. Some of you are complaining that I should keep away from politics and stay in my lane. I haven’t moved. The lane just got a lot wider. I am, however, looking forward to writing about more tech-y stuff. Can the world cooperate, please?

Starting next week, this column will be subscriber-only, with the occasional exception. Get full access to every edition by subscribing to WIRED (50% off for Plaintext readers) today.

The Plain View

In June 2017, Mark Zuckerberg changed Facebook’s mission. Speaking at the company’s first Community Summit in Chicago, he explained that the best part of Facebook is its “meaningful groups,” those that address a user’s passions or needs and connect them with others who share those interests. At the time, there were 100 million people in meaningful groups; he wanted to grow it to a billion. Zuckerberg believed this so much that he changed Facebook’s core goal from “connecting the world” to “giving people the power to build community and bring the world closer together.” In a post explaining this, he wrote, “Communities give us that sense that we are part of something greater than ourselves, that we are not alone, and that we have something better ahead to work for.”

More than three years later, some of those groups have done exactly what Mark Zuckerberg envisioned: They bound together for a passionately held common cause. But the “something greater than ourselves” probably wasn’t what he had in mind: overthrowing the peaceful transfer of power following a fair and certified election in the United States. Other platforms like Parler might have been instrumental in organizing extremists to assault the US Capitol building. But Parler’s members were already committed to the cause. Facebook’s community-building algorithms were effective in drawing some of its massive audience from the sidelines and into the maw of radicalism and sedition.

In fact, Facebook’s own algorithms seem to pump up membership in those groups. A Wall Street Journal article from May 2020 reported an alarming finding from Facebook’s own researchers. According to a 2016 internal study, “64 percent of all extremist group joins are due to our recommendation tools … Our recommendation systems grow the problem.” The article also revealed that the company’s efforts to address this were stifled by interference from the company’s political wing, ever sensitive to criticisms from the right. And just this week a New York Times article outlined several cases where relatively sane people were driven deep into seditious crazytown once they discovered that Facebook widely circulated their most transgressive anti-democratic posts, winning them status and followers. One user found that the more he posted deranged Trumpist messages, the more followers Facebook sent his way, and soon he was hosting a meaningful group based on election denial, with tens of thousands of members. It was almost like an embodiment of what Zuckerberg had described as the feeling of “we are not alone.” By becoming an anti-democracy person, he’d found other people. All reinforcing everyone else’s awfulness.

This week Facebook COO Sheryl Sandberg gave a rare interview. As always, she cautioned that the company wasn’t perfect, but her overall message was that Facebook’s policies were by and large working. “Was there anything you thought Facebook could have done sooner?” asked her interlocutor. Sandberg replied that while Facebook knew that the protests were being organized online, it had generally done its job by removing violent groups like Proud Boys, QAnon, and Stop the Steal. (The latter group garnered 320,000 followers before Facebook took it down, and the corresponding hashtag wasn’t banned until five days after the January 6 insurrection.) She assigned serious blame to others. “I think these events were largely organized by platforms that don’t have our ability to stop hate and don’t have our standards and don’t have our transparency,” she said.",Yes,"It creates a feedback loop that amplifies extremism, disinformation, and conspiracy theories, leading to mass psychosis and political unrest.

The Social Dilemma is the best documentary I’ve seen on this topic. It’s available on Netflix.

If you’re not subscribed to WIRED yet, now’",https://www.wired.com/story/plaintext-facebook-cant-fix-what-it-wont-admit-to/,2021-01-15 14:00:00+00:00,LABEL_1_relevant,0.9955435395240784,Social Media,social_media,Wired,"Information, Discourse & Governance"
137,"Snap Is No Facebook, and Spiegel Insists He Wants It That Way","Evan Spiegel wants the world to know something: His company, Snap, doesn’t admire Facebook, doesn’t want to be like Facebook, and believes that Snap’s approach to its users and their data is better for the world. Appearing onstage at the Code Conference in Palos Verdes, California, Tuesday night, Spiegel said that Facebook may have changed its products and mission but “fundamentally they will have a hard time changing the DNA of the company.” That DNA, Spiegel said, “is about having people compete online for attention.”

It made for great theater, and Spiegel won applause from the well-heeled crowd of tech and media elites. But the reason Spiegel was on stage in the first place was because it appears Snap’s business will never look anything like Facebook’s. Sure, Facebook is facing withering criticism for how it has handled a slew of problems, from protecting users’ data to Russian bots to hate speech. But as a business it mints money. Snap’s does not yet, and Spiegel has been under enormous scrutiny from investors and employees for the way he is running the place.

Facebook’s Instagram is projected to surpass one billion users this year, while Snapchat claims around 191 million daily users, according to its most recent earnings report. Its growth has slowed. While Snapchat has managed to capture a rabid audience of younger users who love its ephemeral messaging---and its impressive augmented-reality filters---some of the company’s recent moves have shaken investors. Its stock price has been slashed in half since February, when it redesigned the app.

Snap is also facing scrutiny around its culture and commitment to diversity, after a newly published memo from a former Snap employee highlighted a “pervasive sexist vibe.” The memo described a toxic macho culture, and it included complaints about a company party that featured scantily clad women and a separate complaint about a senior vice president of engineering who allegedly made inappropriate comments.

Spiegel said the memo was a “really good wake-up call” for the company, and that his team is constantly talking about its culture and values and how to improve. Snap may be located in “Silicon Beach,” not Silicon Valley, but its early responses to the memo resembled those of its northern neighbors facing similar accusations.

Five years ago, people wondered whether Snapchat could be the new Facebook, so much so that Facebook reportedly tried to buy Snapchat for $3 billion. Now, Facebook seemingly waits for Spiegel to roll out new features, then copies them.

The Evan Spiegel onstage Tuesday night was a more thoughtful, poised, and undoubtedly coached CEO than the Spiegel of the past. He crafted the image of a company that communicates well internally and develops new products based not just on data but also on intuition—in a way, pushing his own image as a visionary tech iconoclast. He vouched for Snap as a better alternative to other social media sites that are “unpleasant” for users. “Fundamentally, it’s important to understand that Snapchat is not just a bunch of features,” Spiegel said. “It has an underlying philosophy that runs counter to other social media.”",Yes,It encourages people to compete for attention.,https://www.wired.com/story/snap-is-no-facebook-and-spiegel-insists-he-wants-it-that-way/,2018-05-30 20:05:11.837000+00:00,LABEL_1_relevant,0.9943053126335144,Social Media,social_media,Wired,"Information, Discourse & Governance"
138,What we can learn from the Facebook-Australia news debacle,"Australia’s approach is now being considered by lawmakers and regulators in multiple other governments. Reuters reports that Canadian heritage minister Steven Guilbeault said Canada will model its own legislation on the Australian law. There are also some similarities in a bill proposed by US congressman David Cicilline of Rhode Island that would “provide a temporary safe harbor for the publishers of online content to collectively negotiate with dominant online platforms regarding the terms on which their content may be distributed.”

In general, these measures seek to boost the bargaining power of news organizations and help them extract value from tech giants for the content that newsrooms produce. The Australian model’s novelty lies in its arbitration mechanism, a kind of membrane between the parties intended to help them arrive at a fair exchange of value.

The Australian law will likely pass, so this grand experiment in pushing capital back to the news media will soon be under way. We’ll get to see how it works out, and whether opponents’ concerns bear out—if larger news organizations are privileged over small ones, for instance, or whether the money actually ends up being spent on producing more journalism.

But in view of the objections to this approach, what other options exist? If new subscription models are not enough to sustain the media industry, what else can be done to push billions of dollars back into journalism?",Yes,"It has drastically decreased the amount of revenue that news organizations receive from advertisements. In order to sustain the media industry, new subscription models have been created. However, these models have not been effective in generating enough revenue. Some governments are now considering legislation that would require social media companies to pay news organizations for their content. The effectiveness of this approach is",https://www.technologyreview.com/2021/02/20/1019365/what-we-can-learn-from-facebook-australia-news-debacle/,2021-02-20 00:00:00,LABEL_1_relevant,0.9956542253494264,Social Media,social_media,MIT Tech Review,Economy
139,Facebook and Twitter Want to Keep the Justice System Skewed,"In recent weeks, social media executives have jumped in to pledge their support for the protest movement that emerged in response to the killing of George Floyd by Minneapolis police. In a June 1 Facebook post, Mark Zuckerberg wrote about the need to fight for racial justice and noted, “One of the areas Priscilla and I have personally worked on and where racism and racial disparities are most profound is in the criminal justice system.” On the same day, Twitter’s Jack Dorsey tweeted “Police policy reform now”; he later pledged $3 million to Colin Kaepernick’s Know Your Rights Camp. Instagram CEO Adam Mosseri recently promised to take steps to improve black users’ experience on the platform, and Google’s Sundar Pichai published a blog post this week affirming the company’s support for “organizations working to advance criminal justice reform.”

When it comes to the role these companies actually play in the criminal justice system, however, a different story emerges, according to defense attorneys. As things stand, law enforcement routinely gets access to private social media records, including direct messages, that it can use to investigate and prosecute suspects. But criminal defendants—a group in which poor people of color, particularly black people, are wildly overrepresented—can’t make the companies turn over the same information, even when it could make their case. And at this very moment, Facebook and Twitter are fighting in court to make sure this doesn’t change.

The dispute is about the Stored Communications Act of 1986, a federal law that prohibits an “electronic communications service provider” from sharing users’ communication and account records, subject to a list of exceptions. There’s an exception for law enforcement, but none for defendants. This sets up an imbalance in which privacy law is less restrictive toward the police than it is toward someone trying to prove their innocence. If you’re accused of a crime, police and prosecutors can get a warrant to access your social media accounts, or other users’ accounts that could serve as evidence in the case. But if you go to these companies with a subpoena, they won’t honor it—even if you’ve convinced a judge that the data might prove that someone else committed the crime, that a witness against you is lying, or that you acted in self-defense.

This state of affairs is intensely frustrating to defense attorneys, who in general are already outgunned by vastly better-resourced prosecutors. Jeffrey Stein, a public defender in Washington, DC, said that in serious felony cases, it’s almost automatic for the police to request and receive access to a defendant’s social media accounts. But information that could help his clients’ cases is generally out of reach.

“In a criminal case, where an individual’s life may hinge on whether the jury has a single reason to doubt a witness’s testimony, or whether to believe that your client acted in self-defense, social media often houses the most critical, case-shattering evidence,” he said in an email—hardly a situation Congress could have foreseen when it passed the law in 1986. “That is why law enforcement routinely collects it and uses it against our clients. But, as in so many other ways in our criminal system, the playing field is not level.”

To be clear, social media platforms didn’t create this imbalance. Generally, when they turn down a defendant’s request for user data, the law is firmly on their side. But occasionally, trial court judges have ruled in a defendant’s favor, usually based on the constitutional right to due process. That’s where things get interesting. In these cases, the platforms could simply comply with the order. Instead, they fight tooth and nail to resist it. In one California case, Facebook and Twitter appealed all the way to the US Supreme Court rather than hand over account information to a defendant, even after being placed in contempt for violating the trial court’s order. (The Supreme Court recently declined to take the appeal, possibly because it’s not at the right procedural step.)",Yes,"It has given law enforcement a powerful new tool for investigating and prosecuting crimes, while doing nothing to level the playing field for defendants. This has led to frustration among defense attorneys, who say that vital evidence is often out of their reach. The social media platforms themselves have contributed to this problem by fighting tooth and nail to resist court orders that would require",https://www.wired.com/story/facebook-twitter-criminal-justice-stored-communications-act/,2020-06-19 11:00:00+00:00,LABEL_1_relevant,0.9902134537696838,Social Media,social_media,Wired,"Information, Discourse & Governance"
140,Elon Musk’s “Free Speech” Social Network Mocked by Employee Who Got Fired for Saying Things Tesla Didn’t Like,"Do as he says, not as he does.

Speech

It’s all fun and free speech until someone gets hurt — especially if that someone is Elon Musk.

After tweeting that he’s “giving serious thought” to starting his own social network because Twitter fails “to adhere to free speech principles” and “fundamentally undermines democracy,” the world’s richest man was immediately called out for not practicing what he preaches.

Former Tesla employee John Bernal — who was fired earlier in March after he posted a video to his YouTube channel, AI Addict, that was critical of the company’s full self-driving (FSD) software — quipped that Musk “is a free speech absolutist… unless it involves safety concerns.”

In a video he posted before being fired, Bernal demonstrated that Tesla’s beta FSD software had serious issues, including randomly disengaging while it was driving, forcing him to scramble to take back control of the vehicle. In a later update, the former Tesla employee said he “was fired from Tesla in February with my YouTube being cited as the reason why.”

Advertisement

Advertisement

Chilling Effect

As Business Insider notes, Bernal isn’t the only person whose speech Musk has arguably chilled. In 2018, the Tesla and SpaceX CEO reportedly found the identity of a then-anonymous blogger who posted something criticizing the electric car company’s stock profile and then contacted their employer with threats of a lawsuit.

In a farewell post, the blogger wrote that they had deactivated their Twitter and would no longer write about Tesla. The blogger added that their friend had spoken by phone to Musk, who intimated that if Montana Skeptic “continued to write, he would engage counsel and sue.”

Just this month, Musk insisted that he is “a free speech absolutist,” but as Bernal and others have noted, his ideals don’t always match up to his actions.

“Elon Musk is not Tony Stark,” one user wrote. “Elon Musk is Lex Luther.”

Advertisement

Advertisement

READ MORE: Despite calling himself a ‘free speech absolutist,’ Elon Musk has a history of retaliation against employees and critics [Business Insider]

More on Musk: Elon Musk Says Sometimes He Feels Lonely

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Yes,It creates echo chambers whereby users only see content that supports their existing beliefs. This can lead to the reinforcement of false narratives and the spread of misinformation.,https://futurism.com/the-byte/elon-musk-social-network-mocked,,LABEL_1_relevant,0.9967365860939026,Social Media,social_media,Futurism,"Information, Discourse & Governance"
141,Facebook Moves to Limit Toxic Content as Scandal Swirls,"Mark Zuckerberg would like you to know that despite a scathing report in The New York Times, which depicts Facebook as a ruthless and selfish corporate behemoth, things are getting better—at least, the way he sees it.

In a lengthy call with reporters Thursday, and an equally lengthy ""note"" published on Facebook, the company's CEO laid out a litany of changes Facebook is making, designed to curb toxic content on the platform and provide more transparency into the decisions on content. But perhaps the most consequential update is that the Facebook News Feed algorithm will now try to limit the spread of sensationalist content on the platform, which represents a major change from how the social network traditionally has approached moderation. All of it is in service of restoring trust in a company whose reputation—and that of its leaders—has taken near constant body blows over the past two years.

""When you have setbacks like we’ve had this year, that’s a big issue, and it does erode trust, and it takes time to build that back,"" Zuckerberg said on the call. ""Certainly our job is not only to have this stuff at a good level and to continually improve, but to be ahead of new issues. I think over the last couple of years that’s been one of the areas where we’ve been most behind, especially around the election issues.""

These words come a day after the Times published a damning report that portrays Facebook as not merely behind on issues of election interference, as Zuckerberg suggested, but actively working to downplay what it knew about that interference. It argued that Facebook's executives, wary of picking sides in a partisan battle over Russian interference in the 2016 elections, aimed to minimize Russia's role in spreading propaganda on the platform. The story states that Facebook's former head of security, Alex Stamos, was chastised by the company's chief operating officer, Sheryl Sandberg, for investigating Russian actions without the company's approval and berated him again for divulging too much about it to members of Facebook's board.

In his remarks, Zuckerberg flatly denied this allegation. ""We’ve certainly stumbled along the way, but to suggest that we weren't interested in knowing the truth or that we wanted to hide what we knew or that we tried to prevent investigations is simply untrue,"" he said. (Stamos, for his part, tweeted earlier on Thursday that he was “never told by Mark, Sheryl or any other executives not to investigate.”)

The Times story also alleges that Facebook waged a smear campaign against its competitors through an opposition research firm called Definers Public Relations. The firm repeatedly worked to tie Facebook's detractors, including groups like the Open Markets Institute and Freedom from Facebook, to billionaire George Soros. Critics say that in doing so, Facebook engaged with the same anti-Semitic tropes that have been used by white nationalists and other hate groups that regularly vilify Soros.",Yes,"The way it has changed the way we view information.

The way we view information has changed dramatically over the past few years, thanks in large part to social media. On one hand, social media has made it easier than ever for us to find and consume information that is relevant to our interests. On the other hand, the way that information",https://www.wired.com/story/facebook-limits-hate-speech-toxic-content/,2018-11-15 23:13:29.467000+00:00,LABEL_1_relevant,0.9962140917778016,Social Media,social_media,Wired,"Information, Discourse & Governance"
142,How to manipulate Facebook and Twitter instead of letting them manipulate you,"Fight algorithms with algorithms

Algorithms on Facebook and Twitter control so much of what you see in your feed—they’re the reason, for instance, that you always seem to see posts from your best friend but not those from an old roommate you don’t interact with very much online. Or, more insidiously, why you see news only from certain sources and not from others.

You can mess with these algorithms by using a tool like Gobo, a project from researchers at the MIT Media Lab’s Center for Civic Media. Gobo connects with your Twitter and Facebook accounts and lets you precisely control your feeds in a way that the companies themselves won’t. Sliders let you adjust factors like how serious or silly the stories in your news feed are, what percentage of posts you see are from men or women, or the amount of political diversity you’re exposed to. The tool also lets you see what it removed from your feed and why.

Later, feeds

What if you’d rather just not deal with feeds at all? Try using something like Feedless, an iPhone app that blocks much of the content in Facebook, Instagram, and Twitter feeds on the iOS Safari browser (it’s free for Facebook, though you’ll have to pay $10 per year to use it to block feeds from the other two).

Once you enable it for Facebook, you will still be able to see your full profile and notifications, as well as any posts that those notifications are related to. You can also post status updates. But your news feed—that ever-present, algorithmically driven river of stuff—simply disappears.

Go niche

It’s nearly impossible for new worldwide social networks to make a dent in the market because it’s just so hard to move everyone away from the networks we’re already using.

But there are a lot of other online communities to join, many of them devoted to specific interests.

Are you into books? Try Litsy. Obsessed with design and showing off what you’re working on? Perhaps Dribbble will do it for you. Want to connect anonymously with others who work at your company so you can dish honestly about what’s going on there? Check out Blind.

Not today, advertisers

Ever wonder what kinds of data about you Twitter and Facebook share with advertisers? On Twitter, at least, it’s not that hard to figure out how ads are targeted and either change the targeting or make it impossible to target you, period.

If you click on “Your Twitter Data,” you can enter your password and see all kinds of information about yourself that the social network has collected, including a list called “Interests from Advertisers” that shows all the things those advertisers think you like. (Mine includes everything from dog treats—weird, since I don’t have a dog—to specific brands like Kleenex.)

Some of the many things Twitter advertisers think I like, including dog treats (not so much) and yogurt (absolutely). Twitter

You can disable specific items on this list by un-checking them individually, or simply turn off ad personalization altogether. To get to that option, scroll to the bottom of the list and click “Personalization and Data Settings.”

You can see and manage similar information on Facebook, too. Under “Settings,” click “Ads” to view a slew of data the social network has collected about you and shares with advertisers. Tap “Your Interests” to uncheck individual interests Facebook is using to target ads to you in all kinds of categories (everything from “Interior Design” to “Food Allergy”), or tap “Your Categories” to remove categories advertisers are lumping you into (like “Technology Early Adopters” or “Frequent Travelers”).

The nuclear option

If you’re really, truly sick of Facebook and Twitter, you can take more drastic steps, up to and including deleting your accounts—though the networks make it annoyingly difficult to do.

On Twitter, if you’re using a Web browser, click your user profile image in the upper-right part of the page, and tap “Settings and Privacy.” Once in there, tap “Account” and scroll down to the bottom of the page, where there’s a “Deactivate Your Account” link. Keep in mind that while deactivating will remove your account from Twitter, killing it off actually takes some time; regular Twitter accounts are deleted a month after deactivation, while verified accounts are deleted a year afterwards.",Yes,It can be addictive.,https://www.technologyreview.com/2018/03/20/144567/how-to-manipulate-facebook-and-twitter-instead-of-letting-them-manipulate-you/,2018-03-20 00:00:00,LABEL_1_relevant,0.9955293536186218,Social Media,social_media,MIT Tech Review,User Experience & Entertainment
143,"Russian Oligarchs’ Superyachts Are Being Seized, and the Internet Is Loving It","People on Twitter are even turning it into a game of Bingo.

Yoink

Law enforcement agencies around the world are seizing superyachts owned by Russian oligarchs — and people on social media are watching with glee, The Washington Post reports.

Government sanctions are dialing up the pressure on obscenely rich Russian oligarchs. And while money can easily be hidden in offshore bank accounts, gigantic mega vessels are a lot harder to keep secret.

So now netizens are now watching in fascination as superyachts get seized by authorities worldwide, even turning it into a game of Bingo.

That’s because trackers like VesselFinder or MarineTraffic allow practically anybody with an internet connection to monitor the whereabouts of private jets and luxury yachts.

Advertisement

Advertisement

Even 19-year-old Jack Sweeney, that kid who was keeping track of Elon Musk’s every move, now has a Twitter account dedicated to snooping on Russian zillionaire yachts.

Snatched Up

Amid Russia’s bloody invasion of Ukraine, governments are aiming their sights at “the assets Russian elites and their family members hold in our respective jurisdictions — their yachts, luxury apartments, money, and other ill-gotten gains,” according to a March 3 White House statement.

The thinking goes that if Russia’s ultra rich — many of whom are members of president Vladimir Putin’s inner circle — lose their precious toys, they might be more inclined to oppose the atrocities happening in Ukraine right now.

And yachts are indeed being picked off one by one. Just last week, French officials seized a massive $120 million superyacht owned by a Russian oil tsar.

Advertisement

Advertisement

They’re powerful symbols of excessive wealth. For many, that’s the reward for staying loyal to Putin’s regime.

No wonder people are entranced by watching the oligarchs get a much-needed reality check.

READ MORE: Schadenfreude at sea: The Internet is watching with glee as Russian oligarchs’ yachts are seized



Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.

Advertisement

Advertisement",Yes,"We are creating a generation of people who are extremely narcissistic and self-centered.

While social media does have some positive effects, such as allowing us to connect with friends and family, the negatives far outweigh the positives.

The constant need for validation, the comparison to others, and the striving for perfection can all lead to a downward",https://futurism.com/the-byte/russian-oligarchs-yachts-seized,,LABEL_1_relevant,0.9952329993247986,Social Media,social_media,Futurism,Social Norms & Relationships
144,Twitter says it may warn users about deepfakes—but won’t remove them,"The news: Twitter has drafted a deepfake policy that would warn users about synthetic or manipulated media, but not remove it. Specifically, it says it would place a notice next to tweets that contain deepfakes, warn people before they share or like tweets that include deepfakes, or add a link to a news story or Twitter Moment explaining that it isn’t real. Twitter has said it may remove deepfakes that could threaten someone’s physical safety or lead to serious harm. People have until November 27 to give Twitter feedback on the proposals.

The context: It’s become relatively easy to make convincing doctored videos thanks to advances in artificial intelligence. That’s led to a huge panic over the potential for deepfakes to subvert democracy, as they can be used to make politicians seem to say or do whatever the creator wants.",Yes,"The spread of fake news. This can have dangerous consequences, as it can lead to the spread of misinformation and potentially cause physical harm to people. To combat this, Twitter has drafted a policy that would warn users about deepfakes, but not remove them. This has caused some backlash, as many people believe that Twitter should be doing more to combat",https://www.technologyreview.com/2019/11/12/217/twitter-says-it-may-warn-users-about-deepfakesbut-wont-remove-them/,2019-11-12 00:00:00,LABEL_1_relevant,0.9965264201164246,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
145,Twitter Insiders Allegedly Spied for Saudi Arabia,"In charges released Wednesday, the Justice Department accused two former Twitter employees, Ahmad Abouammo and Ali Alzabarah, of abusing their internal system privileges to spy on target users and pass the information they collected to Saudi Arabia. The criminal complaint also alleges that it was trivial for them to do so—a chilling reminder of how much damage an insider can cause.

The court documents, first reported by The Washington Post, also reference a third suspect, Ahmed Almutairi, who allegedly worked as an intermediary between the Twitter insiders and the Saudi government. Alzabarah and Almutairi are both Saudi citizens, while Abouammo is a United States citizen. He was arrested in Seattle on Tuesday.

Alzabarah joined Twitter in August 2013 as a site reliability engineer, the complaint says, and gained more responsibility over time until he could access users accounts and personal data—like phone numbers and IP addresses—as part of his job. He also allegedly developed relationships with Saudi intelligence agents during this time, and is accused of looking up private information from more than 6,000 Twitter accounts, including those of dissidents and political activists, on Saudi Arabia's behalf over the course of a few months in 2015. Saudi Arabia is known for aggressively exerting influence and tracking detractors on social media. Crown Prince Mohammed bin Salman and his regime have also fostered close ties to Silicon Valley.

""Insiders can do major damage and often go undetected for large periods of time."" Dave Kennedy, TrustedSec

The Justice Department alleges that Abouammo accessed data from three user accounts, at least one of which was that of an outspoken critic of the Saudi royal family. But unlike Alzabarah, Abouammo's role as media partnerships manager at Twitter does not necessarily seem to necessitate access to private user data. The complaint asserts that the Saudi government wired at least $300,00 to Abouammo and his family. He left Twitter in May 2015, but allegedly still attempted to get information about users from some former Twitter colleagues. Abouammo worked for Amazon after leaving Twitter, but apparently left that job over a year ago.

Twitter said on Wednesday that it appreciated the work of the Justice Department and Federal Bureau of Investigation on the case. ""We recognize the lengths bad actors will go to try and undermine our service,"" the social media giant said in a statement. ""Our company limits access to sensitive account information to a limited group of trained and vetted employees. We’re committed to protecting those who use our service to advocate for equality, individual freedoms, and human rights.""

But the fact that even a company with the resources of Twitter was unable to head off an insider threat speaks to just how difficult they are to defend against. Most organizations are woefully under-defended against those attempts, according to multiple cybersecurity professionals WIRED spoke with Wednesday. They emphasize that the risk can never be totally eliminated, but that there are necessary data access controls and siloing efforts that many organizations overlook or implement weakly.

For example, many companies aren't strict enough about limiting which employee accounts have ""permission"" or ""privilege"" to access sensitive data.

""Privileged access is one of the toughest things in any organization and especially in tech companies,"" says Dave Kennedy, founder of TrustedSec, a cybersecurity firm that conducts so-called penetration tests, the practice of probing a system for weaknesses. ""Companies are not doing enough to protect sensitive consumer data. This is a great example with Twitter. Insiders can do major damage and often go undetected for large periods of time.""

LEARN MORE The WIRED Guide to Personal Data

Many organizations find it difficult to prioritize the work it takes to stratify employee access to data based on specific need, a process often called provisioning. Uber infamously allowed employees access to a ""God mode"" that let them track users and view their account details—a feature staffers extensively abused. On the other end of the spectrum, making it more difficult for insiders to access and exfiltrate large amounts of sensitive data is possible but takes stringent, often frustrating rules. When companies grow from relaxed small businesses or startups into massive organizations, imposing those restrictive controls can be deeply unpopular among the people who work there.

""The corporate world is coming to realize the value of focusing on foundational security principles used in the military and finance worlds,"" says Kenn White, security principal at the database company MongoDB. ""Insider threats and administrator privileged access are serious issues for any company, because at some point, you have to trust someone to manage your most trusted, confidential data. Encryption has a role, but only in a broader strategy of separation of duties and compartmentalization.""",Yes,Our private lives are now public. We are constantly sharing information about ourselves that we would never share in person. This is leading to a loss of privacy for everyone. The other main unintended consequence is that social media is being used by governments and corporations to track and influence people.,https://www.wired.com/story/twitter-insiders-saudi-arabia-spy/,2019-11-07 03:31:14.560000+00:00,LABEL_1_relevant,0.9964261651039124,Social Media,social_media,Wired,Security & Privacy
146,The Moral Panic over Social-Networking Sites,"It’s a “monumentally ill-considered piece of legislation” that “by any rational measure” should never have left the House, says Henry Jenkins, professor of literature and director of the Comparative Media Studies Program at MIT. Jenkins believes the act plays on parents’ lack of understanding, and their resulting fears, about their kids’ activities on the Internet. “But the price of standing up to that fear may be too high for liberal Democrats,” he says.

If the Senate approves a similar bill and the legislation reaches President Bush’s desk, the price to young people will be even higher, say Jenkins and other critics. “If it would actually prevent predation, I would be fine with it,” says Danah Boyd, a PhD candidate in the School of Information Management Sciences at the University of California, Berkeley, who is considered one of the leading scholarly authorities on social-networking sites. “But it’s not going to help at all. Out of 300,000 child abductions every year, only 12 are by strangers. This is just going to stifle the social-networking industry and completely segment youth around economic status.”

The impact on youth from economically disadvantaged families is what Jenkins worries about most. “Already, you have a gap between kids who have 10 minutes of Internet access a day at the public library and kids who have 24-hour-a-day access at home,” he says. “Already, we have filters in libraries [required under the Child Internet Protection Act of 2001] blocking access to much of the Internet. Now we’re talking about adding even more restrictions. It exaggerates the ‘participation gap’ – not a technology gap, but a difference in access to the defining cultural experiences that take place around technology today.”

Current Internet filters at schools and libraries – some aimed at pornography and obscene materials, some already targeting social-networking sites – have “a tremendous chilling effect on education,” agrees Jeff Cooper, an educational-technology consultant and former high-school teacher in Portland, OR. “The ‘Just Say No’ philosophy has never worked,” Cooper says. “You’re lumping all social networking into the negative basket, and not giving kids any alternative. But there is so much good stuff online that nobody ever talks about.”

Indeed, while it might be easy to agree that teens shouldn’t be wasting time on MySpace or other social-networking sites while they’re at school, DOPA would cover any site that allows networking and chatting. As one example, Cooper points to TappedIn.org, a social-networking and professional-development site for teachers. Students often use personal and public “rooms” on the site as part of virtual classroom activities. “It allows teachers to bring their students online in a very safe and secure environment,” explains Cooper. “My concern isn’t really that MySpace won’t be accessible from schools, but that other sites like TappedIn will be banned.”

DOPA supporters frequently cite a 2000 report about online sexual victimization funded by the National Center for Missing and Exploited Children, which concluded that one-fifth of children have been sexually solicited in chat rooms, by instant message, or by e-mail. But in fact, as Boyd and other opponents point out, the same report states that most solicitations come from other young people – only 4 percent are from adults over 25 – and that most kids deal with these solicitations simply by not answering or logging off. “To clamp down on a bunch of new networking sites really doesn’t do anything” to stop sexual predators, says Cooper. “You might as well shut off the Internet entirely.”

Opponents of DOPA misunderstand the bill, says Jeff Urbanchuck, a press officer for Representative Fitzpatrick. He says it is intended only to reduce the risk to teens from one particular category of websites – those where members can create online profiles and fill them with personal details, including e-mail or instant-messaging addresses, that help predators contact them. Critics are “extending beyond the MySpaces and Facebooks and arguing that the technology of social networking is so pervasive now that the Internet is going to become one big social-networking site,” Urbanchuck says. “But the objective of the bill is to deal with the growing threat of online predators on specific sites that allow profiles. We want to tailor the bill to those sites.”

Even banning access just to sites that allow profiles, however, would affect scores of educational, community, and media-sharing sites, including sites as popular as Flickr and as specialized as TappedIn. And in the longer term, predicts Boyd, the law would simply drive teen networking underground, where it would be more difficult for adults to monitor. “They’ll be moving from site to site with a level of ephemerality that no one can keep up with,” she says. “Not the cops – not even the designers of the technology.”",Yes,"The potential for a 'chilling effect' on education. If the Senate approves a similar bill and the legislation reaches President Bush’s desk, the price to young people will be even higher, say Jenkins and other critics.",https://www.technologyreview.com/2006/08/07/228481/the-moral-panic-over-social-networking-sites/,2006-08-07 00:00:00,LABEL_1_relevant,0.9960076808929444,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
147,We've Got the Screen Time Debate All Wrong. Let's Fix It,"In 1995, New York City psychiatrist Ivan Goldberg logged onto PsyCom.net, then a popular message board for shrinks, to describe a new disease he called ""internet addiction disorder,"" symptoms of which, he wrote, included giving up important social activities because of internet use and ""voluntary or involuntary typing movements of the fingers.""

It was supposed to be a joke.

But to his surprise, many of his colleagues took him seriously. Their response led him to create an online support group for internet addicts—though he quickly downgraded the affliction, renaming it ""pathological internet-use disorder."" The word addiction ""makes it sound as if one were dealing with heroin, a truly addicting substance,"" Goldberg told the New Yorker in 1997. ""To medicalize every behavior by putting it into psychiatric nomenclature is ridiculous.""

LEARN MORE The WIRED Guide to Internet Addiction

Today, more than two decades after Goldberg's joke fell flat, mental health professionals find themselves in a similar bind. Public anxiety over the side effects of screen time—the hours we spend staring at our various devices—is the highest it's been in years. That anxiety has manifested in the form of self-help books, social movements, major media outlets foretelling the ""the worst mental-health crisis in decades,"" and no shortage of guilt. (You let your kid play with an iPad at restaurants? You spent 30 minutes browsing Instagram when you could have been exercising? Or playing board games with your family? Or learning a second language? You sad/selfish/lonely monster!) And yet, there exists little clear evidence that we are locked in an unambiguously harmful relationship with our devices—let alone addicted to them in any clinical sense. ""For the past twelve months, the narrative surrounding technology use and screen time has been consistently negative, but it's been driven more by fear than facts,"" says UC Irvine psychologist Candice Odgers.

Experts like Odgers say we'll never get good answers about the effects of screen time, unless we start asking better questions. And that means being honest with ourselves about what we mean by ""screen time"" in the first place.

This year, the conversation around digital dependence entered a new phase when Facebook CEO Mark Zuckerberg resolved to spend 2018 fixing Facebook, vowing, among other things, to ensure that time spent on the social network would be ""time well spent."" (Zuckerberg borrowed the phrase from former Google design ethicist Tristan Harris, who has popularized the term in recent years by characterizing it as the opposite of time surrendered involuntarily to devices, apps, and algorithms designed to ""hijack our minds."") A few days after Zuck's post went public, major Apple shareholders urged that company to study its products' effects on children and equip parents with better tools for managing their kids' screen time. The following month, Harris formed the Center for Humane Technology—an alliance of tech-giant turncoats united in opposition against the attention-grabbing products they helped create.

These events helped set the tone of the year to come. ""For right or wrong, big tech companies have seen which way the wind is blowing and responded,"" says Andrew Przybylski, an experimental psychologist at the Oxford Internet Institute. Google led the charge, pledging its commitment to digital well-being and releasing new tools designed to help Android users monitor their tech habits. Apple followed suit, unveiling features designed to help users understand and manage the time they spend on their iOS devices. Then came Facebook and Instagram, each of which released features designed to help users track, and set limits on, the time they spend in-app.

Want more? Read all of WIRED’s year-end coverage

None of those companies has shared whether its tools have been effective. It's possible they never will release any data—and even if they do, researchers say, taking them at face value could be hard. ""There wasn't a good empirical basis for their creation in the first place, so there probably won't be good evidence for their effectiveness,"" Przybylski says.

Look what happened earlier this year, when Congress asked the National Institutes of Health what science had to say about tech's addictive potential and the effects of screen time on kids. NIH director Francis Collins' response amounted to a big shrug: What limited research exists has been inconclusive, he wrote. Tech addiction? Scientists don't even agree on how to define it, let alone measure it. As for screen time's impact on developing minds, Collins said researchers are still gathering evidence on how best to balance technology's ""obvious benefits"" with its ""potential harms.""",Yes,"It makes us feel bad.

At the same time, many experts have been quick to point out that the link between screen time and mental-health problems is far from clear. In a recent paper, Jim Steyer—the CEO of Common Sense Media, a nonprofit that aims to help kids thrive in a world of media and technology—",https://www.wired.com/story/year-in-review-tech-addiction-debate-all-wrong/,2018-12-20 13:00:00+00:00,LABEL_1_relevant,0.9827038645744324,Social Media,social_media,Wired,"Information, Discourse & Governance"
148,Facebook Rolls Out More Features for Dead People,"Facebook was designed for the living, but as the social network grew older, it also needed to decide what should happen when users die. In 2015, Facebook began allowing people to assign a legacy contact to be in charge of their account in the event they pass away, but the system wasn't perfect and users found some of the associated policies upsetting.

The company announced Tuesday that it will give the legacy contact more control over the deceased’s presence on Facebook, including the ability to manage a new “Tributes” section where loved ones can reminisce about the person they lost. The feature underscores how Facebook wants to be the place for life’s most important events, even when that event is death.

“I had very close friends, particularly one, who had lost people and told me how important Facebook was to preserving their memories,” says Sheryl Sandberg, Facebook’s COO. “My friend Kim had lost her brother, he died by suicide, and she and her mother were not up to having a memorial service, so Facebook became that memorialization place for them.”

Louise Matsakis covers cybersecurity, internet law, and online culture for WIRED.

Facebook has encouraged its users to share major life milestones on the platform, like when they get married, have a kid, or get a new job. It’s no longer just a social network; it’s also a scrapbook. When users die, they may leave behind accounts containing over a decade of memories, and they might not have specified how they want that archive to be maintained. Facebook has to balance the interests of grieving loved ones with the privacy of someone who is now dead, says Alice Ely, a product manager at Facebook.

“We have a hard stance against never allowing anybody to log into [someone else’s] account, but that’s exactly why we wanted to create a legacy contact to give [people] these controls to manage the profile after the person has passed away,” she explains.

The updates Facebook announced Tuesday will address some of the complaints raised by users who have lost someone close to them. Before a legacy contact can take over managing an account, it must be “memorialized,” meaning Facebook has verified the person is actually dead. In the past, anyone could memorialize an account, for example by sending Facebook a news story, and the process sometimes happened before the person’s loved ones were ready. Now Facebook says it will allow only friends and family to request to have an account memorialized.

Facebook doesn’t allow users under 18 to appoint a legacy contact, but going forward parents who have lost their children can apply to be theirs. Previously, parents didn’t have the option of changing anything on their child’s profile after they passed away; it remained frozen in time. “We had one instance where parents wrote in to us and their daughter’s profile picture was a picture of her at a Halloween party. It was really distressing to see because she had died at that point, and they didn’t want to remember her in this Halloween costume,” says Ely. “That was one instance where we allowed them to become the legacy contact, and going forward this will be the policy.”

Facebook

Once a person’s account is memorialized, the designated legacy contact can update their profile picture or cover photo and write a pinned post to their profile, for example to share information about a memorial service. They can’t read private messages or delete anything. As of Tuesday, the legacy contact will also have the option to manage a Tributes section, a separate portion of the deceased’s Facebook profile where friends and family can share content about their loved one. The legacy contact will have the ability to edit who can post to Tributes and who can see what content is shared there, among other controls.

Tributes is a dedicated section separate from a dead person’s Facebook timeline. The posts shared to it won’t show up alongside things like status updates or photos uploaded while the person was still alive. The section functions like a digital memorial or gravestone, which the legacy contact is in charge of maintaining.",Yes,"It can have a negative impact on people's social lives and real-world relationships. Some research has found that social media can make people feel more isolated and less connected to others, while other studies have found that social media can actually help people connect and build relationships.",https://www.wired.com/story/facebook-rolls-out-more-features-dead-people/,2019-04-09 13:00:00+00:00,LABEL_1_relevant,0.9931916594505308,Social Media,social_media,Wired,"Information, Discourse & Governance"
149,"As EU says it'll ban Russia's 'toxic media machine', social media firms face pressure to act","Late yesterday the European Union announced an “unprecedented” step against Russian disinformation, saying it would ban Kremlin-based media outlets, Russia Today (aka RT) and Sputnik — extending measures targeting the country following president Putin’s invasion of Ukraine.

“The state-owned Russia Today and Sputnik, as well as their subsidiaries will no longer be able to spread their lies to justify Putin’s war and to saw division in our Union,” said the EU’s president, Ursula von der Leyen. “So we are developing tools to ban their toxic and harmful disinformation in Europe.”

Details on the EU’s planned ban on Russian state media content are still thin on the ground.

At the time of writing EU officials we’ve spoken to were unable to confirm whether or not the ban will extend to online platforms such as Google’s YouTube — where RT and Sputnik both have a number of channels, and the US platform hosts thousands of videos they upload for viewing on demand as well as enabling them to reach viewers via livestreaming.

However in the past few hours EU officials have been cranking up the public pressure on mainstream tech platforms on the disinformation issue.

Today the office of internal market commissioner Thierry Breton announced that both he and the values & transparency commissioner, Vera Jourova, had spoken to the CEOs of Google and YouTube urging them to step up efforts against Russian propaganda.

EU’s @ThierryBreton & @VeraJourova spoke with Google & YouTube CEOs, urged online platforms to step up efforts to address Russian war propaganda 👇#EUvsDisinfo 🇪🇺 🇺🇦 https://t.co/8d9hUk8Q8u — Lucía Caudet (@LuciaCaudet) February 28, 2022

On Saturday Google announced it had demonetized RT and other Russian-backed channels, meaning they can no longer receive ad revenue via its platforms or ad networks. But in a video call with the two tech CEOs Breton it reported to have said they need to go further, per Reuters.

“Freedom of expression does not cover war propaganda. For too long, content from Russia Today and other Russian state media has been amplified by algorithms and proposed as ‘recommended content’ to people who had never requested it,” the commissioner said in a statement after the call.

“War propaganda should never be recommended content — what is more, it should have no place on online platforms at all. I count on the tech industry to take urgent and effective measures to counter disinformation.”

The EU’s high commissioner for foreign affairs, Josep Borrell, was also asked for details of the ban by the BBC this morning and also declined to specify if it will apply online.

“We’ll do whatever we can to prevent them to disseminate [toxic content to] a European audience,” he told BBC Radio 4’s Today program when questioned on how workable a ban would be, adding: “We have to try to cut them.”

He also aggressively rebutted a line of questions querying the consistency of the EU’s move vis-a-vis Western liberal democratic principles which center free speech — saying the two channels do not distribute free information but rather pump out “massive disinformation” and create an “atmosphere of hate” that he asserted is “against the freedom of thinking” and is “toxifying minds”.

“If you start telling lies all the time, if you create an atmosphere of hate this has to be forbidden,” Borrell added.

The EU’s previous proposals for dealing with online disinformation have largely focused on getting voluntary buy-in from the tech sphere, through a Code of Practice on disinformation. However it has been pressing for tougher action in recent years, especially around COVID-19 disinformation which presents a clear public safety danger.

But an out-and-out ban on media entities — even those clearly linked to the Kremlin — is a major departure from the usual Commission script.

Of the two sanctioned channels, Russia Today appears to garner the most eyeballs on YouTube — where its main channel has some 4.65M subscribers, although content produced by the state-backed media outlet can be found far more widely across the platform.

The channel’s marketing even claims it is “the most watched news network on YouTube” — with a touted 10BN+ views — which looks more than a little awkward for Google in the context of Putin’s land war in Europe.

We reached out to Google to ask if it intends to take further action against the Kremlin-backed media channels in light of the EU’s decision to ban them.

At the time of writing the company had not responded but we’ll update this report if it does.

While the Kremlin deploys the very thinly veiled camouflage of (claimed) media operations to launder its propaganda as professional news commentary, its infowars tactics are of course far more sprawling online — where multiple user generated platforms provide Putin with (essentially) limitless opportunities to pass his influence ops off as genuine public opinion by making his propaganda look like organic user generated content.

That means, even if pressure from the EU forces mainstream platforms like YouTube to ban RT and Sputnik it can’t hope to stop the Kremlin’s propaganda machine — it will just drive Russia to produce more less directly attributable propaganda, via sock puppet/fake accounts and the like.

And — notably — among the new sanctions that the EU announced at the weekend it added the notorious Russian troll factory, the Internet Research Agency, to the expanded list — along with its oligarch funder.

Although that took isn’t likely to be the only troll farm operating at Russia’s bidding, even if it is the most well known. Reports have long suggested Russian’s web brigade has used outsourcing tactics to try to better cover its manipulative tracks, for example.

In a small sign of some of the less immediately visible Kremlin-backed propaganda activity dialling up around the Ukraine war, Facebook’s parent company Meta put out an update today — saying the teams it has monitoring disinformation (aka “coordinated inauthentic behavior”, as Facebook terms it) have been on “high alert” since the invasion, and have taken down a network, of circa 40 accounts/Pages/Groups on Facebook and Instagram, being run out of Russia, which had been targeting people in Ukraine.

In a neat illustration of the malicious duality that can be applied to even disinformation reporting tools, Meta said the Russian network it identified had been reporting Ukrainians for violating its policies on coordinated inauthentic behavior — including by posing as independent news outlets.

“They ran websites posing as independent news entities and created fake personas across social media platforms including Facebook, Instagram, Twitter, YouTube, Telegram and also Russian Odnoklassniki and VK,” write Nathaniel Gleicher, Meta’s head of security policy, and David Agranovich, a director, threat disruption.

“They were operated from Russia and Ukraine and targeted people in Ukraine across multiple social media platforms and through their own websites. We took down this operation, blocked their domains from being shared on our platform, and shared information with other tech platforms, researchers and governments. When we disrupted this network on our platform, it had fewer than 4,000 Facebook accounts following one of more of its Pages and fewer than 500 accounts following one or more of its Instagram accounts.”

Meta also said some of the accounts it found using fictitious personas had also used profile pictures which is suggests were generated using artificial intelligence techniques like generative adversarial networks (GAN).

“They claimed to be based in Kyiv and posed as news editors, a former aviation engineer, and an author of a scientific publication on hydrography — the science of mapping water. This operation ran a handful of websites masquerading as independent news outlets, publishing claims about the West betraying Ukraine and Ukraine being a failed state,” it adds.

“Our investigation is ongoing, and so far we’ve found links between this network and another operation we removed in April 2020, which we then connected to individuals in Russia, the Donbass region in Ukraine and two media organizations in Crimea — NewsFront and SouthFront, now sanctioned by the US government.”

Meta’s security update also warns that it’s seen increased targeting of people in Ukraine — including Ukrainian military and public figures — by Ghostwriter, a threat actor it notes has been tracked for some time by the security community.

“We detected attempts to target people on Facebook to post YouTube videos portraying Ukrainian troops as weak and surrendering to Russia, including one video claiming to show Ukrainian soldiers coming out of a forest while flying a white flag of surrender,” it notes, adding: “We’ve taken steps to secure accounts that we believe were targeted by this threat actor and, when we can, to alert the users that they had been targeted. We also blocked phishing domains these hackers used to try to trick people in Ukraine into compromising their online accounts.”

Twitter also confirmed that it has taken some action against suspected Russian disinformation on its platform since the invasion when we asked.

A spokesperson for the social network told us:

On Feb. 27, we permanently suspended more than a dozen accounts and blocked the sharing of several links in violation of our platform manipulation and spam policy. Our investigation is ongoing; however, our initial findings indicate that the accounts and links originated in Russia and were attempting to disrupt the public conversation around the ongoing conflict in Ukraine. As is standard, further details will be shared through our information operations archive once complete.

Twitter said it will be continuing to monitor its platform for “emerging narratives” which violate platform rules as the situation in Ukraine develops — such as rules on synthetic and manipulated media and its platform manipulation policy — adding that it will take enforcement action when it identifies content and accounts that violate its policies.

The platform does already label state-affiliated accounts belonging to the Russian Federation.

Meta, meanwhile, faced restrictions on its service inside Russia Friday — after the state internet regulator appears to have retaliated over fact-checking labels Facebook had placed on four Kremlin-linked media firms.

On Saturday, Twitter also said access to its service had been restricted in Russia following street protests against the war.

Both Meta and Twitter are urging users to beef up their account security in light of the threat posed by Russian cyberops, suggesting people be cautious about accepting Facebook friend requests from people they don’t know, for example, and implementing 2FA on their accounts to add an additional security layer.

We also reached out to TikTok to ask if it’s taken any measures against Russian propaganda but at the time of writing it had not responded.

The Russian military has been a keen user of TikTok in recent years as Putin’s propaganda tacticians leveraged the viral video clip sharing platform to crank up a visual display of power which looked intended to psyche out and sap the will of Ukraine to resist Russian aggression.

Albeit, if anti-Ukraine ‘psyops’ was the primary goal of Russian military TikTokking the tactic appears to have entirely failed to hit the mark.",Yes,"We are no longer as easily controlled by governments and large corporations as we once were. The internet gives a voice to the voiceless, and a platform for the oppressed to share their stories.",https://techcrunch.com/2022/02/28/eu-rt-sputnik-ban-social-media-pressure/,2022-02-28 00:00:00,LABEL_1_relevant,0.9964003562927246,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
150,Instagram now lets users reply to comments with Reels,"Instagram has rolled out a new “Reels Visual Replies” feature that lets users reply to comments on posts with Reels. Now, when users choose to reply to a comment, they’ll see a new option that allows them to select the Reels button to create a video reply. The social media giant notes that the video reply will appear as a sticker.

“We love the communities that creators have built on Instagram,” Instagram said in a tweet announcing the new feature. “That’s why we’re excited to launch Reels Visual Replies, a new feature to interact with your audience.”

We love the communities that creators have built on Instagram. 😊❤️ That’s why we’re excited to launch Reels Visual Replies, a new feature to interact with your audience. You can now reply to comments with Reels and the comment will pop up as a sticker. pic.twitter.com/dA3qj1lAwE — Instagram (@instagram) December 10, 2021

Instagram’s newest feature further integrates Reels into its wider social media platform. It’s worth noting that the new feature is similar to TikTok’s video replies, which allow users to reply to comments on their content with video responses. TikTok rolled out the feature last year to give users a way to offer more context about their videos and answer questions. Considering the popularity of the feature, it’s no surprise that Instagram has launched a similar feature for its own TikTok-clone.

The launch of Reels Visual Replies comes as Instagram has been rolling out several new features. Most recently, the company introduced a new end-of-year “Playback” IG Stories time capsule feature. The feature leverages the Stories Archive, so users can customize and select up to 10 stories to share with their followers. To be eligible for this feature, users need to have posted more than three stories this year or have Stories Archive turned on. Instagram suggests Playback posts for you, but you also have the option to pick what you want to share. The new feature is currently available to users for “several weeks.”

Instagram has also rolled out a new “Rage Shake” feature that lets users shake their phones to report a problem with the app. Once you shake to report, you can explain what happened with the app and report the issue. The social media giant also introduced the ability to delete single items from a carousel post with at least three images or videos. For context, carousel feed posts can combine up to 10 photos and videos in one post. With this update, you can delete items from an existing carousel.

The company is also testing a new feature called “Take a Break,” which allows users to remind themselves to take a break from Instagram after either 10, 20 or 30 minutes. There’s also a new “Add Yours” sticker that creates public threads in Stories. The new feature allows users to respond to other users’ Stories with their own following a prompt or a certain topic.",Yes,It can lead to addiction.,https://techcrunch.com/2021/12/13/instagram-now-lets-users-reply-to-comments-with-reels/,2021-12-13 00:00:00,LABEL_1_relevant,0.9946584105491638,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
151,I Woke Up to Australia's Newsless News Feed,"By late Tuesday afternoon, Sydney time, Facebook had announced it would restore access to news on its platform in Australia. But logging on five days after its surprise ban, there was still no news. We cannot share it. We cannot see it.

It will return soon, we are told, but for now, when I log on to Facebook, this is what I see:

WIRED OPINION ABOUT Celina Ribeiro is an Australia-based freelance writer whose work has appeared in The Guardian, the BBC, Australian Financial Review and The Saturday Paper.

A friend from London has taken a picture of crocus blooming in a field. Someone in my neighborhood is selling an Ikea table and two chairs for $100. Someone else nearby is selling a baby carrier for $50. A flatmate from my university days went out with friends on the weekend and took a photo. My mother’s best friend likes a video about Gordon Ramsay’s pick-up lines. A man I met on holiday in Turkey wishes a friend he met in Lisbon a happy birthday. My local café has expanded its range of house-made lemonades. And my high school drama teacher has bought many, many pieces of bargain vintage furniture and homeware.

Facebook without news is different.

Last Thursday morning local time, amid a global pandemic and ahead of a state election, millions of Australians woke to find their Facebook feed so altered. The move, Facebook said, was one it made with a “heavy heart” ahead of the federal government putting its media bargaining code, which seeks to make Google and Facebook negotiate payment with news publishers in exchange for their content, for a vote in the upper house.

At the core of the Australian move is the argument that both news publishers and the platforms benefit from content being shared on Facebook and Google. But given that the tech giants are essentially gatekeepers of the internet, news publishers are unable to negotiate with them for what may be a fair price for their content. Australia argues this code redresses that imbalance.

Now the government has made some seemingly minor concessions to its law. News is coming back. But Facebook has made clear: It can, and may, take news away again.

As we first discovered our News Feeds void of news last week—with Facebook pages belonging to the likes of Guardian Australia emptied and featuring a gray, bold-type claim that there were “No posts yet”—there erupted a broad sense of disbelief and indignation. The tech giant had not only blocked news, but apparently unintentionally blocked numerous other information pages including the national Bureau of Meteorology, which issues emergency weather warnings; government health departments; charities; and a state opposition leader’s Facebook page just weeks before an election. On Monday the still-blocked Australian Medical Association flipped to Twitter to plead “Hey @facebook—we’re not a media company, we’re doctors and this is a pandemic—how about restoring our content?”

My own feed became full of friends declaring that Facebook had instantly become boring and companies trying to sell me stuff, or as one friend in Melbourne put it: “a featureless wasteland, populated by boomer memes and dog photos.” I instantly began to regret nearly every Facebook group I had ever joined. And I have no idea why I ever voluntarily opted to like a company’s page.

And now, as Australians drown in dog photos and 30-second captioned made-for-social videos, news is to return to Australia. That’s good. But we know this isn’t over.",Yes,"It creates echo chambers in which people only see information that aligns with their own beliefs.

Facebook without news is different. It’s a place where people can only share information that they agree with, and that’s not healthy.",https://www.wired.com/story/newsless-news-feed-facebook-australia/,2021-02-23 13:00:00+00:00,LABEL_1_relevant,0.9955190420150756,Social Media,social_media,Wired,"Information, Discourse & Governance"
152,Why Elizabeth Warren's Feeds Are Flooded With Snake Emoji,"Senator Elizabeth Warren’s social media feeds are crawling with snakes. If you scroll through the replies to every new tweet and the comments of her most recent Instagram posts, you won’t find much discussion about the actual content, whether its donations or the student loan debt crisis. Instead, it’s line after line of acid-green snake emoji, intermixed with people apologizing for the people inundating Warren’s accounts with snake emoji. Lest ye think that Senator Warren’s campaign for president has suddenly become much beloved by reptile enthusiasts, #NeverWarren is also trending, alongside #WarrenIsASnake.

The people slinging snake emoji are mostly supporters of Senator Bernie Sanders’ campaign for president. (Or so they claim. This is still the internet, after all.) On Monday, CNN reported (and Warren confirmed) that, during a private meeting between the two in 2018 during which they discussed their presidential ambitions, Sanders stated that he did not think a woman could win the election. Sanders has denied making this comment, calling it “ludicrous” in a statement to CNN and “incomprehensible” when asked about the situation during last night’s Democratic debate. Warren again disagreed with Sanders’ account, but said she was “not here to fight with Bernie.”

Still, the atmosphere between the senators, who are friends, looked tense. As the debate came to an end, an image of the pair having a terse discussion (with Tom Steyer looming between them like a nonsequitur stork in a suit) has become a meme of its own. But it’s nowhere nearly as ubiquitous as the snakes.

Twitter content This content can also be viewed on the site it originates from.

Warren is not the first woman to be visited with a social media plague of snake emoji in the midst of a controversy. Calling someone a snake is hardly a social media invention, but employing the snake emoji as a form of memetic harassment started with popstar Taylor Swift. In July 2016, people began spamming Swift’s accounts with snake emoji because they believed she was duplicitous—partially due to situations with ex-boyfriend Calvin Harris and Katy Perry, but mostly because of a longtime feud with Kanye West (and, by extension, Kim Kardashian West). After the release of West’s single “Famous,” Swift said that she was offended by the song’s lyrics, in which West both takes credit for Swift’s fame and says he thinks the pair might sleep together. In response, Kardashian West posted audio on Snapchat of Swift seeming to approve the lyrics prior to the track’s release. She also joined incensed fans in the snake emoji flood.",Yes,"It allows for the spread of information and memes at an incredibly rapid pace. This can obviously be a good thing, as it allows for the rapid dissemination of news and information. However, it also allows for the rapid spread of misinformation and memes that can be used to harass and bully people.",https://www.wired.com/story/elizabeth-warren-snakes-meme/,2020-01-15 20:59:09.615000+00:00,LABEL_1_relevant,0.9959132075309752,Social Media,social_media,Wired,"Information, Discourse & Governance"
153,Trump’s Latest Tariffs Could Hurt Tech—and Even Social Media,"Do you like your smartphone? Your television? Your social network? Whether or not they proudly boast of being “Designed in California,” all rely to some extent on China’s technology and manufacturing industries. And all could potentially be affected by a new campaign of economic sanctions on China announced Thursday by President Trump.

Speaking at the White House, the president said that his administration will slap tariffs on “about $60 billion” worth of goods imported from China each year. He argued the levies are necessary to compensate for how China has slurped up the fruits of American invention by nefarious means. “We have a tremendous intellectual property theft situation going on,” Trump said.

Where exactly the 25 percent levies will fall is still to be worked out, but they will focus on technologies in areas including computing and communications, industrial machinery such as factory robots, and aerospace.

The action illustrates the centrality of the technology industry and new developments such as artificial intelligence to the economic and national security strategies of the world’s superpowers. The sanctions plan also creates a headache for US companies reliant on Chinese circuit boards, servers, chips, or manufacturing plants—and potentially their customers, too.

The effects could be felt beyond hardware companies. For example, Chinese suppliers are important to the Open Compute Project that internet companies such as Facebook, Google, and Microsoft support to make servers and other data center equipment cheaper.

Silicon Valley lobbying group The Internet Association, whose members include Google, Amazon, and Facebook, said Thursday that it fears knock-on effects such as higher prices for consumers, and industry job losses. Adam Segal, who tracks Chinese technology policy at the Council on Foreign Relations, says the announcement fired the starting gun on a backroom battle over the products that will be subject to the tariffs. “There’s going to be a huge amount of lobbying now as specific sectors try to make sure they’re not caught up in the thing,” he says.

Trump’s new sanctions are informed by a 215-page report into China’s technology strategy by the Office of the United States Trade Representative. It concludes that the country’s government uses cyber espionage, investments and acquisitions in the US, and pressure on American companies operating in China to collect valuable US intellectual property. The report says that since 2010 China has made, and then broken, eight separate commitments to reform its policies on technology transfer.

Speaking alongside Trump at the White House Thursday, US Trade Representative Robert Lighthizer summed up the findings as portraying an assault on America. “Technology is really the backbone of the future of the US economy,” Lighthizer said. “China [has] a policy of forced technology transfer.”’ Lighthizer said his agency will also ask the World Trade Organization to take action against China’s technology licensing policies.

In order to operate in China, overseas companies must often transfer intellectual property, or even technology assets, to a locally owned partner. The trade representative’s report cites a recent raft of tough new rules for overseas cloud computing providers operating in China, which effectively force them to transfer their hardware and operations to local companies. Amazon made that move last November.",Yes,It can lead to addiction and isolation.,https://www.wired.com/story/trumps-latest-china-tariffs-could-hurt-techand-even-social-media/,2018-03-22 23:19:53.139000+00:00,LABEL_1_relevant,0.8711563944816589,Social Media,social_media,Wired,"Information, Discourse & Governance"
154,'Fake News Victims' Meet With Twitter and Facebook,"On paper, they would seem to have little in common. Tun Khin is a human rights activist who advocates for the persecuted Rohingya Muslims in his home country of Myanmar. Jessikka Aro is a Finnish journalist who exposed the international influence of Russian propagandists at the Internet Research Agency long before the rest of the world had ever heard of them. Lenny Pozner is an American father who lost his 6-year-old son, Noah, in the shooting at Sandy Hook Elementary in 2012. Ethan Lindenberger is almost a kid himself, a high school student who’s become a vaccination proponent despite his parents’ anti-vaccination beliefs.

But all four of them are bound by one unfortunate and common thread: They’ve all seen firsthand just how ugly---and downright dangerous---the spread of fake news and disinformation online can be. Which is why this week, they gathered in Silicon Valley to talk with tech executives about what they’ve been through and what they want tech companies to do about it. The group met with Twitter on Tuesday, and another meeting was planned at Facebook Wednesday afternoon.

The meetings, which were organized by a nonprofit advocacy group called Avaaz, come at a time of fierce debate over what responsibility tech companies have to limit the spread of toxic content on their platforms. Just last week, Facebook announced it was banning seven people, including Infowars conspiracy theorists Alex Jones and Paul Joseph Watson, under a policy that prohibits “dangerous individuals” from having any presence on Facebook. The bans prompted President Trump to lash out against tech companies over the weekend, ramping up accusations of censorship that have become a constant drumbeat on the right.

The discussions organized by Avaaz served as a counterpoint to all that pressure, as individual victims of online harassment campaigns came forward to tell tech companies exactly how they’ve been hurt by the hate and hoaxes that have festered on their platforms. “Our job as advocates is to make them stop for a minute and think about the implications of not acting fast enough,” says Oscar Soria, a senior campaigner with Avaaz.

Issie Lapowsky covers the intersection of tech, politics, and national affairs for WIRED.

During Tuesday’s meeting with Twitter, the attendees took turns telling their stories. Aro shared the details of the global smear campaign that was lodged against her, after her reporting outed the Internet Research Agency. She explained the threats that have been made against her life and read a recent direct message she received while traveling in the Czech Republic, in which a stranger threatened to “castrate” her if she ever came back to the country.

Aro says the harassment she’s received violates Finnish defamation laws, and she is in the process of pursuing cases against some of her harassers in court. And yet, she says, the complaints she’s filed to Twitter and Facebook often go unanswered, leaving local investigators to do the work the American companies won't. “I'm basically here, to put it simply, to give a user report live, because they haven't reacted to the ones that I have made online,” Aro says.

Khin described the trauma he’s seen in Rohingya refugee camps and pressed Twitter about why it continues to provide safe haven for Senior General Min Aung Hlaing, the commander-in-chief of the Myanmar military. The military was behind some of the accounts that notoriously flooded Facebook with anti-Islam rhetoric, and the United Nations called for its leaders to face genocide charges last year. Facebook has since banned Min Aung Hlaing and other accounts and pages that the UN linked to human rights abuses in the country. While the general's Twitter account hasn’t been active since last year, it remains up on the platform today.",Yes,"The spread of false information.

One of the most significant problems associated with social media is the spread of false information. This is often perpetuated by people sharing inaccurate news stories or rumors without verifying the information. As a result, people can mistakenly believe false information to be true. This can lead to serious consequences, such as making poor decisions based",https://www.wired.com/story/fake-news-victims-meet-twitter-facebook/,2019-05-08 23:15:54.246000+00:00,LABEL_1_relevant,0.9962363839149476,Social Media,social_media,Wired,"Information, Discourse & Governance"
155,Can the Wisdom of Crowds Help Fix Social Media’s Trust Issue?,"Social media misinformation outrage cycles tend to go through familiar phases. There’s the initial controversy over some misleading story that goes viral, then the platform’s response. Then someone asks “What about Fox News?” Finally, someone points out that the real problem, as far as social media is concerned, is the algorithms that determine who sees what. Those algorithms are primarily optimized for engagement, not accuracy. False and misleading stories can be more engaging than true ones, so absent some intervention by the platform, that’s what people are going to see. Fixing the algorithm, the argument goes, would be a better way to deal with the problem than taking down viral misinformation after the fact.

But fix it how? To change the ranking to favor true stories over false ones, say, the platforms would need a way to systematically judge everything that gets shared, or at least everything that gets shared a nontrivial amount. The current prevailing approach to false material involves punting the judgment to some outside party. Facebook, for example, partners with organizations like Factcheck.org to determine whether a given link merits a warning label. Twitter builds its fact-checks by linking to external sources. That could never be scaled up to the level of the algorithm. There aren’t enough professional fact checkers in the world to go over every article that might get posted on social media. Research has found that this creates an “implied truth effect”: If you only check a subset of content, some users will assume any article that isn’t labeled must therefore be accurate, even if it simply was never checked.

A new paper published in Science Advances suggests a promising solution to these issues: fact-checking by the crowd. In the study, a team of researchers led by David Rand, a professor at MIT, set out to show whether groups of random laypeople could approximate the results of professional fact checkers. Using a set of 207 articles that had been flagged for fact-checking by Facebook’s AI, they had three professional fact checkers score them on several dimensions to produce an overall score from 1 (totally false) to 7 (totally trustworthy). Then they recruited about 1,100 ordinary people from Amazon Mechanical Turk, divided them into groups equally balanced between self-identified Democrats and Republicans, and had them do the same thing, but with a twist: While the fact checkers read the entire article and did their own research to verify the claims, the laypeople only looked at the headline and first sentence of each story.

Amazingly, that was enough for the crowd to match and even exceed the fact checkers’ performance.

To measure the crowd’s performance, the researchers first measured the correlation between the scores assigned by the three fact checkers themselves. (The correlation came out to .62—high, but far from uniform agreement. When judging stories on a binary true/false scale, however, at least two out of three fact checkers agreed with each other more than 90 percent of the time.) Then they measured the correlation between the crowd-assigned scores, on the one hand, and the average of the three fact checkers’ scores, on the other. The basic idea was that the average of the professionals’ ratings represents a better benchmark of accuracy than any one fact checker alone. And so if the laypeople ratings correlated with the average fact checker score as closely as the individual fact checkers agreed with each other, it would be fair to say that the crowd was performing as well as or better than a professional. The question: How many laypeople would you need to assemble to hit that threshold?

The study found that with a group of just eight laypeople, there was no statistically significant difference between the crowd performance and a given fact checker. Once the groups got up to 22 people, they actually started significantly outperforming the fact checkers. (These numbers describe the results when the laypeople were told the source of the article. When they didn’t know the source, the crowd did slightly worse.) Perhaps most important, the lay crowds outperformed the fact checkers most dramatically for stories categorized as “political,” because those stories are where the fact checkers were most likely to disagree with each other. Political fact-checking is really hard.",Yes,"It has made it easier for people to spread false information. The study found that with a group of just eight laypeople, there was no statistically significant difference between the crowd performance and a given fact checker. Once the groups got up to 22 people, they actually started significantly outperforming the fact checkers.",https://www.wired.com/story/could-wisdom-of-crowds-help-fix-social-media-trust-problem/,2021-09-01 18:00:00+00:00,LABEL_1_relevant,0.9962142109870912,Social Media,social_media,Wired,"Information, Discourse & Governance"
156,"The Internet, Politics, and the Politics of Internet Debate","Social networking, for example, is different in Egypt than it is in China. Users in Egypt expect and do different things on social networks than do people in China—which makes perfect sense given that they live in different cultures, with different political, social, and cultural concerns. In Egypt, much social networking happens on Facebook, an American site; in China, social networking happens on local sites that are tightly controlled by the government.

China probably has a team of native speakers to do censorship—not necessarily the case in Egypt/Facebook. Such differences have profound implications on users’ freedom, ways of relating, and ability to openly express discontent, as well as on the ability of state authorities to monitor users’ actions.

Is social networking good for protesters? For dictatorships? For democracy? These are not questions we can answer in the abstract. The idea that social networking—or other technologies such as search engines, databases, Wikipedia, smartphones, sensors, Big Data, and algorithms—will have similar effects across political cultures seems delusional.

Questions about the utility of Twitter for protest and the challenges involved in containing the sprawling apparatus built by democracies require a lot of soul-searching and force a lot of uncomfortable questions, about the future of capitalism, privacy, personal data, responsibility of companies and governments, the Western obsession with the war on terror, and so forth. None of these questions will be easy to answer on their own, but they will get maddeningly difficult to answer if we also confuse ourselves with an unnecessary urge to somehow make sure that our answers cohere to some vision of the Internet as a singular network.

Given the immense technological resources available, the failure to predict the Arab Spring looks far more remarkable than the failure to predict the fall of the Soviet Union. Should we give up trying to predict anything, and just blindly hope that somehow, now that everyone has access to a smartphone and Google, things will work themselves out and democracy will eventually prevail? Well, no: this would be too irresponsible. The best we can do is to develop a better set of optical tools—the ones that would allow us to zoom in on particular practices and notice the actual bits and pieces of the many infrastructures hiding behind the Internet label—and embrace a form of epistemological modesty where, when we are asked to opine on “what does the Internet do to Subject X?”, we politely decline and stay silent. Or, if we are of a more dissenting breed, we point out the explicit danger of asking such questions.

Read the full article here.



Evgeny Morozov is a contributing editor at the New Republic and the author of The Net Delusion: The Dark Side of Internet Freedom (PublicAffairs, 2011) and To Save Everything, Click Here: The Folly of Technological Solutionism (PublicAffairs, 2013). In 2010–12 he was a visiting scholar at Stanford University and a Schwartz Fellow at the New America Foundation. In 2009–10 he was a fellow at Georgetown University and in 2008–09 was a fellow at the Open Society Foundations (where he was on the board of the Information Program between 2008 and 2012). Between 2006 and 2008 he served as Director of New Media at Transitions Online. He has written for the New York Times, the Economist, the Wall Street Journal, Financial Times, London Review of Books, Times Literary Supplement, and other publications. His monthly Slate column is syndicated in El País, Corriere della Sera, Frankfurter Allgemeine Zeitung, Folha de S. Paulo, and several other newspapers.",Yes,"It creates a false sense of intimacy and superiority among users, which in turn fuels division and conflict rather than genuine communication and understanding.",https://www.technologyreview.com/2014/12/03/170219/the-internet-politics-and-the-politics-of-internet-debate/,2014-12-03 00:00:00,LABEL_1_relevant,0.6509577035903931,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
157,FTC Reportedly Hits Facebook With Record $5 Billion Fine,"After months of negotiations, the Federal Trade Commission fined Facebook a record-setting $5 billion on Friday for privacy violations, according to multiple reports. The penalty comes after an investigation that lasted over a year, and marks the largest in the agency’s history by an order of magnitude. If approved by the Justice Department's civil division, it will also be the first substantive punishment for Facebook in the US, where the tech industry has gone largely unregulated. But Washington has taken a harsher stance toward Silicon Valley lately, and Friday’s announcement marks its most aggressive action yet to curb its privacy overreaches.

Full details of the settlement were unavailable Friday afternoon, and the FTC and Facebook both declined to comment. The Wall Street Journal first reported the news. It's unclear how long it will take for the Justice Department to review the terms. In the meantime, important questions remain unanswered, including whether the FTC has opted to hold Facebook CEO Mark Zuckerberg personally liable for the company’s privacy violations, and what sort of external oversight Facebook must submit to going forward.

The FTC opened its investigation into Facebook’s data practices last March, one week after news broke that Cambridge Analytica, a political consulting firm that worked with the Trump campaign in 2016, had improperly obtained information on tens of millions of Facebook users. The data was purchased from an academic who used a personality profiling app to collect information not just from consenting users but, thanks to Facebook’s lax privacy policies at the time, from all those users’ friends—without their knowledge. Facebook didn’t cut off that access until 2015.

But in 2011, Facebook had promised the FTC that it would not share data with third parties without users’ affirmative consent, as part of a settlement agreement over charges that the company deceived consumers about its privacy practices. It appears that the regulator has found that Facebook violated that consent decree.

""We don’t think a fine matters. We need a structural solution here."" Matt Stoller, Open Markets Institute

The reported fine far surpasses the previous largest privacy-related penalty that the FTC has levied, a $22.5 million strike against Google in 2012 over its privacy policies on the Safari browser. But even $5 billion would be a drop in the bucket for Facebook, which generated $15 billion in revenue last quarter alone. When Facebook disclosed in its Q1 earnings report that it had set aside $3 billion to $5 billion to cover the costs of the settlement, its stock price soared.

Some of Facebook’s biggest critics had previously expressed doubt that any amount of money could sufficiently punish a company of Facebook’s size. “They can issue a really big fine, which is just a parking ticket,” Matt Stoller, a fellow at the anti-monopoly think tank Open Markets Institute, recently told WIRED. “We don’t think a fine matters. We need a structural solution here.”

In a letter to the FTC in early May, Senators Richard Blumenthal (D-Connecticut) and Josh Hawley (R-Missouri) argued that the FTC should “compel sweeping changes to end the social network’s pattern of misuse and abuse of personal data.”

“Personal responsibility must be recognized from the top of the corporate board down to the product development teams,” the letter read. “If the FTC finds that any Facebook executive knowingly broke the consent order or violated the law, it must name them in any further action.” Whether the FTC did so remains one of the biggest open questions around this settlement, one made all the more intriguing after the recent disclosure of emails, again in The Wall Street Journal, that appear to indicate that Zuckerberg was aware of the company’s “questionable” privacy practices.",Yes,It can lead to addiction. Social media addiction is a real and growing problem that needs to be addressed.,https://www.wired.com/story/facebook-ftc-fine-five-billion/,2019-07-12 20:44:25.575000+00:00,LABEL_1_relevant,0.9964160919189452,Social Media,social_media,Wired,Health & Wellbeing
158,Gab's CTO Introduced a Critical Vulnerability to the Site,"Over the weekend, word emerged that a hacker breached far-right social media website Gab and downloaded 70 gigabytes of data by exploiting a garden-variety security flaw known as an SQL injection. A quick review of Gab’s open source code shows that the critical vulnerability—or at least one very much like it—was introduced by the company’s chief technology officer.

ARS TECHNICA This story originally appeared on Ars Technica, a trusted source for technology news, tech policy analysis, reviews, and more. Ars is owned by WIRED's parent company, Condé Nast.

The change, which in the parlance of software development is known as a “git commit,” was made sometime in February from the account of Fosco Marotto, a former Facebook software engineer who in November became Gab’s CTO. On Monday, Gab removed the git commit from its website. An image on a site that provides saved commit snapshots shows the February software change.

The commit shows a software developer using the name Fosco Marotto introducing precisely the type of rookie mistake that could lead to the kind of breach reported this weekend. Specifically, line 23 strips the code of “reject” and “filter,” which are API functions that implement a programming idiom that protects against SQL injection attacks.

This idiom allows programmers to compose an SQL query in a safe way that “sanitizes” the inputs that website visitors enter into search boxes and other web fields to ensure that any malicious commands are stripped out before the text is passed to backend servers. In their place, the developer added a call to the Rails function that contains the “find_by_sql” method, which accepts unsanitized inputs directly in a query string. Rails is a widely used website development toolkit.

“Sadly Rails documentation doesn't warn you about this pitfall, but if you know anything at all about using SQL databases in web applications, you'd have heard of SQL injection, and it's not hard to come across warnings that find_by_sql method is not safe,” Dmitry Borodaenko, a former production engineer at Facebook who brought the commit to my attention wrote in an email. “It is not 100% confirmed that this is the vulnerability that was used in the Gab data breach, but it definitely could have been, and this code change is reverted in the most recent commit that was present in their GitLab repository before they took it offline.”

Ironically, Fosco in 2012 warned fellow programmers to use parameterized queries to prevent SQL injection vulnerabilities. Marotto didn’t respond to an email seeking comment for this post. Attempts to contact Gab directly didn't succeed.

Besides the commit raising questions about Gab’s process for developing secure code, the social media site is also facing criticism for removing the commits from its website. Critics say the move violates terms of the Affero General Public License, which governs Gab’s reuse of Mastodon, an open source software package for hosting social networking platforms.

Critics say the removal violates terms that require forked source code be directly linked from the site. The requirements are intended to provide transparency and to allow other open source developers to benefit from the work of their peers at Gab.

Gab had long provided commits at https://code.gab.com/. Then, on Monday, the site suddenly removed all commits—including the ones that created and then fixed the critical SQL injection vulnerability. In their place, Gab provided source code in the form of a Zip archive file that was protected by the password “JesusChristIsKingTrumpWonTheElection” (minus the quotation marks).",Yes,"The way it spreads information. Social media, by its very nature, is designed to share information quickly and easily. This can be a good thing, but it also has some negative consequences. One of these is the way that false information can spread quickly and easily through social media. This can lead to people believing things that are not true, and it",https://www.wired.com/story/gab-cto-critical-vulnerability/,2021-03-03 19:30:00+00:00,LABEL_1_relevant,0.9960177540779114,Social Media,social_media,Wired,"Information, Discourse & Governance"
159,Australia's Standoff Against Google and Facebook Worked—Sort Of,"Over Zoom, Australia’s communications minister, Paul Fletcher, has the air of a man in the middle of a victory speech. He credits his team and the country’s competition regulator for succeeding where others had failed: forcing tech giants to pay for news. “There were a lot of people saying you can't really succeed in taking on the global digital giants,” he says, sitting beneath strip lighting in his Sydney constituency office. But Fletcher and Australia’s federal treasurer, Josh Frydenberg, persevered. In 2020, when the Australian government asked the competition regulator to develop a law that would force tech giants to pay for the news that appears on their feeds, Fletcher was aware of the stories others used as warnings. When Germany’s biggest news publisher, Axel Springer, tried to block Google from running snippets of its articles in 2014, it backtracked after just two weeks once traffic plunged. When Spain tried to force Google to pay for news in 2014, the search giant just left—blocking Google News in the country for seven years.

Google threatened Australia with even more drastic action. In January 2021, the tech giant suggested Australians could lose access to its entire search engine if Fletcher and Frydenberg’s “news media bargaining code,” which would force platforms to pay news publishers for links, came into force. Facebook also lobbied hard against the code, arguing that news makes up less than 4 percent of the content people see in their news feed. On February 17, Australians woke up to discover that all news links had been wiped off the platform, leaving the Facebook pages of the country’s biggest media companies completely blank. Traffic to news websites sank 13 percent, illustrating exactly what the government said it was worried about. Facebook’s actions “confirm for all Australians [the] immense market power of these media digital giants,” Frydenberg said at the time.

Still, the government did not back down. According to Fletcher, the code was Australia’s answer to a problem that was first and foremost about competition. The argument was simple—Australia’s news industry should be compensated for helping Google and Facebook attract eyeballs. “What we're trying to do is replicate the ordinary commercial dealings that would occur in a market where there wasn't a huge imbalance of bargaining power,” he says.

But others suspect the code was really an attempt to subsidize the media industry, which was suffering from intense online competition for advertising. Out of every 100 Australian dollars spent on advertising in 2019, AD$53 ($38) went to Google, AD$28 to Facebook, and AD$19 to all other websites including media outlets, according to Australia’s competition watchdog. If this was the reason for the code, Bloomberg editors described it as a misdiagnosis in an op-ed. “Journalism’s business model wasn’t broken by digital platforms,” they said, “[the internet] offered consumers a wealth of free news and opinion and gave advertisers options and audiences that traditional publishers haven’t been able to match.”

Australians experienced this standoff through their Facebook feeds. For eight days, the site featured no news. Then, at 1 am on February 26, 2021, news content started to reappear, reversing users’ feeds to how they always looked. But behind the scenes, tech’s relationship with the media had permanently shifted.

Google and Facebook did not leave; they paid up, striking deals with news organizations to pay for the content they display on their sites for the first time. The code was formally approved on March 2, 2021, writing into law that tech platforms had to negotiate a price to pay news publishers for their content. If they didn’t, an arbiter would step in not only to force the platforms to pay but also to set the price. One year after the media code was introduced, Google has 19 content deals with news organizations and Facebook has 11, according to Fletcher.",Yes,"It has given a voice to everyone, regardless of qualifications or experience. This has led to the spread of misinformation, which can have harmful consequences.",https://www.wired.com/story/australia-media-code-facebook-google/,2022-02-25 12:00:00+00:00,LABEL_1_relevant,0.9940690994262696,Social Media,social_media,Wired,"Information, Discourse & Governance"
160,Foreign Trolls Are Targeting Veterans on Facebook,"I first came across the imposter Facebook page by accident. The page was made to look like that of my employer, Vietnam Veterans of America, complete with our organization's registered trademark and name. As an Iraq veteran and the office’s designated millennial policy guy, I was helping run VVA's social media accounts. The discovery kicked off what would become a 15-month-long amateur investigation into digital trolls in Bulgaria, the Philippines, and 27 other countries—all running Facebook pages targeting American troops and veterans with political propaganda.

WIRED OPINION ABOUT Kristofer Goldsmith served in the United States Army and was deployed in support of Operation Iraqi Freedom in 2005. He is the founder of High Ground Veterans Advocacy and the assistant director for Policy and Government Affairs at Vietnam Veterans of America.

Last year, an Oxford study revealed that military veterans are ripe targets for exploitation by foreign powers seeking to undermine American democracy. The report concluded that veterans are more likely than the average person to be community leaders and that their political opinions have significant influence on those around them. Recognizing this, foreign powers have sought to infiltrate our community, impersonating individuals and organizations with tens of thousands of members in an effort to gain veterans' trust.

At first, what I found on the imposter Vietnam Vets account didn’t make sense. The Facebook page had recycled old news stories about the Department of Veterans Affairs and veterans’ benefits, as well as a post about a “Vietnam Veterans song of the day.” Even though the latter didn’t have any audio attached, it was nonetheless shared by followers hundreds of times each day.

My employer reported the imposter, and the page's administrators quickly scrubbed it of any trace of our logo to avoid banishment from Facebook. Digging deeper, we realized that the news the page was sharing was scraped from legitimate military and veteran-focused newspapers, but that the stories' dates and content were altered to provoke emotional responses—specifically outrage.

The fake page's most viral video was a looping, 58-second local media story depicting what looked like berries smeared on a Vietnam Veterans’ monument. However, now it ran in a post that read “EXCLUSIVE: Vietnam Veterans Monument Vandalized… Share and Vote!” Text superimposed over the video prompted “Do you think the criminals must suffer?” More sinister still, the trolls had figured out how to game Facebook's algorithms into thinking that the video was a live feed. As a result, Facebook treated the looped video as if it were important breaking news, pushing it into the newsfeeds of tens of thousands of Americans.

It took three months and a handful of press releases before Facebook shut down the fraudulent page. The company's mostly automated reporting features found that even the fake live video hadn’t violated community standards.

Five months after the imposter page was shut down, we discovered two more Facebook pages that were sharing the same content and linking to new websites. One appeared to be a page that had been dormant since 2015, a year before Russia’s election interference reportedly began. Because its creator had forgotten to register the affiliated website anonymously, we were able to identify one of the trolls by name and location: Plovdiv, Bulgaria.

It’s unclear whether this particular troll was financially motivated or part of a network of troll farms in Eastern Europe that are targeting American democracy. Whatever the motivation, the effect was the same. Changing the dates on old stories about Congress making cuts to veterans’ benefits can spread panic, anger, and confusion throughout the community and influence political beliefs and voting behavior.

This troll’s persistence sparked further investigation. We eventually found scores of American-veteran-focused Facebook pages producing politically polarizing content from outside the United States.

Vietnam Veterans of America produced a report on our earliest findings for 11 committees in Congress and a host of alphabet agencies. It’s important for Congress and federal agencies to investigate these foreign entities to find out what damage has been done. But we're also calling on the Department of Veterans Affairs take a more proactive role in inoculating veterans against this type of threat to prevent future harm.",Yes,"It gives foreign powers a direct line to American veterans. Not only does this threaten our democracy and national security, but it’s also putting the lives of our service members and veterans at risk.

The VA must do more to educate veterans about how to identify and avoid foreign influence. The agency should also consider creating a network of social",https://www.wired.com/story/trolls-are-targeting-vets-on-facebook/,2018-12-04 14:00:00+00:00,LABEL_1_relevant,0.9965234398841858,Social Media,social_media,Wired,"Information, Discourse & Governance"
161,Facebook and Instagram will now allow users to hide 'Like' counts on posts,"Facebook this week will begin to publicly roll out the option to hide Likes on posts across both Facebook and Instagram, following earlier tests beginning in 2019. The project, which puts the decision about Likes in the hands of the company’s global user base, had been in development for years, but was deprioritized due to the COVID-19 pandemic and the response work required on Facebook’s part, the company says.

Originally, the idea to hide Like counts on Facebook’s social networks was focused on depressurizing the experience for users. Often, users faced anxiety and embarrassment around their posts if they didn’t receive enough Likes to be considered “popular.” This problem was particularly difficult for younger users who highly value what peers think of them — so much so that they would take down posts that didn’t receive enough Likes.

Like-chasing on Instagram, especially, also helped create an environment where people posted to gain clout and notoriety, which can be a less authentic experience. On Facebook, gaining Likes or other forms of engagement could also be associated with posting polarizing content that required a reaction.

As a result of this pressure to perform, some users grew hungry for a “Like-free” safer space, where they could engage with friends or the wider public without trying to earn these popularity points. That, in turn, gave rise to a new crop of social networking and photo-sharing apps such as Minutiae, Vero, Dayflash, Oggl and, now, newcomers like Dispo and newly viral Poparazzi.

Though Facebook and Instagram could have chosen to remove Likes entirely and take its social networks in a new direction, the company soon found that the metric was too deeply integrated into the product experience to be fully removed. One key issue was how the influencer community today trades on Likes as a form of currency that allows them to exchange their online popularity for brand deals and job opportunities. Removing Likes, then, is not necessarily an option for these users.

Instagram realized that if it made a decision for its users, it would anger one side or the other — even if the move in either direction didn’t really impact other core metrics, like app usage.

“How many likes [users] got, or other people got — it turned out that it didn’t actually change nearly as much about the experience, in terms of how people felt or how much they use the experience, as we thought it would. But it did end up being pretty polarizing,” admitted Instagram head, Adam Mosseri. “Some people really liked it and some people really didn’t.”

“For those who liked it, it was mostly what we had hoped — which is that it depressurized the experience. And, for those who didn’t, they used Likes to get a sense for what was trending or was relevant on Instagram and on Facebook. And they were just super annoyed that we took it away,” he added. This latter group sometimes included smaller creators still working on establishing a presence across social media, though larger influencers were sometimes in favor of Like removals. (Mosseri name-checked Katy Perry as being pro Like removals, in fact.)

Ultimately, the company decided to split the difference. Instead of making a hard choice about the future of its online communities, it’s rolling out the “no Likes” option as a user-controlled setting on both platforms.

On Instagram, both content consumers and content producers can turn on or off Like and View counts on posts — which means you can choose to not see these metrics when scrolling your own Feed and you can choose whether to allow Likes to be viewed by others when you’re posting. These are configured as two different settings, which provides for more flexibility and control.

On Facebook, meanwhile, users access the new setting from the “Settings & Privacy” area under News Feed Settings (or News Feed Preferences on desktop). From here, you’ll find an option to “Hide number of reactions” to turn this setting off for both your own posts and for posts from others in News Feed, groups and Pages.

The feature will be made available to both public and private profiles, Facebook tells us, and will include posts you’ve published previously.

Instagram last month restarted its tests on this feature in order to work out any final bugs before making the new settings live for global users, and said a Facebook test would come soon. But it’s now forging ahead with making the feature available publicly. When asked why such a short test, Instagram told TechCrunch it had been testing various iterations on this experience since 2019, so it felt it had enough data to proceed with a global launch.

Mosseri also pushed back at the idea that a decision on Likes would have majorly impacted the network. While removal of Likes on Instagram had some impact on user behavior, he said, it was not enough to be concerning. In some groups, users posted more — signaling that they felt less pressure to perform, perhaps. But others engaged less, Mosseri said.

“Often people say, ‘oh, this has a bunch of Likes. I’m gonna go check it out,’ ” the exec explained. “Then they read the comments, or go deeper, or swipe to the carousel. There’s been some small effects — some positive, some negative — but they’ve all been small,” he noted. Instagram also believes users may toggle on and off the feature at various times, based on how they’re feeling.

In addition, Mosseri pointed out, “there’s no rigorous research that suggests Likes are bad for people’s well-being” — a statement that pushes back over the growing concerns that a gamified social media space is bad for users’ mental health. Instead, he argued that Instagram is still a small part of people’s day, so how Likes function doesn’t affect people’s overall well-being.

“As big as we are, we have to be careful not to overestimate our influence,” Mosseri said.

He also dismissed some of the current research pointing to negative impacts of social media use as being overly reliant on methodologies that ask users to self-report their use, rather than measure it directly.

In other words, this is not a company that feels motivated to remove Likes entirely due to the negative mental health outcomes attributed to its popularity metrics.

It’s worth mentioning that another factor that could have come into play here is Instagram’s plan to make a version of its app available to children under the age of 13, as competitor TikTok did following its FTC settlement. In that case, hiding Likes by default — or perhaps adding a parental control option — would necessitate such a setting. Instagram tells TechCrunch that, while it’s too soon to know what it would do with a kids app, it will “definitely explore” a no Likes by default option.

Facebook and Instagram both told TechCrunch the feature will roll out starting on Wednesday but will reach global users over time. On Instagram, that may take a matter of days.

Facebook, meanwhile, says a small percentage of users will have the feature Wednesday — notified through an alert on News Feed — but it will reach Facebook’s global audience “over the next few weeks.”",Yes.,It makes people feel bad about themselves. A lot of people think that the solution to this problem is to hide the number of likes that a post gets. Facebook is now testing this solution.,https://techcrunch.com/2021/05/26/facebook-and-instagram-will-now-allow-users-to-hide-like-counts-on-posts/,2021-05-26 00:00:00,LABEL_1_relevant,0.9959622025489808,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
162,OnlyFans’ Explicit Content Ban Betrays Its Creators,"OnlyFans creators are getting screwed—and not in the way many subscribers are used to seeing. This week, the adult fan site, which rose to prominence last year and was even name-checked by Beyoncé on Megan Thee Stallion’s Grammy-winning “Savage Remix,” announced that it was further distancing itself from the business that made it an in-demand enterprise: hardcore porn.

The sharp pivot comes on the eve of the company’s five-year anniversary and the introduction of a new app, OFTV, in what is a clear attempt to become more of a general-purpose marketplace for creators.

In a statement released Thursday, the company said that, starting October 1, it would forbid the “posting of any content containing sexually explicit conduct” that doesn’t adhere to its Acceptable Use Policy. Details about the ban are still forthcoming, but OnlyFans representatives did confirm that nudity would be allowed as long as it abided by the company’s updated guidelines. The changes are, in part, to appease its “banking partners” as well as to “ensure the long-term sustainability of the platform,” the company said, and seem to be in line with its recent efforts to widen the composition of its creator pool beyond sex workers.

What remains unclear in the wake of OnlyFans’ announcement is whether those guidelines could become even more restrictive as the company’s financial interests evolve. A BBC report confirmed that the company decided to update its guidelines in part because of Mastercard’s decision to implement new rules for adult content sites that use their payment processing system. Last year, both Mastercard and Visa terminated their relationship with Pornhub. Mastercard’s new policies are meant to make sure adult sites have controls in place to monitor and remove illegal content, but OnlyFans’ move to ban nearly all explicit material left many worried about the fate of sex workers on the platform.

Across the internet, the surprise announcement was met with a mix of cynicism, support, and humor. Creators say they are skeptical of the changes, though hardly surprised. “They used us,” says Xavier Blanco. “They exploited our connections, only to toss us out.” Blanco, who is 34 and lives in New York City, joined the platform in the summer of 2019 and has made a considerable living through the site, where he uploads all manner of gay erotica. “They are laying off countless individuals without caring about their livelihood.”

In December 2019 when I spoke with Dominic Ford, the founder of JustFor.Fans, a rival adult fansite, he predicted such a future. “As has been the trend, sex-positive imagery of any kind is slowly being washed away from social media,” he said at the time, pointing to websites such as Tumblr, Craigslist, and Patreon cranking up restrictions on pornographic content.

Still, the impact of OnlyFans—and the site’s abrupt change in course—cannot be understated. Since its founding in 2016, the site has slowly ushered in a new frontier for adult entertainment. In what was once a hostile, and sometimes dangerous, landscape for sex workers, OnlyFans became a lucrative alternative to conventional services, such as camming, escorting, and studio porn. Conceptually, what made OnlyFans feel like such a radical leap forward was how it melded a growing need for personalized porn with our obsession with influencer culture. Eventually, Big Tech followed suit. Twitter, YouTube, and other major platforms have since developed tools that allow users to monetize their unique skills.",Yes,The way it has desensitized us to the point of making it difficult to have genuine connections with others.,https://www.wired.com/story/onlyfans-betrays-creators/,2021-08-20 19:25:42.143000+00:00,LABEL_1_relevant,0.9963956475257874,Social Media,social_media,Wired,"Information, Discourse & Governance"
163,Facebook suspends the Trump campaign’s data-mining firm amid revelations of a major data breach,"Facebook has blocked Cambridge Analytica, which provided data to the Trump election campaign

The news: In a blog post late on March 16, Facebook’s deputy general counsel said Strategic Communications Laboratories (SCL) and its political data arm, Cambridge Analytica, had their access to the platform suspended for violating data use policies.

The details: According to Facebook, Aleksander Kogan, a psychology professor at Cambridge University in the UK, had collected various kinds of personal data using an app on the social network. Billed as a research vehicle for psychologists, the app, downloaded by approximately 270,000 people, asked them to share things such as content they had liked and the city they’d listed on their Facebook profile.

The social network says that Kogan passed the information to SCL/Cambridge Analytica and Christopher Wylie of Eunoia Technologies in violation of its rules. When it discovered this in 2015, it removed the app and asked Kogan, SCL/Cambridge Analytica, and Wylie to certify they had destroyed the data collected. All three parties said they had done so.

But according to its post, Facebook recently received reports that not all the information had been deleted. The social network says it’s looking into these claims and is suspending Kogan, Wylie, and SCL/Cambridge Analytica pending further information.

But wait, there’s more: In a related story on March 17, the New York Times reported that Cambridge Analytica collected private information from more than 50 million Facebook profiles without permission, in what it described as “one of the largest data leaks in the social network’s history.” The story cites documents and former employees, including on-the-record statements from Wylie.

Why this matters: The use of social media to target political messages during the 2016 election has caused plenty of controversy. Facebook’s move will be seen as a sign it’s taking seriously accusations that it was used to manipulate public opinion, but it also raises the question of whether other data was siphoned off without the network’s knowledge.",Yes,"It can be used to manipulate public opinion. This is because social media platforms like Facebook collect a lot of data about their users, which can then be used to target them with personalized messages. In the case of Cambridge Analytica, this data was collected without users' knowledge or consent, and was then used to support the Trump election campaign.",https://www.technologyreview.com/2018/03/17/144608/facebook-suspends-the-trump-campaigns-data-mining-firm-amid-revelations-of-a/,2018-03-17 00:00:00,LABEL_1_relevant,0.996518850326538,Social Media,social_media,MIT Tech Review,Politics
164,They called it a conspiracy theory. But Alina Chan tweeted life into the idea that the virus came from a lab.,"The obvious problem with the lab-leak theory, though, is that there remains no concrete evidence for it. Chan has no particular view about how exactly an accident might have happened—whether a student got sick in a bat cave, say, or secret research to infect mice with a novel virus went awry. After reading Chan’s posts, I noticed that many of her claims don’t even relate to direct evidence at all; more often, they revolve around its absence. She tends to point out things that Chinese researchers didn’t do or say, important facts they did not quickly reveal, the infected market animal they never found, or a database that’s no longer online. She’s plainly suggesting there is a cover-up—and, therefore, a plot to conceal the truth.

Pre-adapted

Last February, when leading scientists convened to analyze the virus genome, they ended up publishing two letters. One, in The Lancet, dismissed the lab-accident possibility outright as a “conspiracy theory” (its authors included a scientist who funded research at the Wuhan lab). The other was the “Proximal Origins” letter in Nature Medicine, coauthored by Kristian Andersen, an evolutionary biologist at the Scripps Research Institute in La Jolla, California. Andersen and his coauthors looked at the genome of the virus and marshaled arguments for why it was very likely a natural occurrence—backed by evidence that it was similar to others found in nature.

The 30,000 genetic letters in that genome remain the most widely studied clue to the virus’s origin. Coronaviruses frequently swap parts—a phenomenon called recombination. Andersen found that all the components of the virus had been seen before in samples collected over the years from animals. Evolution could have produced it, he believed. The Wuhan Institute had been genetically engineering bat viruses for scientific experiments, but the SARS-CoV-2 genome did not match any of the favorite “chassis” viruses used in those experiments, and it did not contain any other obvious sign of engineering.

According to Clarivate, an analytics company, the Nature Medicine letter was the 55th most cited article of 2020, with over 1,300 citations in the journals tracked. Email records would later show that starting in January 2020, the letter had been the subject of urgent, high-level messages and conference calls between the letters’ authors, Anthony Fauci, head of the National Institute of Allergy and Infectious Diseases; top virologists; and the head of the Wellcome Trust, a major pharmaceutical research funding organization in the United Kingdom. Early on, the authors had worried that the virus looked suspicious before quickly coming together around a scientific analysis supporting a natural cause. Initially one of their aims was to quash rumors that the virus was a bioweapon or a result of engineering gone wrong, but they ended up going further, writing: “We do not believe that any type of laboratory-based scenario is plausible.”

Working from her home in Massachusetts, Chan soon found a way to revive the lab-accident theory by looking for differences with SARS, a similar virus that broke out in 2002 but caused only about 8,000 illnesses. With Shing Zhan, a bioinformatics specialist at the University of British Columbia, Chan looked at the early human cases of covid and saw that the new virus hadn’t mutated as fast as SARS had. If it were an animal virus from a market, she thought, its genome would show signs of adjusting more quickly to fit its brand-new human host. She prepared an analysis arguing that the virus was “pre-adapted” to humans and offered some theories as to why. Maybe it had been spreading undetected in people elsewhere in China. Or maybe, she thought, it had been growing in a lab somewhere, perhaps multiplying in human cells or in transgenic mice that had had human genes spliced into them.

The chance that a non-engineered virus could have “adapted to humans while being studied in a laboratory,” she wrote, “should be considered, regardless of how likely or unlikely.”

On May 2, 2020, Chan posted a preprint paper, coauthored with Deverman and Zhan, to the website bioRxiv, an online venue for quickly communicating results that haven’t yet been reviewed by other scientists. “Our observations suggest that by the time SARS-CoV-2 was first detected in late 2019, it was already pre-adapted to human transmission,” they wrote. The Broad Institute communications department also pointed Chan to examples of how to compose a “tweetorial,” a daisy chain of posts, with pictures, that present a compact scientific argument to a wider public. She posted her first tweetorial the following day.

For journalists suspicious about China’s handling of the virus, the thread—and those that followed—were dynamite. Here was an actual scientist at America’s biggest gene center who was explaining why the official story might be wrong. “Coronavirus did NOT come from animals in Wuhan market,” screamed a Mail on Sunday headline, in what became Chan’s first breakout into the public conversation.

While her report was a media success, what the Daily Mail described as Chan’s “landmark paper” has still never been formally accepted by a scientific journal. Chan says that’s because of censorship due to her raising the lab-origin possibility. Eisen of UC Davis, however, thinks Chan’s expectations for how the covid-19 virus should have behaved remain conjecture. He doesn’t think we’ve traced enough outbreaks in enough molecular detail to really know what's normal. And, he notes, covid-19 has continued to change and adapt.

“My colleagues said, This is a conspiracy—don’t bother. I said, No, I am going to treat this like any other paper,” says Eisen, who took time to study the manuscript. “I think it’s interesting what she tried to do, but I am not convinced by the conclusion, and I think the inferences were wrong. I do commend her for posting it. Many of the people pushing the lab-origin theory are not making claims based on logic, but she presented her evidence. I don’t agree with it, but that is science.”

Wrong or right, though, the word Chan used—“pre-adapted”—sent shivers up the spine of people like author Nicholson Baker. “We were dealing with a disease that was exceptionally good, right out of the gate, at chewing up human airways,” says Baker, who got in touch with Chan to learn more. Several months later, in January of this year, Baker would publish a lengthy report in New York magazine saying he’d become convinced a laboratory accident was to blame. He cited a variety of sources, including Chan.

Pangolin problem

Chan wasn’t done knocking holes in the natural-origins narrative. She next took on four papers that had been rapidly published early in 2020, two of them in Nature, describing viruses in pangolins—endangered scale-covered mammals sometimes eaten as delicacies in China—that shared similarities to SARS-CoV-2. If researchers could find all the components of the pandemic virus, especially in wild animals illegally trafficked as food, they could cinch the case for a spillover from nature, given the way coronaviruses swap parts. The pangolin papers, published in quick succession in early 2020, were a promising start. To the authors of “Proximal Origins,” these similar viruses offered “strong” and “parsimonious” evidence for natural emergence.

Chan and Zhan noticed that all the papers described the same batch of animals—even though some failed to acknowledge the overlap. One even relabeled the data, which made it appear novel. To Chan, that wasn’t just sloppy work or scientific misconduct. There could, she believed, have been “coordination” between the overlapping authors of all these papers, some of whom had published together before. She created the hashtag #pangolinpapers—calling to mind the Panama Papers, documents that exposed secret offshore financial dealings.

Maybe, she thought, researchers were now laundering data to make it seem that nature was swimming with similar viruses.

Chan started emailing authors and journals to get the raw data she needed to more fully analyze what they had done. Making such data available is usually a condition of publication, but it can still be hard to obtain. After what she calls months of stonewalling, Chan finally lost her cool and blasted an accusation out from her browser. “I need the scientists + editors who are directly or indirectly covering up severe research integrity issues surrounding some of the key SARS-2-like viruses to stop and think for a bit,” she posted to Twitter. “If your actions obscure SARS2 origins, you're playing a hand in the death of millions of people.”

Eddie Holmes, a prominent Australian virologist and coauthor of one of those papers (as well as “Proximal Origins”), called the tweet “one of most despicable things I read on the origins issue.” He felt accused, but he wondered what he was being accused of, since his paper had correctly accounted for its pangolin data sources. Holmes then circulated an intricate time line prepared by Chan of the publication dates and past connections between the authors. The chart’s dense web of arrows and connections bore an unmistakable resemblance to an obsessive’s cork board covered with red string and thumbtacks.",Yes,"It can connect people to others who hold similar beliefs and enter them into a 'echo chamber'.

The creation of echo chambers has contributed to the spread of conspiracy theories and the reinforcement of false beliefs.",https://www.technologyreview.com/2021/06/25/1027140/lab-leak-alina-chan/,2021-06-25 00:00:00,LABEL_1_relevant,0.9962835907936096,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
165,Lina Khan’s Theory of the Facebook Antitrust Case Takes Shape,"When federal judge James Boasberg dismissed the Federal Trade Commission’s antitrust lawsuit against Facebook in June, he gave the agency pretty specific instructions on how to salvage it. The problem, he wrote in his opinion, was that the FTC hadn’t offered even the barest evidence that Facebook is a monopoly, beyond the vague claim that it “maintained a dominant share of the US personal social networking market (in excess of 60 percent).” As Boasberg noted, that inexplicably left some basic questions unanswered, such as: 60 percent of what? Who makes up the leftover 40 percent? It was a bit like accusing a driver of speeding without even mentioning the speed limit.

To get back into court and advance to the next stage of litigation, the FTC would have to come back with something a lot more specific. That presented an interesting early assignment for Lina Khan, who was confirmed as commissioner of the agency a mere two weeks before Boasberg issued his ruling. (Facebook has sought to have Khan recused from the case on the basis of her public criticism of big tech companies before her current job, though experts see little chance of that succeeding.)

On Thursday, the FTC filed its revised complaint answering those previously unanswered questions. While it’s impossible to predict how a given judge will rule, the new material seems likely to satisfy Boasberg and keep the case alive. “To my eye, they’ve scratched Boasberg’s itch,” said Paul Swanson, an antitrust attorney in Denver. Facebook, he said, may not be able to avoid “a long slog of document productions and depositions.”

To prove that Facebook is a monopoly for legal purposes, the FTC doesn't have to show that it’s literally the only social network. They have to show that it has “market power.” In a nutshell, having market power means you face so little competition that you can do things your customers don’t like without losing any business. It’s one of the main reasons antitrust law exists: When there isn’t enough competition, companies will stop trying to please their customers and start trying to squeeze them. Think about how frustrating it is when your internet provider raises prices and you realize no one else serves your neighborhood. That’s market power.

There are two ways to show market power: indirect evidence and direct evidence. Indirect evidence usually refers to dominant market share. (That might sound counterintuitive, but the reason it’s indirect is because being big on its own doesn’t prove a company is doing anything wrong—it just raises the strong possibility.) In its initial complaint, the FTC only offered indirect evidence, and very little of it: that feeble 60 percent statistic, which Boasberg ruled was inadequate. The revised complaint, on the other hand, goes into great detail on market share. Drawing on data from the analytics company Comscore—which, the complaint notes, Facebook itself relies on—the FTC argues that just about any way you slice it, Facebook controls a dominant chunk of the market for “personal social networking services.” According to the Comscore data, Facebook has accounted for more than 80 percent of time spent since 2011, at least 70 percent of daily active users, and at least 65 percent of monthly active users.

The new complaint also tightens up the FTC’s definition of the market itself, which is another crucial part of any monopolization case. You can’t prove a company has market power without explaining which market they have power in. According to the agency, the market for personal social networking services has three key attributes: First, a network has to be “built on a social graph that maps the connections between users and their friends, family, and other personal connections.” Second, it has to have features for users to interact with each other in a “shared social space,” like a news feed or group. Third, it has to allow users to look each other up. (Think about how you can search for someone by name on Facebook, but not in iMessage.)",Yes,It's desensitized us to reading long articles.,https://www.wired.com/story/lina-khan-theory-facebook-antitrust-case-takes-shape/,2021-08-19 23:10:02.759000+00:00,LABEL_1_relevant,0.9961438179016112,Social Media,social_media,Wired,"Information, Discourse & Governance"
166,The Top Political Advertiser on Facebook Is ... Facebook,"On Tuesday, Facebook released a new tool that shows who's spending the most money on political ads on the platform in the US. At a glance, the Ad Archive Report suggests that Texas senate candidate Beto O'Rourke is the biggest spender, having plowed more than $5 million into Facebook ads since May. But the fine print reveals a more surprising finding: The advertiser spending the most on political and issue ads on Facebook is, well, Facebook.

Since May, Facebook has spent the equivalent of over $12 million on political and issues ads across Facebook and Instagram. That's about as much as the next four top advertisers, including O'Rourke and President Trump's super PAC, spent combined. The ads Facebook ran promoted the company's efforts to secure elections. They also encouraged Instagram users to vote.

It should come as no surprise that Facebook would be the biggest power user of its own technology. But the size of the ad campaign, which according to Facebook’s ad archive only began in August, reflects the company’s eagerness to restore its reputation. Since the 2016 election, Facebook has been blamed for the spread of fake news, for allowing its users' data to be misappropriated for political means, and for being blind to the ways its platform could be misused by foreign actors. Facebook has since launched television ads and wallpapered cities with posters warning against the dangers of fake friends and vowing to protect user privacy. Facebook's own Facebook ads are, it seems, a core part of that strategy.

But arguably the most important part of Facebook's reputational turnaround process is not what the company says about itself, but what it does. In May, Facebook released its political advertising archive for the first time, keeping a promise to introduce more transparency to digital political ads. The tool was a start, but it proved difficult to navigate. There was no way to see who was spending the most money on Facebook, how advertisers stacked up against each other, or how much they were spending on Facebook overall.

The new Ad Archive Report tool makes that possible. It ranks every political advertiser on Facebook by ad spend, with links to the ads themselves, as well as information on how many ads they've run. It's not a perfect system; Facebook has been caught automatically labeling ads for Bush's Beans as political, simply because they happen to include the name of two former presidents. Other clearly political ads, including some run by Senator Kamala Harris to oppose the nomination of Justice Brett Kavanaugh, have not always been flagged. Still, the Ad Archive Report provides a window into how campaigns and interest groups are spending their money on Facebook compared to one another---and compared to other mediums.

It reveals, for instance, that not only is the O'Rourke campaign outspending every other congressional campaign by millions of dollars, but that President Trump's campaign and super PAC are still as active as ever on Facebook. While the Trump Make America Great Again Committee ranks just after O'Rourke, the president's campaign comes in at number nine. In between are some of the usual suspects. Billionaire Tom Steyer's Need to Impeach group is among the top ten spenders, while ExxonMobil has spent big in Colorado, fighting against Proposition 112, which would limit the location of new oil and gas projects.

According to the report, since May, advertisers have spent more than $256 million on nearly 1.7 million political and issue ads on Facebook. That's still small compared with the $8.8 billion that's expected to be spent on political advertising this year. Nevertheless, given the sheer volume of ads on Facebook, a tool like this offers valuable insights into who's dominating the political conversation on the world's biggest social media platform---even if the answer is Facebook itself.

More Great WIRED Stories",Yes,"It can be used to spread misinformation and cause division.

When Facebook released its political advertising archive for the first time, it showed that the company was spending the most on political and issue ads.

The new Ad Archive Report tool from Facebook shows that the O'Rourke campaign is outspending every other congressional campaign by millions of",https://www.wired.com/story/top-political-advertiser-on-facebook-is-facebook/,2018-10-23 19:55:20.649000+00:00,LABEL_1_relevant,0.9948062300682068,Social Media,social_media,Wired,Politics
167,The Internet Has Changed How We Deal with Death,"If anyone doubts how important the online world has become in our grieving process, they have only to look to the biggest social media platform. Facebook has long allowed people’s profiles to be memorialized when they die, and as of last year, users can designate a legacy contact—someone who can assume limited control of the deceased’s account and continue maintaining it. Even the law is beginning to recognize how important our digital existence is to our loved ones after we shuffle off. Startups allow people to create a “digital will” that only releases access to social media accounts and digital assets to a designated executor.

As Garber points out, people were similarly accused of shedding crocodile tears over the death of Princess Diana. And such “grief policing” isn’t likely to go away. But neither is public, online grieving—in fact, it has already become part of the ritual of death.

(Source: The Atlantic, New Scientist)",Yes,"It has given people a platform to publicly grieve, which has led to a lot of ""grief policing"" from people who think that this grieving is not genuine.",https://www.technologyreview.com/2016/01/21/163779/the-internet-has-changed-how-we-deal-with-death/,2016-01-21 00:00:00,LABEL_1_relevant,0.9925903081893921,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
168,Americans Are Split On Online Dating—but Swipe More Than Ever,"In 1965, two Harvard students hacked together a computerized matchmaking program—a punch-card survey about a person and their ideal match, recorded by the computer, then crunched for compatibility—and the world’s first dating site was born. Over the next half-century, the idea would evolve into Match.com and eHarmony, OkCupid and Grindr, Tinder and Bumble, and Facebook Dating. But even then, the basic truth was the same: Everyone wants to find love, and with a computer to narrow the pool, it gets a little easier. Punch-cards turned to finger-swipes, but the computerized matchmaking magic remained the same.

In the decades that people have been finding love online, there has been surprisingly little anthropological research on how technology has changed the dating landscape. There are some notable exceptions—like Dan Slater's 2013 book Love in the Time of Algorithms—but research that takes stock of the swiping, matching, meeting, and marrying of millions of online daters has been thin, when it exists at all.

A new survey from the Pew Research Center updates the stack. The group last surveyed Americans about their experiences online dating in 2015—just three years after Tinder launched and, in its wake, created a tidal wave of copycats. A lot has changed: The share of Americans who have tried online dating has doubled in four years (the survey was conducted in October 2019) and is now at 30 percent. The new survey was also conducted online, not by phone, and “for the first time, gives us the ability to compare experiences within the online dating population on such key dimensions as age, gender and sexual orientation,” said Monica Anderson, Pew’s associate director of internet and technology research, in a Q&A published alongside the survey.

Ask WIRED What is the safest way to use dating apps?

The new survey is far from sweeping, but it qualifies with new data many of the assumptions about online dating. Pew surveyed 4,860 adults from across the United States, a sample that’s small but nationally representative. It asked them about their perceptions of online dating, their personal usage, their experiences of harassment and abuse. (The term “online dating” refers not just to websites, like OkCupid, but also apps like Tinder and platform-based services like Facebook Dating.) Half of Americans said that online dating had “neither a positive nor negative effect on dating and relationships,” but the other half was divided: A quarter said the effect was positive, a quarter said it was negative.

“Americans who have used a dating site or app tend to think more positively about these platforms, while those who have never used them are more skeptical,” Anderson notes in her Q&A. But there are also demographic differences. From the survey data, people with higher degrees of education were more likely to have positive perceptions of online dating. They were also less likely to report receiving unwanted, explicit messages.

Young adults—by far the biggest users of these apps, according to the survey—were also the most likely to receive unwanted messages and experience harassment. Of the young women Pew surveyed, 19 percent said that someone on a dating site had threatened violence. These numbers were even higher for young people who identify as lesbian, gay, or bisexual, who are also twice as likely to use online dating than their straight peers. “Fully 56% of LGB users say someone on a dating site or app has sent them a sexually explicit message or image they didn’t ask for, compared with about one-third of straight users,” the survey reports. (Men, however, are more likely to feel ignored, with 57 percent saying they didn’t get enough messages.)

None of this is surprising, really. Unpleasant encounters on dating platforms are well documented, both by the media and the public (see: Tinder Nightmares), and have even spurred the creation of new dating platforms, like Bumble (its original tagline: “The ball is in her court”). Researchers have made these observations before, too. In a 2017 survey on online harassment, Pew found that young women were much likelier than young men to have received unwanted and sexually explicit images.",Yes,It isn't as safe as people thought it was. Social media makes it easier for predators to find their victims and for bullies to hide behind a screen.,https://www.wired.com/story/americans-online-dating-pew-survey-2020/,2020-02-06 22:36:42.119000+00:00,LABEL_1_relevant,0.9940914511680604,Social Media,social_media,Wired,Security & Privacy
169,Wellness Startup's Generic Viagra Ads Flout Facebook Rules,"On television and radio, the ads are fairly innocuous: “Hey guy,” a female narrator says playfully in one TV spot for Hims, a men’s wellness brand that sells prescription drugs to treat erectile dysfunction, oral herpes, social anxiety, hair loss, and other conditions. “Hi there. Welcome to Hims.”

The ad invites viewers to “get ED treatment started for only $5,” next to a close-up of a young man pressing a white pill seductively to his lips. What appear to be customer reviews are superimposed over the image: “Should have done it years ago and I feel like the young stud that I always imagined I was,"" says one. ""Outstanding product, works above and beyond our expectations,"" reads another.

Much like other ads for Hims—and its sister brand, Hers, which sells prescription drugs and wellness products for women—that are broadcast on television, radio, podcasts, or appear in print or on billboards, this ad is rather generic. It describes a medical problem, alludes to the company’s business model—which skips a trip to the doctor in favor of an “online visit” with a physician—and invites the viewer to check out its website for more details.

Online, however, the brands take a different tack. Hims and Hers ads on Facebook and Instagram are more specific, offering a quick and easy way for users to buy prescription medications directly. “men [sic] can get Sildenafil (active ingredient in Viagra) delivered directly to the door,” reads one active Facebook ad by Hims. “try it out today for only $5. free shipping!”

Such ads violate Facebook policies that prohibit promoting the sale or use of prescription drugs or suggesting that users have a specific condition. Earlier this month, Facebook removed three Hims ads for sildenafil that had been flagged by WIRED; a Facebook spokesperson said the ads violated Facebook policies. But hundreds of other ads from Hims and Hers touting specific prescription drugs were active on Facebook as of Friday.

Many online ads from Hims and Hers also may run afoul of US Food and Drug Administration guidelines that require marketers to disclose the side effects associated with a drug. A Hers ad for prescription anti-acne medication tretinoin, for example, calls the drug ""your skin's BFF” and offers ""Serious results without the hassle."" The ad suggests, “Skip the drive over to the doctor's office and start today for just $5.""

Facebook Ad Library

The ad does not mention any of the side effects required by the FDA, which in tretinoin’s case can include burning, itching, stinging, scaling, peeling, or redness of the skin, or sensitivity to sunlight, soaps, cosmetics, and other skincare products. Nor does the Hims ad for sildenafil mention side effects including low blood pressure, loss of vision or hearing, headaches, or insomnia. TV ads from Hims and Hers don’t carry these disclosures because they don’t mention specific drugs, referring instead to “treatment.”

The FDA disclosure requirements apply to online, as well as traditional, media, says Hyosun Kim, a professor at the University of Wisconsin Stevens Point whose research focuses on online direct-to-consumer advertising of prescription drugs. “The FDA has these guidelines to protect consumers from misleading information, because [when it comes to] prescription drugs, it's really important for the patient to understand the risk factors and any other [interactions the drug might have],” she says. Kim says that, in her opinion, ads like those run by Hims and Hers—which do not include any risk information and in some cases claim to provide “serious results without the hassle”—violate FDA guidelines.",Yes,It's very easy to advertise prescription drugs without disclosing any of the risks or side effects.,https://www.wired.com/story/health-brands-hims-hers-flout-facebooks-rules-drug-ads/,2019-07-01 11:00:00+00:00,LABEL_1_relevant,0.9963472485542296,Social Media,social_media,Wired,"Information, Discourse & Governance"
170,"Regardless of Its Influence on the Election, Facebook Needs to Change","Trump even explained over the weekend that social media was pivotal to his campaign. “The fact that I have such power in terms of numbers with Facebook, Twitter, Instagram, etc.—I think it helped me win all of these races where they’re spending much more money than I spent,” he said during an interview on CBS’s 60 Minutes on Sunday.

The more contentious issue is whether truly fictitious news also played a role in swinging the election in Trump’s favor. Stories that claimed Hillary Clinton was an arms dealer, or that Donald Trump was endorsed by the pope, for example, were complete fabrications. But as Wired notes, people read them. And surely many believed them to be fact.

Mark Zuckerberg, however, is adamant that they did not decide the election—a position he’s now spoken out about twice. Last week, he claimed that “the idea that fake news on Facebook ... influenced the election in any way is a pretty crazy idea.” And over the weekend he posted a lengthy note explaining that “of all the content on Facebook, more than 99 percent of what people see is authentic.” This, he claims, “makes it extremely unlikely hoaxes changed the outcome of this election.”

The question of whether such news did have an impact has commentators scrambling for a hot take that comes down on one side or the other. New York magazine claims that Trump won because of fake news on Facebook. Recode says that “if you’re blaming Facebook for the presidential election results, you’re an idiot.”

Neither is right.

It is undeniably the case that such stories will have shaped some opinions. It’s also the case that people consume media from sources other than Facebook. The truth is, Facebook alone did not bring about Donald Trump’s win.

The furor does, however, bring one thing into stark relief: Facebook needs to solve its fake news problem. Protestations from Zuckerberg that Facebook is not and shall not be a media company seem harder to accept when it continues to deliver such overtly fallacious content.

As Sam Biddle neatly puts it in a piece for The Intercept: “You can blame Facebook outright for Trump’s victory, or not. But at the very least, we should demand from them some accountability for their role in spreading the present toxic sea of deliberate misinformation and non-factual chaos.”

Mercifully, the New York Times suggests that some senior executives at Facebook are worried about the company’s influence. Zuckerberg has, somewhat vaguely, claimed that the social network will “continue to work on” blocking fake news, though he seemed to backpedal even from that, saying he wants to “proceed very carefully” because “identifying the ‘truth’ is complicated.”

He’s right, of course. But when you’re in control of one of the primary conduits through which over one billion people receive news and information, things can’t always be simple. It’s time to step up to the mark, Mark.

(Read more: New York Times, The Intercept, Bloomberg, “Facebook Tweaks Its News Feed Algorithm, and the Winner Is: Facebook”)",Yes,"The spread of fake news. While Facebook CEO Mark Zuckerberg has downplayed the role of his company in the election of Donald Trump, it is undeniable that some people will have shaped some opinions based on the fallacious content they read on the site. It is therefore crucial that Facebook solve its fake news problem.",https://www.technologyreview.com/2016/11/14/107182/regardless-of-its-influence-on-the-election-facebook-needs-to-change/,2016-11-14 00:00:00,LABEL_1_relevant,0.9953598380088806,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
171,Evidence Grows That Online Social Networks Have Insidious Negative Effects,"The question Sabatini and Sarracino set out to answer is whether the use of online networks reduces subjective well-being and if so, how.

Sabatini and Sarracino’s database is called the “Multipurpose Survey on Households,” a survey of around 24,000 Italian households corresponding to 50,000 individuals carried out by the Italian National Institute of Statistics every year. These guys use the data drawn from 2010 and 2011. What’s important about the survey as that it is large and nationally representative (as opposed to a self-selecting group of undergraduates).

The survey specifically asks the question “How satisfied are you with your life as a whole nowadays?” requiring an answer from extremely dissatisfied (0) to extremely satisfied (10). This provides a well-established measure of subjective well-being.

The survey also asks other detailed questions such as how often people meet friends and whether they think people can be trusted. It also asked about people’s use of online social networks such as Facebook and Twitter.

This allowed Sabatini and Sarracino to study the correlation between subjective well-being and other factors in their life, particularly their use of social networks. As statisticians they were particularly careful to rule out spurious correlations that can be explained by factors such as endogeneity bias where a seemingly independent parameter is actually correlated with an unobserved factor relegated to the error.

They found for example that face-to-face interactions and the trust people place in one another are strongly correlated with well-being in a positive way. In other words, if you tend to trust people and have lots of face-to-face interactions, you will probably assess your well-being more highly.

But of course interactions on online social networks are not face-to-face and this may impact the trust you have in people online. It is this loss of trust that can then affect subjective well-being rather than the online interaction itself.

Sabatini and Sarracino tease this apart statistically. “We find that online networking plays a positive role in subjective well-being through its impact on physical interactions, whereas [the use of] social network sites is associated with lower social trust,” they say. “The overall effect of networking on individual welfare is significantly negative,” they conclude.

That’s an important result because it is the first time that the role of online networks has been addressed in such a large and nationally representative sample.

Sabatini and Sarracino particularly highlight the role of discrimination and hate speech on social media which they say play a significant role in trust and well-being. Better moderation could significantly improve the well-being of the people who use social networks, they conclude.

Facebook, Twitter, and others take note.

Ref: arxiv.org/abs/1408.3550: Online Networks and Subjective Well-Being",Yes,"It can reduce trust and social capital, which in turn affects subjective well-being.",https://www.technologyreview.com/2014/08/29/12613/evidence-grows-that-online-social-networks-have-insidious-negative-effects/,2014-08-29 00:00:00,LABEL_1_relevant,0.9958112239837646,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
172,David Mamet files short story as brief supporting Texas social media ...,"David Mamet is known for plays like Glengarry Glen Ross and Speed-the-Plow, films like The Spanish Prisoner and Heist, books like On Directing Film and The Wicked Son, and as of last week, a short story about a lost airplane pilot filed as a legal brief supporting social media regulation in Texas.

Mamet’s amicus brief is titled “Lessons from Aerial Navigation,” and as it’s two pages long, you should really just go read it. Here’s a sample of the prose:

The Map is not the territory. The territory is the territory. The pilot’s answer to the question “where am I?” lies not on the map, but out the windscreen. That’s where he is. It doesn’t matter where he calculated he should be, the territory below him is where he is.

In case you’re wondering, the map is a metaphor for the internet. The implied legal argument is that social media platforms have distorted “the Map” by moderating content in a way Mamet — a vocal supporter of former President Donald Trump, who’s been banned from major platforms — objects to. In response Mamet is supporting HB 20, a currently blocked Texas law that’s meant to discourage web services from moderating conservative posts and users. This gets a little more explicit later on:

Navigating requires using tools correctly. The confused citizen has a map. But, if he worked from his observations back to it, he might discover that he can’t find his position pictured there. Looking out he might, for example, see a free, prosperous, and good country, in which there was little actual poverty, scant racism, and no “systemic” racism, where minorities and women, rather than being discriminated against were treated preferentially. (This belief might be correct or incorrect, but unless we prefer a Ministry of Truth, the belief is his own and surely he’s entitled to it.) Referring back, then, to his “information,” the citizen might not be able to correlate it with his observations. He knew where he was, as he’d just looked around. But he found no corresponding position on his map.

But along the way you can also find diversions like an etymology lesson referencing Greek mythology:

I report as an outdoorsman, that Panic is real. It is the loss of the mind and will to Pan, God of the Woods. The affected loses his reason, and runs about unable to recognize those actual signs (a road, his own footprints), which might bring him back to safety.

Mike Masnick of Techdirt has dissected Mamet’s argument (which includes a nonfictional preface) as well as his decision to copyright the legal filing — something that’s theoretically possible but pretty rare. To sum up, Mamet’s logic hinges on casting a partial government ban on moderation by private companies as protection from government censorship, by somewhat dubiously calling web platforms “firms that control information conduits, and are privileged and subsidized by the government.”

At the risk of stating the obvious: a judge probably will not be swayed by a legal brief that includes no actual legal references or arguments. On the other hand, if state legislatures are going to make me keep covering ill-conceived and possibly unconstitutional social media laws, I’m not going to personally complain about writers throwing some florid allegorical prose into the mix.",Yes,"People are exposed to a much wider range of viewpoints than they would be otherwise. This can lead to echo chambers and filter bubbles, where people only see information that reinforces their existing beliefs.",https://www.theverge.com/2022/3/15/22979026/david-mamet-amicus-brief-texas-hb20-social-media-regulation-texas,2022-03-15 00:00:00,LABEL_1_relevant,0.9950724244117736,Social Media,social_media,The Verge,"Information, Discourse & Governance"
173,Facebook Let Cybercrime Groups Operate in Plain Sight,"Facebook's failure to moderate bad behavior on the sprawling online world it created, what with political trolls, extremist content, and livestreamed acts of horrific violence, has received a torrent of criticism. But researchers have found that the social media giant is also failing to police a far more basic and decades-old internet problem among its users: plain old cybercrime.

Researchers at Cisco's Talos security division on Friday revealed that they'd uncovered 74 Facebook groups devoted to the sale of stolen credit card data, identity info, spam lists, hacking tools, and other cybercrime commodities. The researchers say those groups sat in plain sight, with names like Spam Professional and Spammer and Hacker Professional, attracting 385,000 members in all. Anyone could find them with a site search for basic terms like ""carding"" or ""CVVs,"" a reference to the security codes on the back of credit cards.

""Effectively, what we found was a huge number of Facebook groups openly trading crime stuff online,"" says Craig Williams, Cisco Talos' director of outreach. ""The user base in these groups is basically the size of Tampa.""

""It's ridiculous ... This company operates on a set of rules that are backward and are only in its own commercial interest."" Dipayan Ghosh, Shorenstein Center Platform Accountability Project

Screenshots that Cisco published in a blog post summarizing its findings capture Facebook users publishing pictures of purportedly stolen credit cards and IDs, offering lists of CVVs priced at $5 each, as well as collections of thousands of emails ripe for spamming and phishing—the type of data usually sold on dark-web markets or password-protected, invite-only hacker forums. Williams says many of the users he saw in those groups even appeared to be conducting business in Facebook's cybercrime bazaars under their real accounts.

Some of the posts that Cisco researchers found selling credit card data, including CVV security codes, as well as counterfeit credit cards and IDs. Cisco

And finding the groups, Williams says, wasn't particularly difficult: Once Cisco's researchers identified a handful of them, Facebook's recommendation algorithm offered them other groups with similar black market focuses.

This isn't the first time Facebook has faced this exact problem. Last year, cybersecurity reporter Brian Krebs identified a similar-sized crop of Facebook cybercrime groups, totaling 300,000 members, and reported them to Facebook. Facebook banned those groups at the time, but it took less than a year for an even larger population of fraudsters and hackers to make homes on the site.

And while Facebook has removed the groups Cisco identified—after the researchers alerted the company to its findings—its cleanup remains incomplete. In a few minutes of searching, WIRED found users and groups with names like Carder Philippines and Anonymous Carding India openly hawking credit card information, along with what appeared to be stolen goods like cameras and iPhones bought with hijacked ecommerce accounts.",Yes.,"Cybercrime. The ease of access to potential victims as well as the anonymous nature of most social media platforms creates a perfect environment for cybercriminals to ply their trade. The article cites a study by Cisco which found that 385,000 members were in 74 Facebook groups devoted to the sale of stolen credit card data, identity info, spam lists, hacking",https://www.wired.com/story/facebook-cybercrime-groups-again/,2019-04-05 13:00:00+00:00,LABEL_1_relevant,0.9960824251174928,Social Media,social_media,Wired,Security & Privacy
174,Facebook Gave Russian Internet Giant Special Data Extension,"Since March, when news broke that the political consulting firm Cambridge Analytica used a Facebook app to amass data on as many as 87 million people without their consent, the social networking giant has been forced to repeatedly answer for how it has given away user data and who it's given that data to. In the immediate wake of the scandal, Facebook rushed to defend itself in a blog post, saying that in 2014, it changed an element of its API to prevent apps from collecting data on their users' friends, as the Cambridge Analytica app did. Facebook has since clarified that while it announced this change in 2014, apps that already had access to people's friends' data continued to have access until May 2015.

Then, in more than 700 pages of written responses delivered to the House Energy and Commerce Committee late last month, Facebook acknowledged that some apps had this access for up to six months longer, to allow them to ""come into compliance"" with the new rules. There were dozens of companies on the list, including dating apps like Hinge and music-streaming services like Spotify, but one may raise more than a few eyebrows in Washington: the Russian internet giant Mail.ru.

According to Facebook, Mail.ru was given a two-week extension to wind down a feature on two messaging apps that enabled users to see their Facebook friend lists and message with people who also had the Mail.ru apps. During the extension, at least, the app only had access to people's friend lists, not any information about those friends' likes or interests. And yet, long before that extension was in place, Facebook says Mail.ru ran hundreds of apps on the platform, all of which operated under Facebook's old rules, which did allow app developers to collect their users' friends' data. Some of those apps began operating as early as 2009.

""Some apps were built prior to the platform change in 2015, so they did have access to the earlier version of our platform,"" a Facebook spokesperson said. ""That made it possible for users to consent to sharing information about themselves, as well as their friends.""

Facebook says the majority of Mail.ru's apps were test apps that remained private and that only a handful actually launched publicly. It did not share details on how many users may have had their information exposed to Mail.ru apps without their consent. The company adds that Mail.ru's collection of apps have not had access to people's friends' data since May 2015, when Facebook changed its API. Still, Facebook is now investigating Mail.ru, along with all other apps that had access to large quantities of user data prior to the changes. But, the spokesperson says the investigation is not itself a condemnation. ""We found no indication of misuse with Mail.ru. If we detect any suspicious activity or potential misuse, that’s when we formally audit a company.""

Facebook granted thousands of other companies the same data access as Mail.ru prior to 2015. And yet, recent concern over Russia's manipulation of social networks in the run-up to the 2016 election may cast the relationship between the two companies in a new light.",Yes,It can be used to spread false information and cause panic.,https://www.wired.com/story/facebook-gave-russian-internet-giant-special-data-extension/,2018-07-10 21:31:55.807000+00:00,LABEL_1_relevant,0.9961766004562378,Social Media,social_media,Wired,"Information, Discourse & Governance"
175,Why Teens Are Falling for TikTok Conspiracy Theories,"Of all the bits of sleepover gossip ever uttered, this was the most unexpected. “Basically, Hillary Clinton sex traffics children and sacrifices them to Satan or something,” said my friend Beth, straightfaced. “It’s called Pizzagate, or whatever.” It was a Friday night a few weeks ago and even though it was a typical girls' night in, it was not a typical conversation—but I took the bait. “It’s called Pizza what?” I asked, totally unsure of what I was getting myself into. “Dude, you haven’t heard? Hillary and that one guy, John something? Podesta? They’re involved in this massive underground sex trafficking operation. I saw it on TikTok. It’s everywhere.” Half convinced she was kidding, I asked if she actually believed something so obviously ridiculous. “I swear, it’s legit,” Beth replied. “There’s been like five videos talking about it in a row. It’s clearly not a coincidence.”

At the completion of Beth’s ode to Pizzagate, I stared at her, eyebrows raised, mouth open. Taking note of my expression, she rolled her eyes. Then I did too. But for the rest of the night, I watched my two best friends gush over the medley of conspiracy theories that had been flickering across their TikTok “For You” pages and Twitter feeds, listening to them debate which celebrity had exploited the most children and who had sacrificed who to what Hollywood god. As the conversation intensified, and the lack of actual evidence became more and more obvious, I was overwhelmed by a wave of hysteria. It all seemed so ridiculous. How could these seemingly well-educated teenagers—rising high school seniors, even—be gullible enough to take a 60-second social media video for the truth? Conspiracy theories always seemed so juvenile to me. I knew better. Others, it seemed, did not.

They’re not alone. Conspiracy theories are finding a whole new generation of acolytes amongst the teens of TikTok, where—according to a recent New York Times report—#Pizzagate posts have been viewed more than 82 million times in the past few months. This became more obvious as the sleepover’s conversation wore on. Honorable mentions of the evening: the (false) claim that home furnishings company Wayfair is involved in a sex-trafficking operation, the (also false) notion that Oprah Winfrey is involved in a similar plot, and—my personal favorite—that one-time pop-punk princess Avril Lavigne has been replaced by a lookalike. (This is, of course, also not true.) Yet, as the night continued it became obvious the people in my friend circle would be willing to bet their life savings, albeit minimal, on the idea that Ellen DeGeneres coerced Oprah into recruiting children for human trafficking after watching a young girl rant through a 30-second clip. The reason for their faith? The girl had made a PowerPoint and even animated the transitions.

On the surface, it makes sense that young people would latch on to conspiracy theories on TikTok. The platform skews young—reportedly one-third of its daily users in the US are 14 or younger—and celebrity gossip has long been the lingua franca of social media for people of all ages. Right-wing conspiracy groups like QAnon have been spreading made-up stories about those in power on networks like Facebook for years. Now those ideas have jumped to TikTok, where they’re being metabolized by much younger consumers. Those things all scan. What doesn’t, however, is why teens believe them.

ABOUT THE AUTHOR Sofia Barnett is a 17-year-old writer living in Frisco, Texas. Currently a high school senior, she plans to attend college next year and eventually pursue a career in journalism or politics. Despite trying to convince her friends to stay off of social media, she spends approximately 10 hours per week on TikTok.

The short answer? TikTok is full of crazy ideas—conspiracies are no different. They've been normalized by the platform where many young people spend most of their time. “Many of these conspiracy sites and stories are entertaining. They are social gathering spots. They are exciting,” says Nancy Rosenblum, Senator Joseph S. Clark Professor of Ethics in Politics and Government at Harvard University and coauthor of A Lot of People Are Saying: The New Conspiracism and the Assault on Democracy. “It’s small wonder that teenagers who ‘live on the screen’ would be drawn to the drama.”

Easy access to social media’s redistribution tools worsens this problem. With every like, share, send, and retweet, teenagers are popularizing this content worldwide. “On social media, repetition substitutes for validation,” says Russel Muirhead, a professor of democracy and politics at Dartmouth College and Rosenblum’s coauthor. “Repetition is what breathes air into conspiracy theories, and social media is all about repetition. But repeating something that’s false does not make it more true! Teenagers are just as vulnerable to this as grown-ups.”

This wouldn’t be such a problem if teenagers weren’t so attached to social media. So fond, in fact, that some 54 percent of teens get the bulk of their news from it. If this sounds concerning, that’s because it is. With teenagers relying on TikTok as their sole source of information, it makes sense for my generation to become absorbed in webs of falsities and to live as largely uninformed citizens.",Yes,It allows teenagers to became easily gullible and to live as largely uninformed citizens.,https://www.wired.com/story/teens-tiktok-conspiracy-theories/,2020-09-19 11:00:00+00:00,LABEL_1_relevant,0.9960340857505798,Social Media,social_media,Wired,"Information, Discourse & Governance"
176,Russia says it is restricting access to Facebook in the country,"Russia says it is restricting access to Facebook in the country

The Russian government announced Friday that it will begin to “partially restrict” access to Facebook, according to an announcement from its internet regulatory agency Roskomnadzor.

Russia claimed that it would implement the measures, which were not specified, after Facebook put its own restrictions on four Russian state-linked media outlets, the television network Zvezda, news agency RIA Novosti, and the websites Lenta.ru and Gazeta.ru.

“On February 24, Roskomnadzor sent requests to the administration of Meta Platforms, Inc. remove the restrictions imposed by the social network Facebook on Russian media and explain the reason for their introduction,” Roskomnadzor wrote, adding that Facebook “ignored” its requests.

Meta Global Affairs VP Nick Clegg elaborated on the situation on Twitter, indicating that the Russian request came in response to either Facebook’s fact-checking practices or its policy of labeling of state-run media accounts. “Ordinary Russians are using @Meta’s apps to express themselves and organize for action,” Clegg wrote.

Ordinary Russians are using @Meta's apps to express themselves and organize for action. We want them to continue to make their voices heard, share what’s happening, and organize through Facebook, Instagram, WhatsApp and Messenger. pic.twitter.com/FjTovgslCe — Nick Clegg (@nickclegg) February 25, 2022

Facebook did not respond to TechCrunch’s request for comment on the Russian government’s claims and what actions it may have taken against state-backed media on the platform. For now, it’s not clear what partial restrictions mean for Russian Facebook users or if those limitations would extend to Meta-owned platforms like WhatsApp and Instagram.

As Russia deepens its invasion of neighboring Ukraine, thousands of Russians have taken to the streets to demonstrate against the war — in most cases at great personal risk. It’s possible that the Russian government is seeking to tamp down anti-war sentiment online and further shape the narrative of the war, starting with one of the biggest U.S.-owned social networks.

In the U.S., there’s some support for the idea that tech platforms like Facebook should take more strident action to limit the spread of Russian disinformation. A widely shared blog post published Thursday likened Russian President Vladimir Putin to Hitler, encouraging platforms to “choose sides” and crack down on the vast network of Russian propaganda accounts that spread disinformation online.

“Would you demand those publishers pull the plug on Hitler, or would you defend the German state’s right to some confused notion of ‘free speech,’ even as Hitler sets out to destroy the lives of millions?” Justin Hendrix, CEO and editor of Tech Policy Press, wrote in the post.

“That is the situation Google, Facebook, Twitter and other U.S. tech firms are faced with right now.”",Yes.,The way it can spread disinformation and cause divisions in society. This is what the Russian government is exploiting with its latest announcement.,https://techcrunch.com/2022/02/25/russia-facebook-restricted-censorship-ukraine/,2022-02-25 00:00:00,LABEL_1_relevant,0.9956036806106568,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
177,Are Facebook Ads Discriminatory? It’s Complicated,"The Facebook media cycle took a head-whipping quadruple turn over the past few weeks. First, in a surprise move by the Trump administration, the Department of Housing and Urban Development sued the company for violations of the Fair Housing Act, alleging it engaged in discriminatory advertising practices for housing ads.

Within days, researchers at Northeastern University published a preview of a paper describing an interesting set of advertising experiments showing that, indeed, Facebook was differentially showing ads for housing and jobs by gender and race. Lastly, in April the Algorithmic Accountability Act—AAA, naturally—was introduced in both the House and Senate, requiring large tech companies to test for evidence of discriminatory bias in the artificial intelligence that runs platforms like Facebook and Google.

This drama comes on the heels a bevy of other lawsuits alleging that Facebook abetted discriminatory advertising by allowing the targeting of housing ads by race and gender, something the company essentially admitted to doing by vowing to bar that functionality.

Antonio García Martínez (@antoniogm) is a writer and Ideas contributor for WIRED. Previously he worked on Facebook’s early monetization team, where he headed its targeting efforts. His 2016 memoir, Chaos Monkeys, was a New York Times best seller and NPR Best Book of the Year.

The HUD suit and AAA bill are really a step beyond the previous algorithmic bias lawsuits, with potentially far-reaching implications. To understand why requires digging a bit into the semantics and practice of Facebook advertising.

Start with the term targeting. As commonly used, it’s an overly broad category that can mean very different things. Within the industry, targeting refers to the data an advertiser uses to segment the audience, either data supplied by Facebook (say, 25 to 35-year-old Californians) or data that an advertiser brings to Facebook (say, the browsing and buying history of users on an ecommerce site). The key point is that whether the data comes from Facebook or the advertiser, the advertiser’s hands are on the targeting levers, deciding which set of users sees their ad.

But given the complexity of the ad buying and auction process, targeting isn’t the only thing that determines who sees an ad. Within the segment specified by the advertiser, Facebook can skew which type of user eventually views that targeted ad.

That’s typically called optimization, which is what’s at issue in the HUD suit and the AAA. Particularly for broadly targeted ads—say, every US millennial—Facebook itself further narrows the targeted set based on what it knows about the user and its platform. If the ad is about fashion, then it’ll pick users who’ve shown interest in fashion brands. If the initial run of the ad over a broad audience shows that some subsegment—say, people in Texas—engage with it more than others, then it’ll quickly start biasing the showing of the ad to Texans only.

In a world of perfect targeting, where the advertiser has absolute control of the user experience and knows all Facebook user data, no optimization would be necessary. A sophisticated advertiser could simply train the same machine-learning models that Facebook does.

In a world of perfect optimization, where Facebook knows even off-Facebook data like purchases, then no targeting would be necessary. Facebook could simply take what the advertiser knows into account about what the user has bought or browsed, and provide as near-perfect an ad experience as possible.

In reality, of course, neither targeting nor optimization is perfect, so both work interdependently: The advertiser doesn’t trust Facebook enough to hand over all of its targeting data, and Facebook doesn’t want to share its optimization data (at least not intentionally) with outsiders.

What happens in the commercial reality of ad buying is the advertiser shows up with a best guess at a target audience, some ad creative, and a bid for a desired user action such as a click or app install. Facebook uses the advertiser’s targeting to whittle down the set of potential targets, and when one such user shows up, estimates how likely they are to click on the ad or download the app.",Yes,"It has become an amplifier of hate speech, fake news, and conspiracy theories.

The advertiser has a maximum amount they’re willing to pay for that click or install, and if that estimated probability is high enough, Facebook sells the ad impression to the advertiser. But if that estimate is too low, then Facebook keeps the",https://www.wired.com/story/are-facebook-ads-discriminatory-its-complicated/,2019-06-11 15:00:00+00:00,LABEL_1_relevant,0.9962271451950072,Social Media,social_media,Wired,"Information, Discourse & Governance"
178,Police in India visited Twitter offices over 'manipulated media' label,"Delhi police, controlled by India’s central government, on Monday evening visited two offices of Twitter — in the national capital state of Delhi and Gurgaon, in the neighboring state of Haryana — to seek more information about Twitter’s rationale to label one of the tweets by ruling partly BJP spokesperson as “manipulated media.”

An hour into the search attempt, Delhi Police Special Cell team, which investigates terrorism and other crimes, vacated Twitter’s offices because they were closed and there were no Twitter employees to engage with at the premises, according to live broadcast by several local news channels.

A Twitter spokesperson declined to comment. India, the world’s second-largest internet market, is a key overseas region for Twitter and many other American technology firms.

New Delhi sent a notice to Twitter last week after the social network labeled a tweet from Sambit Patra, the spokesperson of India’s ruling party BJP, as “manipulated media.”

In the tweet, Patra had claimed that Congress, the leading opposition party in India, was using a so-called “toolkit” to derail the Indian government’s efforts against the coronavirus pandemic. Alt News, a leading fact-checking organization in India, had debunked Patra’s claim.

Delhi police said it had received a complaint about the classification of Patra’s tweet, which it said it was investigating, and visited the offices to serve Twitter India’s head a notice of the inquiry. In a statement, the police said Twitter India’s managing director’s replies on the subject had been “very ambiguous.”

Delhi Police Special Cell team returns from Gurgaon after they find the Twitter India offices shut. Apparently there is work from home at @TwitterIndia since March last year. Was this move by Government to send out a message? pic.twitter.com/aCBfjhb5CC — Aditya Raj Kaul (@AdityaRajKaul) May 24, 2021

“Delhi Police is enquiring into a complaint in which clarification is sought from Twitter regarding the classification of a tweet by Shri Sambit Patra (BJP spokesman) as ‘manipulative’. It appears that Twitter has some information which is not known to us on the basis of which they have classified it as such,” Delhi Police said in an earlier statement to local TV channels and other journalists.

“This information is relevant to the enquiry. Special Cell which is conducting the enquiry wants to find out the truth. Twitter which has claimed to know the underlying truth should clarify,” it added.

In a follow-up statement, it disputed the characterization of today’s event as “raiding.”

Several policy executives and the like questioned Delhi Police’s motives.

As India seeks priority assistance from the international community & esp from the US, I guess this is one way to convince them that it is focused on the real problem at hand. https://t.co/QqBhbx0oYN — Tanvi Madan (@tanvi_madan) May 24, 2021

Label a ruling party spokesperson's tweet as containing manipulated media? Well, the Union Govt will bsend in the Special Cell stormtroopers of the Delhi Police to your India subsidiary's office in the national capital region to show you what's what. Blatant authoritarianism. https://t.co/rzt8Nf0bVd — Raman Chima (@tame_wildcard) May 24, 2021

This would be funny if it wasn’t tragic. @DelhiPolice knows that no data is physically stored at @TwitterIndia offices hence purpose of raid is nothing but intimidation. https://t.co/jgfWTMO2L2 — Raheel Khursheed (@Raheelk) May 24, 2021

The move also comes at a time when Twitter and Facebook are inching closer to the deadline to comply with India’s new guidelines to regulate social media.

In a new notice to both the firms, New Delhi warned that “failure to comply with IT rules could lead to loss of status and protections as intermediaries.”

Today’s development is the latest headache for the American firm, which has been struggling to maintain peace with the Indian government for several months.

After briefly complying with a New Delhi order early this year, the company faced heat from the government for restoring accounts that had posted tweets critical of the Indian government’s policy or the Prime Minister Narendra Modi.

The two faced off again publicly last month after New Delhi ordered Twitter and Facebook to take down posts that were critical of the government’s handling of the coronavirus pandemic.

The story was updated throughout with additional details.",Yes,"It leads to echo chambers and filter bubbles.

This is because people tend to follow and interact with people who share similar views and interests, and social media platforms use algorithms to show users content that is similar to what they have engaged with in the past.

As a result, people are exposed to a limited range of perspectives and information",https://techcrunch.com/2021/05/24/delhi-police-run-by-indias-central-government-raids-twitter-offices-over-manipulated-label/,2021-05-24 00:00:00,LABEL_1_relevant,0.9967471361160278,Social Media,social_media,TechCrunch,"Information, Discourse & Governance"
179,California Reveals It’s Been Investigating Facebook,"Over the last two months, nearly every state and territory has announced investigations into the market dominance of Facebook, Google, or both. Missing from those probes was the place Big Tech calls home: California.

The Golden State’s attorney general, Xavier Becerra, has been curiously silent as his peers made plans to examine the biggest tech firms in his backyard. Becerra's supposed inaction garnered him a wave of negative local press, and last week led The New York Times to wonder what, exactly, his office was up to.

On Wednesday, California’s top cop finally revealed he has actually been very busy investigating Facebook, in an inquiry that has lasted for over a year. At a press conference announcing a new lawsuit, Becerra said Facebook has refused to fully cooperate with his probe. “We make our work public when there is a legal action to make public, otherwise we do not discuss our investigations,” he said.

The California attorney general is suing Facebook to turn over key information the company has allegedly failed to release, including email correspondence from both CEO Mark Zuckerberg and COO Sheryl Sandberg. The documents are connected to an 18-month investigation into the social network’s data protection and privacy practices.

Is there something you think we should know about Facebook's privacy practices? Email the writer at louise_matsakis@wired.com. Signal: 347-966-3806. WIRED protects the confidentiality of its sources, but if you wish to conceal your identity, here are the instructions for using SecureDrop. You can also mail us materials at 520 Third Street, Suite 350, San Francisco, CA 94107.

The California probe began in June 2018, shortly after news broke that Cambridge Analytica, a political consulting firm whose clients included the Trump campaign, had accessed data on tens of millions of Facebook users. “What initially began as an inquiry into the Cambridge Analytica scandal expanded over time to become an investigation into whether Facebook has violated California law by, among other things, deceiving users and ignoring its own policies,” reads the lawsuit, which was filed in state superior court Wednesday.

Becerra said Facebook has hampered the investigation by refusing to fully cooperate with subpoenas. “If Facebook had complied with our legitimate investigative requests, we would not be making this announcement today. But we must move our investigation forward,” he added. The lawsuit notes Facebook allegedly was uncooperative as well during a Federal Trade Commission investigation into the company’s privacy practices, which resulted in a $5 billion penalty in July.

“We have cooperated extensively with the state of California’s investigation,"" Will Castleberry, Facebook's vice president of state and local policy, said in a statement. ""To date we have provided thousands of pages of written responses and hundreds of thousands of documents.”

Facebook has been very familiar with government scrutiny lately, and just not in its own home state. The company is already facing similar investigations in Massachusetts and the District of Columbia. The attorney general of New York, Letitia James, is leading a group of 46 states and territories examining the company’s market dominance and privacy practices. The FTC has opened another inquiry into the social network too, this time over antitrust concerns. You also can’t forget the House Judiciary Committee’s antitrust investigation, which is looking at tech companies more generally but certainly has Facebook in its sights.

No one knows yet what each of these inquiries might mean for Facebook in the end. Some politicians, including Democratic presidential candidate Elizabeth Warren, have suggested breaking up Facebook by unwinding acquisitions like Instagram. In an internal meeting transcript published by the Verge last month, Zuckerberg acknowledged the way regulators have been discussing his company isn’t exactly ideal. “I think that the direction of the discussion is concerning,” he said.

Content This content can also be viewed on the site it originates from.

More Great WIRED Stories",,...,https://www.wired.com/story/california-attorney-general-facebook-investigation/,2019-11-06 22:05:36.381000+00:00,LABEL_1_relevant,0.9959883093833924,Social Media,social_media,Wired,"Information, Discourse & Governance"
180,People Are Tweeting About Videogames Now More Than Ever,"Greetings, and welcome to this Monday's edition of The Monitor, WIRED's culture news roundup. What's happening? Glad you asked. First up, videogames are huge on Twitter. Second, Universal has canceled the release of one of its upcoming films. And finally, folks can finally get a glimpse of the new Apple TV+ show from Jennifer Aniston and Reese Witherspoon. Let's go!

People Are Tweeting About Videogames Now More Than Ever

Here's some news that's pretty surprising—and yet, totally not. According to new data, tweets focusing on videogames, and the videogame industry, are having a bang-up 2019. In the first half of this year, there were more than 500 million gaming-related messages posted on Twitter. That number marks an increase of 20 percent year over year. Another interesting stat: This year has seen a 15 percent increase in accounts that tweet about games. The data signals not only an increase in gaming chatter generally, but also points to recent news events—like Ninja leaving Twitch, and President Trump's recent comments blaming videogames for gun violence—dominating online conversation. ""It's been crazy to look at the past couple weeks and see a huge influx of trends coming from gaming,"" Rishi Chadha, head of gaming content partnerships at Twitter, told the Hollywood Reporter.

Universal Cancelled the Release of The Hunt

Following the mass shootings in Ohio and Texas that left dozens dead and wounded, Universal Pictures has canceled the release of The Hunt, which was set to hit theaters on September 27. The movie, which the studio calls a ""satirical social thriller,"" focuses on a group of wealthy people who hunt others for sport. Universal's decision came on the heels of Trump making a presumed dig at the film, tweeting ""Liberal Hollywood is Racist at the highest level, and with great Anger and Hate! They like to call themselves 'Elite,' but they are not Elite. In fact, it is often the people that they so strongly oppose that are actually the Elite. The movie coming out is made in order to inflame and cause chaos."" Universal, meanwhile, said the reason for canceling the release was that ""now is not the right time to release this film."" It's unclear if the release will ever be rescheduled.

Here's the First Trailer for That Jennifer Aniston-Reese Witherspoon Apple TV+ Show

You've been hearing about it for some time and now it's almost here. The Morning Show, which stars Jennifer Aniston, Reese Witherspoon, and Steve Carrell, will be landing on Apple TV+ this fall. The first trailer for the series about an AM news program is below. No word yet on whether or not there will be an Apple News+ tie-in.

Content This content can also be viewed on the site it originates from.

More Great WIRED Stories",,"We can't stop thinking about it.

The tech industry's history of discrimination is long, and it's time for a change.

The massive wildfire in the Amazon is spreading—and climate change has made it worse.

Review: The new iPadOS is what the iPad has needed since the beginning

The world's",https://www.wired.com/story/videogames-twitter/,2019-08-12 17:44:08.567000+00:00,LABEL_1_relevant,0.9951571822166444,Social Media,social_media,Wired,Environment & Sustainability
181,These Trump Socks Went Viral—and a Nightmare on Amazon Began,"When Donald Trump visited Louisiana earlier this month, he was greeted with an unexpected hairy surprise. Billy Nungesser, the state’s lieutenant governor, got dressed with the commander-in-chief in mind that morning. With the president and news cameras as his witness at the airport, Nungesser joyfully lifted the leg of his pants to reveal a goofy pair of socks: Each ankle bore Trump’s face, complete with a signature tuft of fake blond locks waving daintily in the breeze. The bizarre-looking socks quickly went viral and were covered by a smattering of news outlets. Stephen Colbert even mentioned them on The Late Show.

For Erica Easley, all the attention was great, at least at first. Easley is the founder of Gumball Poodle, a small Los Angeles–based sock company that originally came up with the hirsute design during the last presidential election. “They went really viral, beyond anything we’d ever experienced before,” she says about the aftermath of Nungesser’s photo-op. “And these socks have been on Rachel Maddow, The View, a bunch of things in 2016.” Wholesale orders started ticking up. Several media outlets linked to Gumball Poodle’s Amazon listing, and soon the Trump socks reached Amazon's best-seller list for men's novelty socks.

Louisiana Lieutenant Governor Billy Nungesser shows Donald Trump his socks. BRENDAN SMIALOWSKI/AFP/Getty Images

About a week passed before Easley noticed that something had gone horribly wrong. Dozens of third-party merchants, most of whom looked to be from China, had jammed her Amazon listing with what Easley believed to be knockoffs, selling for a fraction of the original $30 price tag. (Included in that price, for the record, is a tiny comb, to style your socks' hair. Everything is made in the USA.) To make matters worse, Amazon had chosen one of the frauds as the default seller, shutting Gumball Poodle out. Meanwhile, other third-party sellers appeared to have taken Easley’s photos and set up their own, much cheaper listings.

Louise Matsakis covers Amazon, internet law, and online culture for WIRED.

Easley had done everything to protect her business from exactly this kind of attack. Her hairy sock design is patented in the US, and her logo, which is stamped on the bottom of the socks, is trademarked. What’s more, Gumball Poodle is enrolled in the Amazon Brand Registry, an enhanced suite of tools the company provides eligible brands to protect their intellectual property. But Easley found Amazon’s protections weren’t enough, and she says the company largely ignored her pleas for help. Only after WIRED reached out to Amazon for this story were the counterfeits removed.

“Amazon strictly prohibits the sale of counterfeit products and we invest heavily in both funds and company energy to ensure our policy is followed,” Maxine Tagay, a spokesperson for Amazon, said in a statement.

Contending With a $1 Trillion Industry

Gumball Poodle isn’t alone. “There are thousands of other trademark owners who face the same kind of nonsense every single day,” says James Thomson, a former Amazon employee and a partner at Buy Box Experts, a firm that consults with Amazon sellers. “Amazon does have a problem with counterfeits.”

Counterfeiting is a booming, trillion-dollar industry that costs businesses around the world billions of dollars a year. Its growth has been fueled by the rise of ecommerce, which a government report says has led to “a fundamental change in the market for counterfeit goods.” That report, published last year by the Government Accountability Office, found that the volume and variety of counterfeit goods seized by officials has grown year after year, and it’s increasingly difficult to distinguish knockoffs from the real thing. It also notes that fraudulent goods are sold on a number of different ecommerce platforms. The Trump administration has signaled it wants to take the issue more seriously. In April, President Trump asked the Justice, Commerce, and Homeland Security Departments for recommendations on potential regulatory and legislative fixes to address counterfeits on third-party marketplaces.

But on Amazon, counterfeits can be uniquely devastating, in part because of the site’s sheer scale. Half of all US ecommerce sales go to Amazon, and the site is also where about half of all product searches on the web begin. Not many companies can afford to avoid it. If they do stay away, they risk letting other sellers determine how their brand is marketed on one of the biggest online retailers in the world.",Yes,"It has made it much easier for people to connect with others who share their interests. This has led to a number of positive outcomes, such as the ability to connect with like-minded people and the spread of information. However, it has also led to a number of negative outcomes, such as the spread of misinformation and theecho chamber effect.",https://www.wired.com/story/viral-trump-socks-amazon-counterfeits/,2019-05-31 17:12:38.038000+00:00,LABEL_1_relevant,0.99537992477417,Social Media,social_media,Wired,"Information, Discourse & Governance"
182,Facebook's Targeted Ads Are More Complex Than It Lets On,"In a recent blog post, Facebook's vice president for ads, Rob Goldman, argues his platform's users aren't its product. Even though Facebook primarily makes money by selling targeted ads based on what it knows about you, Goldman says that the real product is the ability to connect people—ads merely exist to ""fund that experience.""

To help support that stance, Goldman paints a simple picture of the role advertising plays on Facebook, downplaying the information it collects about you. Using a hypothetical example about a small bike shop in Atlanta, he emphasizes that targeted ads help small businesses reach customers—like, say, female cyclists who live nearby—more efficiently. The example, which the social network also uses on a general page explaining how ads work, does represent how some companies use Facebook. But ad industry experts say Goldman's explanation leaves out many important realities of Facebook's advertising machine.

Relevant to What?

Four times in his blog post, Goldman stresses that Facebook's targeting mechanisms allow users to see relevant ads. But nowhere does he define what ""relevant"" means in this context. In some ways, it's broadly intuitive across all industries; advertisements for dentures or funeral insurance don't run on Nickelodeon for a reason. But beyond simple demographics, a ""relevant"" ad to a marketer might target a specific personality type, or perceived emotional state. It might also be designed to take advantage of an already vulnerable population. That can quickly get a lot more involved than just people who like bikes.

'There’s 60,000 channels and weird ways to combine them.' Kane Jamison, Content Harmony

“We already have been seeing the results of negative segmentation we saw in the past before, like when cigarette companies were targeting low-income people,” says Juan Mundel, a professor at DePaul University who has studied Facebook advertising. Because the social network has so much data, it's possible to target hyper-specific audiences with extreme precision. That means, as Bloomberg reported in March, predatory advertisers can exploit Facebook's tools to sell shady products to the masses, like diet pills.

“Facebook also knows when you’re motivated to do something, when you’re feeling down, when you’re feeling all sorts of emotions,” says Mundel. The social network leverages that information for advertisers; the Intercept discovered earlier this month that the company has developed a new service designed to predict how consumers will behave in the future, like when they're likely to switch from one product brand to another. That level of psychological parsing goes far beyond what Goldman outlines.

Goldman is right to point out that Facebook has much in common with traditional forms of advertising like television and print, but the difference is companies who use Facebook have a near-endless number of data points with which to target their ads, and can show them to much narrower slices of the population. ""Facebook is the same thing, but there’s 60,000 channels and weird ways to combine them as well,” says Kane Jamison, the founder and managing director of Content Harmony, a marketing agency that frequently uses Facebook to advertise.

What Facebook Knows

Throughout Goldman's post, he stresses that users can control their ad experience by visiting their Ad Preferences menu. At the top of the screen, you will see Your interests, which Facebook says it generates based on your activities on Facebook, such as pages you may have liked. It's not clear whether some of these categories are algorithmically generated, and Facebook would only say they are based on past actions on the platform.

The categories range from intuitive to bizarre. Mine, for example, include head-scratching topics like ""Laser,"" ""Steel,"" ""Everything,"" and ""Authority."" If you hover your cursor over each one, Facebook ostensibly tells you why it first appeared: ""You have this preference because you clicked on an ad related to Everything."" Huh? Advertisers have a stunning number of categories to sift through—ProPublica has collected over 50,000, including those only marketers can see. You can remove any interest associated with your profile by clicking the X in the top right-hand corner.

Toward the bottom of the screen, under Your information > Your categories, you can see other information Facebook has collected about you, based on your activities both on and off the platform. There, you might notice that Facebook has categorized you as a ""newlywed,"" ""away from family,"" or ""close friends with expats."" You will likely also see the type of browser and phone you use. You can remove any of these categories as well.",Yes,"It can give you a false sense of intimacy with others.

Intimate social media can make you feel as if you know people better than you actually do, which can lead to inappropriate behavior.

It can also lead to feeling like you are a part of a group when you are not, which can make you more susceptible to peer",https://www.wired.com/story/facebooks-targeted-ads-are-more-complex-than-it-lets-on/,2018-04-25 20:04:38.005000+00:00,LABEL_1_relevant,0.9957196116447448,Social Media,social_media,Wired,Social Norms & Relationships
183,"Elon Musk's Mouth, and Tweets, Land Him in Trouble Again","Twitter CEO Jack Dorsey once said that Tesla and SpaceX CEO Elon Musk is the most exciting and influential person on Twitter. Musk loves to kibbitz with fans, answering questions about his tech, making reading and music suggestions, and solving customer-service issues in real time. But court documents released late Monday show Musk and associates scrambling to deal with what one might call excessive openness on Twitter—openness that has gotten Musk in (more) legal trouble.

According to emails included in the filing, Musk has regrets. “I'm a fucking idiot,” he wrote to a public relations adviser in September 2018.

Stay in the know with our Transportation newsletter. Sign up here!

The court filings are from defamation lawsuit filed by a Thailand-based British cave expert named Vernon Unsworth, who aided in the rescue of a Thai youth soccer team and their coach in July 2018. Unsworth’s lawyers contend that Musk is “a thin-skinned billionaire” who “orchestrated a malicious, false, and anonymous leak campaign in the UK and Australian press” that sought to paint Unsworth as a pedophile. The drama included a $52,000 private investigation by a convicted felon, as well as a Musk associate acting under an assumed name.

Musk had his own plan to rescue the soccer team, involving a child-sized mini-submarine. Unsworth told a CNN reporter that Musk’s plan was a publicity stunt that wouldn’t work. Musk, he said, could “stick his submarine where it hurts.”

A few days later, Musk saw Unsworth’s comments online, and spent less than an hour searching the man’s name online, Musk said in an August 2019 deposition. The billionaire was frustrated, he told lawyers during the deposition: “Who the hell is this guy, and what the hell is he doing insulting me, insulting everything my team did, the hard work everyone put in to try to help these kids?” he said. After finding an article linking Unsworth’s home city of Chiang Rai with the sex trade, Musk defended his submarine plan in a series of tweets to his 20 million followers, and called Unsworth a “pedo guy.” The CEO apologized for the tweets three days later.

But Musk didn't let the matter drop there, according to court filings. According to the court filings, a British man named James Howard-Higgins reached out to Musk in August 2018 about Unsworth, claiming to be a private investigator and alleging the diver had “skeletons in his cupboard.” Musk allegedly passed Howard-Higgins’ information to the head of his family office, a man named Jared Birchall. Birchall assumed the name James Brickhouse for his dealings with Howard-Higgins, and as Brickhouse, used more than $50,000 of Musk’s funds to hire the investigator to dig into the expat’s history.

Musk said Howard-Higgins passed Birchall information alleging that Unsworth had married a girl when she was as young as 12, though Unsworth’s lawyers say Howard-Higgins never told Birchall that Unsworth’s wife was any younger than 18 or 19 at the time of their marriage. (In fact, the diver’s wife was 32 when they met, the lawyers say.) BuzzFeed News later revealed that Howard-Higgins was in fact a fraud who had previously been convicted for stealing from a business partner, and is now in prison for violating the conditions of his parole. Either way: In August 2018, Musk responded to a BuzzFeed News reporter’s email about the “pedo guy” tweet by alleging that Unsworth was a “child rapist.” The reporter published Musk’s allegations in an article—allegations, Unsworth lawyer’s say, that are false.",Yes,"People can connect with each other very easily, which can lead to a variety of both positive and negative outcomes. On the one hand, social media can be used to connect with friends and family members, which can lead to increased communication and bonding. On the other hand, social media can also be used to spread false information, which can lead to",https://www.wired.com/story/elon-musks-mouth-tweets-trouble-again/,2019-10-08 23:03:53.666000+00:00,LABEL_1_relevant,0.996409833431244,Social Media,social_media,Wired,"Information, Discourse & Governance"
184,Facebook Hosted Three Huge Concerts in the Metaverse and They Seriously Flopped,"Well, Facebook’s metaverse is off to a depressing start.

Meta — aka the artist formerly known as Facebook — announced a slate of big name virtual reality concerts set to take place entirely in in its Horizon Venues metaverse last month. The star-studded lineup included rapper Young Thug on December 26, DJ David Guetta on December 31, and finally EDM duo The Chainsmokers for a New Years’ Eve concert to ring in 2022.

The only problem? Not many people noticed — and those who did didn’t seem to really care.

Despite being completely free, the concerts seemed to get incredibly little traction or notice from anyone outside of Meta offices. There was barely any mention of any of the performances on social media platforms like Twitter or even Facebook, let alone on news and music publications.

Advertisement

Advertisement

It probably wasn’t helped by the fact that the concerts were barely optimized for a VR experience. Guetta just had a 2D live streamed video of his performance at the Louvre Abu Dhabi. Meanwhile, Young Thug and The Chainsmokers offered just a 180 degree view of their concert for Oculus users.

Young Thug’s performance racked up just north of 100,000 views, while Guetta’s performances accumulated almost a million. The Chainsmokers’ performance, for its part, seems to have been removed. And keep in mind, too, that Facebook has been known to put a very optimistic spin on view counts previously in order to build hype for its fledgling video streaming platform.

If you think about it for just a second, it makes sense. Watching a concert alone while wearing a VR headset at home isn’t how a lot of people want to spend their New Year’s Eve — even if they are massive fans of The Chainsmokers for some reason.

Overall, it seems like the metaverse concerts weren’t the hit Zuckerberg was hoping they’d be. And the elephant in the room, of course, is Travis Scott’s Fortnite concert last year, which drew tens of millions of participants and commanded huge amounts of media attention.

Advertisement

Advertisement

So while Meta has set some ambitious goals for itself, it seems like creating a successful metaverse and drawing new users in is going to take a lot more than some free concerts.

More on Oculus troubles: Christmas Present VR Headset Sends Boy to the Hospital

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Yes,"Creating echo chambers and filter bubbles.

Advertisement

Advertisement

In other words, people tend to only see content that reaffirms their existing beliefs, which can lead to a lot of misinformation and tribalism.",https://futurism.com/facebook-concert-metaverse-flopped,,LABEL_1_relevant,0.9885566234588624,Social Media,social_media,Futurism,"Information, Discourse & Governance"
185,Facebook says it’s going to use machine learning to catch revenge porn,"That Facebook thinks it can tackle the problem in this way shows how just powerful its ability to identify people with AI has become.

The news: Facebook announced today that it will use machine learning to detect and block nude or near-nude images and videos that have been shared without permission—before they have even been reported. Revenge porn (the sharing of sexual videos of someone, usually a past partner, without consent) has become a serious problem, with devastating consequences for victims. Facebook also says it will overhaul the process by which victims can report unapproved images.

Face time: Facebook didn't say exactly what sort of machine learning it was going to use, but it has an almost unparalleled ability to identify people in images, thanks to a vast corpus of labeled training data supplied by its own users.

Training sets: Although many users are unaware, social-media photos are widely used to train machine-learning algorithms. The state-of-art programs are often now better than humans at recognizing people in snaps.

Coming threat: New detection technology could become especially important as it becomes ever easier to generate convincing-looking fake video with AI. The rise of easy-to-use face-swapping software has already led to a proliferation of fake celebrity porn and other weird video mashups.

Silver bullets: The Facebook effort is a worthwhile use of machine learning, but AI is no silver bullet for dealing with harassment, abuse, or fake news on social media (regardless of what Mark Zuckerberg might tell Congress). Humans will always find ways to outwit the best algorithms. Besides that, the problem sadly extends far beyond the walls of Facebook.

Sign up here to our daily newsletter The Download to get your dose of the latest must-read news from the world of emerging tech.",Yes,"It has become a breeding ground for harassment, abuse, and fake news. The problem is only compounded by the fact that it is now easy to generate convincing-looking fake video with AI. While Facebook's new machine learning effort is a worthwhile use of AI, it is no silver bullet for dealing with these problems.",https://www.technologyreview.com/2019/03/15/136597/facebook-says-its-going-to-use-machine-learning-to-catch-revenge-porn/,2019-03-15 00:00:00,LABEL_1_relevant,0.9967086315155028,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
186,Rumble Sends Viewers Tumbling Toward Misinformation,"“I'm not really expecting things to ever be what they were,” says Sarah. “There's no going back.” Sarah’s mother is a QAnon believer who first came across the conspiracy theory on YouTube. Now that YouTube has taken steps toward regulating misinformation and conspiracy theories, a new site, Rumble, has risen to take its place. Sarah feels the platform has taken her mother away from her.

Rumble is “just the worst possible things about YouTube amplified, like 100 percent,” says Sarah. (Her name has been changed to protect her identity.) Earlier this year, her mother asked for help accessing Rumble when her favorite conservative content creators (from Donald Trump Jr. to “Patriot Streetfighter”) flocked from YouTube to the site. Sarah soon became one of 150,000 members of the support group QAnon Casualties as her mother tumbled further down the dangerous conspiracy theory rabbit hole.

Between September 2020 and January 2021, monthly site visits to Rumble rose from 5 million to 135 million; as of April, they were sitting at just over 81 million. Sarah’s mother is one of these new Rumble users, and, according to Sarah, is now refusing to get the Covid-19 vaccine. Explaining her decision, says Sarah, her mother cites the dangerous anti-vax disinformation found in many videos on Rumble.

ABOUT Ellie House is a UK-based investigative journalist writing about tech and companies; she has previously worked with Private Eye, the BBC World Service, and the Investors Chronicle. Alice Wright is an investigative journalist based in London covering politics and environmental issues; she has written for Private Eye, The Times, Prospect, and others. Isabelle Stanley is an investigative journalist covering social justice issues for publications including the Sunday Times and Byline Times.

Rumble claims that it does not promote misinformation or conspiracy theories but simply has a free-speech approach to regulation. However, our research reveals that Rumble has not only allowed misinformation to thrive on its platform, it has also actively recommended it.

If you search “vaccine” on Rumble, you are three times more likely to be recommended videos containing misinformation about the coronavirus than accurate information. One video by user TommyBX featuring Carrie Madej—a popular voice in the anti-vax world—alleges, “This is not just a vaccine; we’re being connected to artificial intelligence.” Others unfoundedly state that the vaccine is deadly and has not been properly tested.

Even if you search for an unrelated term, “law,” according to our research you are just as likely to be recommended Covid-19 misinformation than not—about half of the recommended content is misleading. If you search for “election” you are twice as likely to be recommended misinformation than factual content.

Courtesy of Ellie House, Isabelle Stanley and Alice Wright; Created with Datawrapper

The data behind these findings was gathered over five days in February 2021. Using an adaptation of a code first developed by Guillaume Chaslot (an ex-Google employee who worked on YouTube’s algorithm), information was collected about which videos Rumble recommends for five neutral words: “democracy,” “election,” “law,” “coronavirus,” and “vaccine.” The code was run five times for each word, on different days at different times, so that the data was reflective of Rumble’s consistent recommendation algorithm.

Over 6,000 recommendations were manually analyzed. There can be disagreements about what can and cannot be classed as misinformation, so this investigation erred on the side of caution. For example, if a content creator said “I won’t take the vaccine because I think there might be a tracking chip in it,” the video was not categorized as misinformation. Whereas if a video stated “there is a tracking device in the vaccine,” it was. Our conclusions are conservative.",Yes,It has allowed misinformation to spread.,https://www.wired.com/story/rumble-sends-viewers-tumbling-toward-misinformation/,2021-05-11 13:00:00+00:00,LABEL_1_relevant,0.9961834549903872,Social Media,social_media,Wired,"Information, Discourse & Governance"
187,A Growing Frontier for Terrorist Groups: Unsuspecting Chat Apps,"Heads up, tech companies: If your product appeals to the masses, it likely also holds allure for terrorist groups like ISIS.

WIRED OPINION ABOUT Rita Katz is the Executive Director and founder of the SITE Intelligence Group, the world’s leading non-governmental counterterrorism organization specializing in tracking and analyzing the online activity of the global extremist community.

ISIS has effectively exploited the power of technology to fuel its rise around the globe, from streaming and file-sharing platforms to messenger applications and social media services. Many tech companies have responded in turn, strengthening their oversight and security measures. But while major platforms like Facebook, Twitter, YouTube, and Telegram are becoming increasingly inhospitable to ISIS, the group's reach is growing on lesser-known messenger apps designed for businesses and gamers.

In the aftermath of major territory losses in Iraq and Syria, ISIS is reconfiguring how it uses technology to drive its recruitment and coordination efforts.

The move towards free public messenger apps

ISIS uses the encrypted messenger platform Telegram as its primary app for media releases. In seeking new venues to disseminate its content, the terrorist group has made repeated attempts to set up web pages and blogs on services like Tumblr and WordPress. Such platforms are fitting places for ISIS propaganda, where content can be transferred from Telegram into organized, easily accessible layouts.

Throughout 2018, the group's Amaq News Agency invested significant effort into staying on these webpage and blogging platforms, from implementing Cloudflare to protect against DDOS attacks to creating browser extensions that immediately provide users with new URLs to their sites. Those measures ultimately failed, as each of their sites was hacked or removed.

Thus, emerging messenger applications—particularly those with features modeled after social media, such as chat groups, channels, and media-sharing—have become an effective alternative for ISIS.

RocketChat

In mid-December 2018, some of the most prominent ISIS-linked media groups announced channels on RocketChat, an open-source messenger service designed for businesses. The platform is both mobile and desktop enabled; since its founding in 2015 it has grown to 10 million users. Nashir News Agency, the ISIS-linked media dissemination group, urged supporters to join the app, stating, ""God willing, media will be published on RocketChat before Telegram.""

Many other ISIS-linked media groups embraced RocketChat nearly simultaneously. Some provide instructions for carrying out terrorist attacks, while others mirror content from ISIS's mother-channels on Telegram. In no time, RocketChat was host to content from the ISIS-linked Khilafah News, the attack-guide channel “Just Terror,” the English language ISIS media outlet Halummu, and the longstanding deep-web discussion forum Shumukh al-Islam. A technical manual published by the ISIS-linked “Afaaq Electronic Foundation” on December 15 instructed followers on how to install and anonymously use RocketChat.

A review of these RocketChat rooms shows that Amaq News Agency, ISIS’ official news outlet, was the first to hold an account on the platform, suggesting that the migration to Rocket was instructed by ISIS central. These groups continue to grow on the platform. As of January 8, there were over 700 users on the server that houses ISIS's channels. It requires only a private invite link and an email address to join.

RocketChat offers both cloud-hosting on its own servers and self-hosting. A server hosted by RocketChat risks being taken down by the platform, whereas a self-hosted server is immune to takedowns, except by the web host.

Yahoo Together

Around the same time they were promoting RocketChat, some major ISIS-linked channels also announced accounts on Yahoo Together, a new mobile-only messaging application launched in July to replace Yahoo Messenger. The platform has thus far been used in a similar fashion to RocketChat, mirroring communiques, photo reports, and media statements first posted to Telegram.

Unlike RocketChat, however, Yahoo Together appears to have quickly removed the ISIS accounts. Since then, ISIS groups have not advertised accounts on Yahoo's app.

Viber

Last month, ISIS also announced a Nashir News Agency account on Viber, another cross-platform instant messenger app. Though ISIS-linked media groups and supporters have long used the platform, their official promotion of Viber signaled a new level of investment.

The Nashir News Agency account appears to have since been removed, but its initial establishment on Viber is likely encourage other ISIS media groups and supporters create accounts on the platform.

Discord

Beyond the group’s media officials, die-hard jihadi supporters are expanding their online outreach efforts in more unexpected places.

One such example is Discord, a messenger application for gamers with more than 130 million registered users. The platform organizes its chat communities into “servers,” each of which contains numerous text and voice channels.",Yes,"It allows for easy connection and communication. This is a double edged sword, as it can be used for good or bad. For example, social media can be used to connect with friends and family, but it can also be used to spread fake news or coordinate terrorist attacks.",https://www.wired.com/story/terrorist-groups-prey-on-unsuspecting-chat-apps/,2019-01-09 20:00:00+00:00,LABEL_1_relevant,0.9965245127677916,Social Media,social_media,Wired,"Information, Discourse & Governance"
188,Your Tweets Can Help Map the Spread of Wildfire Smoke,"This story originally appeared on High Country News and is part of the Climate Desk collaboration.

At the end of July, Twitter user Alicia Santana posted a photo of a man sitting in a plastic folding chair in his yard. He’s looking away from the camera, towards a monstrous, orange cloud of smoke filling the sky beyond a wire fence. “My dad not wanting to leave his home,” Santana wrote, ending it with #MendocinoComplexFire.

As wildfires spark, parts of the internet glow with them. The #CarrFire, #FergusonFire, #RanchFire and other hashtags spread quickly this summer on Twitter. If past seasons are any indication, there will be thousands more tweets like this, and they will continue as smoke—the insidious second wave of wildfires—spreads across the West. They can also be used for data. In a recently published study, US Forest Service researchers Sonya Sachdeva and Sarah McCaffrey found that, when analyzed in large numbers, tweets about wildfires can accurately model the way smoke moves.

In their study, published by the International Conference on Social Media & Society, Sachdeva and McCaffrey analyzed close to 39,000 tweets posted between May and September 2015 in California. They stripped the tweets to reveal their core subjects: smoke in the air, fallen ash, haziness, smell. By tagging the tweets with the location in which they were posted, the researchers created a verbatim map: a landscape of fire based on the people who experienced it. Their modeling proved accurate when compared to figures from air quality monitors.

Content This content can also be viewed on the site it originates from.

Their results show that what we write online could fill the gaps that traditional data collection leave behind. While still nascent, using social media to study environmental events is a growing field. Images from Flickr can help researchers understand tourism rates in natural areas, and social media often acts as a crucial tool in disaster relief.",Yes.,"It can be used for data. In a recently published study, US Forest Service researchers Sonya Sachdeva and Sarah McCaffrey found that, when analyzed in large numbers, tweets about wildfires can accurately model the way smoke moves.",https://www.wired.com/story/twitter-wildfire-model/,2018-08-15 12:00:00+00:00,LABEL_1_relevant,0.995292067527771,Social Media,social_media,Wired,Security & Privacy
189,Facebook’s ex security boss: Asking Big Tech to police hate speech is “a dangerous path”,"Three things happened in 2016. There was a disinformation and propaganda campaign by [Russia’s] Internet Research Agency and related groups. There was the leak campaign where the GRU [Russian military intelligence] broke into the e-mail of the DNC [Democratic National Committee] and then planted stories [in the media]. And there was an exploratory penetration of the election systems of 21 states.

The first problem has had the most work [done on it] because it completely falls within the responsibility of the platforms. They have defined what is a political ad and what is an issue ad, created ad transparency, defined what is inappropriate coordination, and started enforcing those rules.

""The Russians were able to send their B team to hack into the DNC. I’m not sure people are ready for the A team.""

On the GRU hack and leak campaign, very little has happened. There have been upgrades in security at the DNC. I expect most campaigns are being more careful. But there does not seem to be a wholesale upgrade of security among campaigns and candidates. The Russians were able to send their B team to hack into the DNC. I’m not sure people are ready for the A team.

On the third we’ve done almost nothing. You still have 10,000 election authorities [in the US] running elections. We have a number of states with no paper backup [for ballots]. There are way too many authorities responsible for their own security for us to secure [elections] without the federal government providing a huge amount of resources. That’s where I think we have some of the most vulnerability.

So is the disinformation/propaganda problem mostly solved?

In a free society, you will never eliminate that problem. I think the most important thing [in the US] is the advertising transparency. With or without any foreign interference, the parties, the campaigns, the PACs [political action committees] here in the US are divvying up the electorate into tiny little buckets, and that is a bad thing. Transparency is a good start.

The next step we need is federal legislation to put a limit on ad targeting. There are thousands of companies in the internet advertising ecosystem. Facebook, Google, and Twitter are the only ones that have done anything, because they have gotten the most press coverage and the most pressure from politicians. So without legislation we’re just going to push all of the attackers into the long tail of advertising, to companies that don’t have dedicated teams looking for Russian disinformation groups.

Facebook has been criticized over Russian political interference both in the US and in other countries, the genocide in Myanmar, and a lot of other things. Do you feel Facebook has fully grasped the extent of its influence and its responsibility?

I think the company certainly understands its impact. The hard part is solving it. Ninety percent of Facebook users live outside the United States. Well over half live in either non-free countries or democracies without protection for speech. One of the problems is coming up with solutions in these countries that don’t immediately go to a very dark place [i.e., censorship].

""I think [Facebook] certainly understands its impact. The hard part is solving it.""

Another is figuring out what issues to put engineering resources behind. No matter how big a company is, there are only a certain number of problems you [can tackle]. One of the problems that companies have had is that they’re in a firefighting mode where they jump from emergency to emergency.

So as they staff up that gets better, but we also need a more informed external discussion about the things we want the companies to focus on—what are the problems that absolutely have to be solved, and what aren’t. You mentioned a bunch of a problems that are actually very different, but people blur them all together.

Do you think tech firms have too much power? Should Facebook be forced to divest itself of Instagram and WhatsApp, for instance?

If antitrust folks think it’s appropriate to force divestment of individual platforms, that’s up to them. I don’t think that either solves these problems or makes them worse. What would make things worse is breaking up specific products. Ten WhatsApps is worse than one WhatsApp, and ten Facebooks is worse than one Facebook, because you lose the economies of scale [and] the ability to have well-staffed teams that are experts in these kinds of abuse [hate speech, propaganda, etc.].

The truth is the big companies have been the responsive ones. Everybody is ignoring the small companies because it doesn’t make for good headlines. Facebook has done more than the vast majority of other companies.

How do you regulate in a world in which tech is advancing so fast while regulation moves so slowly? How should a society set sensible limits on what tech companies do?

But right now, society is not asking for limits on what they do. It’s asking that tech companies do more. And I think that’s a dangerous path. In all of the problems you mentioned—Russian disinformation, Myanmar—what you’re telling these companies is, “We want you to have more power to control what other people say and do.”

""Five or ten years from now there could be... machine-speed, real-time moderation of everything we say online.""

That’s very dangerous, especially with the rise of machine learning. Five or ten years from now, there could be machine-learning systems that understand human languages as well as humans. We could end up with machine-speed, real-time moderation of everything we say online. So the powers we grant the tech companies right now are the powers those machines are going to have in five years.

What is the basic problem the Stanford Internet Observatory is trying to solve?

There is no specific academic field studying the misuse of technology, outside of highly technical flaws. Computer science departments do research into new types of exploits, new types of bugs, esoteric cryptographic solutions, but you can’t get a PhD studying bullying and harassment and the technical solutions to them. You might have political scientists studying the impact of social networks on democracy, and people in the psychology department studying the impact of the use of Instagram on teenagers who are suicidal, but they lack the technical skills and infrastructure.

So the Stanford Internet Observatory will be a permanent program, staffed with data scientists, software engineers, investigators, and analysts who understand how to interact with the tech platforms and how to do data analytics at a very large scale. That group can then provide services to academic groups all over and do its own research. Then we can catalyze work that probably wouldn’t happen because it doesn’t fall cleanly into any one academic sphere.

What are some of the solutions you’re working on?

If you look at 2016, what you see is intelligence failures between the US government [and] allied governments, and between those governments and the tech platforms. We need to think about the responsibilities for these different groups and how you align the fact that the tech companies are acting in a quasi-governmental manner. What kind of controls should they have in place, and at what point does their responsibility begin and end? We’re working on recommendations for Congress for next year.

We’re also building the capability to monitor the use of disinformation in various elections. Our goal is to have that up and running for the Indian and European elections next year.

Democrats are inevitably going to be more receptive than Republicans to these kinds of solutions. How do you make this nonpartisan?

The idea that election interference only helps Republicans is insane. The Russian playbook is out there. The weaknesses in our system have been demonstrated. We have signaled to our adversaries that they can interfere in our election and we will do nothing to really punish that.

""My message to Republicans is, 'Let’s fix this problem before you guys have to have your own 2016.'""

So I would fully expect other adversaries to get involved in future elections. China, Iran, North Korea—the idea that all of these countries are going to support Republicans is ridiculous. So my message to Republicans is, “Let’s fix this problem before you guys have to have your own 2016.”

This discussion needs to move past Trump. Republicans end up with a brain freeze if you imply that Trump was not elected fair and square. So we just have to talk about the vulnerabilities and what possible impact it has in 2020. When I talk privately with Republicans they’re much more receptive to this.

A related issue is the lack of technological literacy among politicians. How do you solve that?

These folks generally have pretty smart staffers, but most of the staffers have not worked in tech. So we need to incentivize people who have worked in tech to go work in DC. There are examples like Chris Soghoian [a technology activist, now working for Senator Ron Wyden] where techies go to Congress and have huge impact individually.

And it goes both ways. We need to teach computer scientists about history and ethics, and we need to teach liberal arts majors about the fundamentals of technology so that they have the ability to be influential. The ability to talk to nerds in a way that they respect is hugely powerful, and that’s something that’s missing from Congress right now.",Yes,"Not that it spread Russian disinformation and propaganda, or that it was used by the military in Myanmar to commit genocide. it has given a small number of companies, like Facebook, too much power.

The solution to this problem is for the government to step in and regulate the tech industry, setting limits",https://www.technologyreview.com/2018/10/23/139334/facebooks-ex-security-boss-asking-big-tech-to-police-hate-speech-is-a-dangerous-path/,2018-10-23 00:00:00,LABEL_1_relevant,0.9967449903488159,Social Media,social_media,MIT Tech Review,"Information, Discourse & Governance"
190,Zuckerberg's View of Speech on Facebook Is Stuck in 2004,"Three days after Donald Trump was elected president of the United States, Mark Zuckerberg was asked the question on many people’s minds: Did the explosion of fake news and caustic political rhetoric on Facebook help Trump win? Zuckerberg dismissed the idea. ""The idea that fake news … influenced the election in any way is a pretty crazy idea,"" he said. The line has been reprinted so frequently, many can cite it from memory.

It didn't matter whether his comments were willful or accidental. The world asked, “How could someone so rich and powerful be so out of touch, and not appreciate the impact Facebook had on the election?” And Zuckerberg didn’t have a good answer. He and Facebook have been scrambling to repair their reputations ever since.

For a while, it looked like they would never figure it out. Russia's manipulation of News Feed made them look negligent. The Cambridge Analytica scandal made them look reckless and greedy. But Zuckerberg kept pledging to spend as much as it took to make things right. He said he understood that Facebook needed to exert more oversight over what appears on its platform. And by the beginning of 2019, it was starting to look like he was getting traction—that Facebook, Instagram, WhatsApp, and Messenger were becoming safer, more responsible platforms for their users.

That hope took a body blow in the past two weeks. In his recent appearances, Zuckerberg doesn’t sound like a changed man at all. In a speech at Georgetown, his testimony to Congress, and his insistence that Facebook will allow politicians to run false ads, Zuckerberg has reignited the fights that followed Trump's election. At the end of 2016, we worried about how Facebook had become a cesspool of lies, bigotry, and hate, skewing election results in pursuit of profits. We’re having the same conversation today.

Meanwhile, Zuckerberg is still lecturing us with the sophistication of a college student about the importance of free speech in politics. And he's showing up in public forums like Congress woefully unprepared—or unwilling—to answer the obvious questions about those views. Why couldn’t he answer the simplest questions about his position on false ads from Representative Alexandria Ocasio-Cortez last week? It wasn't a trick question. It was the same question that had been in the news for two weeks before Zuckerberg appeared in Congress.

Zuckerberg is still pushing the utopian view about news and information as when he started Facebook 15 years ago: Voters and citizens can figure out for themselves what is true and what is false. They can distinguish between a news story and an editorial in The Wall Street Journal, or a screed in Breitbart versus a news story in The New York Times. Society has much bigger problems when mainstream media has control of the conversation, he believes.

Read the transcript of Zuckerberg’s recent speech at Georgetown—or his appearance just after the election three years ago, or a decade ago—and they all make the same point: ""People having the power to express themselves at scale is a new kind of force in the world—a Fifth Estate alongside the other power structures of society. People no longer have to rely on traditional gatekeepers in politics or media to make their voices heard, and that has important consequences,"" he said at Georgetown on October 17.",Yes,"It gives a voice to people who would otherwise not be heard, and that has made society as a whole more open and diverse. But it also has a dark side, as we’ve seen in the past three years. It gives a platform to people who want to spread lies and hate, and it amplifies their voices.",https://www.wired.com/story/zuckerbergs-view-speech-facebook-stuck-2004/,2019-10-31 23:05:53.042000+00:00,LABEL_1_relevant,0.9963405728340148,Social Media,social_media,Wired,"Information, Discourse & Governance"
