,title,text,sector,magazine,url,gpt_summary,label
0,COVID-19 made our tech addiction worse: It’s time to do something about it – TechCrunch,"COVID-19 made our tech addiction worse: It’s time to do something about it A former tech CEO says Americans 'need to take control' of their media consumption

The coronavirus pandemic accelerated America’s addiction to technology, and it’s making us sad, anxious and unproductive.

Companies like Facebook, TikTok and Snapchat earn more advertising revenue the more frequently we use their products. These firms use push notifications and personalized feeds to capture our attention, manipulate our emotions and influence our actions.

Business is good. Americans now spend more than five hours each day on their devices.

So what? As discussed in Netflix’s “The Social Dilemma,” tech firms will continue to follow their profit motive to capture our attention. Governments are no more likely to help manage unhealthy tech consumption than consumption of sugar or illegal drugs. We need to take control.

The coronavirus pandemic accelerated America’s addiction to technology, and it’s making us sad, anxious and unproductive.

My perspective is as a former tech CEO and technology addict. The marketing platform I founded raised over $100 million, grew to 350 employees and sold to a private equity firm last year. Along the way I picked up some terrible tech habits; I checked email constantly and allowed push notifications to interrupt every in-person interaction.

My tech use hit rock bottom last year on a visit with family. I resolved to put down my phone and garden with my mom, who has advanced Parkinson’s and moves slowly and with intention.

I felt like an addict in withdrawal. My phone was like a magnet pulling me to check for missed work emails or breaking news. Tech overuse had rewired my brain, lowered the quality of everyday consciousness and prevented me from being present.

I stepped down as CEO of my company earlier this year. I’ve spent my time off learning about mindfulness, neuroplasticity and technology addiction. Most importantly, I developed a strategy for managing my tech use that’s made me happier and more productive.

Here’s what I learned.

Tech firms exploit our brains to capture our attention

In their quest for our attention, some tech firms target the oldest parts of our brain, what UCLA psychiatrist Daniel Siegel calls the downstairs brain. The downstairs brain includes your brainstem and limbic regions, which control innate reactions and impulses (fight or flight) and strong emotion (like anger and fear). In contrast, your upstairs brain, including your cerebral cortex, is where intricate mental processes take place, like thinking, imagining and planning.

The downstairs brain is reactive. It’s designed to protect us in emergencies; it can make quick judgements, hijack our consciousness and drive action through strong emotion. The downstairs brain is what is targeted by attention-seeking products. Headlines that make us feel outraged and TikTok notifications that make us feel reactive appeal to our downstairs brain.

Spending time in a reactive state rewires our brains

Our brains change with training. Research has shown that our brains are reprogrammed with the firing patterns of neurons. Our nervous system can be rewired and transformed through repetitive, focused attention or activity in a process called neuroplasticity.

Repetitive device usage is a perfect example of neuroplasticity at work. The more time we spend responding to push notifications, watching videos in infinite scroll or looking for social validation from social media, the more our brains will rewire to want the same.

Our addiction will get worse as firms get better at capturing attention

While many tech firms acknowledge problems from overusing their products, none will make radical changes needed to decrease their share of the attention profit pool. If they did, someone else would eat their lunch.

These firms are selling us sugary drinks. The taste is improving exponentially and the sweetest drinks haven’t been invented yet. The more we drink, the harder it gets to stop. We need to take control of our consumption and habits — we need to follow a technology diet — or we will suffer the mental equivalent of morbid obesity.

We can can rewire our brains to be more productive and happier by changing our habits

If we think of technology consumption as an analog to food consumption, tech products fall into four food groups based on the quality of information and method of delivery. Content quality is important: Some content is valuable (e.g., MIT’s online courseware) or critical (work email), while most is not useful (TikTok).

The delivery model is also important. Healthy platforms give agency to the user and allow us to pull content that’s useful when we need it. Conversely, harmful platforms often rely on push, sending us information that’s often not useful at a time when we’re doing something else. Based on my experience, here are three steps we can take to implement a tech diet:

1. Eliminate products that reinforce your downstairs brain (low-quality content pushed to you)

Willpower is finite. If we don’t want sugary drinks, don’t keep them in the house. We keep the most distracting applications ever developed within arms reach at all times. These applications prey on our downstairs brain, which hijacks our better intentions and delivers negative value for most people. I believe our best defense is abstinence; we shouldn’t use these apps.

Tip: I use Apple’s Content Restrictions on the iPhone and MacBook. I added the obvious offenders: TikTok, Instagram, Facebook, Snapchat, and some specific to me, which includes Zillow, StreetEasy and NYPost. My spouse has the override code. I can break it if needed, but the process is hard enough that it doesn’t enter everyday consciousness.

2. Consume more products that reinforce your upstairs brain (high-quality content that’s available when we need it)

Good content expands our knowledge and skills and may contribute to rewiring our upstairs brain in a way that adds to our empathy, imagination and mindfulness.

Consuming good content is rewarding but effortful. It requires uninterrupted focus. Unlike sugary beverages, which we’re wired to consume subconsciously, leafy greens have to be consumed intentionally.

Tip: Make a list of your favorite leafy greens. For me, this includes Kindle, Feedly, tech periodicals and my favorite curation platforms: HackerNews and Product Hunt. Calm, one of several booming mindfulness apps, also makes the list. These are the only apps on my home screen, which encourages me to use them more often. Like a food diet, I set attainable goals for “good” consumption and monitor my progress.

I recommend fasting on technology periodically; I leave my phone at home for walks with my son and dinner with friends. I also recommend nontech activities that promote upstairs brain rewiring like an outdoor hike or learning to play an instrument.

3. Redesign consumption patterns for productivity tools

Email is required for most people. It has the potential to make us productive. But the average message quality is low, and the always-on, high frequency, push-by-default design prevents us from doing our best work.

Tip: I’ve turned off notifications on everything that’s not meant for urgent or timely messages (e.g., texts, Lyft, Tovala oven). Boomerang’s Chrome Extension can be set up to deliver all of your emails every hour on the hour. Batch processing email every hour dramatically reduces the volume of interruption without impacting my responsiveness.

We live in relative abundance, with food, goods and security that would make even our recent ancestors envious. But abundance doesn’t make us happy; we’re the least happy on record. We seem to be living in a collective state of downstairs brain, a continuous adult temper tantrum focused on strong feelings, emotion and impulsiveness.

But there’s hope.

As individuals, I found that even a few months of technology dieting helped me become less impulsive and more mindful. As employees, we can stop working for companies that profit from the attention economy. As managers, we can insist that our teams turn off their devices at night, turn off their Slack notifications and take real vacations. As parents, we can help our children develop healthy consumption patterns.

Collective action — and rewiring of our brains — could change the course of our politics and our ability to collaborate and solve the most important challenges of the 21st century.

American innovation dominates the attention economy. It’s time for American innovation to dominate the way we use technology.",Social Media,TechCrunch,https://techcrunch.com/2020/12/10/covid-19-made-our-tech-addiction-worse-its-time-to-do-something-about-it/,"The coronavirus pandemic has accelerated Americans' addiction to technology, leading to increased sadness, anxiety and unproductivity. Companies are using push notifications and personalized feeds to capture our attention, manipulate our emotions and influence our actions, leading to a rewiring of our brains to crave the same. We need to take control and learn to",User Experience & Entertainment
1,By the Numbers: Has Facebook Fatigue Set In?,"Have we reached the Facebook saturation point?

That's one possibility suggested by monthly growth data from Inside Facebook, which reports that Facebook gained only 320,000 new U.S. users in June after a blockbuster gain in May of more than 7.8 million. Moreover, the net's dominant social networking site lost active users in the 18-25, 26-34 and 35-44 demographic ranges, while gaining users in their mid-teens and middle years.

One possibility is that the May and June controversies over Facebook's privacy policies and dominance have kept the company from tremendous new growth and even led some to curtail their use. Another possibility is that it's just a statistical aberration, or a result of changes to the advertising system, where Inside Facebook says it gleans its numbers.

But there's also the possibility that almost every American who has any interest in joining Facebook already has. Facebook says it has about 125 million active U.S. accounts and is likely closing on half a billion accounts worldwide. According to Inside Facebook, approximately 41 percent of Americans have Facebook accounts, a rate surpassed only by the U.K and Canada, which clock in at 44 percent and 48 percent, respectively.

What would peak-Facebook mean for the company? Certainly not irrelevance, since Facebook is now the second most visited site in the world after Google and is the largest photo-sharing site on the net. However, it's yet to prove that it can create a money-making machine like Google found in tiny search advertising tied to what users search for, not who they are.

Facebook's ads, by contrast, are all about knowing what you are interested in, according to the data you tell it. Advertisers have powerful ways to target ads at users. One techie even created an ad specifically for his wife by targeting his ad at very select criteria -- geography, degree, gender and interest. As a private company, Facebook's revenues are not publicly disclosed, but the company has confirmed that they are close to $1 billion. That's a pittance compared to Google's $25 billion in annual revenue.

Investors, including Microsoft, have been valuing the company at $15 billion.

But if Facebook has hit its peak in terms of U.S. users, the company will have to turn its attention to finding ways to make money off the site, not just add more users. It's increasingly weaving itself into the fabric of the web, and it now seems odder to see a news story without a Facebook Like button than with one, even though that feature has only been around a few months.

The other problem with the possibility of hitting peak-Facebook? There's only one way to go from there and that's down. Users are finicky, and Facebook could find itself caught in a battle of just trying to retain users who might jump ship to Google's rumored new network or who just decide they are tired of maintaining a social networking profile.

Facebook did not immediate respond to an e-mail seeking comment.

Photo: Anita Hart

See Also:",Social Media,WIRED,https://www.wired.com/2010/07/facebook-fatigue/,"If Facebook has indeed reached its peak saturation point in terms of US users, it will have to find ways to make money off the site and retain existing users, as the only way to go from here is down.",User Experience & Entertainment
2,Anti-vaxxers are weaponizing Yelp to punish bars that require vaccine proof,"Smith’s Yelp reviews were shut down after the sudden flurry of activity on its page, which the company labels “unusual activity alerts,” a stopgap measure for both the business and Yelp to filter through a flood of reviews and pick out which are spam and which aren’t. Noorie Malik, Yelp’s vice president of user operations, said Yelp has a “team of moderators” that investigate pages that get an unusual amount of traffic. “After we’ve seen activity dramatically decrease or stop, we will then clean up the page so that only firsthand consumer experiences are reflected,” she said in a statement.

It’s a practice that Yelp has had to deploy more often over the course of the pandemic: According to Yelp's 2020 Trust & Safety Report, the company saw a 206% increase over 2019 levels in unusual activity alerts. “Since January 2021, we’ve placed more than 15 unusual activity alerts on business pages related to a business’s stance on covid-19 vaccinations,” said Malik.

The majority of those cases have been since May, like the gay bar C.C. Attles in Seattle, which got an alert from Yelp after it made patrons show proof of vaccination at the door. Earlier this month, Moe’s Cantina in Chicago’s River North neighborhood got spammed after it attempted to isolate vaccinated customers from unvaccinated ones.

Spamming a business with one-star reviews is not a new tactic. In fact, perhaps the best-known case is Colorado’s Masterpiece bakery, which won a 2018 Supreme Court battle for refusing to make a wedding cake for a same-sex couple, after which it got pummeled by one-star reviews. “People are still writing fake reviews. People will always write fake reviews,” Liu says.

But he adds that today’s online audience know that platforms use algorithms to detect and flag problematic words, so bad actors can mask their grievances by blaming poor restaurant service like a more typical negative review to ensure the rating stays up — and counts.

That seems to have been the case with Knapp’s bar. His Yelp review included comments like “There was hair in my food” or alleged cockroach sightings. “Really ridiculous, fantastic shit,” Knapp says. “If you looked at previous reviews, you would understand immediately that this doesn’t make sense.”

Liu also says there is a limit to how much Yelp can improve their spam detection, since natural language — or the way we speak, read, and write — “is very tough for computer systems to detect.”

But Liu doesn’t think putting a human being in charge of figuring out which reviews are spam or not will solve the problem. “Human beings can’t do it,” he says. “Some people might get it right, some people might get it wrong. I have fake reviews on my webpage and even I can’t tell which are real or not.”

You might notice that I’ve only mentioned Yelp reviews thus far, despite the fact that Google reviews — which appear in the business description box on the right side of the Google search results page under “reviews” — is arguably more influential. That’s because Google’s review operations are, frankly, even more mysterious.

While businesses I spoke to said Yelp worked with them on identifying spam reviews, none of them had any luck with contacting Google’s team. “You would think Google would say, ‘Something is fucked up here,’” Knapp says. “These are IP addresses from overseas. It really undermines the review platform when things like this are allowed to happen.”",Social Media,MIT,https://www.technologyreview.com/2021/06/12/1026213/anti-vaxxers-negative-yelp-google-reviews-restaurants-bars/,"The main undesirable consequence of Social Media discussed in this article is the rise of spam reviews that can be hard to detect and filter out. Yelp and Google both have their own measures in place to try to limit this, but it's still a problem that can go undetected and undermine the trustworthiness of reviews.",Discourse & Governance
3,"TikTok expands mental health resources, as negative reports of Instagram’s effect on teens leak – TechCrunch","TikTok announced this morning that it is implementing new tactics to educate its users about the negative mental health impacts of social media. As part of these changes, TikTok is rolling out a “well-being guide” in its Safety Center, a brief primer on eating disorders, expanded search interventions and opt-in viewing screens on potentially triggering searches.

Developed in collaboration with International Association for Suicide Prevention, Crisis Text Line, Live For Tomorrow, Samaritans of Singapore and Samaritans (UK), the new well-being guide offers more targeted advice toward people using TikTok, encouraging users to consider how it might impact them to share their mental health stories on a platform where any post has the potential to go viral. TikTok wants users to think about why they’re sharing their experience, if they’re ready for a wider audience to hear their story if sharing could be harmful to them and if they’re prepared to hear others’ stories in response.

The platform also added a brief, albeit generic memo about the impact of eating disorders under the “topics” section of the Safety Center, which was developed with the National Eating Disorders Association (NEDA). NEDA has a long track record of collaborating with social media platforms, most recently working with Pinterest to prohibit ads promoting weight loss.

Already, TikTok directs users to local resources when they search for words or phrases like #suicide,* but now, the platform will also share content from creators with the intent of helping someone in need. The platform told TechCrunch that it chose this content following consultation with independent experts. Additionally, if someone enters a search phrase that might be alarming (TikTok offered “scary makeup” as an example), the content will be blurred out, asking users to opt-in to see the search results.

As TikTok unveils these changes, its competitor Instagram is facing scrutiny after The Wall Street Journal leaked documents that reveal its parent company Facebook’s own research on the harm Instagram poses for teen girls. Similar to the Gen Z-dominated TikTok, more than 40% of Instagram users are 22 or younger, and 22 million teens log into Instagram in the U.S. each day. In one anecdote, a 19-year-old interviewed by The Wall Street Journal said that after searching Instagram for workout ideas, her explore page has been flooded with photos about how to lose weight (Instagram has previously fessed up to errors with its search function, which recommended that users search topics like “fasting” and “appetite suppressants”). Angela Guarda, director for the eating-disorders program at Johns Hopkins Hospital, told The Wall Street Journal that her patients often say they learned about dangerous weight loss tactics via social media.

“The question on many people’s minds is if social media is good or bad for people. The research on this is mixed; it can be both,” Instagram wrote in a blog post today. Like TikTok, Instagram has anti-harassment tools, well-being guides, and warnings about sensitive content.

As TikTok nods to with its advice on sharing mental health stories, social media can often be a positive resource, allowing people who are dealing with certain challenges to learn from others who have gone through similar experiences. So, despite these platforms’ outsized influence, it’s also on real people to think twice about what they post and how it might influence others. Even when Facebook experimented with hiding the number of “likes” on Instagram, employees said that it didn’t improve overall user well-being. These revelations about the negative impact of social media on mental health and body image aren’t ground-breaking, but they generate a renewed pressure for these powerful platforms to think about how to support their users (or, at the very least, add some new memos to their security center).

*If you or someone you know is struggling with depression or has had thoughts of harming themselves or taking their own life, The National Suicide Prevention Lifeline (1-800-273-8255) provides 24/7, free, confidential support for people in distress, as well as best practices for professionals and resources to aid in prevention and crisis situations.",Social Media,TechCrunch,https://techcrunch.com/2021/09/14/tiktok-expands-mental-health-resources-as-negative-reports-of-instagrams-effect-on-teens-leak/,"Social media platforms like TikTok and Instagram have been linked to mental health issues such as eating disorders and depression, leading to an increased pressure on these platforms to take measures to protect their users.",Social Norms & Relationships
4,An Undiscovered Facebook Bug Made Me Think I Was Hacked,"My legs were sticking to the vinyl back seat of a NYC cab when I received the email on a Thursday this July. I was running late to an afternoon dentist appointment, and sending messages on Facebook Messenger. Most of the conversations were for a story I was reporting about a Facebook group for sexual assault survivors, which had been overtaken by abusers.

At the time, I was messaging with one of the abusers—who was using a fake profile—hoping to find out how they weaponized the group for harassment. In the middle of our exchange, I received an email from Facebook, which said, “We wanted to let you know that your mobile number was removed from your account. Because of this, we’ve turned off two-factor authentication on your account to make sure you don’t get locked out when using an unrecognized computer or mobile device to log in.”

I hadn't removed my phone number; I immediately assumed I had been hacked, especially given the story I was reporting. Like hundreds of millions of people around the world, my Facebook account contains the record of a decade of my life. But in this case, my messages also contained stories of harassment by the same person I believed had breached my account.

The message didn’t include an easy way to notify Facebook that I hadn’t authorized the change, though there was a button informing me I could add a new mobile number if I wished. From the taxi, I called my editor, as well as another colleague, in an effort to contact Facebook as soon as possible.

While I paced my dentist’s office and tried to explain the situation to the receptionist, my coworker reset my password from a laptop at work. She checked the “active sessions” on my account, the devices on which I was logged in. She didn't find anything amiss—my Facebook looked normal.

At the time, Facebook also could find nothing wrong. I switched from SMS two-factor authentication to one of Facebook’s newer, more secure methods of safeguarding my account, and hoped that everything was OK.

It’s evidence of the implicit trust we all put in Facebook to safeguard our most sensitive communications.

As it turns out, it mostly was. This week, Facebook confirmed that I had actually encountered a bug that automatically turned off two-factor authentication when users changed their phone number, or adjusted the privacy settings associated with it. In my case, as part of undergoing a Facebook ""privacy checkup"" before messaging the troll, I had made the number on my account visible only to me. Because of the bug, Facebook thought I was removing my number altogether, and turned SMS two-factor authentication off.

Facebook says the issue affected “a very select number of people,” though it did not specify a number. “We thank Ms. Matsakis for bringing this to our attention. We addressed the issue as soon as we were made aware of it. We continue to encourage people to apply two-factor authentication, and if this security feature is deactivated for any reason, Facebook will notify you of the change,” Pete Voss, Facebook’s security communications manager, said in a statement.

He added that these sorts of problems are brought to Facebook’s attention regularly, and you can report your own issue here. As a journalist, I was able to get someone from Facebook’ s communications team on the phone quickly, and she made sure my case was addressed. But the vast majority of Facebook users who experience a security problem aren’t able to talk to someone right away. A normal Facebook user in my situation may have also ignored or missed the initial email about two-factor authentication being turned off—leaving their account far less secure than they intended.",Social Media,WIRED,https://www.wired.com/story/facebook-bug-two-factor-hack/,"The lack of secure protocols on many social media accounts can leave users vulnerable to hacking and other forms of exploitation, with many users unable to access the support they need to secure their accounts and protect their data.",Security & Privacy
5,You Are Not Your Name and Photo: A Call to Re-Imagine Identity,"At some point in the last few years, ""identity"" became a nasty word. It's not just identity theft, identity politics or identity requirements. It's everywhere — maybe especially on the web.

People like Google's Eric Schmidt began to talk about ""identity services"" instead of social networks. Identity became synonymous with fixed, verified, monetizable personhood.

Meanwhile, its opposite, Anonymous, became synonymous for many with sheer chaos, whether they were attacking online businesses or careless celebrities. Fights over pseudonyms and identity verification at Google+ (aka ""the Nym Wars"") only showed that sorting out online identity had reached an unhappy, polarized stalemate.

Christopher ""moot"" Poole, founder of message- and mediaboards 4chan and Canvas, might seem like an unlikely voice to advance or complicate this discussion. Now 23 years old, Poole created 4chan at 15; over time the site became as famous for its embrace of anonymous posting and anarchic subculture, as its ability to generate and spread internet memes like lolcats or Rickrolling. Because of 4chan, Poole's generally been treated by the media and at high-profile idea salons as an apologist for anonymity, even when his exact position has always been more complicated.

At the Web 2.0 conference this week, Poole gave a compelling talk that mapped this complexity, and which I hope will help reframe our discussion of identity. It's hard to summarize, but in addition to the full video, I'll try to pull out a few of the big ideas:

Both Google+ (with Circles) and Facebook (with Smart Lists) misunderstand the core problem of online identity: It's not only about who you're sharing with, but how you represent yourself. ""It's not who you share with, but who you share as.""

""Identity is prismatic."" We're all viewed through multiple lenses; we always represent ourselves through multiple personae; and this isn't a strange aberration or attempt at deceit but a fact of being human.

Facebook (but not only Facebook) have fostered the assumption that our identity is consistently not just verified but represented online through a first and last name attached to a photograph of our face. This is actually a diminishing of our plural identities, not a flourishing of it. It consolidates our identity, which distorts how we truly are.

If you're looking to keep score between the major social media companies: Twitter handles identity better than Facebook, because it allows for handles, multiple accounts, fake accounts and other features that keep Twitter interest-driven, not identity-driven. Google, in turn, ""missed a gigantic opportunity to innovate"" the representation of identity online by allowing for something as rich as Circles for self-representation, not just choice of audience. ""Facebook and Google do identity wrong; Twitter does it better; and I want to think about what the world would be like if we did it right.""

Ultimately, though, big companies don't determine identity on the web, even if they shape its contours. Users and other developers do, through their behavior and choices.

Content View Iframe URL

What's refreshing about Poole's talk is how pragmatic he actually is. Canvas, for instance, uses Facebook to verify identity at signup as a simple hedge to keep out trolls and spammers, but still allows users to represent themselves anonymously, through handles, or under real names, and go back and reclaim or ""author-ize"" anonymous posts. To paraphrase George Carlin, not every forum post deserves a name.

With identity, once you take the point of view of all-or-nothing terminal purity, there are no limits and there is no compromise. Tim O'Reilly points this out in a joint interview with Poole conducted by TechCrunch's Alexia Tsotsis: from logged IP addresses on the web and facial recognition software on the street, true anonymity is increasingly impossible.",Social Media,WIRED,https://www.wired.com/2011/10/you-are-not-your-name-and-photo-a-call-to-re-imagine-identity/,"Social media has led to a narrowing of our identities, with many platforms forcing a single, verifiable identity on users. This has limited our ability to express ourselves in multiple personas, and has distorted how we are represented online. Companies like Google and Facebook have failed to innovate in this area, but it is ultimately the users and developers who",Social Norms & Relationships
6,Twitter’s deletion of its Facebook app caused old cross-posts to temporarily disappear – TechCrunch,"(Update: Axios has a post explaining why the cross-posted tweets disappeared. Essentially, Twitter deleted its app from Facebook after Facebook stopped allowing cross-posts earlier this month, since without that feature it was basically useless. This unexpectedly caused old posts to disappear. TechCrunch also heard from a source with understanding of the situation that the deletion of the app took Facebook by surprise, as well as the fact that Twitter didn’t immediately tell them to restore the content.)

Facebook users are complaining the company has removed the cross-posted tweets they had published to their profiles as Facebook updates. The posts’ removal took place following the recent API change that prevented Twitter users from continuing to automatically publish their tweets to Facebook. According to the affected parties, both the Facebook posts themselves, as well as the conversation around those posts that had taken place directly on Facebook, are now gone. Reached for comment, Facebook says it’s aware of the issue and is looking into it.

TechCrunch was alerted to the problem by a reader, Lawrence Miller, who couldn’t find any information about the issue in Facebook’s Help Center. We’ve since confirmed the issue ourselves with several affected parties and confirmed it with Facebook.

Given the real-time nature of social media — and how difficult it is to pull up old posts — it’s possible that many of the impacted Facebook users have yet to realize their old posts have been removed.

In fact, we only found a handful of public complaints about the deletions, so far.

For example:

@facebook I used the Twitter for Facebook app for years, and I realize it's not working and isn't going to. But I just discovered all the Facebook updates it put have been deleted and dissappeared from my timeline! Is there a way to retrieve this? — Omer Lev (@omerlev) August 26, 2018

A recent update to the Facebook Platform Policies ended the ability to automatically post Tweets to our Facebook profile or page and all of our previous Twitter posts were deleted by Facebook. #dfwwx #txwx #plano https://t.co/sAOsbdBjVO — Plano, Texas Weather (@PlanoWX) August 24, 2018

My post on Facebook via Twitter was deleted.

Why?

Can someone explain it? #twitter — Tarin (@lestarindah499) August 26, 2018

Above: selected complaints from Twitter about the data loss

Above: a comment on TechCrunch following our post on the API changes

Some of those who were impacted were very light Facebook users and had heavily relied on the cross-posting to keep their Facebook accounts active. As a result of the mass removals, their Facebook profiles are now fairly empty.

TechCrunch editor Matthew Panzarino is one of those here who was impacted. He points out that the ability to share tweets to Facebook was a useful way to reach people who weren’t on Twitter in order to continue a discussion with a different audience.

“I’ve had tweet cross-posting turned on for years, from the early days of it even existing. This just removed thousands of posts from my Facebook silently, with no warning,” Matthew told me. “Even though the posts didn’t originate on Facebook, I often had ongoing conversations about the posts once my Facebook friends (and audience) saw them. Many of them would never see them on Twitter either because they don’t follow me or they don’t use it,” he said.

“It’s wild to have all of that context just vanish,” he added.

As you may recall, Facebook earlier this month made a change to its API platform to prevent third-party apps from publishing posts to Facebook as the logged-in user. The change was a part of Facebook’s larger overhaul and lockdown of its API platform in the wake of the Cambridge Analytica scandal, where as many as 87 million Facebook users had their data improperly harvested and shared.

Since then, Facebook has been trying to plug up the holes in its platform to prevent further data misuse. One of the changes it made was to stop third-parties from being able to post to Facebook as the logged-in user.

For existing apps, like Twitter, that permission was revoked on August 1, 2018.

Above: Twitter’s cross-posting feature, on the day it was disabled by the Facebook API change

Before the API changes, Twitter users were able to visit the “Apps” section from Twitter on the web, then authenticate with Facebook to have their tweets cross-posted to Facebook’s social network. Once enabled, the tweets would appear on the user’s page as a Facebook post they had published, and their friends could then like and comment on the post as any other.

In theory, the API changes should only have prevented Twitter users from continuing to cross-post their tweets to Facebook automatically. It shouldn’t have also deleted the existing posts from Facebook users’ profiles and business users’ Facebook Pages.

This is a breach of trust from a company that’s in the process of trying to repair a broken trust with its users across a number of fronts, including data misuse. Regardless of whatever new policy is in effect around apps and how they can post to Facebook, no one would have ever expected that Facebook would actually remove their old posts without warning.

We’re hoping that the problem is a bug that Facebook can resolve, and not something that will result in permanent data loss.

Facebook tells us while it doesn’t have further information about the problem at this time, it should have more to share tonight or tomorrow about what’s being done.",Social Media,TechCrunch,https://techcrunch.com/2018/08/28/facebook-has-removed-all-cross-posted-tweets/,"Facebook users are reporting that the company has silently removed their cross-posted tweets from their profiles, as well as the conversations around them, following the recent change to its API platform. Facebook has said it is aware of the issue and is looking into it.",Security & Privacy
7,How the Kavanaugh Information War Mirrors Real Warzones,"As the controversy surrounding the Supreme Court confirmation for Judge Brett Kavanaugh escalates, the online conversation around it has started to feel less like a debate and more like a war. That’s because it is one.

It's been more than three decades since the alleged sexual assaults. But in making their case, defenders of both Kavanaugh and Ford have embraced many of the same information warfare tactics favored by terrorist propagandists and foreign militaries, including those infamous Russian trolls. The aim, in this case, is not collusion. Rather, these are the new and necessary means to “win” on the web. If cyberwar is the hacking of networks, these are the tools of what we call the “LikeWar,” the hacking of people. As these methods merge across flame wars and real wars, there’s no escaping them.

One way the sheer scale of online information has transformed discourse is through what is known as OSINT, open-source intelligence. By mobilizing networks in the hunt for digital clues, formerly shrouded secrets can be pieced together. In war, examples have ranged from revealing details of foreign weapons to documenting war crimes to unmasking assassins. Now those same techniques are being deployed in an attempt to answer the mysteries of the Kavanaugh debate. ProPublica, for instance, is running a crowdsourced investigation of the nominee’s $200,000 baseball ticket debt, trying to identify his network of potential connections by cataloging who sat with him.

At the other end of the spectrum was the effort by Kavanaugh defender Ed Whelan, assisted by CRC, a public relations firm previously known for its role in the “Swiftboat” smears of John Kerry in the 2004 election. Drawing on Facebook comments, Google Maps, and even home layouts from Zillow, Whelan posted a Twitter screed that claimed to “prove” that Ford had actually been assaulted by a Kavanaugh look-alike. Instead, the theory was quickly picked apart by a countering online crowd.

The effort to introduce a doppelganger aligned with another key method used in LikeWars around the world: muddying the debate by throwing out alternative theories. Russia has long been the master of this disinformation tactic. After its 2014 shootdown of the MH-17 airliner over Ukraine, for instance, Russia spread over a dozen different theories of what had really happened. Many were contradictory and debunked previous claims. But the goal wasn’t to find the truth—it was to obscure it behind a smokescreen of lies.

""The most momentous battle in the Kavanaugh saga has been the one fought with keywords and hashtags, as millions of Americans broadcast their thoughts in real time.""

Similarly, the Kavanaugh debate has given rise to false claims and ridiculous photoshopped images, often spread under fake identities. There have been debunked rumors that Kavanaugh had ruled against Ford’s parents in a house foreclosure and that Ford’s brother was part of the Russia investigation. There was even a flurry of unsubstantiated sexual assault charges leveled against Kavanaugh in the hours before the hearing. His supporters were outraged; those opposed to Kavanaugh's nomination speculated that they were placed so that his defenders could point to the media’s unreliability and cast doubt on Ford's credibility.

Not even history itself is safe—at least the online version of it, which we increasingly depend on. When Kavanaugh testified that Devil's Triangle, as mentioned on his high school yearbook page, was a drinking game, there was no online evidence to back up his claim. (Other sources asserted it was a known sexual term.) So an anonymous person immediately updated Wikipedia to support Kavanaugh's definition. It was a near perfect parallel to how Russian operatives repeatedly edited the Wikipedia entry for “MH17” in the hours after the airliner was shot down to try to provide an alternative history.",Social Media,WIRED,https://www.wired.com/story/how-the-kavanaugh-information-war-mirrors-real-warzones/,"The use of Social Media in the Kavanaugh controversy has led to an increase in dangerous tactics such as OSINT, muddying the debate with false claims and ridiculous photoshopped images, as well as editing of online history, which can lead to the spread of misinformation and confusion.","Information, Discourse & Governance"
8,How Instagram Is Transforming Professional Cooking,"Dining has reached its Instagram era, when a camera is as central to the experience as a fork and anyone with a decent eye is making magazine-quality photos of food.

People have always loved eating, and photographers have long recognized the inherent beauty of food. But smartphones with pin-sharp lenses and apps that make editing as easy as swiping and tapping turn anyone into a food photographer. There are more than 178 million photos tagged #food on Instagram and 56 million tagged #foodporn. People are obsessed with photographing what they eat, something professional chefs are catering to—and learning from.

“It’s all about exposure,” says Dominique Crenn of Atelier Crenn in San Francisco. Crenn, who was among the chefs I interviewed at the Terroir Hospitality symposium in Toronto, is the first woman in the US to earn two Michelin stars. “Instagram came to give a voice to chefs and to the food they serve.”

Instagram content View on Instagram

Chefs are embracing this in a big way. A shot of a new dish posted to their own accounts, or a diner’s, can cause reservations to spike. Stunning dishes, daring ingredients and thoughtful presentations add to the experience, and encourage people to post post post photos on their social media accounts.

Of course, chefs don’t set out to create viral dishes, and others abhor the very idea of it. More than a few chefs ban cameras from their dining rooms, as French chef Alexandre Gauthier did last year at La Grenouillère. But many others are well aware of the power of social media, and embrace it. “I’ll be honest. If I have a better looking dish, I give that one to the people taking photos,” says Benedict Reade, the former chef at Nordic Food Lab in Copenhagen who recently opened a pop-up restaurant in Scotland.

Instagram content View on Instagram

Instagram and social media are word-of-mouth marketing for the digital era, and can help build a chef’s rep and clientele—as long as the photos don’t suck. “It affects me when I see a bad review,” says chef Ned Bell of the Four Seasons Hotel in Vancouver. “But it affects me more when someone takes a bad photo of my food. I worry about what my food looks like on the social media world.”

Instagram content View on Instagram

The intense focus on getting it just right goes beyond plating to the entire experience, from the menu to the décor. People photograph everything, and post it all. “Chefs worry about this stuff,” says chef Eric Werner of Hartwood in Tulum, Mexico, and designers are putting “more emphasis on less cooking areas and more plating spaces.”

But Crenn says there is “a thin line between annoying dinners and exposure.” Many chefs loathe patrons who approach their meal as if it were a photo shoot and try to meticulously stage a scene, often to the detriment of the food—and occasionally other patrons. It’s not uncommon to see patrons futzing with a camera or trying to frame a shot as their dish goes cold. “What really annoys me is when people Instagram live,” Reade says. “When I’m serving someone’s food and put a beautiful hot plate on the table but they are so concerned to post and food gets cold because they are trying to find the perfect caption before they eat the fucking food. Do you know how much I sweated to make the food the right temperature for you? Are you here to show off to your friends?”",Social Media,WIRED,https://www.wired.com/2015/06/instagram-transforming-professional-cooking/,"Although Social Media has given chefs an opportunity to get their dishes and restaurants noticed, some customers can be overly focused on capturing the perfect photo, resulting in dishes going cold and other diners' experiences being disrupted.",User Experience & Entertainment
9,"Google's Sergey Brin: China, SOPA, Facebook Threaten the 'Open Web'","Google's search engine was created when most of the web's information was open and available to anyone willing to capture it. In today's more restrictive environment, Sergey Brin and Google CEO Larry Page may not have even tried.

""The kind of environment that we developed Google in, the reason that we were able to develop a search engine, is the web was so open,"" Brin told The Guardian. ""Once you get too many rules, that will stifle innovation.""

In an interview published Sunday, Google's co-founder cited a wide range of attacks on ""the open internet,"" including government censorship and interception of data, overzealous attempts to protect intellectual property, and new communication portals that use web technologies and the internet, but under restrictive corporate control.

There are ""very powerful forces that have lined up against the open internet on all sides and around the world,"" says Brin. ""I thought there was no way to put the genie back in the bottle, but now it seems in certain areas the genie has been put back in the bottle.""

Not coincidentally, these forces map directly onto three of Google's biggest headaches as a business in the past few years. There's no way for Google's servers to crawl Facebook's pages or Apple's smartphone apps for information. YouTube's video clips, Google Books and other key initiatives have had to grapple with both the media industries and government court rulings or legislation. And besides having to withdraw from China to Hong Kong after a series of attacks and new censorship rules, Google has been compelled to hand over user information to the U.S. government, sometimes without being able to legally notify those users.

""If we could be in some magical jurisdiction that everyone in the world trusted, that would be great,"" says Brin. ""We're doing it as well as can be done.""

Brin lists several other threats to the open web (and to Google):

Smartphone apps, as led by Apple: ""all the information in apps – that data is not crawlable by web crawlers. You can't search it"";

Facebook, where data goes in but never comes out: ""Facebook has been sucking down Gmail contacts for many years"";

SOPA and PIPA, which Brin says would have led to the U.S. using the same content-screening technology it has criticized China and Iran for using. With SOPA and PIPA, says Brin, fears of piracy had reduced the media industry to ""shooting itself in the foot, or maybe worse than in the foot.""

Still, there's a profound audacity in Brin bundling internet censorship in regimes like China, Saudi Arabia and Iran, which restrict user access to the web, with Facebook and Apple's platforms, which restrict Google's.

There may be a continuum of control and closure of the internet that connects repressive governments at one end and overbearing corporations at the other. The fight over the SOPA/PIPA legislation, where entertainment and technology companies, along with their users, fought it out in the halls of Congress, doubtlessly lies somewhere in between.

But Google is likewise doubtlessly a part of that continuum, not apart from it.

Because of its origin and the nature of its business, Google's prospects are inexorably tied to the fate of the open web. But we have to resist the urge to make the two identical. Google isn't just a web-crawling search company any more.

Press photo of Sergey Brin courtesy Google.",Social Media,WIRED,https://www.wired.com/2012/04/open-web-google-brin/,"The open internet is facing numerous threats from government censorship, overzealous IP protection and corporate control of communication tools, all of which are adversely affecting Google's business prospects.",Security & Privacy
10,It’s Time for Facebook to Deal With the Grimy History of Revenge Porn,"Revenge porn is a big problem for the internet. In other words, it's a big problem for Facebook. This became abundantly clear with the recent revelation that a secret Facebook group called Marines United, which consisted of active and veteran members of the Marine Corps, circulated nude and otherwise invasive photos of women without their consent. The photos included ex-girlfriends as well as strangers, service members and civilians alike. The Department of Defense has since launched an investigation, and Senate Armed Services Committee hearings began this morning.

With Mark Zuckerberg pulling more and more of the internet into his domain, Facebook has emerged as fertile soil for revenge porn—especially so-called ""closed"" groups, restricted-membership areas of the site that are nearly unpoliceable. (Even since Marines United's closure, other similar groups have rushed in to fill the void.) But with US revenge porn law still a patchwork of difficult-to-enforce statutes, it's increasingly incumbent on Facebook to come up with the solution itself.

Given the history of revenge porn, though, you may not want to hold your breath.

YouTube, Crowdsourcing, and the Rise of Revenge

The existence of revenge porn stretches back to the 1980s, when *Hustler *magazine published a stolen nude photo in its reader-submission feature, ""Beaver Hunt."" But the phenomenon flourished in the anonymous, lawless jungles of the early internet—especially on Usenet groups, where by 2000, photos and videos of users' ex-girlfriends became a genre called ""realcore porn."" (Dear millennials: Usenet groups were basically decentralized Reddit. No, really!)

From there, revenge porn exploded along with the rest of internet pornography. That's not to say that porn wasn't already popular in the early 2000s, but after YouTube launched in 2005 and online video became fast and easy, NSFW copycats like YouPorn or RedTube made it ubiquitous. Given that ease of uploading, professionally produced content wasn't the the only stuff on those sites; user-generated amateur porn (which can be pretty exploitative in its own right) became more popular than ever.

That explosion of consensual amateur content, and rise of texting nude pics, opened the door for revenge porn. ""In 2007, I was laboring alone in the dark on this,"" says Danielle Citron, who teaches law at the University of Maryland. ""But with the saturation of porn and camera phones, you got a lot of young girls being pressured to share nude photos, and this trend came to a head."" Sites dedicated to the ""genre""---sporting names like realexgirlfriend.com---start to pop up around the same time, and porn aggregators like XTube reported receiving complaints from revenge porn victims.

If there's one person most associated with pushing revenge porn into the national conversation, it's a man named Hunter Moore, who in 2010 launched the infamous IsAnyoneUp.com. The site posted pornographic images and video that men had submitted of their ex-girlfriends—as well as the women's full names and links to their Facebook profiles. Further, he taunted his victims, one of whom tracked him down and stabbed him with a pen. (Outside the US, legal precedents were starting to mount; in 2010, a New Zealand man went to prison for posting naked pictures of his ex-girlfriend on Facebook.) Moore shuttered his site in 2012, but revenge porn obviously didn't end there.",Social Media,WIRED,https://www.wired.com/2017/03/revenge-porn-facebook/,"Revenge porn has become increasingly prevalent on social media, with sites like Facebook being fertile soil for the phenomenon. It is becoming increasingly evident that legal statutes are not enough to address the issue, making it more and more incumbent on Facebook to come up with a solution.",Security & Privacy
11,Twitter warns investors of possible fine from FTC consent order probe – TechCrunch,"Twitter has disclosed it’s facing a potential fine of more than a hundred million dollars as a result of a probe by the Federal Trade Commission (FTC), which believes the company violated a 2011 consent order by using data provided by users for a security purpose to target them with ads.

In an SEC filing, reported on earlier by The New York Times, Twitter revealed it received the draft complaint from the FTC late last month. The activity the regulator is complaining about is alleged to have taken place between 2013 and 2019.

Last October the social media firm publicly disclosed it had used phone numbers and email addresses provided by users to set up two-factor authentication to bolster the security of their accounts in order to serve targeted ads — blaming the SNAFU on a tailored audiences program, which allows companies to target ads against their own marketing lists.

Twitter found that when advertisers uploaded their own marketing lists (of emails and/or phone numbers) it matched users to data they had submitted purely to set up two-factor authentication on their Twitter account.

“The allegations relate to the Company’s use of phone number and/or email address data provided for safety and security purposes for targeted advertising during periods between 2013 and 2019,” Twitter writes in the SEC filing. “The Company estimates that the range of probable loss in this matter is $150.0 million to $250.0 million and has recorded an accrual of $150.0 million.”

“The matter remains unresolved, and there can be no assurance as to the timing or the terms of any final outcome,” it adds.

We’ve reached out to Twitter with questions. Update: A company spokeswoman said it had nothing to add outside this statement:

Following the announcement of our Q2 financial results, we received a draft complaint from the FTC alleging violations of our 2011 consent order. Following standard accounting rules we included an estimated range for settlement in our 10Q filed on August 3.

The company has had a torrid few weeks on the security front, suffering a major security incident last month after hackers gained access to its internal account management tools, enabling them to access accounts of scores of verified Twitter users, including Bill Gates, Elon Musk and Joe Biden, and use them to send cryptocurrency scam tweets. Police have since charged three people with the hack, including a 17-year-old from Florida.

In June Twitter also disclosed a security lapse may have exposed some business customers’ information. It was forced to report another crop of security incidents last year — including after a researcher identified a bug that allowed him to discover phone numbers associated with millions of Twitter accounts.

Twitter also admitted it gave account location data to one of its partners, even if the user had opted-out of having their data shared; and inadvertently gave its ad partners more data than it should have.

Additionally, the company is now at the front of a long queue of tech giants pending enforcement in Europe, related to major GDPR complaints — where regional fines for data violations can scale to 4% of a company’s global annual turnover. Twitter’s lead data protection regulator, Ireland’s DPC, submitted a draft decision related to a probe of one of its security breaches to the bloc’s other data agencies in May — with a final decision slated as likely to land this summer.

The decision relates to an investigation the regulator instigated following yet another major security fail by Twitter in 2018 — when it revealed a bug had resulted in some passwords being stored in plain text.

As we reported at the time, it’s pretty unusual for a company of such size to make such a basic security mistake. But Twitter has a very long history of failing to protect users’ data — with additional hacking incidents all the way back in 2009 leading to the 2011 FTC consent order.

Under the terms of that settlement Twitter was barred for 20 years from misleading consumers about the safety of their data in order to resolve FTC charges that it had “deceived consumers and put their privacy at risk by failing to safeguard their personal information.”

It also agreed to establish and maintain “a comprehensive information security program,” with independent auditor assessments taking place every other year for 10 years.

Given the terms of that order, a fine does indeed look inevitable. However, the wider failing here is that of U.S. regulators — which, for over a decade, have failed to grapple with the exploitative, surveillance-based business models that have led to breaches and security lapses by a number of data-mining adtech giants, not just Twitter.",Social Media,TechCrunch,https://techcrunch.com/2020/08/04/twitter-warns-investors-of-possible-fine-from-ftc-consent-order-probe/,"Twitter faces a potential fine of over $100 million due to a Federal Trade Commission (FTC) probe into its violation of a 2011 consent order through using user data for security purposes to target them with ads. This is just one of many security incidents since 2009, and regulators have failed to take meaningful action against exploitative, surveillance-based",Security & Privacy
12,Sex tech companies and advocates protest unfair ad standards outside Facebook’s NY HQ – TechCrunch,"A group of sex tech startup founders, employees and supporters gathered outside of Facebook’s NY office in Manhattan to protest its advertising policies with respect to what it classifies as sexual content. The protest, and a companion website detailing their position we reported on Tuesday, are the work of “Approved, Not Approved,” a coalition of sex health companies co-founded by Dame Products and Unbound Babes.

These policies as applied have fallen out of step with “the average person’s views of what should or shouldn’t be approved of ads,” according to Janet Lieberman, co-founder and CTO of Dame Products.

“If you look at the history of the sex toy industry, for example, vibrators were sexual health products until advertising restrictions were put on them in the 1920s and 1930s — and then they became dirty, and that’s how the industry got shady, and that’s why we have negative thoughts towards them,” she told me in an interview at the protest. “They’re moving back towards wellness in people’s minds, but not in advertising policies. There’s a double standard for what is seen as obscene, talking about men’s sexual health versus women’s sexual health and talking about products that aren’t sexual, and using sex to sell them, versus taking sexual products and having completely non-sexual ads for them.”

It’s a problem that extends beyond just Facebook and Instagram, Lieberman says. In fact, her company is also suing NYC’s MTA for discrimination for its own ad standards after it refused to run ads for women’s sex toys in their out-of-home advertising inventory. But it also has ramifications beyond just advertising, because in many ways what we see in ads helps define what we see as acceptable in terms of our everyday lives and conversations.

“Some of this stems from society’s inability to separate sexual products from feeling sexual, and that’s a real problem that we see that hurts women more than men, but hurts both genders, in not knowing how to help our sexual health,” Lieberman said. “We can’t talk about it without being sexual, and that we can’t bring things up, without it seeming like we’re bringing up something that is dirty.”

“A lot of the people you see here today have Instagrams that have been shut down, or ads that have been not approved on Facebook,” said Bryony Cole, CEO at Future of Sex, in an interview. “Myself, I run Future of Sex, which is a sex tech hackathon, and a podcast focused on sex tech, and my Instagram’s been shut down twice with no warning. It’s often for things that Facebook will say they consider phallic imagery, but they’re not […] and yet if you look at images for something like HIMS [an erectile dysfunction medication startup, examples of their ads here], you’ll see those phallic practice images. So there’s this gross discrepancy, and it’s very frustrating, especially for these companies where a lot of the revenue in their business is around community that are online, which is true for sex toys.”

Online ads aren’t just a luxury for many of these startup brands and companies — they’re a necessary ingredient to continued success. Google and Facebook together account for the majority of digital advertising spend in the U.S., according to eMarketer, and it’s hard to grow a business that caters to primarily online customers without fair access to their platforms, Cole argues.

“You see a lot of sex tech or sexual wellness brands having to move off Instagram and find other ways to reach their communities,” she said. “But the majority of people, that’s where they are. And if they’re buying these products, they’re still overcoming a stigma about buying the product, so it’s great to be able to purchase these online. A lot of these companies started either crowdfunding, like Dame Products, or just through e-commerce sites. So the majority of their business is online. It’s not in a store.”

Earlier this year, sex tech company Lora DiCarlo netted a win in getting the Consumer Technology Association to restore its CES award after community outcry. Double standards in advertising is a far more systemic and distributed problem, but these protests will hopefully help open up the conversation and prompt more change.",Social Media,TechCrunch,https://techcrunch.com/2019/07/31/sex-tech-companies-and-advocates-protest-unfair-ad-standards-outside-facebooks-ny-hq/,"The protest and website from ""Approved, Not Approved"" is aiming to change the double standard in advertising policies from Social Media platforms which have led to negative views of women's sexual health products, and have hurt brands depending on online sales as it is difficult to reach their customers on platforms like Facebook and Instagram.",Equality & Justice
13,Tweeters Tilt at Canada Election Law,"Call it a sit-in for the digital age.

As Canadians go to the polls Monday to elect members of the 41st Parliament, some citizens say they intend to stick it to the man -- by tweeting results throughout the day.

Section 329 of the Elections Act forbids Canadians from broadcasting local election results publicly, before all polls in Canada have officially closed. When the law was first enforced in 1938, “broadcasting” was defined by radio and television. But now, tweeting, blogging and Facebooking all make the list.

""This legislation is ridiculously outdated,"" says Peter Raaymaker (@rymkrs), a blogger from Ontario, Canada, who intends to be a scofflaw. ""It doesn’t reflect the world we live in today.""

Raaymakers isn't a lone voice. NYU professor Jay Rosen (@jayrosen_nyu), an active social media commentator, stoked the fire by tweeting ""So is anyone in Canada organizing a mass tweet-in to protest the (absurd) ban on election night tweeting?""

Apparently, the answer is yes.

A host of deliberate tweets about local election results is expected on Election Day, May 2. “Either we flood the system and [Elections Canada] gets overwhelmed, or we all get fined and end up paying for #elxn41,"" tweeted Denis Gagnon (@DenisGagnonJr), a forester from Timmins, Ontario.

In response, he received a chorus of ""I'm in'""s.

As Gagnon says, the consequences of tweeting results are fines up to CN$25,000 [about U.S. $26,400].

Is he willing to do it anyway? ""I will tweet my results,"" he says. ""And if I had to, yeah, I’d pay the fine.”

John Enright, Election Canada’s spokesperson, points out that there have been no fines in the last three elections and that the system is strictly complaints-based. ""We are not monitoring social media or any other form of transmission,"" he says. The plan is to just wait and see how events play out.

To track the protest and see how many flout the law Monday, two Vancouver social media enthusiasts, Darren Barefoot and Alexandra Samuel, have created Tweettheresults.ca, a live Twitter feed that will follow the hashtag #tweettheresults.

Although both have spoken out against the law and believe it is totally unfeasible, Barefoot said in an interview with the National Post, ""We’re not actually, arguably, publishing the results ourselves."" Even their tagline, ""Bringing you the people who are bringing you Canada’s first real-time election,"" is a sort of cop-out on the entire issue.

Even if they don’t tweet, all those participating in the conversation hope this social media rebellion will cause the Canadian government to take note and amend the law. ""I do know parliament is aware of this question,” says Enright. “They have heard of this community and its opinions.""

As for the question of what will happen on election night, the reality remains to be seen — will this be the united defiance of Canadian tweeters or the revolution that never was?

See Also:",Social Media,WIRED,https://www.wired.com/2011/05/tweeters-tilt-at-canada-election-law/,"Canadian citizens risk fines up to $26,400 for tweeting local election results before polls close, but a protest of this outdated law is being organized for Election Day.",Politics
14,Facebook moves to shrink its legal liabilities under GDPR – TechCrunch,"Facebook has another change in the works to respond to the European Union’s beefed up data protection framework — and this one looks intended to shrink its legal liabilities under GDPR, and at scale.

Late yesterday Reuters reported on a change incoming to Facebook’s T&Cs that it said will be pushed out next month — meaning all non-EU international are switched from having their data processed by Facebook Ireland to Facebook USA.

With this shift, Facebook will ensure that the privacy protections afforded by the EU’s incoming General Data Protection Regulation (GDPR) — which applies from May 25 — will not cover the ~1.5BN+ international Facebook users who aren’t EU citizens (but current have their data processed in the EU, by Facebook Ireland).

The U.S. does not have a comparable data protection framework to GDPR. While the incoming EU framework substantially strengthens penalties for data protection violations, making the move a pretty logical one for Facebook’s lawyers thinking about how it can shrink its GDPR liabilities.

Reuters says Facebook confirmed the impending update to the T&Cs of non-EU international users, though the company played down the significance — repeating its claim that it will be making the same privacy “controls and settings” available everywhere. (Though, as experts have pointed out, this does not mean the same GDPR principles will be applied by Facebook everywhere.)

Critics have couched the T&Cs shift as regressive — arguing it’s a reduction in the level of privacy protection that would otherwise have applied for international users, thanks to GDPR. Although whether these EU privacy rights would really have been enforceable for non-Europeans is questionable.

At the time of writing Facebook had not responded to a request for comment on the change. Update: It’s now sent us the following statement — attributed to deputy chief global privacy officer, Stephen Deadman: “The GDPR and EU consumer law set out specific rules for terms and data policies which we have incorporated for EU users. We have been clear that we are offering everyone who uses Facebook the same privacy protections, controls and settings, no matter where they live. These updates do not change that.”

The company’s generally argument is that the EU law takes a prescriptive approach — which can make certain elements irrelevant for international users outside the bloc. It also claims it’s working on being more responsive to regional norms and local frameworks. (Which will presumably be music to the New Zealand privacy commissioner‘s ears, for one…)

According to Reuters the T&Cs shift will affect more than 70 per cent of Facebook’s 2BN+ users. As of December, Facebook had 239M users in the US and Canada; 370M in Europe; and 1.52BN users elsewhere.

The news agency also reports that Microsoft-owned LinkedIn is one of several other multinational companies planning to make the same data processing shift for international users — with LinkedIn’s new terms set to take effect on May 8, moving non-Europeans to contracts with the U.S.-based LinkedIn Corp.

In a statement to Reuters about the change LinkedIn also played it down, saying: “We’ve simply streamlined the contract location to ensure all members understand the LinkedIn entity responsible for their personal data.”

One interesting question is whether these sorts of data processing shifts could encourage regulators in international regions outside the EU to push for a similarly extraterritorial scope for their local privacy laws — or face their citizens’ data falling between the jurisdiction cracks via processing arrangements designed to shrink companies’ legal liabilities.

Interesting example of Delaware effect, however if other jurisdictions are smart they will have their own art 3 GDPR, that would be Brussels effect. Be sure the lobbying has started. Now if ever we need thinkers (who are also doers) like @linnetelwin — Mireille Hildebrandt (@mireillemoret) April 19, 2018

Another interesting question is how Facebook (or any other multinationals making the same shift) can be entirely sure it’s not risking violating any of its EU users’ fundamental rights — i.e. if it accidentally misclassifies an individual as an non-EU international users and processes their data via Facebook USA.

Keeping data processing processes properly segmented can be difficult. As can definitively identifying a user’s legal jurisdiction based on their location (if that’s even available). So while Facebook’s contract change for international users looks largely intended to shrink its legal liabilities under GDPR, it’s possible the change will open up another front for individuals to pursue strategic litigation in the coming months.",Social Media,TechCrunch,https://techcrunch.com/2018/04/19/facebook-moves-to-shrink-its-legal-liabilities-under-gdpr/,"Facebook’s incoming change to its terms and conditions of non-EU international users may be an attempt to reduce its liabilities under GDPR, but it could also risk violating users' fundamental rights while opening up a new front of strategic litigation.",Security & Privacy
15,"Fed up with Facebook, activists find new ways to defend their movements – TechCrunch","In the wake of revelations that the personal information of as many as 87 million Facebook users was used by data analysis firm Cambridge Analytica in 2016 for political purposes, reports indicate Facebook will contribute raw, anonymized data to a new Social Data Initiative via what is described as an independent, transparent and peer–reviewed process.

Will greater data sharing place the information of communities of color at greater risk? Or will making aggregated user data available data better inform our understanding of social media’s impact on society? Caught between these questions are activists of color and the vulnerable communities they represent.

Activists of color weren’t surprised by the Cambridge Analytica revelations. This scandal is only the latest in a string of worrisome disclosures about the use of social media by third parties, from foreign governments and electoral candidates to law enforcement agencies, to spy on the activities of users – especially immigrant, Black and other vulnerable communities.

With half of all U.S. adults already in police facial recognition databases and the 2018 midterm election season upon us, the issue of political data mining feels urgent to Black activists. “We are tracked by data mining companies that have contracts with law enforcement that profile and criminalize us. This works in tandem with designations like ‘Black Identity Extremism’, a made up term by the FBI to attack Black organizers,” said Janaya Khan, a Black Lives Matter activist and organizer with the national civil rights group Color of Change.

A supporter of a handful of protesters from the activist group the Raging Nannies who gathered outside of Facebook to demand greater data protection, Electronic Frontier Foundation Organizer Nathan Sheard, also raised concerns, “Facebook has a responsibility to its users.” He goes on to note that, “By default their [user] info should be kept secure.” Yet user information on Facebook remains extraordinarily vulnerable and far too available to third parties, without the consent of Facebook users.

Congress has joined the chorus of voices seeking answers. Mark Zuckerberg, Facebook Founder and CEO will testify at a joint hearing before the Senate Judiciary and Senate Commerce, Science, and Transportation committees on Tuesday, April 10 at 2:15 pm Eastern time. He’ll be back on Capitol Hill the following day for another hearing before the House Energy and Commerce Committee on Wednesday, April 11, at 10 am ET.

Congress’ failure to protect the data of vulnerable users has created real world fears for immigrant rights activists working tirelessly to protect undocumented families facing a wave of deportation under President Trump. Co-founder and Executive Director of United We Dream, Cristina Jimenez explains, “Our movement is led by undocumented immigrants and people of color, and under Trump we’ve seen our members targeted in phishing attacks online and chased by white supremacists out in the streets.”

These conditions have prompted some to delete Facebook, which must be done skillfully to ensure all personal data has actually been removed. Given that two–thirds of Americans get their news from platforms like Facebook, the likelihood that users will delete the social media giant is low. For others, the call to action is for Congress to pass laws that require greater data protection in order for Facebook to operate in the U.S. — which can take time.

Activists from the movements for Black lives, immigrant rights, Muslim freedom, and others protesting to save their lives, protect their families, or defend their environment and land can’t wait for data protection. These activists and the technologists who support them have come together to create a resource for keeping their accounts secure and to protect their critical work: defendourmovements.org.

Activists like Southwest Organizing Project activist Roberto Roibal are already responding to the site.

I really like https://t.co/cTQYtrtDmo. I posted a question this morning on wifi security and got a really quick response. I think its a great resource for community orgs. #defendourmovements — Roberto Roibal (@Roberto_SWOP) April 4, 2018

The site, which includes a help desk and crowd-sourced knowledge base, was built to provide activists at the greatest risk of surveillance with culturally relevant digital safety tips, tools and support, vetted by technologists that understand and are participants in social movements. Its launch is accompanied by ongoing digital security trainings nationwide. Together, these tools and trainings offer a starting place for securing social movements in an increasingly frightening political environment.

After all, the Cambridge Analytica debacle is far from the first time corporations and government institutions have used Facebook and other social media platforms to spy on the most vulnerable in our society.

Just last month The Intercept reported that the Immigration and Customs Enforcement agency has also been using Facebook to do its “extreme vetting” dirty work, and it’s been confirmed that Russian government officials utilized multiple social media platforms to influence the 2016 election. And in that same year, Facebook, Instagram, and Twitter were forced to update their platforms after providing user’s data to Geofeedia – a social media surveillance company which marketed its tools to police officers nationwide, in order to monitor protesters and activists of color.

The ACLU has outlined immediate steps Facebook should take in response to this latest privacy disclosures, including implementing better auditing procedures and enforcement of its policies for developers, but the fight to preserve our right to resist online will continue regardless. The human rights organization Witness also chimed in with a thoughtful analysis of next steps the company could take.

Beyond the urgent need for digital security, what movement leaders understand is that if they don’t create these tools, no one else will. Activists cannot wait for Facebook and Mark Zuckerberg to change.

Hacker and security specialist Matt Mitchell said, “ All movements have those who secure the fight because they believe in it. They are the ones folks trust, the ones who sacrifice over and over again. They put time and love into the struggle. Belonging is what brings them to this work. Look, people working for justice and freedom have adversaries who work nine to five to slow things down. We’re being secured 24/7 by our organizing. That’s why we will win.”

We are the ones we’ve been waiting for. In a time when it is hard to tell what’s real and what’s not, digital security grounded in authentic relationships can make all the difference.

Learn more at DefendOurMovements.org or mediajustice.org",Social Media,TechCrunch,https://techcrunch.com/2018/04/10/fed-up-with-facebook-activists-find-new-ways-to-defend-their-movements/,"The Cambridge Analytica scandal has raised urgent questions about the vulnerability of personal data on social media, particularly for Black, immigrant, and other vulnerable communities, who have been tracked by data mining companies and targeted by law enforcement. Activists of color are now responding with digital security tools and trainings to protect their critical work.",Security & Privacy
16,"In a Fake Fact Era, Schools Teach the ABCs of News Literacy","""This is no longer something that if we have time to expose children to, that would be great,"" says Michelle Ciulla Lipkin, executive director of the National Association for Media Literacy Education. ""This is a crisis situation. We do not teach our students enough about what they need to understand about the world they live in."" Checkology, she says, is one important tool helping to change that.

Infograzers

On the day I returned to my alma mater, the students were categorizing online posts as news, entertainment, propaganda, publicity, advertising, raw information, or opinion. As Craige stood by, 13-year-old Catherine Aaron, an 8th grader already dressed in her softball uniform for that day's game, puzzled over a headline from the left-leaning outlet Daily Beast. It read, ""And Then They Came for Big Bird: Public Broadcasting Reels From Trump’s Plan to Destroy It."" The sub-headline continued, ""Next on President Trump’s hit list: public broadcasting. His plan to defund it will have a decimating effect on access to nuanced journalism and educational TV.""

Aaron had a hunch this was the author's opinion. ""What makes you think that?"" Craige prompted the 8th grader.

""The language of it is more of an opinion,"" Aaron says. ""Decimating. Destroying.""

Sophie Giovonnone, 14, isn't so sure. She thinks it might be working as publicity for Democrats, ""because it could cause some conflict"" for Trump.

I ask Giovonnone whether she knows what the Daily Beast is. She doesn't. In fact, most of the students say that outside of class, they rarely encounter much news online at all. Only one student in the whole class uses Twitter. No one even has a Facebook account. Their social media lives consist mainly of Instagram and Snapchat, one of the few platforms that still meticulously curates what news is and isn't allowed in its Discover feature. (WIRED recently joined Discover.)

For a moment, I think, maybe the fact that these students aren't using Facebook or Twitter is a promising sign. Maybe the very nature of the platforms this generation is growing up with will shield it from the internet's onslaught of misinformation. But Adams stops me short. Kids today, he says, are ""infograzers."" Without realizing it, the memes they share and and viral videos they watch each day are telling them stories about the world they live in—not all of them true.

""What counts as news has broadened for this generation,"" he says. ""Unless they learn to flag content and figure out why something might not be sound evidence, it sticks with them.""

And even if they're not skimming social media, it's become second nature to them to whip out their smartphones and Google the answers to any questions they don't know. Tools like Checkology encourage them to dig deeper than the first headline that turns up.

As they get older, the spectrum of online sources they use will broaden even further, and that's when these skills will matter most, says Ciulla Lipkin. ""When we were growing up some of the work we’re doing in school might not have seemed relevant at the time, but it’s teaching students skills they need for the future,"" she says. ""It gets students to practice asking questions.""

Or, as Sachs puts it, ""We're arming them before they hit the battle.""

The question is—as it is for all school subjects—will that practice stick as students grow up and technology evolves? The company is currently crunching the numbers on its first quantitative survey that measures how students' understanding of the topic changes from the beginning of the course to the end.

Catalan, Aaron, Giovannone, and the rest of the 8th grade class walked away from Norwood on Monday for the last time. This fall, they'll head off for high school. If by some chance they return to this place 20 years down the road, as I did, they will no doubt find that the world of communication has changed even more drastically since they sat in these very seats. Now, as the country continues to fight over the fundamental definition of truth, it falls to educators across the country to prepare their students for whatever mayhem those changes may bring.""",Social Media,WIRED,https://www.wired.com/2017/06/fake-fact-era-schools-teach-abcs-news-literacy/,"Due to Social Media, the current generation of students are exposed to an onslaught of misinformation, leading to them having difficulty distinguishing between true and false information. Educators must equip them with the skills to identify and evaluate online sources in order to help them navigate the digital world.","Information, Discourse & Governance"
17,"House Democrats Release 3,500 Russia-Linked Facebook Ads","Another ad, meanwhile, purchased by the page Stop A.I.—short for Stop All Invaders—showed a photoshopped President Obama in the Oval Office, with an ISIS flag behind him. ""Obama was always a mere pawn in the hands of the Arabian Sheikhs,"" it reads in part. ""All these refugees, which we are about to take in, are soldiers with one simple goal. They are going to try to terrorize the nation.""

House Permanent Select Committee on Intelligence Minority

Some of the ads promote real-world events organized by unwitting Facebook users. One such event, promoted by the page Black Matters, promoted a night of free legal help for immigrants. Another, published by the page Fit Black, promoted free self-defense classes run by a club called Black Fist. Buzzfeed News previously reported that IRA trolls had lured American fitness trainers to lead these classes, even going so far as to pay them $320 a month.

HOUSE PERMANENT SELECT COMMITTEE ON INTELLIGENCE MINORITY

What made these ads so deceptive is they rarely looked like traditional political ads. Often, they don't mention a candidate or the election at all. Instead, they tear at the parts of the American social fabric that are already worn thin, stoking outrage about police brutality or the removal of Confederate statues.

HOUSE PERMANENT SELECT COMMITTEE ON INTELLIGENCE MINORITY

Throughout Congress's investigation into Russian meddling in the election, legislators have questioned tech leaders about how the IRA targeted the ads. Facebook has repeatedly said that the majority of ads were geographically targeted broadly to the United States, and the House's trove backs that up. But some ads in the new collection, including a few published by the page Black Matters, specifically target cities with a history of headlines about racial unrest and police brutality, like Baltimore, Maryland, and Ferguson, Missouri. Others target by interest category. In January of 2016, a single ad for a page called Williams&Kalvin, two YouTubers the IRA hired to promote news and video about the black community, targeted users age 18 to 45 who were interested in BlackNews.com, the color black, or HuffPost Black Voices but were not Hispanic or Asian American. Facebook says it has since revisited some of its targeting categories, eliminating one-third of the categories the IRA used.

Williams&Kalvin appears to be one of the pages that tested which messages would resonate most with audiences. Within a single hour on January 14, 2016, that page posted the same ad with two different messages. The ad that read ""Community about black social and racial issues! Like to subscribe!"" received two clicks. The ad that read ""Black Discrimination Awareness! Like to join!"" got 2,592. That kind of testing is common in digital advertising, but it suggests the IRA was operating at a certain level of sophistication.

HOUSE PERMANENT SELECT COMMITTEE ON INTELLIGENCE MINORITY HOUSE PERMANENT SELECT COMMITTEE ON INTELLIGENCE MINORITY

Facebook has sought to downplay the damage caused by these ads, often emphasizing that the IRA spent just $100,000 on them, compared to the nearly $40 billion in ad revenue the company earned in 2017. But the House's disclosure shows how far even a small sum can go. One ad, purchased on June 23, 2015, by the page LGBT United, cost 99.95 Russian rubles, or the equivalent of $1.59. That ad garnered 26 clicks and 374 impressions in a single day. Looked at this way, Facebook's argument that 50 percent of the ads cost less than $3 doesn't sound all that compelling.

The ads also represent just a sliver of what the Russian trolls posted on Facebook and Instagram. While 3,500 sounds like a sizable number, IRA accounts published some 80,000 organic posts on Facebook and another 120,000 on Instagram, reaching roughly 146 million Americans between the two.",Social Media,WIRED,https://www.wired.com/story/house-democrats-release-3500-russia-linked-facebook-ads/,"The House's trove of 3,500 ads purchased by the Russian Internet Research Agency reveals the deceptive tactics used by malicious actors to target American audiences on social media. These ads often targeted vulnerable parts of our society and sought to stoke outrage, demonstrating how far even a small sum of money can go. Combined with organic posts, it is estimated","Information, Discourse & Governance"
18,Study of YouTube comments finds evidence of radicalization effect – TechCrunch,"Research presented at the ACM FAT 2020 conference in Barcelona today supports the notion that YouTube’s platform is playing a role in radicalizing users via exposure to far-right ideologies.

The study, carried out by researchers at Switzerland’s Ecole polytechnique fédérale de Lausanne and the Federal University of Minas Gerais in Brazil, found evidence that users who engaged with a middle ground of extreme right-wing content migrated to commenting on the most fringe far-right content.

A March 2018 New York Times article by sociologist Zeynep Tufekci set out the now widely reported thesis that YouTube is a radicalization engine. Followup reporting by journalist Kevin Roose told a compelling tale of the personal experience of an individual, Caleb Cain, who described falling down an “alt right rabbit hole” on YouTube. But researcher Manoel Horta Ribeiro, who was presenting the paper today, said the team wanted to see if they could find auditable evidence to support such anecdotes.

Their paper, called “Auditing radicalization pathways on YouTube,” details a large-scale study of YouTube looking for traces of evidence — in likes, comments and views — that certain right-leaning YouTube communities are acting as gateways to fringe far-right ideologies.

Per the paper, they analyzed 330,925 videos posted on 349 channels — broadly classifying the videos into four types: Media, the Alt-lite, the Intellectual Dark Web (IDW) and the Alt-right — and using user comments as a “good enough” proxy for radicalization (their data set included 72 million comments).

The findings suggest a pipeline effect over a number of years where users who started out commenting on alt-lite/IDW YouTube content shifted to commenting on extreme far-right content on the platform over time.

The rate of overlap between consumers of Media content and the alt-right was found to be far lower.

“A significant amount of commenting users systematically migrates from commenting exclusively on milder content to commenting on more extreme content,” they write in the paper. “We argue that this finding provides significant evidence that there has been, and there continues to be, user radicalization on YouTube, and our analyses of the activity of these communities… is consistent with the theory that more extreme content ‘piggybacked’ on the surge in popularity of I.D.W. and Alt-lite content… We show that this migration phenomenon is not only consistent throughout the years, but also that it is significant in its absolute quantity.”

The researchers were unable to determine the exact mechanism involved in migrating YouTube users from consuming “alt lite” politics to engaging with the most fringe and extreme far-right ideologies — citing a couple of key challenges on that front: Limited access to recommendation data; and the study not taking into account personalization (which can affect a user’s recommendations on YouTube).

But even without personalization, they say they were “still able to find a path in which users could find extreme content from large media channels.”

During a conference Q&A after presenting the paper, Horta Ribeiro was asked what evidence they had that the radicalization effect the study identifies had occurred through YouTube, rather than via some external site — or because the people in question were more radicalized to begin with (and therefore more attracted to extreme ideologies) versus the notion of YouTube itself being an active radicalization pipeline.

He agreed it’s difficult to make an absolute claim that YouTube is to blame. But also argued that, as host to these communities, the platform is responsible.

“We do find evident traces of user radicalization, and I guess the question asks why is YouTube responsible for this? And I guess the answer would be because many of these communities they live on YouTube and they have a lot of their content on YouTube and that’s why YouTube is so deeply associated with it,” he said.

“In a sense I do agree that it’s very hard to make the claim that the radicalization is due to YouTube or due to some recommender system or that the platform is responsible for that. It could be that something else is leading to this radicalization and in that sense I think that the analysis that we make it shows there is this process of users going from milder channels to more extreme ones. And this solid evidence towards radicalization because people that were not exposed to this radical content become exposed. But it’s hard to make strong causal claims — like YouTube is responsible for that.”

We reached out to YouTube for a response to the research but the company did not reply to our questions.

The company has tightened its approach toward certain far-right and extremist content in recent years, in the face of growing political and public pressure over hate speech, targeted harassment and radicalization risks.

It has also been experimenting with reducing algorithmic amplification of certain types of potentially damaging nonsense content that falls outside its general content guidelines — such as malicious conspiracy theories and junk science.",Social Media,TechCrunch,https://techcrunch.com/2020/01/28/study-of-youtube-comments-finds-evidence-of-radicalization-effect/,"Research presented at the ACM FAT 2020 conference in Barcelona supports the notion that YouTube's platform is playing a role in radicalizing users by exposing them to far-right ideologies. The study found evidence that users engaging with extreme right-wing content on the platform migrate to commenting on the most fringe far-right content, pointing to a pipeline effect",Politics
19,UK now expects compliance with children’s privacy design code – TechCrunch,"In the U.K., a 12-month grace period for compliance with a design code aimed at protecting children online expires today — meaning app makers offering digital services in the market which are “likely” to be accessed by children (defined in this context as users under 18 years old) are expected to comply with a set of standards intended to safeguard kids from being tracked and profiled.

The Age Appropriate Design Code (aka the ‘Children’s Code’) came into force on September 2 last year however the U.K.’s data protection watchdog, the ICO, allowed the maximum grace period for hitting compliance to give organizations time to adapt their services.

But from today it expects the standards of the code to be met.

Services where the code applies can include connected toys and games and edtech but also online retail and for-profit online services such as social media and video sharing platforms which have a strong pull for minors.

Among the code’s stipulations are that a level of “high privacy” should be applied to settings by default if the user is (or is suspected to be) a child — including specific provisions that geolocation and profiling should be off by default (unless there’s a compelling justification for such privacy hostile defaults).

The code also instructs app makers to provide parental controls while also providing the child with age-appropriate information about such tools — warning against parental tracking tools that could be used to silently/invisibly monitor a child without them being made aware of the active tracking.

Another standard takes aim at dark pattern design — with a warning to app makers against using “nudge techniques” to push children to provide “unnecessary personal data or weaken or turn off their privacy protections.”

The full code contains 15 standards but is not itself baked into legislation — rather it’s a set of design recommendations the ICO wants app makers to follow.

The regulatory stick to make them do so is that the watchdog is explicitly linking compliance with its children’s privacy standards to passing muster with wider data protection requirements that are baked into U.K. law.

The risk for apps that ignore the standards is thus that they draw the attention of the watchdog — either through a complaint or proactive investigation — with the potential of a wider ICO audit delving into their whole approach to privacy and data protection.

“We will monitor conformance to this code through a series of proactive audits, will consider complaints, and take appropriate action to enforce the underlying data protection standards, subject to applicable law and in line with our Regulatory Action Policy,” the ICO writes in guidance on its website. “To ensure proportionate and effective regulation we will target our most significant powers, focusing on organisations and individuals suspected of repeated or wilful misconduct or serious failure to comply with the law.”

It goes on to warn it would view a lack of compliance with the kids’ privacy code as a potential black mark against (enforceable) U.K. data protection laws, adding: “If you do not follow this code, you may find it difficult to demonstrate that your processing is fair and complies with the GDPR [General Data Protection Regulation] or PECR [Privacy and Electronics Communications Regulation].”

In a blog post last week, Stephen Bonner, the ICO’s executive director of regulatory futures and innovation, also warned app makers: “We will be proactive in requiring social media platforms, video and music streaming sites and the gaming industry to tell us how their services are designed in line with the code. We will identify areas where we may need to provide support or, should the circumstances require, we have powers to investigate or audit organisations.”

“We have identified that currently, some of the biggest risks come from social media platforms, video and music streaming sites and video gaming platforms,” he went on. “In these sectors, children’s personal data is being used and shared, to bombard them with content and personalised service features. This may include inappropriate adverts; unsolicited messages and friend requests; and privacy-eroding nudges urging children to stay online. We’re concerned with a number of harms that could be created as a consequence of this data use, which are physical, emotional and psychological and financial.”

“Children’s rights must be respected and we expect organisations to prove that children’s best interests are a primary concern. The code gives clarity on how organisations can use children’s data in line with the law, and we want to see organisations committed to protecting children through the development of designs and services in accordance with the code,” Bonner added.

The ICO’s enforcement powers — at least on paper — are fairly extensive, with GDPR, for example, giving it the ability to fine infringers up to £17.5 million or 4% of their annual worldwide turnover, whichever is higher.

The watchdog can also issue orders banning data processing or otherwise requiring changes to services it deems non-compliant. So apps that chose to flout the children’s design code risk setting themselves up for regulatory bumps or worse.

In recent months there have been signs some major platforms have been paying mind to the ICO’s compliance deadline — with Instagram, YouTube and TikTok all announcing changes to how they handle minors’ data and account settings ahead of the September 2 date.

In July, Instagram said it would default teens to private accounts — doing so for under-18s in certain countries which the platform confirmed to us includes the U.K. — among a number of other child-safety focused tweaks. Then in August, Google announced similar changes for accounts on its video sharing platform, YouTube.

A few days later TikTok also said it would add more privacy protections for teens. Though it had also made earlier changes limiting privacy defaults for under-18s.

Apple also recently got itself into hot water with the digital rights community following the announcement of child safety-focused features — including a child sexual abuse material (CSAM) detection tool which scans photo uploads to iCloud; and an opt-in parental safety feature that lets iCloud Family account users turn on alerts related to the viewing of explicit images by minors using its Messages app.

The unifying theme underpinning all these mainstream platform product tweaks is clearly “child protection.”

And while there’s been growing attention in the U.S. to online child safety and the nefarious ways in which some apps exploit kids’ data — as well as a number of open probes in Europe (such as this Commission investigation of TikTok, acting on complaints) — the U.K. may be having an outsized impact here given its concerted push to pioneer age-focused design standards.

The code also combines with incoming U.K. legislation which is set to apply a “duty of care” on platforms to take a broad-brush safety-first stance toward users, also with a big focus on kids (and there it’s also being broadly targeted to cover all children; rather than just applying to kids under 13 as with COPPA in the U.S., for example).

In the blog post ahead of the compliance deadline expiring, the ICO’s Bonner sought to take credit for what he described as “significant changes” made in recent months by platforms like Facebook, Google, Instagram and TikTok, writing: “As the first of its kind, it’s also having an influence globally. Members of the U.S. Senate and Congress have called on major U.S. tech and gaming companies to voluntarily adopt the standards in the ICO’s code for children in America.”

“The Data Protection Commission in Ireland is preparing to introduce the Children’s Fundamentals to protect children online, which links closely to the code and follows similar core principles,” he also noted.

And there are other examples in the EU: France’s data watchdog, the CNIL, looks to have been inspired by the ICO’s approach — issuing its own set of child-protection-focused recommendations this June (which also, for example, encourage app makers to add parental controls with the clear caveat that such tools must “respect the child’s privacy and best interests”).

The U.K.’s focus on online child safety is not just making waves overseas but sparking growth in a domestic compliance services industry.

Last month, for example, the ICO announced the first clutch of GDPR certification scheme criteria — including two schemes which focus on the age-appropriate design code. Expect plenty more.

Bonner’s blog post also notes that the watchdog will formally set out its position on age assurance this autumn — so it will be providing further steerage to organizations which are in scope of the code on how to tackle that tricky piece, although it’s still not clear how hard a requirement the ICO will support, with Bonner suggesting it could be actually “verifying ages or age estimation.” Watch that space. Whatever the recommendations are, age assurance services are set to spring up with compliance-focused sales pitches.

Children’s safety online has been a huge focus for U.K. policymakers in recent years, although the wider (and long in train) Online Safety (neé Harms) Bill remains at the draft law stage.

An earlier attempt by U.K. lawmakers to bring in mandatory age checks to prevent kids from accessing adult content websites — dating back to 2017’s Digital Economy Act — was dropped in 2019 after widespread criticism that it would be both unworkable and a massive privacy risk for adult users of porn.

But the government did not drop its determination to find a way to regulate online services in the name of child safety. And online age verification checks look set to be — if not a blanket, hardened requirement for all digital services — increasingly brought in by the backdoor, through a sort of “recommended feature” creep (as the ORG has warned).

The current recommendation in the age appropriate design code is that app makers “take a risk-based approach to recognising the age of individual users and ensure you effectively apply the standards in this code to child users,” suggesting they: “Either establish age with a level of certainty that is appropriate to the risks to the rights and freedoms of children that arise from your data processing, or apply the standards in this code to all your users instead.”

At the same time, the government’s broader push on online safety risks conflicting with some of the laudable aims of the ICO’s non-legally binding children’s privacy design code.

For instance, while the code includes the (welcome) suggestion that digital services gather as little information about children as possible, in an announcement earlier this summer U.K. lawmakers put out guidance for social media platforms and messaging services — ahead of the planned Online Safety legislation — that recommends they prevent children from being able to use end-to-end encryption.

That’s right; the government’s advice to data-mining platforms — which it suggests will help prepare them for requirements in the incoming legislation — is not to use “gold standard” security and privacy (E2E encryption) for kids.

So the official U.K. government messaging to app makers appears to be that, in short order, the law will require commercial services to access more of kids’ information, not less — in the name of keeping them “safe.” Which is quite a contradiction versus the data minimization push on the design code.

The risk is that a tightening spotlight on kids privacy ends up being fuzzed and complicated by ill-thought-through policies that push platforms to monitor kids to demonstrate “protection” from a smorgasbord of online harms — be it adult content or pro-suicide postings, or cyberbullying and CSAM.

The law looks set to encourage platforms to ‘show their workings’ to prove compliance — which risks resulting in ever-closer tracking of children’s activity, retention of data — and maybe risk profiling and age verification checks (that could even end up being applied to all users; think sledgehammer to crack a nut). In short, a privacy dystopia.

Such mixed messages and disjointed policymaking seem set to pile increasingly confusing — and even conflicting — requirements on digital services operating in the U.K., making tech businesses legally responsible for divining clarity amid the policy mess — with the simultaneous risk of huge fines if they get the balance wrong.

Complying with the ICO’s design standards may therefore actually be the easy bit.",Social Media,TechCrunch,https://techcrunch.com/2021/09/01/uk-now-expects-compliance-with-its-child-privacy-design-code/,"The government's push for online safety risks conflicting with the laudable aims of the ICO's children's privacy design code, as it encourages platforms to monitor kids to demonstrate protection from a variety of harms, resulting in closer tracking of children's activity and potential risk profiling and age verification checks.",Security & Privacy
20,Me Too and the Problem with Viral Outrage,"Three days into the #MeToo meme, my Facebook News Feed is teeming with posts. Female friends have shared heavy anecdotes about inappropriate events. Men have attempted to express solidarity, or concern, or surprise. Celebrities have run with the meme. A backlash has materialized, in which women voice concerns about those who are speaking up.

Jessi Hempel is Backchannel’s editorial director. Sign up to get Backchannel's weekly newsletter, and follow us on Facebook, Twitter, and Instagram.

On its surface, #MeToo has the makings of an earnest and effective social movement. It’s galvanizing women and trans people everywhere to speak out about harassment and abuse. It’s causing everyone to weigh in on systemic sexism in our culture. In truth, however, #MeToo is a too-perfect meme. It harnesses social media’s mechanisms to drive users (that’s you and me) into escalating states of outrage while exhausting us to the point where we cannot meaningfully act. In other words, #MeToo—despite the best intentions of so many participating—is everything that’s wrong with social media.

Outrage is central to the design of most social media platforms—for very good reason. It’s an emotion that inspires sharing, which causes all of us to spend more time engaged with the platform. And that translates directly to revenue for the companies.

But what’s the impact on us? Yale assistant professor Molly Crockett takes this on in new research on moral outrage in the digital age, in which she looks critically at how digital media changes the expression of moral outrage and its social consequences. Crockett is a trained neuroscientist with a PhD in experimental psychology who studies altruism, morality, and values-based decision-making in humans. (She gives a good TED talk on the subject.) She believes new digital technologies may be transforming the way we experience outrage, and limiting how much we can actually change social realities.

It’s useful here to consider the role that violations of moral norms played in our communities before Facebook. The purpose of passing along this information was to help us establish who we could trust and thus better cooperate with one another, notes Crockett. In other words, the only point of speaking out about outrageous acts like harassment and abuse would be to curtail the abuser from harming others.

Online platforms have changed our incentives for sharing. For one, they compete for our attention, so their algorithms are primed to promote the content we are most likely to click—regardless of whether it benefits us as individuals or a community. People are more likely to share things that elicit moral emotions like outrage, writes Crockett.

As a result, our “outrage” bar continues to move firmly up and to the right as our feeds become saturated by egregious stories. We become numb to tragedies because we’re unable to process the emotions they engender at the speed with which they arise. As Crockett writes, “Just as a habitual snacker eats without feeling hungry, a habitual online shamer might express outrage without actually feeling outraged.” We may also discover that, just as venting anger begets anger, expressing outrage leads us to feel the emotion more deeply and consistently. Neither of these changes is good for humans.",Social Media,WIRED,https://www.wired.com/story/the-problem-with-me-too-and-viral-outrage/,"Social media amplifies our outrage and numbs us to tragedies, making it difficult to process emotions and leading to more frequent and intense feelings of outrage.",Social Norms & Relationships
21,Google’s Vint Cerf voices support for common criteria for political ad targeting – TechCrunch,"Google VP Vint Cerf has voiced support for a single set of standards for internet platforms to apply around political advertising.

Speaking to the U.K. parliament’s Democracy and Digital Technologies Committee today, the longtime Googler — who has been chief internet evangelist at the tech giant since 2005 — was asked about the targeting criteria it allows for political ads and whether he thinks there should be a common definition all platforms should apply.

“Your idea that there might be common criteria for political advertising I think has a certain merit to it,” he told the committee. “Because then we would see consistency of treatment — and that’s important because there are so many different platforms available for purposes of — not just advertising but political speech.”

“In the U.S. we’ve already experienced the serious side effects of some of the abuse of these platforms and the ability to target specific audiences for purposes of inciting disagreement,” he added. “We should make it difficult for our platforms to be abused in that way.”

The committee had raised the point that Google and Facebook currently apply different criteria around political ads — also asking whether advertisers could use Google’s tools to target political issue ads at a particular geographical region, such as South Bend in Northern Indiana.

“I don’t think that criterion is allowed in our advertising system,” Cerf responded on that specific example. “I don’t think that we’re that refined, particularly in the political space… We have a small number of criteria that are permitted for targeting political ads.”

Last November Google announced limits on political microtargeting — saying it would limit the ability for advertisers to target political demographics, and also committing itself to take action against “demonstrably false claims.”

The move remains in stark contrast to Facebook, which dug in at the start of this year — refusing to limit targeting criteria for political ads. Instead it trumpeted a few settings tweaks that it claimed would afford users more controls over ads. As we (and many others) warned at the time, such tweaks offer no meaningful way for Facebook users to prevent the company’s pervasive background profiling of their internet activity from being repurposed as an attack surface to erode democracy.

Last year some of Facebook’s own staff also criticized its decision not to restrict politicians from lying in ads and called for it to limit the use of Custom Audiences — arguing microtargeting works against the public scrutiny that Facebook claims keeps politicians honest. However, the company has held the line on refusing to apply limits to political ads — with the occasional exception.

The committee also asked Cerf if he has any concerns about online misinformation and disinformation emerging on platforms related to the novel coronavirus outbreak.

Cerf responded by saying he’s “very concerned about the abuse of the system and looking for ways to counter that.”

“I use our tools every single day. I don’t think I would survive without having the ability to search through the world wide web — get information — get answers. I exercise critical thinking as much as I can about the sources and the content. I am a very optimistic person with regard to the value of what’s been done so far. I am very concerned about the abuse of the system and looking for ways to counter that — and those ways may be mechanical but they also involve the ‘wet ware’ up here,” he said, gesturing at his head.

“So my position is this is all positive stuff, but how do we preserve the value of what we defend against the abuse? … We’re human beings and we should try very hard to make our tools serve us and our society in a positive way.”",Social Media,TechCrunch,https://techcrunch.com/2020/03/09/googles-vint-cerf-voices-support-for-common-criteria-for-political-ad-targeting/,"Google VP Vint Cerf has expressed concern about the potential abuse of social media platforms for political purposes, with the misuse of targeted ads and false information posing a serious threat to democracy. He has called for a single set of standards across platforms to help reduce such risks.",Politics
22,UK Internet attitudes study finds public support for social media regulation – TechCrunch,"UK telecoms regulator Ofcom has published a new joint report and stat-fest on Internet attitudes and usage with the national data protection watchdog, the ICO — a quantitative study to be published annually which they’re calling the Online Nation report.

The new structure hints at the direction of travel for online regulation in the UK, following government plans set out in a recent whitepaper to regulate online harms — which will include creating a new independent regulator to ensure Internet companies meet their responsibilities.

Ministers are still consulting on whether this should be a new or existing body. But both Ofcom and the ICO have relevant interests in being involved — so it’s fitting to see joint working going into this report.

“As most of us spend more time than ever online, we’re increasingly worried about harmful content — and also more likely to come across it,” writes Yih-Choung Teh, group director of strategy and research at Ofcom, in a statement. “ For most people, those risks are still outweighed by the huge benefits of the internet. And while most internet users favour tighter rules in some areas, particularly social media, people also recognise the importance of protecting free speech – which is one of the internet’s great strengths.”

While it’s not yet clear exactly what form the UK’s future Internet regulator will take, the Online Nation report does suggest a flavor of the planned focus.

The report, which is based on responses from 2,057 adult internet users and 1,001 children, flags as a top-line finding that eight in ten adults have concerns about some aspects of Internet use and further suggests the proportion of adults concerned about going online has risen from 59% to 78% since last year (though its small-print notes this result is not directly comparable with last year’s survey so “can only be interpreted as indicative”).

Another stat being highlighted is a finding that 61% of adults have had a potentially harmful online experience in the past year — rising to 79% among children (aged 12-15). (Albeit with the caveat that it’s using a “broad definition”, with experiences ranging from “mildly annoying to seriously harmful”.)

While a full 83% of polled adults are found to have expressed concern about harms to children on the Internet. The UK government, meanwhile, has made child safety a key focus of its push to regulate online content.

At the same time the report found that most adults (59%) agree that the benefits of going online outweigh the risks, and 61% of children think the internet makes their lives better.

While Ofcom’s annual Internet reports of years past often had a fairly dry flavor, tracking usage such as time spent online on different devices and particular services, the new joint study puts more of an emphasis on attitudes to online content and how people understand (or don’t) the commercial workings of the Internet — delving into more nuanced questions, such as by asking web users whether they understand how and why their data is collected, and assessing their understanding of ad-supported business models, as well as registering relative trust in different online services’ use of personal data.

The report also assesses public support for Internet regulation — and on that front it suggests there is increased support for greater online regulation in a range of areas. Specifically it found that most adults favour tighter rules for social media sites (70% in 2019, up from 52% in 2018); video-sharing sites (64% v. 46%); and instant-messaging services (61% v. 40%).

At the same time it says nearly half (47%) of adult internet users expressed recognition that websites and social media platforms play an important role in supporting free speech — “even where some people might find content offensive”. So the subtext there is that future regulation of harmful Internet content needs to strike the right balance.

On managing personal data, the report found most Internet users (74%) say they feel confident to do so. A majority of UK adults are also happy for companies to collect their information under certain conditions — vs over a third (39%) saying they are not happy for companies to collect and use their personal information.

Those conditions look to be key, though — with only small minorities reporting they are happy for their personal data to be used to program content (17% of adult Internet users were okay with this); and to target them with ads (only 18% didn’t mind that, so most do).

Trust in online services to protect user data and/or use it responsibly also varies significantly, per the report findings — with social media definitely in the dog house on that front. “Among ten leading UK sites, trust among users of these services was highest for BBC News (67%) and Amazon (66%) and lowest for Facebook (31%) and YouTube (34%),” the report notes. Despite low privacy trust in tech giants, more than a third (35%) of the total time spent online in the UK is on sites owned by Google or Facebook. “This reflects the primacy of video and social media in people’s online consumption, particularly on smartphones,” it writes. “Around nine in ten internet users visit YouTube every month, spending an average of 27 minutes a day on the site. A similar number visit Facebook, spending an average of 23 minutes a day there.”

And while the report records relatively high awareness that personal data collection is happening online — finding that 71% of adults were aware of cookies being used to collect information through websites they’re browsing (falling to 60% for social media accounts; and 49% for smartphone apps) — most (69%) also reported accepting terms and conditions without reading them.

So, again, mainstream public awareness of how personal data is being used looks questionable.

The report also flags limited understanding of how search engines are funded — despite the bald fact that around half of UK online advertising revenue comes from paid-for search (£6.7BN in 2018). “[T]here is still widespread lack of understanding about how search engines are funded,” it writes. “Fifty-four per cent of adult internet users correctly said they are funded by advertising, with 18% giving an incorrect response and 28% saying they did not know.”",Social Media,TechCrunch,https://techcrunch.com/2019/05/30/uk-internet-attitudes-study-finds-public-support-for-social-media-regulation/,"The joint report from Ofcom and the ICO has found that 8 in 10 adults have concerns about some aspects of Internet use and 61% of adults have had a potentially harmful online experience in the past year, with 79% among children. There is also widespread lack of understanding about how search engines are funded, with only 54% correctly saying they",Security & Privacy
23,Dating apps face questions over age checks after report exposes child abuse – TechCrunch,"The UK government has said it could legislate to require age verification checks on users of dating apps, following an investigation into underage use of dating apps published by the Sunday Times yesterday.

The newspaper found more than 30 cases of child rape have been investigated by police related to use of dating apps including Grindr and Tinder since 2015. It reports that one 13-year-old boy with a profile on the Grindr app was raped or abused by at least 21 men.

The Sunday Times also found 60 further instances of child sex offences related to the use of online dating services — including grooming, kidnapping and violent assault, according to the BBC, which covered the report.

The youngest victim is reported to have been just eight years old. The newspaper obtaining the data via freedom of information requests to UK police forces.

Responding to the Sunday Times’ investigation, a Tinder spokesperson told the BBC it uses automated and manual tools, and spends “millions of dollars annually”, to prevent and remove underage users and other inappropriate behaviour, saying it does not want minors on the platform.

Grindr also reacting to the report, providing the Times with a statement saying: “Any account of sexual abuse or other illegal behaviour is troubling to us as well as a clear violation of our terms of service. Our team is constantly working to improve our digital and human screening tools to prevent and remove improper underage use of our app.”

We’ve also reached out to the companies with additional questions.

The UK’s secretary of state for digital, media, culture and sport (DCMS), Jeremy Wright, dubbed the newspaper’s investigation “truly shocking”, describing it as further evidence that “online tech firms must do more to protect children”.

He also suggested the government could expand forthcoming age verification checks for accessing pornography to include dating apps — saying he would write to the dating app companies to ask “what measures they have in place to keep children safe from harm, including verifying their age”.

“If I’m not satisfied with their response, I reserve the right to take further action,” he added.

Age verification checks for viewing online porn are due to come into force in the UK in April, as part of the Digital Economy Act.

Those age checks, which are clearly not without controversy given the huge privacy considerations of creating a database of adult identities linked to porn viewing habits, have also been driven by concern about children’s exposure to graphic content online.

Last year the UK government committed to legislating on social media safety too, although it has yet to set out the detail of its policy plans. But a white paper is due imminently.

A parliamentary committee which reported last week urged the government to put a legal ‘duty of care’ on platforms to protect minors.

It also called for more robust systems for age verification. So it remains at least a possibility that some types of social media content could be age-gated in the country in future.

Last month the BBC reported on the death of a 14-year-old schoolgirl who killed herself in 2017 after being exposed to self-harm imagery on the platform.

Following the report, Instagram’s boss met with Wright and the UK’s health secretary, Matt Hancock, to discuss concerns about the impact of suicide-related content circulating on the platform.

After the meeting Instagram announced it would ban graphic images of self-harm last week.

Earlier the same week the company responded to the public outcry over the story by saying it would no longer allow suicide related content to be promoted via its recommendation algorithms or surfaced via hashtags.

Also last week, the government’s chief medical advisors called for a code of conduct for social media platforms to protect vulnerable users.

The medical experts also called for greater transparency from platform giants to support public interest-based research into the potential mental health impacts of their platforms.",Social Media,TechCrunch,https://techcrunch.com/2019/02/11/dating-apps-face-questions-over-age-checks-after-report-exposes-child-abuse/,"The UK government is considering introducing age verification checks for dating apps following an investigation which revealed over 30 cases of child rape linked to their use. The government is seeking to protect children from inappropriate content, and is also responding to a public outcry over the death of a 14-year-old schoolgirl who was exposed to self-harm imagery on",Security & Privacy
24,Facebook threatens to block news sharing in Australia as it lobbies against revenue share law – TechCrunch,"Adtech giant and self-styled ‘free speech champion’, Facebook, has threatened to pull the plug on the public sharing of news content on Facebook and Instagram in Australia.

The aggressive threat is Facebook’s attempt to lobby against a government plan that will require it and Google to share revenue with regional news media to recompense publishers for distributing and monetizing professionally produced content on their platforms.

Consultation on a draft of the mandatory code — which Australia’s lawmakers say is intended to address “acute bargaining power imbalances” between local news businesses and the adtech duopoly — closed on August 28, with a final version expected imminently from Australia’s Competition and Consumer Commission (ACCC) and then due to be put before parliament.

Facebook’s threat thus looks timed to turn the heat up on lawmakers as they’re about to debate the details of the code. However dangling the prospect of blocking professionally produced news in an attempt to thwart a law change that’s not in its commercial interests will do nothing to reduce lawmakers’ concerns about the level of market power being wielded by tech giants.

The fact that it is even capable of making such a threat have a meaningful impact shows the lie in their 'aw shucks we have so much competition' line. It's proof positive that their very existence exerts unethical levels of control over the web. — Aram Zucker-Scharff (@Chronotope) September 1, 2020

Last month Google also warned that if Australia goes ahead with the plan then the quality of regional search results and YouTube recommendations will suffer — becoming “less relevant and helpful” if the law goes into effect.

Both platform giants are essentially saying that unless the bulk of professional reportage can be freely distributed on their platforms, leaving them free to monetize it via serving ads and through the acquisition of associated user data, then unverified user generated content will be left to fill the gap.

The clear implication is that lower grade content — and potentially democracy-denting disinformation — will be left to thrive. Or, in plainer language, the threat boils down to: Give us your journalism for free or watch your society pay the price as our platforms plug the information gap with any old clickbait.

“The ACCC presumes that Facebook benefits most in its relationship with publishers, when in fact the reverse is true. News represents a fraction of what people see in their News Feed and is not a significant source of revenue for us. Still, we recognize that news provides a vitally important role in society and democracy, which is why we offer free tools and training to help media companies reach an audience many times larger than they have previously,” writes Facebook in the same blog post where it threatens — as a ‘last choice’ — to pull the plug on content it describes as playing “a vitally important role in society and democracy” because it doesn’t want to have to pay for it.

Facebook’s calculus is clearly elevating its own commercial interests above free speech. And indeed above democracy and society. Yet the tech giant’s go-to defence for not removing all sorts of toxic disinformation and/or hateful, abusive content — or indeed lying political ads — from circulating on its platform is a claim that it’s defending ‘free speech’. So this is a specially rank, two-faced kind of platform hypocrisy on display.

Last year the comic Sacha Baron Cohen slammed Facebook’s modus operandi as “ideological imperialism” — warning then that unaccountable Silicon Valley ‘robber barons’ are “acting like they’re above the reach of law”. Well, Australians are now getting a glimpse of what happens when the mask further slips.

The ACCC has responded to Facebook’s flex with a steely statement of its own, attributed to chair Rod Sims.

“Facebook’s threat today to prevent any sharing of news on its services in Australia is ill-timed and misconceived,” he writes. “The draft media bargaining code aims to ensure Australian news businesses, including independent, community and regional media, can get a seat at the table for fair negotiations with Facebook and Google.”

“Facebook already pays some media for news content. The code simply aims to bring fairness and transparency to Facebook and Google’s relationships with Australian news media businesses,” he adds.

“As the ACCC and the Government work to finalise the draft legislation, we hope all parties will engage in constructive discussions.”

A similar battle is playing out in France over Google News, following a recent pan-EU law change which extended copyright to news snippets. France has been at the forefront of implementing the change in national law — and Google has responded by changing how it displays news media content in Google News in the country, switching to showing headlines and URLs only (so removing snippets).

However earlier this year France’s competition watchdog slapped down the tactic — saying Google’s unilateral withdrawal of snippets to deny payment to publishers is likely to constitute an abuse of a dominant market position, which it asserted “seriously and immediately damaged the press sector.”

Google’s share of the search market in Europe remains massively dominant — with the tech giant taking greater than 90% marketshare. (Something that underpins a number of regional antitrust enforcements against various aspects of its business.)

In Australia, Facebook’s position as a news distributor appears to be less strong, with the ACCC citing the University of Canberra’s 2020 Digital News Report which found that 39% of Australians use Facebook for general news, and 49% use Facebook for news about COVID-19.

However information and disinformation do not distribute equally, with plenty of studies indicating a faster spread for fake news — which suggests Facebook’s platform power to distribute bullshit is far greater than its role in informing societies by spreading bona fide news. That in turn makes its threat to block genuine reportage an antisocial weaponization of its dominance of social media.",Social Media,TechCrunch,https://techcrunch.com/2020/09/01/facebook-threatens-to-block-news-sharing-in-australia-as-it-lobbies-against-revenue-share-law/,"The aggressive threat by Facebook to pull the plug on public sharing of news content in Australia is an example of the unethical power being wielded by tech giants and a weaponization of their dominance of social media. This could lead to a decrease in quality news content and an increase in disinformation, which could have a damaging effect on society and democracy.","Information, Discourse & Governance"
25,"In hearing with Snap, TikTok and YouTube, lawmakers tout new rules to protect kids online – TechCrunch","Fallout from revelations around teen mental health on Instagram continues — and not just for Facebook. On Tuesday, policy reps from YouTube, Snap and TikTok faced Congress to talk about kids and online safety, marking the first time the latter two companies appeared in a major tech hearing.

The hearing, held by the Senate Subcommittee on Consumer Protection, Product Safety, and Data Security, managed to stay on topic about half of the time. The committee’s Republican members were keen to steer their rare time with a TikTok executive toward questions about privacy concerns over the company’s relationship with the Chinese government.

Diversions notwithstanding, a few of the hearing’s more useful moments saw the three policy leads pressed to answer yes/no questions about specific policy proposals crawling through Congress. The hearing featured testimony from Snap VP of Global Public Policy Jennifer Stout, TikTok’s VP and Head of Public Policy Michael Beckerman and Leslie Miller, who leads government affairs and public policy at YouTube.

Both YouTube and TikTok called for the U.S. to create comprehensive laws around online privacy, with Beckerman deeming a legal framework for national privacy laws “overdue.” All three companies agreed that parents should have the ability to erase all online data for their children or teens, with Stout pointing out that Snapchat data disappears by design. Still, Snapchat’s own privacy page mentions that the company retains location data for “different lengths of time based on how precise it is and which services you use.”

Senator Ed Markey (D-MA), himself an unlikely TikTok sensation, pushed for what he calls a kids’ “privacy bill of rights for the 21st century” during the hearing. Markey pointed to his proposed changes to the Children and Teens’ Online Privacy Protection Act (COPPA) that would bolster protections for young social media users. That legislation would ban tech companies from collecting the data of users between 13 and 15-years-old without explicit consent, implement an “eraser button” that would make it easy to delete minors’ personal data and more broadly restrict the kind of information that social media platforms can collect to begin with.

Markey pressed each company rep on if they would support the COPPA changes. Speaking for TikTok, Beckerman said the company does support the proposal but views a standard method for platforms to verify the age of their users as just as essential, if not more so.

This is what drives us crazy. Big Tech always says, “we want to talk, we want to talk, we want to talk.” Meanwhile they continue to profit from preying on kids and teens. This is a crisis. We need strong children’s privacy legislation NOW. pic.twitter.com/PI92ba8kjX — Ed Markey (@SenMarkey) October 26, 2021

Snap wouldn’t commit to the COPPA proposal, and Markey derided Stout for playing “the old game” of tech companies refusing to commit to specifics. YouTube, which was slammed with a historic $170 million FTC fine for COPPA violations in the past, didn’t explicitly commit to anything but pointed to “constructive” talks the company has had with Markey’s staff.

In the hearing, Markey and Blumenthal also highlighted their reintroduction of the KIDS (Kids Internet Design and Safety) Act last month. That bill would protect online users under 16 from engagement-juicing features like autoplay, push alerts and like buttons. It would also ban influencer marketing to kids under 16 and force platforms to create a reporting system for the instances in which they serve harmful content to young users.",Social Media,TechCrunch,https://techcrunch.com/2021/10/26/tiktok-snap-youtube-hearing-congress/,"The Senate Subcommittee on Consumer Protection, Product Safety, and Data Security hearing revealed the lack of protection for kids and teens online, with tech companies refusing to commit to specific policy proposals, such as the proposed changes to the Children and Teens' Online Privacy Protection Act. It also highlighted the dangers of influencer marketing and engagement-juicing features",Security & Privacy
26,Google says China used YouTube to meddle in Hong Kong protests – TechCrunch,"Google has disabled 210 YouTube accounts after it said China used the video platform to sow discord among protesters in Hong Kong.

The search giant, which owns YouTube, followed in the footsteps of Twitter and Facebook, which earlier this week said China had used their social media sites to spread misinformation and discord among the protesters, who have spent weeks taking to the streets to demand China stops interfering with the semi-autonomous region’s affairs.

In a brief blog post, Google’s Shane Huntley said the company took action after it detected activity which “behaved in a coordinated manner while uploading videos related to the ongoing protests in Hong Kong.”

“This discovery was consistent with recent observations and actions related to China announced by Facebook and Twitter,” said Huntley.

Earlier this week Twitter said China was using its service to “sow discord” through fake accounts as part of “a coordinated state-backed operation.”

In line with Twitter and Facebook’s findings, Google said it detected the use of virtual private networks — or VPNs — which can be used to tunnel through China’s censorship system, known as the Great Firewall. Facebook, Twitter and Google are all banned in China. But Google said little more about the accounts, what they shared or whether it would disclose its findings to researchers.

When reached, a Google spokesperson only referred back to the blog post and did not comment further.

More than a million protesters took to the streets this weekend to peacefully demonstrate against the Chinese regime, which took over rule from the United Kingdom in 1997. Protests erupted earlier this year after a bid by Hong Kong leader Carrie Lam to push through a highly controversial bill that would allow criminal suspects to be extradited to mainland China for trial. The bill was suspended, effectively killing it from reaching the law books, but protests have continued, pushing back at claims that China is trying to meddle in Hong Kong’s affairs.",Social Media,TechCrunch,https://techcrunch.com/2019/08/22/google-youtube-china-hong-kong-protests/,"Google, Twitter, and Facebook have all reported finding evidence of China using their platforms to spread misinformation and sow discord amongst Hong Kong protesters, resulting in 210 YouTube accounts being disabled by Google.","Information, Discourse & Governance"
27,"Look to Zuck's F8, Not Trump's 100 Days, to See the Shape of the Future","The Circle, a film adaptation of the best-selling novel by David Eggers about a mega-Silicon Valley company that has sinister plans to control the world, opened recently to tepid reviews and unimpressive box office. That shouldn’t obscure the fact that the issues it attempts to address—and which the novel brilliantly took on—are ones that need to be dealt with, urgently.

To wit, what happens as our lives more and more are lived digitally? What are the implications for rights, freedoms, and privacy when the desiderata of our digital incarnations are channeled through only a handful of massive private companies who want to use our data not just to reduce the frictions of everyday life but to augment their own bottom lines? And what happens when technology moves towards ever-more automated and augmented reality when more of our key interactions take place in a digital realm that exists only on the servers that those companies control?

Instead, our public discussion is dominated by parsing the current presidency’s first 100 days, a marker without meaning, anchored to little more than the ease of digesting the number. The preponderance of attention goes to Washington these days, when what goes on in Washington is only one variable designing the future. While the fate of the Trump administration certainly matters, it may shape the world much less decisively in the long-term than the tectonic changes rapidly altering the digital landscape.

As the media attention has veered from whether Congress and the White House would manage to repeal and replace Obamacare (spoiler: they did not) to grading Trump’s 100 days, three things happened that received considerably less play and will have considerably more impact. At the end of March, both the Senate and the House voted to roll back broadband privacy regulations that had been passed by the Federal Communications Commission in 2016. Those would have required internet service providers to seek customers’ explicit permission before selling or sharing their browsing history.

Then a few weeks later, Marc Zuckerberg took to the stage of a developer’s conference to tout a Facebook vision of 24/7 augmented reality with sensors, camera, and chips embedded in clothing, everyday objects, and eventually the human body-–-with Facebook the central processing station for those terabytes upon terabytes of data and the central transaction platform for our commercial lives lived digitally. And finally, last week, the newly appointed head of the FCC announced his intention to revisit, revise, and eliminate the rules of net neutrality that treat internet service providers as utilities and constrain them from charging different prices for speedy data.

Freedom to Control

These disparate developments share no causal link. But they can and should be correlated to form the bare but troubling outlines of a future more like The Circle than not. The utopian view of the Valley is that vast streams of data harnessed by near-infinite processing power will empower us all to lead fully lived, full individual lives. It is a potent and intoxicating hope, and of course, the short history of digital existence so far would suggest that yes, more people are starting to carve out bespoke professional and personal paths thanks to the grace of the digital and data tools of today. Different modes of work, easier access to social and political communities of interest, less friction obtaining needed goods and services at lower costs, all that is evident.

No one should, however, ignore the potential dark side of these goodies. The libertarian creed of the Valley elite says that no good can come from government attempts to restrict how data flows and how it can be used. Government attempts at regulation have rarely been more than ham-fisted. The fact, however, that many regulations impede rather than facilitate desirably social and economic outcomes does not mean that all regulations do. A world where data and experiences are concentrated in a handful of companies with what will soon be trillion dollar capitalizations risks being one where freedom gives way to control.",Social Media,WIRED,https://www.wired.com/2017/05/look-zucks-f8-not-trumps-100-days-see-shape-future/,A world where data and experiences are concentrated in a handful of companies with trillion dollar capitalizations risks leading to a loss of freedom and a risk of control.,Security & Privacy
28,Facebook again under fire for spreading illegal content – TechCrunch,"An investigation by a British newspaper into child sexual abuse content and terrorist propaganda being shared on Facebook has once again drawn critical attention to how the company handles complaints about offensive and extremist content being shared on its platform.

And, indeed, how Facebook’s algorithmically driven user generated content sharing platform apparently encourages the spread of what can also be illegal material.

In a report published today, The Times newspaper accuses Facebook of publishing child pornography after one of its reporters created a fake profile and was quickly able to find offensive and potentially illegal content on the site — including pedophilic cartoons; a video that apparently shows a child being violently abused; and various types of terrorist propaganda including a beheading video made by an ISIS supporter, and comments celebrating a recent attack against Christians in Egypt.

The Times says it reported the content to Facebook but in most instances was apparently told the imagery and videos did not violate the site’s community standards. (Although, when it subsequently contacted the platform identifying itself as The Times newspaper it says some of pedophilic cartoons that had been kept up by moderators were subsequently removed.)

Facebook says it has since removed all the content reported by the newspaper.

A draft law in Germany is proposing to tackle exactly this issue — using the threat of large fines for social media platforms that fail to quickly take down illegal content after a complaint. Ministers in the German cabinet backed the proposed law earlier this month, which could be adopted in the current legislative period.

And where one European government is heading, others in the region might well be moved to follow. The UK government, for example, has once again been talking tougher on social platforms and terrorism, following a terror attack in London last month — with the Home Secretary putting pressure on companies including Facebook to build tools to automate the flagging up and taking down of terrorist propaganda.

The Times says its reporter created a Facebook profile posing as an IT professional in his thirties and befriending more than 100 supporters of ISIS while also joining groups promoting “lewd or pornographic” images of children. “It did not take long to come across dozens of objectionable images posted by a mix of jihadists and those with a sexual interest in children,” it writes.

The Times showed the material it found to a UK QC, Julian Knowles, who told it that in his view many of the images and videos are likely to be illegal — potentially breaching UK indecency laws, and the Terrorism Act 2006 which outlaws speech and publications that directly or indirectly encourage terrorism.

“If someone reports an illegal image to Facebook and a senior moderator signs off on keeping it up, Facebook is at risk of committing a criminal offense because the company might be regarded as assisting or encouraging its publication and distribution,” Knowles told the newspaper.

Last month Facebook faced similar accusations over its content moderation system, after a BBC investigation looked at how the site responded to reports of child exploitation imagery, and also found the site failed to remove the vast majority of reported imagery. Last year the news organization also found that closed Facebook groups were being used by pedophiles to share images of child exploitation.

Facebook declined to provide a spokesperson to be interviewed about The Times report, but in an emailed statement Justin Osofsky, VP global operations, told us: “We are grateful to The Times for bringing this content to our attention. We have removed all of these images, which violate our policies and have no place on Facebook. We are sorry that this occurred. It is clear that we can do better, and we’ll continue to work hard to live up to the high standards people rightly expect of Facebook.”

Facebook says it employs “thousands” of human moderators, distributed in offices around the world (such as Dublin for European content) to ensure 24/7 availability. However given the platform has close to 2 billion monthly active users (1.86BN MAUs at the end of 2016, to be exact) this is very obviously just the tiniest drop in the ocean of content being uploaded to the site every second of every day.

Human moderation clearly cannot scale to review so much content without there being far more human moderators employed by Facebook — a move it clearly wants to resist, given the costs involved (Facebook’s entire company headcount only totals just over 17,000 staff).

Facebook has implemented Microsoft’s Photo DNA technology, which scans all uploads for known images of child abuse. However tackling all types of potentially problematic content is a very hard problem to try to fix with engineering; one that is not easily automated, given it requires individual judgement calls based on context as well as the specific content, while also potentially factoring in differences in legal regimes in different regions, and differing cultural attitudes.

CEO Mark Zuckerberg recently publicly discussed the issue — writing that “one of our greatest opportunities to keep people safe” is “building artificial intelligence to understand more quickly and accurately what is happening across our community”.

But he also conceded that Facebook needs to “do more”, and cautioned that an AI fix for content moderation is “years” out.

“Right now, we’re starting to explore ways to use AI to tell the difference between news stories about terrorism and actual terrorist propaganda so we can quickly remove anyone trying to use our services to recruit for a terrorist organization. This is technically difficult as it requires building AI that can read and understand news, but we need to work on this to help fight terrorism worldwide,” he wrote in February, before going on to emphasize that “protecting individual security and liberty” is also a core plank of Facebook’s community philosophy — which underscores the tricky ‘free speech vs offensive speech’ balancing act the social media giant continues to try to pull off.

In the end, illegal speech may be the driving force that catalyzes a substantial change to Facebook’s moderating processes — by providing harder red lines where it feels forced to act (even if defining what constitutes illegal speech in a particular region vs what is merely abusive and/or offensive entails another judgement challenge).

One factor is inescapable: Facebook has ultimately agreed that all of the problem content identified via various different high profile media investigations does indeed violate its community standards, and does not belong on its platform. Which rather begs the question why was it not taken down when it was first reported? Either that’s systemic failure of its moderating system — or rank hypocrisy at the corporate level.

The Times says it has reported its findings to the UK’s Metropolitan Police and the National Crime Agency. It’s unclear whether Facebook will face criminal prosecution in the UK for refusing to remove potentially illegal terrorist and child exploitation content.

The newspaper also calls out Facebook for algorithmically promoting some of the offensive material — by suggesting that users join particular groups or befriend profiles that had published it.

On that front features on Facebook such as ‘Pages You Might Known’ automatically suggest additional content a user might be interested on, based on factors such as mutual friends, work and education information, networks you’re part of and contacts that have been imported — but also many other undisclosed factors and signals.

And just as Facebook’s New Feed machine learning algorithms have been accused of favoring and promoting fake news clickbait, the underlying workings of its algorithmic processes for linking people and interests look to be being increasingly pulled into the firing line over how they might be accidentally aiding and abetting criminal acts.",Social Media,TechCrunch,https://techcrunch.com/2017/04/13/facebook-under-fire-for-spreading-illegal-content/,"The investigation by The Times newspaper has highlighted how Facebook's algorithmically driven user-generated content sharing platform encourages the spread of potentially illegal material, such as child exploitation imagery and terrorist propaganda, and has raised questions about the efficacy of the platform's content moderation processes.",Security & Privacy
29,"You Purged Racists From Your Website? Great, Now Get to Work","For those who follow the politics of platforms, Monday’s great expulsion of malicious content creators was better late than never. For far too long, a very small contingent of extremely hateful content creators have used Silicon Valley’s love of the First Amendment to control the narrative on commercial content moderation. By labeling every effort to control their speech as “censorship,” these individuals and groups managed to create cover for their use of death threats, harassment, and other incitements to violence to silence opposition. For a long time, it has worked. Until now. In what looks like a coordinated purge by Twitch, Reddit, and YouTube, the reckoning is here for those who use racism and misogyny to gain attention and make money on social media.

For the past five years, I have been researching white supremacists online and how they capitalize on tech’s willful ignorance of the damage they are causing in the real world. At Harvard Kennedy School’s Shorenstein Center, I lead a team of researchers who look into the fraught politics of online life and how platforms connect the wires to the weeds. It’s too often the case that what happens online no longer stays online. Relying on media manipulation techniques to hide their identities and motives, a mass of racists began to come out in public in the lead up to Trump’s election, including the rise of the so-called alt-right. Due to social media we are all witnesses to white supremacist violence, including the murder of Heather Heyer in Charlottesville and the attack on Muslims in Christchurch. Researchers, journalists, and activists have fought to expose these networks and provide the basic research needed to detect, document, and debunk disinformation campaigns.

WIRED OPINION ABOUT Joan Donovan (@BostonJoan) is the research director at the Harvard Kennedy School’s Shorenstein Center on Media, Politics, and Public Policy.

With Monday's expulsion, it feels like researchers, journalists, and activists are finally being heard.

It’s no coincidence that this newfound boldness on the part of social media companies is coming in the middle of a global pandemic. The past few months of work dealing with medical misinformation surrounding Covid-19 has taught these companies an important lesson: They must—and they can—take decisive action to control who and what is on their sites. It’s about time, and it had better be just the beginning.

What exactly happened? First, Twitch removed streamers who had been accused of sexual abuse and even suspended Trump’s campaign account for violating the policy on hateful conduct. The gaming platform has been struggling to contain a Gamergate-like outbreak of misogyny toward female streamers and trans women on its platform. Earlier this year it appointed an advisory council to create policies for community safety—which in turn sparked a backlash from disgruntled users who saw this as foreshadowing censorship. While niche platforms like Twitch do not get much attention compared to the avalanche of coverage about Facebook and Twitter, it is a central space for young folks who spend countless hours chatting while running around virtual worlds. Like the punk scene in the 1990s, gaming has become a prime territory for recruitment of wayward youth who are still forming their political opinions.

With Monday's expulsion, it feels like researchers, journalists, and activists are finally being heard.

Reddit, despite its dark past, has a recent history of quarantining harmful communities containing hate and misinformation. On Monday, it removed more than 2,000 communities for terms-of-service violations, including The_Donald, an infamous subreddit known for building up Trump’s meme army in 2016. Moderators for The_Donald have long planned for this event and are trying to create their own infrastructure elsewhere. Since 2015 a dedicated group of users has organized on r/againsthatesubreddits, working tirelessly to expose the growth of hate on the platform and to keep the community safe for all users. Even after Reddit's recent actions, this group continues its work to ensure that even the smallest communities of racists do not survive the expulsion.

YouTube also took down accounts for several well-known racists, misogynists, and far right influencers who had been using the platform to discuss their racist political positions and solicit donations. Many wondered why it took so long to remove these accounts, especially since YouTube changed its hate speech policy last year in response to white supremacist content.",Social Media,WIRED,https://www.wired.com/story/you-purged-racists-from-your-website-great-now-get-to-work/,"Social media has enabled a rise in white supremacists, misogyny and incitements to violence, which have caused real-world harm, such as the murder of Heather Heyer in Charlottesville and the attack on Muslims in Christchurch. Monday's coordinated purge of malicious content creators was a long-overdue step in the right direction.",Equality & Justice
30,Science Says: The Baby Madness on Your Facebook Feed Is an Illusion,"Ben Wiseman

Everyone knows what a hassle new parents are on Facebook, right? They overshare. They post endless pictures of their new bundle of joy, flooding your feed with drooling infants. Last summer saw the creation of a browser extension—UnBaby.Me, since renamed Rather—that fought back against the tide, auto-detecting baby images and replacing them with less-annoying material, like cats or bacon. ""A brilliant and sanity-preserving idea,"" a Forbes writer gushed. Except for one thing: The entire premise is wrong.

Recently, Meredith Ringel Morris—a computer scientist at Microsoft Research—gathered data on what new moms actually do online. She persuaded more than 200 of them to let her scrape their Facebook accounts and found the precise opposite of the UnBaby.Me libel. After a child is born, Morris discovered, new mothers post less than half as often. When they do post, fewer than 30 percent of the updates mention the baby by name early on, plummeting to not quite 10 percent by the end of the first year. Photos grow as a chunk of all postings, sure—but since new moms are so much less active on Facebook, it hardly matters. New moms aren't oversharers. Indeed, they're probably undersharers. ""The total quantity of Facebook posting is lower,"" Morris says.

And therein lies an interesting lesson about our supposed age of oversharing. If new moms don't actually deluge the Internet with baby talk, why does it seem to so many of us that they do? Morris thinks algorithms explain some of it. Her research also found that viewers disproportionately ""like"" postings that mention new babies. This, she says, could result in Facebook ranking those postings more prominently in the News Feed, making mothers look more baby-obsessed.

I have another theory: It's a perceptual quirk called a frequency illusion. Once we notice something that annoys or surprises or pleases us—or something that's just novel—we tend to suddenly notice it more. We overweight its frequency in everyday life. For instance, if you've decided that fedoras are a ridiculous hipster fashion choice, even if they're comparatively rare in everyday life, you're more likely to notice them. And pretty soon you're wondering, why is everyone wearing fedoras now? Curse you, hipsters!

Frequency illusions are self-perpetuating cycles enhanced by lazy journalism and punditry. One reason people think new mothers post a lot of baby pictures is that trend pieces and op-eds claim they do. (Indeed, trend journalism is essentially a form of intellectual trolling designed to create frequency illusions. ""Why is everyone suddenly listening to Wilco again?"") Yes, some moms post about their kids every 10 minutes. You may have one in your feed right now. But the behavior is not widespread or incessant.

The way we observe the world is deeply unstatistical, which is why Morris' work is so useful. It reminds us of the value of observing the world around us like a scientist—to see what's actually going on instead of what just happens to gall (or please) us. I'd hazard that perceptual illusions lead us to overamplify the incidence of all sorts of ostensibly annoying behavior: selfies on Instagram, people ignoring one another in favor of their phones, Google Glass. We don't have a plague of oversharing. We have a plague of over-noticing. It's time to reboot our eyes.",Social Media,WIRED,https://www.wired.com/2014/04/babies-on-facebook/,"Perceptual illusions lead to a tendency to overamplify the incidence of ostensibly annoying behavior, such as oversharing and ignoring people in favor of phones, on social media.",Social Norms & Relationships
31,The Cleaners is a riveting documentary about how social media might be ruining the world,"Welcome to Cheat Sheet, our brief breakdown-style reviews of festival films, VR previews, and other special event releases. This review comes from the 2018 Sundance Film Festival.

We’re in a cultural moment where the impact of social media isn’t something that we just notice when we catch ourselves heads-down in our phones or computers. It’s something that’s underscored with almost daily news stories, with each new revelation seemingly more sinister than the last. It’s so ever-present that it can be easy to tune out, which makes the Sundance documentary The Cleaners pack such a devastating wallop.

The film, from first-time documentary directors Moritz Riesewieck and Hans Block, starts with one unseen corner of the internet: the content moderators that work for major platforms like Facebook, Twitter, and Google, spending their time looking at every single picture and video that has been flagged for potentially being objectionable. But while revealing the story of those individuals is an interesting place to start, the filmmakers have much bigger ideas they want to tackle. The movie broadens its scope to cover what the directors clearly see as a real-time global catastrophe — a situation where tech companies are so eager to grow, expand, and monetize that they fail to recognize the ways their platforms are fomenting hate, discord, and violence, with devastating results.

What’s the genre?

Good old-fashioned talking heads documentary, spiced up here and there with some fancy computer graphics.

What’s it about?

Ostensibly The Cleaners is about the outsourced workers that these companies use to determine whether photos and videos that have been shared online should be allowed to stay there. The film tracks a handful of people based in Manila that spend their days looking at terrorist videos, political propaganda, self-harm videos, and child pornography, breaking them into binary categories: “ignore,” where they let the post stand, and “delete,” where the imagery is removed for violating community standards.

An ah-ha moment for anyone who’s wondered why benign things are censored

Riesewieck and Block initially seem interested in pointing out how poorly conceived the outsourcing of moderation is as a business practice. One of the “cleaners” that specializes in pornography, for example, admits she didn’t know anything about porn (and maybe not much about sex) before she started her job, making her an odd choice for a subject matter expert. Another employee, who has been told by his supervisor to be extremely vigilant about anything related to ISIS, explains to camera that an infamous photo of an American soldier terrorizing an Abu Ghraib prisoner with a barking dog is actually an image of an ISIS soldier terrifying a captive — and therefore needs to be pulled from the service.

It’s an ah-ha moment for anyone who’s ever wondered why seemingly innocuous material sometimes disappears off social networks, but the story of the cleaners — and the tragic, long-term impact of watching such horrific material day in and day out — is only the vehicle for the film’s larger ambitions.

What’s it really about?

Simply put, it’s how social networking platforms have created a feedback loop that is irrevocably harming our real-world social fabric. Riesewieck and Block pull in journalists, ethicists, and former employees of various tech companies to offer a macro perspective on how the platforms have been designed, and what impact they are actually having upon our global discourse. The filmmakers also thread in footage from the Senate hearings held last year with the top legal executives from Facebook, Google, and Twitter. The latter ends up playing like a lot of obfuscation and hand-waving on the part of the tech companies — but as the film explains, there’s a reason for that.

It’s because the platforms in question have been expressly designed to generate interest and engagement above all else, and it’s in a platform’s best interests to never show a user news or information that would truly challenge their world view or turn them off, leading to insular bubbles where people are only fed the information they already want to see. Compounding that is the fact that outrage is awfully good at generating engagement, so we’re faced with a situation where these platforms have become algorithmically tuned to inspire and provoke as much extreme behavior as possible. On top of that is the idea that the cleaners themselves are tasked with turning what should be complex, nuanced questions — Is artist Illma Gore’s painting of a nude Donald Trump protected political speech, or an act of bullying, as one cleaner claims? — into the simplistic buckets of “ignore” and “delete”. The result is a system that renders the broader population angry, incited, and utterly ill-informed.

Is it good?

It is! At times The Cleaners does feel rambly, as if the filmmakers started out telling this small story about the Manila employees, and ended up being distracted by the much broader social implications. The personal stories of the employees never end up serving as the throughline the film wants them to be, and it’s easy to imagine a version of the documentary that tackled the larger problem, with the cleaners being just one chapter of the story along the way rather than a framing device.

But Riesewieck and Block nevertheless do a tremendous job of taking subject matter that could be too heady, or that people could simply be too tired of hearing about, and making it seem vibrant and incredibly vital. None of the ideas expressed by the interview subjects in the film are necessarily new, but they’re laid out in a way that are easily accessible no matter what level of technical expertise that audience has. There are a few stylistic crutches that the movie leans on too heavily — the filmmakers are particularly fond of cutting to a computer-generated visualization of social network activity, that starts to feel rather dated — but that’s all balanced out by the real-world stakes that the documentary is constantly underscoring.

It paints such a bleak picture that you’ll want to delete your Facebook account in protest

Silicon Valley companies that espouse the power of global communities and open sharing also voluntarily censor their own services at the behest of governments like Turkey, essentially undermining their own stated ethics in the name of gaining a foothold in a new market. Facebook serves as a primary news source for citizens in Myanmar, but the adoption of the platform by hate groups has led to increased violence against refugees there — something that the company apparently doesn’t feel compelled to address. And then there’s the polarization that’s simply become part of the political landscape here in the United States.

With all of its disparate threads, the movie argues that no matter what rhetoric they spout, these tech giants are capitalistic endeavors first and foremost, and as a result are simply refusing to acknowledge the negative impact they’re having upon our larger culture because doing so would be bad for business. The film paints such a bleak picture that it’s hard to not walk away with the feeling that we should all immediately delete our Twitter and Facebook accounts — not out of protest, but out of sheer self-preservation.

What should it be rated?

There is a lot of uncomfortable imagery shown in this film, and the cleaners describe the horrifying things they’ve seen online in graphic detail. This is an “R”.

How can I actually watch it?

There is no distribution deal in place yet, but given Netflix’s general fervor with documentaries I’d be shocked if the company didn’t snap this film up sooner rather than later. The Cleaners is a movie of the moment, and whomever ends up distributing it is going find themselves attracting a lot of attention.",Social Media,Verge,https://www.theverge.com/2018/1/21/16916380/sundance-2018-the-cleaners-movie-review-facebook-google-twitter,"The Cleaners examines how social media platforms, designed to maximize engagement, are creating a feedback loop that is fostering hate, discord, and violence, with devastating real-world effects.",Discourse & Governance
32,"Facebook expands fact-checking program, adopts new technology for fighting fake news – TechCrunch","Facebook this morning announced an expansion of its fact-checking program and other actions it’s taking to combat the scourge of fake news on its social network. The company, which was found to be compromised by Russian trolls whose disinformation campaigns around the November 2016 presidential election reached 150 million Americans, has been increasing its efforts at fact-checking news through a combination of technology and human review in the months since.

The company began fact-checking news on its site last spring, with help from independent third-party fact-checkers certified through the non-partisan International Fact-Checking Network.

These fact checkers rate the accuracy of the story, allowing Facebook to take action on those rated false by lowering them in the News Feed, and reduced the distribution of those Pages that are repeat offenders.

Today, Facebook says it has expanded this program to 14 countries around the world, and plans to roll it out to more countries by year-end. It also claims the impact of fact-checking reduced the distribution of fake news by an average of 80 percent.

The company additionally announced the expansion of its program for fact-checking photos and video to four countries.

First unveiled this spring, Facebook has been working to fact-check things like manipulated videos or misused photos where images are taken out of context in order to push a political agenda. This is a huge issue, because memes have become a popular way of rallying people around a cause on the internet, but they often do so by completely misrepresenting the facts by using images from different events, places, and times.

One current example of this is the photo used by Drudge Report showing young boys holding guns in a story about the U.S.-Mexico border battle. The photo was actually taken nowhere near the border, but rather was snapped in Syria in 2012 and was captioned by the photographer: “Four young Syrian boys with toy guns are posing in front of my camera during my visit to Azaz, Syria. Most people I met were giving the peace sign. This little city was taken by the Free Syrian Army in the summer of 2012 during the Battle of Azaz.”

Using fake or misleading images to stoke fear, disgust, or hatred of another group of people is a common way photos and videos are misused online, and they deserve fact-checking as well.

Facebook also says it’s now taking advantage of new machine learning technology to help it find duplicates of already debunked stories. And it will work with fact-checking partners to use Schema.org‘s Claim Review, an open-source framework that will allow fact-checkers to share ratings with Facebook so the company can act more quickly, especially in times of crisis.

The company is also expanding its efforts in downranking fake news by using machine learning to demote foreign Pages that are spreading financially-motivated hoaxes to people in other countries.

In the weeks ahead, an elections research commission working in partnership with Facebook to measure the volume and effect of misinformation on the social network will launch its website and its first request for proposals.

The company had already announced its plans to further investigate the role social media plans in elections and in democracy. This commission will receive access to privacy-protected data sets with a sample of links that people engaged with on Facebook, which will allow it to understand what sort of content is being shared, says Facebook. The company claim the research will “help keep us accountable and track our progress.”

We’ll see!",Social Media,TechCrunch,https://techcrunch.com/2018/06/21/facebook-expands-fact-checking-program-adopts-new-technology-for-fighting-fake-news/,"Facebook is taking various steps to combat the spread of fake news on its platform, including expanding its fact-checking program, using machine learning to demote foreign Pages that are spreading financially-motivated hoaxes to people in other countries, and creating a research commission to measure the volume and effect of misinformation.","Information, Discourse & Governance"
33,"My Phone Keeps Me Sane During This Crisis … and Insane, Too","In the morning I wake up and turn on my phone. Thirty-eight text messages, mostly jokes and memes about how to keep sane while staying inside, and links to clothes and necklaces my out-of-work friends will buy once we’re back from quarantine and re-employed. But the rest are utter doomsday. They’re chainmail-like copy-and-pasted messages from a friend’s brother’s girlfriend’s cousin who is a nurse, warning me against taking Advil or other NSAIDs. They’re texts that say a friend of a friend of a friend who is “high up in government” knows that New York City will shut down all bridges and tunnels by tomorrow. They’re links to articles from dubious websites on the various Covid-19 related calamities that can’t be stopped, and which are sending us toward apocalypse. Great way to start the morning.

In the afternoon, while my toddler sleeps, I open Instagram; its messaging function has been a nice way to keep up with friends I don’t constantly text with. That inbox is always waiting with a funny video from a friend, or a tender response to one I’d posted the day before. It can also be a comfort to scroll through people’s stories, looking in, briefly, on the silly little things one does to cope with an unbearable situation. But then, again, there are the inevitable anxiety-inducing posts about how we’re at the brink of total disaster or already mired in it. My heart begins to pound, hard. Must close Instagram.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

At night I seek the refuge of friends’ faces. I open a new app, House Party, and start a chat. We laugh, morosely, about springtime inside, our bodies breaking and decrepit, sun-lorn. But looming behind the app are notifications from the New York Times about the rising death toll and economic collapse, and I can’t look away.

The teen-heartbreak adage goes, “The only person who can make you stop crying is exactly the person who is making you cry.” And that’s how it is for many of us, except swap “person” with “phone.” I’m not ignoring the news, far from it. But when I want to turn off for a moment, to look away from the fact that I’ve had symptoms of Covid-19 for a few days, or that my elderly parents and even more elderly grandmother are living a few miles north of me, also in the epicenter of this disaster—I can’t.

My friend Nona feels similarly. “I think we are really seeing the consequences of phones becoming our appendages,” she wrote to me in a message (of course). “Our entire world is crammed into one tiny device, so for me it’s been hard to compartmentalize, which is a very useful skill during this time.” For Nona, the opting-out of phone time is an ethical issue. “Nowadays it feels almost immoral to turn off the news and turn off your texts. Your loved ones need you. Information will save us. But also it doesn’t always feel great!”

Read all of our coronavirus coverage here.

Is there even a way to fully “log off”? My primary-care doctor service, One Medical, also can’t seem to parse the conundrum of how to look away. In a listicle sent out to patients about prioritizing mental health during quarantine, the number 2 suggestion was to go on a digital detox: “While it’s important to stay up to date on the latest public health announcements, too much news consumption can increase feelings of stress and anxiety,” it reads. “If endless scrolling leaves you feeling overwhelmed, try setting aside regular time in the morning or afternoon to check your newsfeed and give yourself a time limit.” But then number 4 advised me to communicate with others as much as possible. “While physical contact may be limited right now, there are several ways to stay in touch with friends and family. Try to still connect with your friends and family through video chat or phone calls. Host a virtual ‘happy hour’ or ‘coffee break’ with one of your co-workers.” It’s good advice, but not really practical for those of us who can’t seem to ignore the news. If One Medical has any tips for how to open up your phone for video chat without sneaking a glance at end-times updates, it’d be much appreciated.

However difficult it may be, stepping back, even for a brief moment, is necessary.

How Long Does the Coronavirus Live on Surfaces? Plus: What it means to “flatten the curve,” and everything else you need to know about the coronavirus.

I was inside of my New York City Quaker high school not far from the World Trade Center when the buildings were struck by planes. I remember that evening, when I stayed at a friend’s house close to school (I lived too far to walk home that day), taking breaks between watching CNN’s constant coverage and turning off the television to do something normal: eat pasta, talk about a crush. I remember feeling guilty, that I should try to think only of the dead, of the firefighters risking their lives, of the dozens of my friends who wouldn’t be returning home for months because of how close they lived to Ground Zero. In the wake of the tragedy, a teacher told us during Quaker meeting to think of the moment we were in like sunset—to look at it, but turn away every so often so as not to burn our eyes.

That was comforting to hear. It needs reminding now, for those of us who are able to follow the advice.

There was plenty of time, back then, to periodically look away from the sunset, from the wreckage; there was time for pondering the epic pain and loss on our own schedule. In the last two decades, though, technology has made this feel impossible. News comes at us differently: not from a television that can be turned off while we talk by phone, but through a single device—an input-output box for both anxiety and solace, terror and connection. The phone is now a sunset in itself. It can be hard to close your eyes even as they burn, but we must try.

More From WIRED on Covid-19",Social Media,WIRED,https://www.wired.com/story/my-phone-keeps-me-sane-during-this-crisis-and-insane-too/,"The use of smartphones, and their ability to provide us with constant news updates, has made it difficult to take time away from worrying about the current pandemic and its consequences. This can be a psychological burden, making it hard to take a break from the news and to focus on other activities.",User Experience & Entertainment
34,"U.K. Kids Increasingly Credulous Online, Finds Ofcom – TechCrunch","While there has been high profile U.K. government backing for furnishing the nation’s youth with digital skills in recent years, including a requirement in the English national curriculum to start teaching coding to primary age children, new research from comms industry regulator Ofcom suggests a parallel push to teach kids to be much better critical thinkers in our information-overloaded era is also desperately needed.

As it stands, a growing segment of the nation’s youth is far too credulous when it comes to the media they are consuming online, with Ofcom finding that British kids sometimes lack the understanding to decide whether the digital content they are viewing is true or impartial.

Its latest media use and attitudes report, which surveyed the views of U.K. children and parents in 2015, charts a rise in trust in online information among kids, with nearly one in ten of eight- to 15-year-olds apparently believing the information they encounter on social media websites and apps is “all true” — a doubling of the rate from last year’s report.

In one sense this is hardly surprising, in an era of increasingly blurred lines dividing editorial content from marketing and advertising. Not to mention the systematic appropriation of user generated content by the likes of giant ad-powered social media platform businesses, such as Facebook, to use as a trusted backdrop in order to dripfeed advertising to users.

But all the more reason, then, to teach kids to be more critical about the digital information they are being fed. Not that this is something the government appears to be prioritizing. Rather it continues to focus attention on the needs of digital businesses, such as funding free online courses teaching the kind of skills tech employers seek — such as social media marketing training. Media literacy? Not so much.

Incredibly Ofcom’s research found that one in five online teens (twelve- to fifteen-year-olds) believe information returned by a search engine such as Google or Bing must be true. Yet only a third are able to identify paid-for adverts within these search results.

Ofcom also found that U.K. kids are increasingly turning to YouTube — another platform owned by ad giant Google — to seek out “true and accurate” information about the goings on in the world. Nearly one in ten (eight per cent) of online kids identified YouTube as their preferred choice for this kind of info, up from just three per cent in 2014.

Yet only half of the 12- to 15-year-olds who watch YouTube are aware that advertising is the main source of funding on the site, and just under half are aware that video bloggers can be paid to endorse products or services.

Truly we are living in a golden age for marketing and misinformation.

The research also suggests the nation’s youth is generally becoming more comfortable with the notion of sharing personal information online — again, hardly a surprise given how they are being groomed to do so by the social media platforms that rely on user generated content to power their businesses.

Ofcom found that fewer teens than last year’s study would not want anyone to see their content details; location; info about what they are doing; and photos and videos from being our with their friends.

That said, the research also found teens are increasingly stating they want this type of personal information sharing to be with their friends only — suggesting growth of a more nuanced position on privacy, such as perhaps kids preferring the more bounded info sharing enabled by messaging apps, for instance, vs the broader social sharing platforms where their parents may also be present.

The full Ofcom media use and attitudes report can be found here.",Social Media,TechCrunch,https://techcrunch.com/2015/11/20/u-k-kids-increasingly-credulous-online-finds-ofcom/,"The research suggests that UK youth are becoming too trusting of the online content they are consuming, with many believing information returned by search engines such as Google or Bing to be true, and increasing numbers turning to YouTube for accurate information. As a result, there is a danger of them being exposed to misinformation and marketing without the understanding to differentiate between the",Security & Privacy
35,Facebook for 6-Year-Olds? Welcome to Messenger Kids,"Facebook says it built Messenger Kids, a new version of its popular communications app with parental controls, to help safeguard pre-teens who may be using unauthorized and unsupervised social-media accounts. Critics think Facebook is targeting children as young as 6 to hook them on its services.

Facebook’s goal is to “push down the age” of when it’s acceptable for kids to be on social media, says Josh Golin, executive director of Campaign for a Commercial Free Childhood. Golin says 11-to-12-year-olds who already have a Facebook account, probably because they lied about their age, might find the animated emojis and GIFs of Messenger Kids “too babyish,” and are unlikely to convert to the new app.

Facebook launched Messenger Kids for 6-to-12-year olds in the US Monday, saying it took extraordinary care and precautions. The company said its 100-person team building apps for teens and kids consulted with parent groups, advocates, and childhood-development experts during the 18-month development process and the app reflects their concerns. Parents download Messenger Kids on their child’s account, after verifying their identity by logging into Facebook. Since kids cannot be found in search, parents must initiate and respond to friend requests.

Facebook says Messenger Kids will not display ads, nor collect data on kids for advertising purposes. Kids’ accounts will not automatically be rolled into Facebook accounts once they turn 13.

DISCUSSION Join Parenting In a WIRED World , a new Facebook Group for parents to discuss kids' health and their relationship to tech.

Nonetheless, advocates focused on marketing to children expressed concerns. The company will collect the content of children’s messages, photos they send, what features they use on the app, and information about the device they use. Facebook says it will use this information to improve the app and will share the information “within the family of companies that are part of Facebook,” and outside companies that provide customer support, analysis, and technical infrastructure.

“It’s all that squishy language that we normally see in privacy policies,” says Golin. “It seems to give Facebook a lot of wiggle room to share this information.” He says Facebook should be clearer about the outsiders with which it may share data.

In response to questions from WIRED, a spokesperson for Facebook said: “It’s important to remember that Messenger Kids does not have ads and we don’t use the data for advertising. This provision about sharing information with vendors from the privacy policy is for things like providing infrastructure to deliver messages.”

Kristen Strader, campaign coordinator for the nonprofit group Public Citizen, says Facebook has proven it cannot be trusted with youth data in the past, pointing to a leaked Facebook report from May that promised advertisers the ability to track teen emotions, such as insecurity, in real-time. ""Their response was just that they will not do similar experiments in the future,"" says Strader. At the time, advocacy groups asked for a copy of the report, but Facebook declined.

On Thursday, Sen. Richard Blumenthal and Sen. Ed Markey sent a long list of questions about the app's privacy controls to Mark Zuckerberg. ""We remain concerned about where sensitive information collected through this app could end up and for what purpose it could be used,"" they wrote in a letter to the Facebook CEO.

Tech companies have made a much more aggressive push into targeting younger users, a strategy that began in earnest in 2015 when Google launched YouTube Kids, which includes advertising. Parents create an account for their child through Google’s Family Link, a product to help parents monitor screentime. FamilyLink is also used for parents who want to start an account for their kid on Google Home, which gets matched to their child’s voice.

“There is no way a company can really close its doors to kids anymore,” says Jeffrey Chester, executive director for the Center of Digital Democracy. “By openly commercializing young children’s digital media use, Google has lowered the bar,” he says, pointing to what toy company Mattel described as “an eight-figure deal” that it signed with YouTube in August.",Social Media,WIRED,https://www.wired.com/story/facebook-for-6-year-olds-welcome-to-messenger-kids/,"Critics of social media's targeting of younger users point to the potential for the misuse of sensitive data collected from children, and the potential for companies to exploit children for commercial gain.",Security & Privacy
36,Europe keeps up the pressure on social media over illegal content takedowns – TechCrunch,"The European Union’s executive body is continuing to pressure social media firms to get better at removing illegal content from their platforms before it has a chance to spread further online.

Currently there is a voluntary Code of Conduct on countering illegal online hate speech across the European Union. But the Commission has previously indicated it could seek to legislate if it feels companies aren’t doing enough.

After attending a meeting on the topic today, Andrus Ansip, the European Commissioner for Digital Single Market, tweeted to say the main areas tech firms need to be addressing are that “takedown should be fast, reliable, effective; pro-activity to detect, remove and disable content using automatic detection and filtering; adequate safeguards and counter notice”.

Main areas to address on #illegalcontent: takedown should be fast, reliable, effective; pro-activity to detect, remove and disable content using automatic detection and filtering; adequate safeguards and counter notice pic.twitter.com/5ErFUBcAsZ — Andrus Ansip (@Ansip_EU) January 9, 2018

Launched 1.5 year ago, the #CodeOfConduct on countering illegal #OnlineHateSpeech has shown good improvement and a successful collaboration of the companies, the regulators, NGOs and users. I want to widen the exercise and invite further platforms to join. pic.twitter.com/icnG1T7hFo — Věra Jourová (@VeraJourova) January 9, 2018

While the notion of tech giants effectively removing illegal content might be hard to object to in principle, such a laundry list of requirements underlines the complexities involved in pushing commercial businesses to execute context-based speech policing decisions in a hurry.

For example, a new social media hate speech law in Germany, which as of this month is being actively enforced, has already draw criticism and calls for its abolition after Twitter blocked a satirical magazine that had parodied anti-Muslim comments made by the far-right Alternative for Germany political party.

Another problematic aspect to the Commission’s push is it appears keen to bundle up a very wide spectrum of ‘illegal content’ into the same response category — apparently aiming to conflate issues as diverse as hate speech, terrorism, child exploitation and copyrighted content.

In September the EC put out a set of “guidelines and principles” which it said were aimed at pushing tech firms to be more pro-active about takedowns of illegal content, and specifically urging them to build tools to automate flagging and re-uploading of such content. But the measures were quickly criticized for being overly vague and posing a risk to freedom of expression online.

It’s not clear what kind of “adequate safeguards” Ansip is implying could be baked into the auto-detection and filtering systems the EC wants (we’ve asked and will update this story with any response). But there’s a clear risk that an over-emphasis on pushing tech giants to automate takedowns could result in censorship of controversial content on mainstream platforms.

There’s no public sign the Commission has picked up on these specific criticisms, with its latest missive flagging up both “violent and extremist content” but also “breaches of intellectual property rights” as targets.

Last fall the Commission said it would monitor tech giants’ progress vis-a-vis content takedowns over the next six months to decide whether to take additional measures — such a drafting legislation. Though it has also previously lauded progress being made.

In a statement yesterday, ahead of today’s meeting, the EC kept up the pressure on tech firms — calling for “more efforts and progress”:

The Commission is counting on online platforms to step up and speed up their efforts to tackle these threats quickly and comprehensively, including closer cooperation with national and enforcement authorities, increased sharing of know-how between online players and further action against the reappearance of illegal content. We will continue to promote cooperation with social media companies to detect and remove terrorist and other illegal content online, and if necessary, propose legislation to complement the existing regulatory framework.

In the face of rising political pressure and a series of content-related scandals, both Google and Facebook last year announced they would be beefing up their content moderation teams by thousands of extra staff apiece.",Social Media,TechCrunch,https://techcrunch.com/2018/01/09/europe-keeps-up-the-pressure-on-social-media-over-illegal-content-takedowns/,"The European Commission is pushing social media companies to take down illegal content quickly, but this could potentially lead to censorship of controversial content on mainstream platforms.",Security & Privacy
37,"‘The problem is Facebook,’ lawmakers from nine countries tell Zuckerberg’s accountability stand-in – TechCrunch","A grand committee of international parliamentarians empty-chaired Mark Zuckerberg at a hearing earlier today, after the Facebook founder snubbed repeat invitations to face questions about malicious, abusive and improper uses of his social media platform — including the democracy-denting impacts of so-called “fake news.”

The U.K.’s DCMS committee has been leading the charge to hold Facebook to account for data misuse scandals and election interference — now joined in the effort by international lawmakers from around the world. But still not by Zuckerberg himself.

In all, parliamentarians from nine countries were in the room to put awkward questions to Zuckerberg’s stand-in, policy VP Richard Allan — including asking what Facebook is doing to stop WhatsApp being used as a vector to spread political disinformation in South America; why Facebook refused to remove a piece of highly inflammatory anti-Muslim hate speech in Sri Lanka (until the country blocked access to its platform); how Facebook continues to track non-users in Belgium and how it justifies doing so under Europe’s tough new GDPR framework; and, more generally, why anyone should have any trust in anything the company says at this point — with the company neck-deep in privacy and trust scandals.

The elected representatives were collectively speaking up for close to 450 million people across the U.K., Argentina, Belgium, Brazil, Canada, France, Ireland, Latvia and Singapore. The most oft-repeated question on their lips was why wasn’t Zuckerberg there?

9 countries.

24 official representatives.

447 million people represented. One question: where is Mark Zuckerberg? pic.twitter.com/BK3KrKvQf3 — Digital, Culture, Media and Sport Committee (@CommonsCMS) November 27, 2018

Allan looked uncomfortable on his absentee boss’ behalf and spent the best part of three hours running the gamut of placative hand gestures as he talked about wanting to work with regulators to find “the right regulation” to rein in social media’s antisocial, anti-democratic impacts.

Canadian MP Bob Zimmer spoke for the room, cutting into another bit of Allan’s defensive pabulum, with: “Here we are again hearing another apology from Facebook — ‘look trust us, y’all regulate us etc but we really don’t have that much influence in the global scheme of things.’

“In this room we regulate over 400 million people and to not have your CEO sit in that chair there is an offence to all of us in this room and really our citizens as well.”

“[BlackBerry co-founder] Jim Balsille said, when I asked him on our committee, is our democracy at risk if we don’t change the laws in Canada to deal with surveillance capitalism?” Zimmer continued. “He said: ‘Without a doubt’. What do you think?” — which Allan took as a cue to ummm his way into another series of “we need tos” and chatter about “a number of problematic vectors” he said Facebook is trying to address with a number of “tools.”

The session was largely filled up with such frustratingly reframed waffle, as Allan sought to deflect, defang and defuse the committee’s questions — leading it to accuse him more than once of repeating the “delay, deny, deflect” tactics recently reported on by The New York Times.

Allan claimed not — claiming to be there “acknowledging” problems. But that empty chair beside him sure looked awkward.

At the close, Canada’s Charlie Angus sought to sweep Facebook’s hot air away by accusing Allan of distracting the debate with discussion of symptoms — to draw the regulatory eye away from the root cause of the problem which he sharply defined as Facebook itself.

“The problem we have with Facebook is there’s never accountability. So I would put it to you when we talk about regulation that perhaps the best regulation would be antitrust,” said Angus. “Because people who don’t like Facebook — oh they could go to WhatsApp. But oh we have some problems in South America, we have problems in Africa, we have to go back to Mr Zuckerberg who’s not here.

“My daughters could get off Facebook. But they’d go to Instagram. But that’s now controlled by Facebook. Perhaps the simplest form of regulation would be to break Facebook up — or treat it as a utility so that we could all then feel that when we talk about regulation we’re talking about allowing competition, counting metrics that are actually honest and true, and that Facebook has broken so much trust to allow you to simply gobble up every form of competition is probably not in the public interest.

“So when we’re talking about regulation would you be interested in asking your friend Mr Zuckerberg if we could have a discussion about antitrust?”

Allan reached for an “it depends upon the problem we’re trying to solve” reply, to try reframing the question again.

“The problem is Facebook,” retorted Angus. “We’re talking about symptoms but the problem is the unprecedented economic control of every form of social discourse and communication. That it’s Facebook. That that is the problem that we need to address.”

Committee chair Damian Collins also gave short shrift to Allan’s attempt to muddily reframe this line of questioning — as regulators advocating “turning off the internet” (instead of what Angus was actually calling for: A way to get “credible democratic responses from a corporation”) — by interjecting: “I think we would also distinguish between the internet and Facebook to say they’re not necessarily the same thing.”

The room affirmed its accord with that.

At the start of the session Collins revealed the committee would not — at least for now — be publishing the cache of documents it dramatically seized this weekend from the founder of a startup that’s been suing Facebook since 2015, saying it was “not in a position to do that.” Though at a later press conference he said the intent is to publish the documents.

At several points during the session DCMS committee members also appeared to tease some new details culled from the documents, asking for example whether Facebook had ever made API decisions for developers contingent on them taking advertising on its platform.

Allan said it had not — and appeared to be attempting to suggest that the emails the committee may have been reading were the result of “normal” internal business discussions about how to evolve Facebook’s original desktop-based business model for the mobile-first era.

Collins did detail one piece of new information during the hearing which he categorically identified as having been sourced from the seized documents — and specifically from an internal email sent by a Facebook engineer dating from October 2014 — suggesting this item looked to be of significant public interest.

“An engineer at Facebook notified the company in October 2014 that entities with Russian IP addresses had been using a Pinterest API key to pull over 3 billion data points a day through the ordered friends API,” he revealed, asking Allan whether Facebook reported that “to any external body at the time?”

The Facebook VP responded by characterizing the information contained in the seized documents as “partial,” on account of it being sourced via a “hostile litigant.”

“I don’t want you to use this opportunity just to attack the litigant,” retorted Collins. “I want you to address the question… What internal process [Facebook] ran when this was reported to the company by an engineer? And did they notify external agencies of this activity? Because if Russian IP addresses were pulling down a huge amount of data from the platform — was that reported or was that just kept, as so often seems to be the case, just kept within the ‘family’ and not talked about.”

“Any information you have seen that’s contained within that cache of emails is at best partial and at worst potentially misleading,” responded Allan. “On the specific question of whether or not we believe, based on our subsequent investigations, that there was activity by Russians at that time I will come back to you.”

We reached out to Pinterest to ask whether Facebook ever informed it about such an abuse of its API key. At the time of writing it had not responded to our request for comment.

Update: A spokesperson for Pinterest told us: “We’re currently investigating and have reached out to Facebook for any information they may have.”

In another development after the hearing, Facebook also unsealed a selection of the documents Collins had been referring to (below) — which appear to suggest there was no external investigation of the Pinterest incident.

Facebook have broken the seal on the court documents from California; will they now ask for the rest to be released? These emails show there was no external investigation of Russian IP addresses calling for Facebook data & don’t confirm how much was taken. https://t.co/5l49CnWtXU — Damian Collins (@DamianCollins) November 28, 2018

This report was updated with comment from Pinterest and to note additional developments",Social Media,TechCrunch,https://techcrunch.com/2018/11/27/the-problem-is-facebook-lawmakers-from-nine-countries-tell-zuckerbergs-accountability-stand-in/,"At a hearing attended by international parliamentarians, Facebook's policy VP was questioned about how Facebook is impacting democracy by spreading malicious and abusive content, as well as tracking non-users and using its platform to spread political disinformation. The empty chair next to him was a reminder that Facebook's CEO, Mark Zuckerberg, had snubbed repeated invitations to","Information, Discourse & Governance"
38,Facebook and eBay pledge to do more to tackle trade in fake reviews after pressure from UK regulator – TechCrunch,"Facebook and eBay pledge to do more to tackle trade in fake reviews after pressure from UK regulator

Facebook and eBay have made commitments to do more to stop fake reviews being sold on their platforms after coming under pressure from a UK markets regulator — even as fresh examples of the problem have been found on Facebook-owned Instagram.

Last June the Competition and Markets Authority (CMA) warned the two platform that they must do more to prevent the sale of fake reviews on their platforms, saying it had found “troubling evidence” of a “thriving marketplace for fake and misleading online reviews.”

The regulator estimates that more than three-quarters of UK shoppers are influenced by reviews when they shop online, with billions of pounds being spent every year based on write-ups of products or services — which in turn encourages an illegal trade in fake and misleading reviews.

A few months after the CMA’s warning UK consumer rights group Which? released the results of its own investigation of the problem — singling out Facebook for having failed to move the needle (while finding eBay had made progress).

Today the CMA says Facebook has removed a total of 188 groups and disabled 24 user accounts as a result of its investigation. While eBay has permanently banned 140 users after the intervention.

The regulator said both companies have now pledged to put measures in place to “better identify, investigate and respond to” the trade in fake reviews, and help prevent such content from appearing in the future — with Facebook agreeing to introduce “more robust systems” to detect and remove such content; and eBay saying it has improved its existing filters to “better identify and block listings” for the sale or trade of online reviews.

Commenting in a statement, CMA chief executive Andrea Coscelli said: “We’re pleased that Facebook and eBay are doing the right thing by committing to tackle this problem and helping to keep their sites free from posts selling fake reviews.”

“Fake reviews are really damaging to shoppers and businesses alike. Millions of people base their shopping decisions on reviews, and if these are misleading or untrue, then shoppers could end up being misled into buying something that isn’t right for them – leaving businesses who play by the rules missing out,” he added.

The CMA’s press release does not contain any detail of the kinds of improvements the pair have agreed to but Facebook told us it’s looking into developing automated technology to help detect and remove the bogus content.

Commenting in a statement, a Facebook spokesperson said:

Fraudulent activity is not allowed on Facebook or Instagram, including offering or trading fake reviews. While we have invested heavily to prevent this kind of activity across our services, we know there is more work to do and are working with the CMA to address this issue. Since we were first contacted by the CMA, we have identified and removed over 180 groups and 24 accounts for violating our rules and have taken robust steps to prevent this type of fraudulent activity from re-appearing on our platforms. This includes exploring the use of automated technology to help us detect and remove this content quickly, before people see it and report it to us.

An eBay spokesperson also told us: “We maintain zero tolerance for fake or misleading reviews and will continue to take action against any seller that breaches our user polices. We welcome today’s CMA report, as well as their acknowledgement of our ongoing enforcement work on this issue.”

Despite the CMA chalking up the platforms’ pledge to ‘do more’ as a win for consumers, it also reveals it’s found fresh examples of fake reviews traded on Facebook-owned Instagram — suggesting the game of whack-a-fake goes on. And will go on, unless or until platforms face more robust regulation and enforcement vis-a-vis the content they spread and monetize.

The CMA notes that websites have a responsibility to ensure that unlawful and harmful content isn’t advertised or sold through their platforms. However, as it stands, there’s little real punishment for failing to tackle the trade in bogus reviews — beyond reputational damage (and the slow burn of user trust).

The UK government recently proposed legislation to tackle a range of online harms, setting out a safety-first plan to regulation Internet firms last year — which could mean more stringent controls on platform content in future. For now, though, regulators only really have tough words in their toolbox to try to make tech giants clean up their act.

The CMA says it reported the instances of fake reviews that it found being traded on Instagram to Facebook, adding: “Facebook has committed to investigate the issue” — and saying it “will be seeking a commitment from Facebook to take action to tackle these further issues.”",Social Media,TechCrunch,https://techcrunch.com/2020/01/08/facebook-and-ebay-pledge-to-do-more-to-tackle-trade-in-fake-reviews-after-pressure-from-uk-regulator/,"The sale of fake reviews on Social Media platforms can be detrimental to shoppers and businesses alike, as millions of people base their shopping decisions on reviews that may be false or misleading. This problem needs to be addressed through better regulation and enforcement to ensure trust and fairness on Social Media.","Information, Discourse & Governance"
39,U.K. Prime Minister Suggests 'Pre-Crime' Blocking of Social Media,"British Prime Minister David Cameron has told Parliament that he is investigating whether to stop people communicating via social networking sites if they are known to be planning criminal activity.

He said to the House of Commons Thursday: ""Free flow of information can be used for good. But it can also be used for ill. So we are working with the Police, the intelligence services and industry to look at whether it would be right to stop people communicating via these websites and services when we know they are plotting violence, disorder and criminality.""

In the statement he released to the media before he spoke to Parliament, he also said: ""when people are using social media for violence we need to stop them.""

When he was later challenged to increase the headcount of police on the streets, he replied that the focus should be on whether ""to give the police the technology to trace people on Twitter or BBM or close it down"" before talking about police resources.

He added: ""The key thing is that the police were facing a new circumstance where rioters were using BBM -- a closed network -- so they knew where they were going to loot next. We've got to examine that and know how to keep up with them.""

Quite how this would work on a practical level is somewhat baffling, but a number of people have already been arrested in Lancashire and Essex for posting Facebook messages inciting criminal damage.

It wasn't all bad for technology. Cameron said ""we are making technology work for us, by capturing the images of the perpetrators on CCTV -- so even if they haven't yet been arrested, their faces are known and they will not escape the law."" Presumably he is referring to initiatives such as the Metropolitan Police uploading photos of suspected looters to its Flickr account.

""No phoney human rights concerns about publishing photos will get in the way of bringing these criminals to justice,"" he added.

When asked about the need to clamp down on people who spread false rumors across the internet, Cameron recognized the need for police to be better technologically equipped. He said: ""Just as police have been using technology more effectively, so have criminals."" He mentioned hoaxes, false trails on Twitter and BlackBerry Messenger. ""There needs to be a major piece of work to ensure that police have all the electronic capabilities to hunt down and beat the criminal,"" he added.

One voice of reason was from Liberal Democrat Julian Huppert who said that social networks are also used to mobilise people for good purposes, such as the clean up operation and that ""clamping down"" on them could have serious negative consequences. However, Cameron reiterated the need to give the police the tools to monitor social media activity.

He continued: ""This is not about poverty, it's about culture. A culture that glorifies violence, shows disrespect to authority, and says everything about rights but nothing about responsibilities.""

""It's time for our country to pull together. To the law abiding people who play by the rules, and who are the overwhelming majority in this country, I say the fightback has begun, we will protect you, if you've had your livelihood and property damaged, we will compensate you. We are on your side.""

He issued a sternly worded warning to the lawless minority: ""We will track you down, we will find you, we will charge you, we will punish you. You will pay for what you have done.""

Photo: Kirsty Wigglesworth/AP

See Also: - Why Has BlackBerry Been Blamed for the London Riots?",Social Media,WIRED,https://www.wired.com/2011/08/uk-block-social-media/,"Prime Minister David Cameron is advocating for the police to be given access to technology that will allow them to trace people on social media and potentially shut down communication when criminal activity is being planned. This has raised concerns about the potential for clamping down on activities that are not criminal, such as peaceful protests and the spread of true information.",Security & Privacy
40,Want to Stop Facebook Violence? You Won't Like the Choices,"No one wants murder videos on Facebook. But no one wants Facebook to censor their baby videos, either. Technology isn’t ready to step in and tell the difference. So what are the legal options for stopping videos like the appalling killing uploaded last week from hitting Facebook? None of them will be easy for Americans to swallow.

The country could regulate Facebook like it does traditional broadcasters and media by holding the company accountable for the country it distributes. Or legislators could create new laws to criminalize the amplification of violence on social media. But doing so could have drastic repercussions, impinging on freedom of expression online and discouraging the dissemination of videos that shine on a light on atrocities, among other things. The entire internet would change.

""We want a free and open internet, and we want a space that we aren't paying a subscription for,"" says Kate Coyer, a fellow at Harvard's Berkman Klein Center for Internet and Society and an expert online extremism. ""But we also don't want to encounter some of the worst elements of humanity on there. ... At a certain point we may have to make a compromise.""

This most recent video is proof that technology moves fast, but not fast enough to preempt the ways humans will use and abuse it. Unfortunately neither can that slow bastion of humanity, the law. It's a game of catch-up, and attempts at creating real safeguards remain far behind.

Back in the salad days of the Internet, lawmakers saw the problems presented by the ease and ubiquity of this new medium approaching fast. In 1996, the internet was a promise wrapped in a mystery inside a tangle of cords and cables. ""Congress looked and said oh god this is going to be a child porn nightmare,"" says law professor Mary Anne Franks, an expert in free speech and digital rights at the University of Miami. So they drafted the Communications Decency Act, a sweeping ruling against obscenities and child pornography. Free speech advocates and companies pushed back, which resulted in the created of Section 230 of the Act.

""Section 230 was created as a shield for online intermediaries to avoid liability for regulating harmful content. It is controversial because it now mostly operates as a sword to allow online intermediaries to avoid liability for not regulating harmful content,"" says franks. ""It started out as a way to try to put some regulation on the internet, and it ended up becoming the thing that keeps us from being able to regulate the internet.""

The law now serves as the backbone of the open internet. Companies like Facebook don't have to worry about being sued for something you do on its site---even if what you do is murder someone. The clear exceptions to that immunity carved out by the courts are child pornography and copyright infringement. The law still holds tech platforms accountable for such ills. But in the most common interpretation of the statute, everything else is outside its scope of culpability.

The Legal Options

Still, if Americans decided that never seeing murder on Facebook trumped the freedom to upload anything, they could push for legal and regulatory steps. But each is riddled with downsides.

Lawmakers could amend Section 230 to expand companies' liability. If the law designated Facebook a content developer like a traditional media company, rather than an intermediary through which others post content, it could face penalties itself for what people put on its site. That's in the same spirit as a law introduced in Germany to fine social media platforms for allowing hate speech or violent content to remain on its site.",Social Media,WIRED,https://www.wired.com/2017/04/face-law-cant-keep-violence-off-facebook-either/,"lawmakers could amend Section 230 to expand companies' liability, potentially leading to the internet becoming regulated and hindering the dissemination of videos that shine a light on atrocities.","Information, Discourse & Governance"
41,Facebook Notification Spam Has Crossed the Line,"Facebook has always nudged truant users back to its platform though emails and notifications. But recently, those prods have evolved beyond comments related to activity on your own profile. Now Facebook will nag you when an acquaintance comments on someone else’s photo, or when a distant family member updates their status. The spamming has even extended to those who sign up for two-factor authentication—which is a great way to turn people off to that extra layer of security.

“The part of it that bugs me is that two-factor authentication is something [Facebook] should be encouraging people to use, but instead the way this is working here is that they’re driving people away from two-factor and making people less secure,” says Matt Green, a professor at the Johns Hopkins University Information Security Institute, who has done contracted security work for Facebook in the past. “It’s abusive, people’s attention is deliberately tweaked by what looks like a two-factor authentication message.”

Green says he’s received near-daily SMS messages from Facebook since January alerting him that one of his friends performed some action on the platform. Before he started receiving the messages, Green says he hadn’t logged into Facebook for a long time and had actually forgotten his password.

The weirdest part about the SMS notifications is what happens if you reply to them. If you respond, your message is posted to your own profile. If the notifications involve someone else’s content, users say the text replies inadvertently show up as comments on other people’s photos or statuses. I set up text two-factor authentication myself to try this out, and my text reply did end up on my own profile.

Facebook

Facebook's response about what was happening with SMS messages didn't shed very much light on the issue. “We give people control over their notifications, including those that relate to security features like two-factor authentication. We’re looking into this situation to see if there’s more we can do to help people manage their communications,” a spokesperson said in a statement.",Social Media,WIRED,https://www.wired.com/story/facebook-notification-spam-two-factor/,"Facebook's SMS notifications related to two-factor authentication can end up on other people's profiles and posts, which is potentially a security risk.",Security & Privacy
42,Indivisible is training an army of volunteers to neutralize political misinformation – TechCrunch,"The grassroots Democratic organization Indivisible is launching its own team of stealth fact-checkers to push back against misinformation — an experiment in what it might look like to train up a political messaging infantry and send them out into the information trenches.

Called the “Truth Brigade,” the corps of volunteers will learn best practices for countering popular misleading narratives on the right. They’ll coordinate with the organization on a biweekly basis to unleash a wave of progressive messaging that aims to drown out political misinformation and boost Biden’s legislative agenda in the process.

Considering the scope of the misinformation that remains even after social media’s big January 6 cleanup, the project will certainly have its work cut out for it.

“This is an effort to empower volunteers to step into a gap that is being created by very irresponsible behavior by the social media platforms,” Indivisible co-founder and co-executive director Leah Greenberg told TechCrunch. “It is absolutely frustrating that we’re in this position of trying to combat something that they ultimately have a responsibility to address.”

Greenberg co-founded Indivisible with her husband following the 2016 election. The organization grew out of the viral success the pair had when they and two other former House staffers published a handbook to Congressional activism. The guide took off in the flurry of “resist”-era activism on the left calling on Americans to push back on Trump and his agenda.

Indivisible’s Truth Brigade project blossomed out of a pilot program in Colorado spearheaded by Jody Rein, a senior organizer concerned about what she was seeing in her state. Since that pilot began last fall, the program has grown into 2,500 volunteers across 45 states.

The messaging will largely center around Biden’s ambitious legislative packages: the American Rescue plan, the voting rights bill HR1 and the forthcoming infrastructure package. Rather than debunking political misinformation about those bills directly, the volunteer team will push back with personalized messages promoting the legislation and dispelling false claims within their existing social spheres on Facebook and Twitter.

The coordinated networks at Indivisible will cross-promote those pieces of semi-organic content using tactics parallel to what a lot of disinformation campaigns do to send their own content soaring (in the case of groups that make overt efforts to conceal their origins, Facebook calls this “coordinated inauthentic behavior.”) Since the posts are part of a volunteer push and not targeted advertising, they won’t be labeled, though some might contain hashtags that connect them back to the Truth Brigade campaign.

Volunteers are trained to serve up progressive narratives in a “truth sandwich” that’s careful to not amplify the misinformation it’s meant to push back against. For Indivisible, training volunteers to avoid giving political misinformation even more oxygen is a big part of the effort.

“What we know is that actually spreads disinformation and does the work of some of these bad actors for them,” Greenberg said. “We are trying to get folks to respond not by engaging in that fight — that’s really doing their work for them — but by trying to advance the kind of narrative that we actually want people to buy into.”

Truth Sandwich:

1. Start with the truth. The first frame gets the advantage.

2. Indicate the lie. Avoid amplifying the specific language if possible.

3. Return to the truth. Always repeat truths more than lies.

Hear more in Ep 14 of FrameLab w/@gilduran76https://t.co/cQNOqgRk0w — George Lakoff (@GeorgeLakoff) December 1, 2018

She cites the social media outrage cycle perpetuated by Georgia Rep. Marjorie Taylor Greene as a harbinger of what Democrats will again be up against in 2022. Taylor Greene is best known for endorsing QAnon, getting yanked off of her Congressional committee assignments and comparing mask requirements to the Holocaust — comments that inspired some Republicans to call for her ouster from the party.

Political figures like Greene regularly rile up the left with outlandish claims and easily debunked conspiracies. Greenberg believes that political figures like Greene who regularly rile up the online left suck up a lot of energy that could be better spent resisting the urge to rage-retweet and spreading progressive political messages.

“It’s not enough to just fact check [and] it’s not enough to just respond, because then fundamentally we’re operating from a defensive place,” Greenberg said.

“We want to be proactively spreading positive messages that people can really believe in and grab onto and that will inoculate them from some of this.”

For Indivisible, the project is a long-term experiment that could pave the way for a new kind of online grassroots political campaign beyond targeted advertising — one that hopes to boost the signal in a sea of noise.",Social Media,TechCrunch,https://techcrunch.com/2021/06/01/indivisible-truth-brigade/,"Social media has created a range of problems, particularly through the spread of misinformation, leading to Indivisible's Truth Brigade project, which trains volunteers to push back against popular misleading narratives on the right and promote progressive policies.","Information, Discourse & Governance"
43,Security lapse exposed Clearview AI source code – TechCrunch,"Since it exploded onto the scene in January after a newspaper exposé, Clearview AI quickly became one of the most elusive, secretive and reviled companies in the tech startup scene.

The controversial facial recognition startup allows its law enforcement users to take a picture of a person, upload it and match it against its alleged database of 3 billion images, which the company scraped from public social media profiles.

But for a time, a misconfigured server exposed the company’s internal files, apps and source code for anyone on the internet to find.

Mossab Hussein, chief security officer at Dubai-based cybersecurity firm SpiderSilk, found the repository storing Clearview’s source code. Although the repository was protected with a password, a misconfigured setting allowed anyone to register as a new user to log in to the system storing the code.

The repository contained Clearview’s source code, which could be used to compile and run the apps from scratch. The repository also stored some of the company’s secret keys and credentials, which granted access to Clearview’s cloud storage buckets. Inside those buckets, Clearview stored copies of its finished Windows, Mac and Android apps, as well as its iOS app, which Apple recently blocked for violating its rules. The storage buckets also contained early, pre-release developer app versions that are typically only for testing, Hussein said.

The repository also exposed Clearview’s Slack tokens, according to Hussein, which, if used, could have allowed password-less access to the company’s private messages and communications.

Clearview has been dogged by privacy concerns since it was forced out of stealth following a profile in The New York Times, but its technology has gone largely untested and the accuracy of its facial recognition tech unproven. Clearview claims it only allows law enforcement to use its technology, but reports show that the startup courted users from private businesses like Macy’s, Walmart and the NBA. But this latest security lapse is likely to invite greater scrutiny of the company’s security and privacy practices.

When reached for comment, Clearview founder Hoan Ton-That claimed his company “experienced a constant stream of cyber intrusion attempts, and have been investing heavily in augmenting our security.”

“We have set up a bug bounty program with HackerOne whereby computer security researchers can be rewarded for finding flaws in Clearview AI’s systems,” said Ton-That. “SpiderSilk, a firm that was not a part of our bug bounty program, found a flaw in Clearview AI and reached out to us. This flaw did not expose any personally identifiable information, search history or biometric identifiers,” he said.

Ton-That accused the research firm of extortion, but emails between Clearview and SpiderSilk paint a different picture.

Hussein, who has previously reported security issues at several startups, including MoviePass, Remine and Blind, said he reported the exposure to Clearview but declined to accept a bounty, which he said if signed would have barred him from publicly disclosing the security lapse.

It’s not uncommon for companies to use bug bounty terms and conditions or non-disclosure agreements to prevent the disclosure of security lapses once they are fixed. But experts told TechCrunch that researchers are not obligated to accept a bounty or agree to disclosure rules.

Ton-That said that Clearview has “done a full forensic audit of the host to confirm no other unauthorized access occurred.” He also confirmed that the secret keys have been changed and no longer work.

Hussein’s findings offer a rare glimpse into the operations of the secretive company. One screenshot shared by Hussein showed code and apps referencing the company’s Insight Camera, which Ton-That described as a “prototype” camera, since discontinued.

According to BuzzFeed News, one of the firms that tested the cameras is New York City real estate firm Rudin Management, which trialed use of a camera at two of its city residential buildings.

Hussein said that he found some 70,000 videos in one of Clearview’s cloud storage buckets, taken from a camera installed at face-height in the lobby of a residential building. The videos show residents entering and leaving the building.

Ton-That explained that, “as part of prototyping a security camera product we collected some raw video strictly for debugging purposes, with the permission of the building management.”

TechCrunch has learned that the Rudin-owned building is on Manhattan’s east side. Several property listings with images of the building’s lobby also confirm this. A representative for the real estate company did not return our emails.

Clearview has come under intense scrutiny since its January debut. It has also attracted the attention of hackers.

In February, Clearview admitted to customers that a list of its customers was stolen in a data breach — though, it claimed its servers were “never accessed.” Clearview also left unprotected several of its cloud storage buckets containing its Android app.

Vermont’s attorney general’s office has already opened an investigation into the company for allegedly violating consumer protection laws, and police departments have been told to stop using Clearview, including in New Jersey and San Diego. Several tech companies, including Facebook, Twitter and YouTube, have already filed cease-and-desist letters with Clearview AI.

In an interview with CBS News in February, Ton-That defended his company’s practices. “If it’s public and it’s out there and could be inside Google’s search engine, it can be inside ours as well,” he said.

Got a tip? You can send tips securely over Signal and WhatsApp to +1 646-755-8849.",Social Media,TechCrunch,https://techcrunch.com/2020/04/16/clearview-source-code-lapse/,"Clearview AI's misconfigured server has exposed the company's internal files, apps, source code, secret keys and credentials, as well as 70,000 videos taken from a prototype security camera, raising serious privacy concerns and inviting greater scrutiny of the company's security and privacy practices.",Security & Privacy
44,Facebook under pressure over Soros smear tactics – TechCrunch,"Facebook is facing calls to conduct an external investigation into its own lobbying and PR activities by an aide to billionaire George Soros.

BuzzFeed reports that Michael Vachon, an advisor to the chairman at Soros Fund Management, made the call in a letter to friends and colleagues.

The call follows an explosive investigation, published yesterday by the New York Times based on interviews with more than 50 sources on the company, which paints an ugly picture of how Facebook’s leadership team responded to growing pressure over election interference, in the wake of the Kremlin ads scandal of 2016, including by engaging an external firm to lobby aggressively on its behalf.

The firm used smear tactics targeted at Soros, according to the NYT report, with the paper writing that: “A research document circulated by Definers [the PR firm engaged by Facebook] to reporters this summer, just a month after the House hearing, cast Mr. Soros as the unacknowledged force behind what appeared to be a broad anti-Facebook movement.”

Wikipedia describes Definers as “an American right leaning opposition research firm… [that] performs media monitoring services, conducts research using the Freedom of Information Act and also creates strategic communication to negatively influence the public image about individuals, firms, candidates and organizations who oppose their clients”.

Facebook has since responded to the NYT article, rejecting some of the report as inaccurate — and denying outright that it ever asked Definers to smear anyone on its behalf.

“The New York Times is wrong to suggest that we ever asked Definers to pay for or write articles on Facebook’s behalf – or to spread misinformation,” the company writes. “Our relationship with Definers was well known by the media – not least because they have on several occasions sent out invitations to hundreds of journalists about important press calls on our behalf.

“Definers did encourage members of the press to look into the funding of ‘Freedom from Facebook,’ an anti-Facebook organization. The intention was to demonstrate that it was not simply a spontaneous grassroots campaign, as it claimed, but supported by a well-known critic of our company. To suggest that this was an anti-Semitic attack is reprehensible and untrue.”

In a follow up report today the NYT says Facebook cut ties with the PR firm on Wednesday, after the publication of its article.

In his letter, Vachon describes it as “alarming that Facebook would engage in these unsavory tactics, apparently in response to George’s public criticism in Davos earlier this year of the company’s handling of hate speech and propaganda on its platform”.

“What else is Facebook up to? The company should hire an outside expert to do a thorough investigation of its lobbying and PR work and make the results public,” he adds.

We contacted Facebook for a response to Vachon’s call for an external investigation of its internal conduct. A company spokesman just directed us to its earlier response to the NYT article.

Facebook has recently faced calls for an external security and privacy audit from the European parliament in the wake of the Cambridge Analytica data misuse scandal.

And calls for its CEO and founder to face up to international politicians’ questions over fake news and election interference. Although Zuckerberg has continued to decline to attend.

So the external pressures keep piling up…

A damning story about Facebook which underlines why we need to hold their top people to account – Delay, Deny and Deflect: How Facebook’s Leaders Fought Through Crisis https://t.co/cwJmKVR3qD — Damian Collins (@DamianCollins) November 15, 2018

The title of the NYT article — “delay, deny and deflect” — hints at the meaty reportage within, with the newspaper presenting a well-sourced view of Facebook’s management team grappling ineptly and then cynically and aggressively with an existential reputation crisis by reaching for smear tactics associated with the worst kind of politics.

“[Facebook COO Sheryl] Sandberg has overseen an aggressive lobbying campaign to combat Facebook’s critics, shift public anger toward rival companies and ward off damaging regulation,” the newspaper writes.

It also alleges that Facebook knew about Russian activity on its platform as early as the spring of 2016 but was slow to investigate.

Again, in its rebuttal, Facebook rejects that characterization — claiming a less inept early handling of the political disinformation threat. “Leading up to Election Day in November 2016, we detected and dealt with several threats with ties to Russia … [including] a group called APT28 … we also saw some new behavior when APT28-related accounts, under the banner of DC Leaks, created fake personas that were used to seed stolen information to journalists. We shut these accounts down for violating our policies,” it writes.

It also denies its then CSO, Alex Stamos, was discouraged by senior management from looking into Russian activity.

Although Stamos clashing with Sandberg over the Russian disinformation threat has previously been causally linked to his departure from Facebook this summer. (And in an internal memo that BuzzFeed obtained earlier this year Stamos does admit to having had “passionate discussions with other execs”.)

“After the election, no one ever discouraged Alex Stamos from looking into Russian activity — as he himself acknowledged on Twitter,” Facebook writes now, rejecting that portion of the NYT report. “Indeed as The New York Times says, “Mark and Sheryl [Sandberg] expanded Alex’s work.”

Facebook has also denied treating Donald Trump’s comments about Muslims — when in December 2015 the US president posted a statement on Facebook calling for a “total and complete shutdown” on Muslims entering the United States — any differently to “other important free speech issues”.

On this the newspaper’s sources told it that Facebook’s management team had delegated key decisions on whether or not Trump’s post constituted hate speech to policy staffers who “construed their task narrowly” yet were also motivated by worries about stoking a conservative backlash.

The post was not deleted. And the NYT writes that it was shared more than 15,000 times on Facebook — “an illustration of the site’s power to spread racist sentiment”.",Social Media,TechCrunch,https://techcrunch.com/2018/11/15/facebook-under-pressure-over-soros-smear-tactics/,"Facebook is facing calls to conduct an external investigation into its own lobbying and PR activities after reports of it engaging an external firm to smear billionaire George Soros in response to growing pressure over election interference. This highlights the potential for Social Media to spread dangerous and damaging information, including racism and hate speech, which could have damaging consequences.",Social Norms & Relationships
45,The online battle for the mental health of service workers,"Morgan Eckroth became famous on TikTok as morgandrinkscoffee. A 21-year-old barista and social-media manager for Tried & True Coffee in Corvallis, Oregon, she shares latte art, dramatic reenactments of customer interactions, and drink tutorials with her 4 million followers. Before the pandemic her content was pretty wholesome—she likes her job! But then in May, someone who was angry about the shop’s temporary safety policy against handling cash assaulted her and a coworker with bear mace.

In a TikTok video Eckroth made about the incident, she was in bed with covers pulled up over her mouth and nose as calming music played. “We don’t deserve this when we are just trying to keep ourselves and customers safe,” read one of the captions. The video has more than 1.3 million views, and Eckroth was flooded with supportive comments.

Nearly every day, a story about a confrontation between a service employee and a shopper or diner upset about pandemic-related safety requirements makes news in the US. Messages exchanged on TikTok, in private Facebook groups, and in other semi-private online spaces have become a form of homebrewed therapy for workers trying to deal with the stress. But alongside this ecosystem of service-worker support is another organizational structure: the private and semi-private online spaces where Americans who refuse to wear masks or abide by other safety requirements promote protests and boycotts over mask policies, and support those who show up in public places without one.

As in every other online information war, these narratives compete for your attention and spread through social-media platforms that remain extremely good at helping misinformation peddlers reach bigger audiences. But unmasked customers, whether motivated by apathy or activism, are a stressful burden for service workers who are largely left to enforce pandemic safety measures in stores and restaurants. And in many instances, these workers are using social media to advocate for themselves and explain the damage these confrontations can do to their mental health.

“There’s a new sense of bonding between people who work in customer service right now,” Eckroth said. “The fact that most of us have had to work people-facing jobs through the pandemic on or near minimum wage has created a whole new community online.”

TikTok was already something of a haven for retail and food service workers before the pandemic, when employees of restaurants and retail chains used the app to vent and share about their work days. Then, when the pandemic hit, jokes and memes gave way to stories of assault, threats, and verbal abuse from anti-mask activists who visited their stores. Suddenly, these clusters of creators and viewers became a kind of support group.

Support can be found elsewhere, too. In a private 5,000-member Facebook group called Retail Life during Covid-19, workers vent about customer confrontations, confusing management directives, and unemployment payment delays. Working in a store now is like “being held hostage by these people who don’t give a shit about me, or you,” wrote one member, who asked to remain anonymous for fear of losing their job, after encountering a mostly maskless group of shoppers who lingered in the store.

Workers in service jobs have long endured stressful conditions for low pay. Many of these workers now face unemployment, fears about job security, and the daily dangers inherent in serving others during a pandemic. On top of all that, the pandemic has made dangerous encounters with customers more frequent—and social media has made such interactions more visible.

Some videos about these encounters have gotten thousands of views: At a Skechers store in Oklahoma City, a customer was caught on video throwing shoeboxes at an employee’s head after being asked to wear a mask. And a Florida insurance agent was fired after he was filmed yelling at a Costco employee when asked why he wasn’t wearing a face covering.

Judy Herrell, the owner of Herrell’s ice cream shop in Northampton, Massachusetts, posted on Facebook begging customers to treat her employees with respect after a customer threw ice cream at one upon learning the shop was not allowing people to eat inside.

Her post got some attention, though Herrell said her store sees many more confrontations than she was able to describe in the post.

“We’re getting one every couple of days, sometimes two or three,” she says. Some of her employees have decided to seek counseling.

There’s an “overall climate of tremendous anxiety” among restaurant workers right now, says John Vincent, a professor of psychology at the University of Houston who is supervising a university program that gives free therapy to restaurant workers in the area. The program, which is a collaboration with a Texas-based crisis relief organization for the food industry, was designed to address concerns about the mental health of these employees, who often do not have access to affordable mental health care. It had been in the works for several years when the pandemic began, prompting coordinators to move the start date up. They launched about a month ago.

States have different rules about wearing masks in public: about 20 currently have mask mandates in place. The US Centers for Disease Control now recommends that people wear cloth face coverings in public to prevent the spread of the coronavirus (although, famously, the agency initially told residents not to wear them). Businesses follow local and state guidelines on masks or, in some cases, set their own policies. As these requirements and recommendations have become a part of daily life, a conspiracy-fueled movement to oppose mask wearing has gained traction on social media, often tapping into the same networks of influencers and communities that have spread other health misinformation.

I found Bare Face Is Legal, a private Facebook group with more than 20,000 members, via a recommendation from a natural healing private Facebook group I'm in that regularly promotes bogus cures or treatments for cancer. In one video posted to the Bare Face Is Legal group, which was also shared publicly, a woman who identifies herself as a nurse films masked store employees and customers as she asserts that she has a right to be in the store without a mask, before law enforcement convinces her to leave. The video has nearly 30,000 views.

The group is an offshoot of Barefoot Is Legal, a Facebook group and nonprofit run by Dave Kelman that promotes going barefoot in public as a legal right. As Mel Magazine noted in a 2018 profile of Kelman and his movement, that group’s online presence is not explicitly political or conspiratorial. However, the piece states that Kelman himself runs an online radio station that plays “a lot of programming on refusing vaccines and ‘fighting the New World Order.’”

Kelman says that he believes that the vast majority of mask wearers have ""just been pressured"" into wearing them by ""social justice warriors"" and the mainstream media. He cited a debunked claim popular in anti-mask circles that wearing a mask over a long period of time can cause an oxygen deficiency, or carbon dioxide toxicity.

The group generally talks about mask policies as a civil rights issue, using terms such as ""discrimination."" This sentiment echoes flyers that circulated on social media in April and encouraged those opposed to wearing masks to claim that they had a medical condition and were exempt from the requirements. As Snopes noted, the Americans with Disabilities Act does not protect people pretending to have a disability. For those who are covered by the US federal law, the ADA would require businesses to make reasonable accommodations, which could include curbside or home delivery.

Other Bare Face Is Legal members trade strategies for avoiding or confronting employees who ask them about masks, tell anecdotes about being able to shop mask free, discuss stores to boycott, and share links to local anti-mask protests. One user discussed a strategy for getting away with not wearing a mask for a four-hour plane ride: by eating nonstop for the entire flight.

Kelman said he agrees that it's unfair to see service workers ""getting beat up on this"" when they're tasked with enforcing mask-related rules, and that he believes these protests should be aimed at the government. However, like in many anti-mask spaces, members of his group also celebrate videos of customers who refuse to wear a mask and become angry when asked to by employees or other shoppers. A recent post shared by administrators to the related public Bare Face is Legal page encouraged people to film interactions with store managers who refuse them entry, but to be ""respectful.""

As the pandemic progresses, these confrontations don’t seem to be slowing down. Anti-mask protesters have filmed themselves destroying mask displays in stores and trying to enter businesses that require masks, and they’ve posted photographs to social media of employees who decline to serve them. They’ve promoted Etsy listings for mesh masks—which wouldn’t stop the spread of the virus—claiming that these designs are a loophole to rules requiring them.

Vincent, the University of Houston professor supervising a free therapy service for area restaurant workers, says the program had more than 50 referrals in three weeks and underscores the need for greater access to affordable health care, including mental health care, in the United States. Programs like his can help meet that need to a degree, but the industry was already facing a mental health crisis before the pandemic began, and now that crisis is undeniably worse.

For Vincent, there’s one small silver lining: he feels that more people are starting to talk openly about mental health in the restaurant industry and beyond. “There’s a growing cognizance of ‘we are all human,’” he says.",Social Media,MIT,https://www.technologyreview.com/2020/07/16/1005278/essential-workers-masks-tiktok-facebook-protests/,"Social media has made the dangerous confrontations between anti-mask customers and service workers more visible, creating an atmosphere of anxiety and stress for these workers who often do not have access to affordable mental health care. This has led to an increase in mental health problems among service workers, highlighting the need for greater access to affordable health care, including mental",Equality & Justice
46,"No Opting Out Of Facebook Turning Your Check-Ins, Likes Into Ads","Better go check your Facebook profile pic to make sure it's suitable for advertising—the company has begun using real users' postings in ads being shown to their friends. The effort is eerily similar to parts of the now-defunct Facebook Beacon, but Facebook is now calling them ""sponsored stories,"" and users won't be able to opt out of their posts being used to advertise to friends.

The new ""feature"" started showing up quietly on Wednesday morning without any kind of fanfare from Facebook, but users began to notice it right away. Things posted by their friends; check-ins at businesses and ""Likes"" clicked from other websites started being highlighted in the right-hand column with the other ads, under the headline of ""Sponsored Story.""

Facebook says that the new ads are an effort to help marketers take advantage of what people are saying about them online. ""Currently, marketers don't have the ability to know or plan word-of-mouth endorsements as part of their campaigns,"" Facebook's product marketing lead Jim Squires told the Wall Street Journal. ""This gives a way for marketers to increase the visibility of stories about their organization… this is word-of-mouth marketing at scale.""

The company has reportedly been testing the feature for months and claims to have seen a positive reaction from users, who apparently prefer hearing about things from their friends rather than a faceless entity.

Still, there are ways in which the system could be improved, especially for those concerned about their privacy being respected. The most obvious is that users are not notified that their posts are being used in ads, and cannot block their posts from becoming ads unless they change their privacy settings to make the posts private. That's one thing Facebook has going for it this time around, though—the company will respect all privacy settings so that only the people you've already authorized to see your posts will see ads with you in them.

In some ways, the sponsored stories are a refined version of the disaster now known as Facebook Beacon. Launched in 2007, Facebook attempted to achieve basically the same goal by offering marketers a way to capitalize on users' off-Facebook activities by publishing purchases to users' walls. This resulted in a class-action privacy lawsuit and Facebook eventually shut Beacon down as part of its settlement.

Now, users' off-Facebook activities are basically part of the Facebook ecosystem thanks to ""Likes"" published all over the Web. If you click the Facebook Like button on any given site, that data is transmitted to your own Facebook profile and can be promoted by marketers in ads to your friends. We knew something like this was coming—it was rumored last year in advance of Facebook's f8 conference — but it's still fascinating to watch the evolution of Beacon and the very different reactions this time around. There are certainly users who are unhappy with their check-ins and likes being used to generate more cash, but the privacy concerns appear to be mostly gone — for now, that is.

Image via AllThingsD

Follow Epicenter on Twitter for disruptive tech news

See Also:",Social Media,WIRED,https://www.wired.com/2011/01/facebook-check-ins-likes-ads/,"The new ""sponsored stories"" feature on Facebook is allowing marketers to use users' posts in ads being shown to their friends without informing them and without allowing them to opt out. This is similar to the now-defunct Facebook Beacon which resulted in a class-action privacy lawsuit and was eventually shut down.",Security & Privacy
47,Mark Zuckerberg's Answer to a World Divided by Facebook Is More Facebook,"When I ask Mark Zuckerberg if the presidential election changed the way he sees Facebook---if he made poor assumptions, if Facebook functioned in ways he didn't intend---he pauses.

I've interviewed Zuckerberg before, and he tends to pause like this, gathering his thoughts in complete silence, sometimes turning to face the empty space across the room. But this dead air lasts particularly long. Five seconds. Six. Seven. Eight. Nine. Ten. ""I don't know,"" he finally says. ""It's a very interesting question."" Pause.

""If you continue giving people voice and work to create a diversity of ideas and common understanding and strengthen the social fabric,"" he says, not directly answering the question, ""then over the long term we will go in the right direction regardless if you disagree on short-term things.""

'When I started Facebook, the mission of connecting people wasn’t a controversial thing.' Mark Zuckerberg

In other words, as Facebook monopolizes the world's attention and helps Americans construct parallel factual universes, undermining old-media business models and altering the dynamics of electoral politics, Zuckerberg isn't budging from the belief that Facebook is on the right path. As he has repeatedly said since the election, he believes his platform brings people together---despite the sea of evidence that in its stated mission to ""connect the world"" Facebook may be helping to tear it apart.

But in that long, painful pause, I get the sense that Zuckerberg is wrestling with Facebook’s flaws. His unease also lies just below the surface of a 5,000-word treatise the company released this afternoon in which Zuckerberg expounds on his vision for Facebook's future. In the letter, he doesn't really concede the point that Facebook may be responsible for the warping of civil discourse. Instead, he reframes. For Zuckerberg, this is a decisive moment in his company's short history, the first major restatement of Facebook's mission since he published his manifesto, ""The Hacker Way,"" before the company went public in 2012. Facebook, as his new letter re-imagines it, should function as a global ""social infrastructure,"" the technological underpinning of a world more tightly connected in real life.

""When I started Facebook, the mission of connecting people wasn't a controversial thing. The default assumption was that the world was coming closer together,"" he tells me. ""But for the last couple of years, it's really been much more of a question."" Now, he wants the world to know he still believes that Facebook is the answer. In his letter, he vows to build a Facebook that forges new connections capable of strengthening ties in the real world. But Zuckerberg still doesn’t seem to know how exactly Facebook will take a platform that has driven so many people apart and use it to pull off a great global coming together.

A New Community

After spending a decade portraying Facebook as a service for connecting friends and family, Zuckerberg’s grand vision is now to build technology that creates far bigger and more complex communities. ""Humanity has always pushed to come together in greater numbers to accomplish better things and improve our lives individually in ways we couldn't in smaller groups,"" he tells me. If your News Feed now feels like a tiny town, Zuckerberg seems to want to build cities. Or at least churches.

In our conversation, he says his model for an online community might look something like Saddleback, the evangelical Southern California megachurch led by pastor Rick Warren. It’s a surprising example from a man who seems steeped in the liberal pluralism of Silicon Valley. But the key for Zuckerberg is that Warren built a community in which tens of thousands of people gather under a capable leader’s guidance, but also divide themselves into smaller groups by interest, affinity, and aspirations.",Social Media,WIRED,https://www.wired.com/2017/02/mark-zuckerbergs-answer-world-divided-facebook-facebook/,"Social media has had a significant effect on society, but not always in a positive way. It has been blamed for contributing to the spread of misinformation and polarization, as well as eroding traditional media business models. As a result, Mark Zuckerberg is trying to move Facebook away from a platform for connecting with friends and family and towards one that creates",Discourse & Governance
48,"Facebook still a great place to amplify pre-election junk news, EU study finds – TechCrunch","A study carried out by academics at Oxford University to investigate how junk news is being shared on social media in Europe ahead of regional elections this month has found individual stories shared on Facebook’s platform can still hugely outperform the most important and professionally produced news stories, drawing as much as 4x the volume of Facebook shares, likes, and comments.

The study, conducted for the Oxford Internet Institute’s (OII) Computational Propaganda Project, is intended to respond to widespread concern about the spread of online political disinformation on EU elections which take place later this month, by examining pre-election chatter on Facebook and Twitter in English, French, German, Italian, Polish, Spanish, and Swedish.

Junk news in this context refers to content produced by known sources of political misinformation — aka outlets that are systematically producing and spreading “ideologically extreme, misleading, and factually incorrect information” — with the researchers comparing interactions with junk stories from such outlets to news stories produced by the most popular professional news sources to get a snapshot of public engagement with sources of misinformation ahead of the EU vote.

As we reported last year, the Institute also launched a junk news aggregator ahead of the US midterms to help Internet users get a handle on manipulative politically-charged content that might be hitting their feeds.

In the EU the European Commission has responded to rising concern about the impact of online disinformation on democratic processes by stepping up pressure on platforms and the adtech industry — issuing monthly progress reports since January after the introduction of a voluntary code of practice last year intended to encourage action to squeeze the spread of manipulative fakes. Albeit, so far these ‘progress’ reports have mostly boiled down to calls for less foot-dragging and more action.

One tangible result last month was Twitter introducing a report option for misleading tweets related to voting ahead of the EU vote, though again you have to wonder what took it so long given that online election interference is hardly a new revelation. (The OII study is also just the latest piece of research to bolster the age old maxim that falsehoods fly and the truth comes limping after.)

The study also examined how junk news spread on Twitter during the pre-EU election period, with the researchers finding that less than 4% of sources circulating on Twitter’s platform were junk news (or “known Russian sources”) — with Twitter users sharing far more links to mainstream news outlets overall (34%) over the study period.

Although the Polish language sphere was an exception — with junk news making up a fifth (21%) of EU election-related Twitter traffic in that outlying case.

Returning to Facebook, while the researchers do note that many more users interact with mainstream content overall via its platform, noting that mainstream publishers have a higher following and so “wider access to drive activity around their content” and meaning their stories “tend to be seen, liked, and shared by far more users overall”, they also point out that junk news still packs a greater per story punch — likely owing to the use of tactics such as clickbait, emotive language, and outragemongering in headlines which continues to be shown to generate more clicks and engagement on social media.

It’s also of course much quicker and easier to make some shit up vs the slower pace of doing rigorous professional journalism — so junk news purveyors can get out ahead of news events also as an eyeball-grabbing strategy to further the spread of their cynical BS. (And indeed the researchers go on to say that most of the junk news sources being shared during the pre-election period “either sensationalized or spun political and social events covered by mainstream media sources to serve a political and ideological agenda”.)

“While junk news sites were less prolific publishers than professional news producers, their stories tend to be much more engaging,” they write in a data memo covering the study. “Indeed, in five out of the seven languages (English, French, German, Spanish, and Swedish), individual stories from popular junk news outlets received on average between 1.2 to 4 times as many likes, comments, and shares than stories from professional media sources.

“In the German sphere, for instance, interactions with mainstream stories averaged only 315 (the lowest across this sub-sample) while nearing 1,973 for equivalent junk news stories.”

To conduct the research the academics gathered more than 584,000 tweets related to the European parliamentary elections from more than 187,000 unique users between April 5 and April 20 using election-related hashtags — from which they extracted more than 137,000 tweets containing a URL link, which pointed to a total of 5,774 unique media sources.

Sources that were shared 5x or more across the collection period were manually classified by a team of nine multi-lingual coders based on what they describe as “a rigorous grounded typology developed and refined through the project’s previous studies of eight elections in several countries around the world”.

Each media source was coded individually by two separate coders, via which technique they say was able to successfully label nearly 91% of all links shared during the study period.

The five most popular junk news sources were extracted from each language sphere looked at — with the researchers then measuring the volume of Facebook interactions with these outlets between April 5 and May 5, using the NewsWhip Analytics dashboard.

They also conducted a thematic analysis of the 20 most engaging junk news stories on Facebook during the data collection period to gain a better understanding of the different political narratives favoured by junk news outlets ahead of an election.

On the latter front they say the most engaging junk narratives over the study period “tend to revolve around populist themes such as anti-immigration and Islamophobic sentiment, with few expressing Euroscepticism or directly mentioning European leaders or parties”.

Which suggests that EU-level political disinformation is a more issue-focused animal (and/or less developed) — vs the kind of personal attacks that have been normalized in US politics (and were richly and infamously exploited by Kremlin-backed anti-Clinton political disinformation during the 2016 US presidential election, for example).

This is likely also because of a lower level of political awareness attached to individuals involved in EU institutions and politics, and the multi-national state nature of the pan-EU project — which inevitably bakes in far greater diversity. (We can posit that just as it aids robustness in biological life, diversity appears to bolster democratic resilience vs political nonsense.)

The researchers also say they identified two noticeable patterns in the thematic content of junk stories that sought to cynically spin political or social news events for political gain over the pre-election study period.

“Out of the twenty stories we analysed, 9 featured explicit mentions of ‘Muslims’ and the Islamic faith in general, while seven mentioned ‘migrants’, ‘immigration’, or ‘refugees’… In seven instances, mentions of Muslims and immigrants were coupled with reporting on terrorism or violent crime, including sexual assault and honour killings,” they write.

“Several stories also mentioned the Notre Dame fire, some propagating the idea that the arson had been deliberately plotted by Islamist terrorists, for example, or suggesting that the French government’s reconstruction plans for the cathedral would include a minaret. In contrast, only 4 stories featured Euroscepticism or direct mention of European Union leaders and parties.

“The ones that did either turned a specific political figure into one of derision – such as Arnoud van Doorn, former member of PVV, the Dutch nationalist and far-right party of Geert Wilders, who converted to Islam in 2012 – or revolved around domestic politics. One such story relayed allegations that Emmanuel Macron had been using public taxes to finance ISIS jihadists in Syrian camps, while another highlighted an offer by Vladimir Putin to provide financial assistance to rebuild Notre Dame.”

Taken together, the researchers conclude that “individuals discussing politics on social media ahead of the European parliamentary elections shared links to high-quality news content, including high volumes of content produced by independent citizen, civic groups and civil society organizations, compared to other elections we monitored in France, Sweden, and Germany”.

Which suggests that attempts to manipulate the pan-EU election are either less prolific or, well, less successful than those which have targeted some recent national elections in EU Member States. And logic would suggest that co-ordinating election interference across a 28-Member State bloc does require greater co-ordination and resource vs trying to meddle in a single national election — on account of the multiple countries, cultures, languages and issues involved.

We’ve reached out to Facebook for comment on the study’s findings. Update: A company spokesperson has now sent the following statement:

We’ve been working hard to stop the spread of false news. Actors seeking to profit from misinformation are highly motivated and continue to employ new tactics to garner clicks, so it’s possible to pick out specific examples of things we miss and there will occasionally be false news posts that perform well–but what we’re really interested in is the overall amount of misinformation on Facebook, and whether that’s trending down. By Oxford’s own admission, overall, mainstream media coverage of the EU elections performed better than “junk news” on Facebook, both in terms of publisher following and engagement.

The company has put a heavy focus on publicizing its self-styled ‘election security’ efforts ahead of the EU election. Though it has mostly focused on setting up systems to control political ads — whereas junk news purveyors are simply uploading regular Facebook ‘content’ at the same time as wrapping it in bogus claims of ‘journalism’ — none of which Facebook objects to. All of which allows would-be election manipulators to pass off junk views as online news, leveraging the reach of Facebook’s platform and its attention-hogging algorithms to amplify hateful nonsense. While any increase in engagement is a win for Facebook’s ad business, so er…",Social Media,TechCrunch,https://techcrunch.com/2019/05/21/facebook-still-a-great-place-to-amplify-pre-election-junk-news-eu-study-finds/,"The study finds that junk news outlets are able to outperform professionally produced news stories on Social Media, with individual stories from these outlets receiving as much as 4x the volume of Facebook shares, likes, and comments. This can have a serious impact on democratic processes, as it allows for the spread of political misinformation and hateful content at a large","Information, Discourse & Governance"
49,The U.S. secretary of state was fired. On Twitter. – TechCrunch,"President Donald Trump just fired his embattled secretary of state, Rex Tillerson, via Twitter this morning, replacing him with former Tea Party Congressman and current Central Intelligence Agency chief Mike Pompeo.

Mike Pompeo, Director of the CIA, will become our new Secretary of State. He will do a fantastic job! Thank you to Rex Tillerson for his service! Gina Haspel will become the new Director of the CIA, and the first woman so chosen. Congratulations to all! — Donald J. Trump (@realDonaldTrump) March 13, 2018

Apparently, it’s completely normal (under the current administration) to announce major policy decisions and staffing changes that impact national security and international diplomacy alongside exhortations to “get out the vote” and lavish self-praise for the state of the economy.

The Economy is raging, at an all time high, and is set to get even better. Jobs and wages up. Vote for Rick Saccone and keep it going! — Donald J. Trump (@realDonaldTrump) March 13, 2018

That tweet immediately preceded Tillerson’s ignominious departure from government service. And the announcement of Tillerson’s ouster was followed by a return to the regularly scheduled publicity tour to showcase the Mexican border wall boondoggle.

Heading to see the BORDER WALL prototypes in California! pic.twitter.com/fU6Ukc271l — Donald J. Trump (@realDonaldTrump) March 13, 2018

Tillerson, who has had a rocky tenure as secretary of state nearly from the outset, was first informed of is imminent ouster with a heads-up call from White House chief of staff John Kelly (also on shaky ground). The New York Times is reporting that Kelly asked Tillerson to cut short his trip to Africa and advised him last Friday that he “may get a tweet.”

It seems that Twitter is now the preferred medium of communication among all White House and cabinet-level staff these days. The undersecretary of state for public diplomacy and affairs took to the service to issue what is, so far, Tillerson’s only response.

The Secretary did not speak to the President this morning and is unaware of the reason, but he is grateful for the opportunity to serve, and still believes strongly that public service is a noble calling and not to be regretted. We wish Secretary-Designate Pompeo well — Under Secretary of State (@UnderSecPD) March 13, 2018

Statement from Under Secretary of State Steve Goldstein: The Secretary had every intention of remaining because of the tangible progress made on critical national security issues. He established and enjoyed relationships with his counterparts. — Under Secretary of State (@UnderSecPD) March 13, 2018

By any measure, Tillerson’s turn at the helm of the State Department was an unmitigated disaster, according to most observers. His policy positions on everything from nuclear proliferation to dealing with Syria, Qatar and the Kremlin were undermined or overturned by the White House.

Here’s a good synopsis from Vox:

The US bombed Syrian dictator Bashar al-Assad in early April — just days after Tillerson suggested the administration would be fine with Assad staying in power. On June 9, Tillerson called on Saudi Arabia and its allies to end their isolation of Qatar; less than two hours later, Trump sided with the Saudis by labeling Qatar “a funder of terrorism at a very high level.” On July 20, after a meeting in which the president reportedly asked for a major expansion of America’s nuclear arsenal, Tillerson told aides that the president was a moron — or, according to some reports, a “fucking moron.” One time, Tillerson tried to open the door to negotiations with North Korea — and Trump slapped him down in a tweet.

Under Tillerson’s tenure, the State Department was also hollowed out. Political appointees were never approved to leadership roles and the rank and file staffers fled the increasingly hamstrung department.",Social Media,TechCrunch,https://techcrunch.com/2018/03/13/the-u-s-secretary-of-state-was-fired-on-twitter/,"When President Trump used Twitter to fire Secretary of State Rex Tillerson, it highlighted the diminished role of the State Department under Tillerson's tenure and the extraordinary way in which major policy decisions and personnel changes are now being announced: via tweets by the president. This approach to governing has been criticized for undermining diplomacy and hollowing out the State Department.",Politics
50,Instagram’s Adam Mosseri to meet UK health secretary over suicide content concerns – TechCrunch,"The still fresh-in-post boss of Instagram, Adam Mosseri, has been asked to meet the UK’s health secretary, Matt Hancock, to discuss the social media platform’s handling of content that promotes suicide and self harm, the BBC reports.

Mosseri’s summons follows an outcry in the UK over disturbing content being recommended to vulnerable users of Instagram, following the suicide of a 14 year old schoolgirl, Molly Russell, who killed herself in 2017.

After her death, Molly’s family discovered she had been following a number of Instagram accounts that encouraged self-harm. Speaking to the BBC last month Molly’s father said he did not doubt the platform had played a role in her decision to kill herself.

Writing in the Telegraph newspaper today, Mosseri makes direct reference to Molly’s tragedy, saying he has been “deeply moved” by her story and those of other families affected by self-harm and suicide, before going on to admit that Instagram is “not yet where we need to be on the issues”.

“We rely heavily on our community to report this content, and remove it as soon as it’s found,” he writes, conceding that the platform has offloaded the lion’s share of responsibility for content policing onto users thus far. “The bottom line is we do not yet find enough of these images before they’re seen by other people,” he admits.

Mosseri then uses the article to announce a couple of policy changes in response to the public outcry over suicide content.

Beginning this week, he says Instagram will begin adding “sensitivity screens” to all content it reviews which “contains cutting”. “These images will not be immediately visible, which will make it more difficult for people to see them,” he suggests.

Though that clearly won’t stop fresh uploads from being distributed unscreened. (Nor prevent young and vulnerable users clicking to view disturbing content regardless.)

Mosseri justifies Instagram’s decision not to blanket-delete all content related to self-harm and/or suicide by saying its policy is to “allow people to share that they are struggling even if that content no longer shows up in search, hashtags or account recommendations”.

“We’ve taken a hard look at our work and though we have been focused on the individual who is vulnerable to self harm, we need to do more to consider the effect of self-harm images on those who may be inclined to follow suit,” he continues. “This is a difficult but important balance to get right. These issues will take time, but it’s critical we take big steps forward now. To that end we have started to make changes.”

Another policy change he reveals is that Instagram will stop its algorithms actively recommending additional self-harm content to vulnerable users. “[F]or images that don’t promote self-harm, we let them stay on the platform, but moving forward we won’t recommend them in search, hashtags or the Explore tab,” he writes.

Unchecked recommendations have opened Instagram up to accusations that it essentially encourages depressed users to self-harm (or even suicide) by pushing more disturbing content into their feeds once they start to show an interest.

So putting limits on how algorithms distribute and amplify sensitive content is an obvious and overdue step — but one that’s taken significant public and political attention for the Facebook-owned company to make.

Last year the UK government announced plans to legislate on social media and safety, though it has yet to publish details of its plans (a white paper setting out platforms’ responsibilities is expected in the next few months). But just last week a UK parliamentary committee also urged the government to place a legal ‘duty of care’ on platforms to protect minors.

In a statement given to the BBC, the Department for Digital, Culture, Media and Sport confirmed such a legal duty remains on the table. “We have heard calls for an internet regulator and to place a statutory ‘duty of care’ on platforms, and are seriously considering all options,” it said.

There’s little doubt that the prospect of safety-related legislation incoming in a major market for the platform — combined with public attention on Molly’s tragedy — has propelled the issue to the top of the Instagram chief’s inbox.

Mosseri writes now that Instagram began “a comprehensive review last week” with a focus on “supporting young people”, adding that the revised approach entails reviewing content policies, investing in technology to “better identify sensitive images at scale” and applying measures to make such content “less discoverable”.

He also says it’s “working on more ways” to link vulnerable users to third party resources, such as by connecting them with organisations it already works with on user support, such as Papyrus and Samaritans. But he concedes the platform needs to “do more to consider the effect of self-harm images on those who may be inclined to follow suit” — not just on the poster themselves.

“This week we are meeting experts and academics, including Samaritans, Papyrus and Save.org, to talk through how we answer these questions,” he adds. “We are committed to publicly sharing what we learn. We deeply want to get this right and we will do everything we can to make that happen.”

We’ve reached out to Facebook, Instagram’s parent, for further comment.

One way user-generated content platforms could support the goal of better understanding impacts of their own distribution and amplification algorithms is to provide high quality data to third party researchers so they can interrogate platform impacts.

That was another of the recommendations from the UK’s science and technology committee last week. But it’s not yet clear whether Mosseri’s commitment to sharing what Instagram learns from meetings with academics and experts will also result in data flowing the other way — i.e. with the proprietary platform sharing its secrets with experts so they can robustly and independently study social media’s antisocial impacts.

Recommendation algorithms lie at center of many of social media’s perceived ills — and the problem scales far beyond any one platform. YouTube’s recommendation engines have, for example, also long been criticized for having a similar ‘radicalizating’ impact — such as by pushing viewers of conservative content to far more extreme/far right and/or conspiracy theorist views.

With the huge platform power of tech giants in the spotlight, it’s clear that calls for increased transparency will only grow — unless or until regulators make access to and oversight of platforms’ data and algorithms a legal requirement.",Social Media,TechCrunch,https://techcrunch.com/2019/02/04/instagrams-adam-mosseri-to-meet-uk-health-secretary-over-suicide-content-concerns/,"Social Media has been criticized for pushing users toward dangerous and extreme content, such as promoting suicide or radicalizing users to far-right or conspiracy theorist views. Adam Mosseri, the CEO of Instagram, is attempting to address the issue by introducing policy changes, such as adding sensitivity screens to images containing cutting and no longer recommending self-harm content",Security & Privacy
51,Facebook is facing an EU data probe over fake ads – TechCrunch,"The UK’s privacy watchdog has asked Facebook’s lead EU regulator to look into ongoing data protection concerns about its ad platform — including how its platform is being used to target and spread fake adverts to try to manipulate voters.

Facebook’s international HQ is in Ireland so the regulator in play here is the Irish Data Protection Commission.

The ICO noted the action in a 113-page report to parliament yesterday giving an update on its long-running investigation into the use of data analytics in political campaigns — writing:

We have referred our ongoing concerns about Facebook’s targeting functions and techniques that are used to monitor individuals’ browsing habits, interactions and behaviour across the internet and different devices to the to the IDPC. Under the GDPR, the IDPC is the lead authority for Facebook in the EU. We will work with both the Irish regulator and other national data protection authorities to develop a longterm strategy on how we address these issues.

A spokesperson for the watchdog told us these concerns fall outside the remit of that still partially ongoing investigation, which was triggered by the Cambridge Analytica data misuse scandal.

So the issues of concern are not the same issues that the ICO fined Facebook for last month, when it handed the company the maximum possible penalty under the UK’s previous data protection regime. Hence the referral to the Irish DPC.

We’ve reached out to Facebook for comment on the referral.

A spokesman for the Irish regulator told us: “The DPC has yet to receive any information from the ICO.”

Giving one example of its concerns, the ICO’s spokesperson pointed to recent news reports flagging fake political ads that had passed Facebook’s checks and been able to circulate on the platform — until being spotted by journalists, after which they got pulled by Facebook.

Hello @Facebook. Shall we talk about how your new ad transparency ""rules""?

Here's a pro-Brexit advert placed two days ago. It was ""paid for by Cambridge Analytica"". Posted by ""Insider Research Group"". And uses an image by the disgraced law-breaking campaign group, BeLeave… pic.twitter.com/8jnK2d2WfL — Carole Cadwalladr (@carolecadwalla) October 31, 2018

Responding to the above ad, badged as being paid for by the now defunct and disgraced data company Cambridge Analytica, Facebook said: “This ad was not created by Cambridge Analytica. It is fake, violates our policies and has been taken down. We believe people on Facebook should know who is behind the political ads they’re seeing which is why we are creating the Ads Library so that you can see who is accountable for any political ad. We have tools for anyone to report suspicious activity such as this.”

Such an obvious fake slipping through Facebook’s checks on political ads — which were only rolled out in the UK a few weeks ago, in first phase form — suggests they can be trivially gamed.

In related news, the Guardian reports that Facebook has delayed a requirement that UK political advertisers verify their identity — pushing it back from an initial deadline of today to sometime in “the next month”, with the company saying it wants to take more time to strengthen the system after a spate of failures.

“We have learnt that some people may try to game the disclaimer system by entering inaccurate details and have been working to improve our review process to detect and prevent this kind of abuse,” a Facebook spokesperson told the newspaper.

The fake ads issue also highlights how self-styled ‘transparency’ without proper accountability can just further muddy already murky waters — where masses of personal data and opaque ad platforms are concerned.

During a hearing in front of the UK’s DCMS committee yesterday, the UK’s information commissioner, Elizabeth Denham, also raised concerns about the use of so-called ‘lookalike audiences’ for targeting voters on Facebook — saying a system that makes inferences in order to target people with political ads needs to be looked at closely in light of Europe’s new GDPR privacy framework.

She also told policymakers that Facebook needs to change its business model. And said all platforms “need to take much greater responsibility”.

“I don’t think that we want to use the same model that sells us holidays and shoes and cars to engage with people and voters. I think that people expect more than that. This is a time for a pause, to look at codes, to look at the practices of social media companies, to take action where they’ve broken the law,” she said.

Committee members raised some of their own political ad concerns with Denham, querying the lawfulness of a crop of ads recently circulating on Facebook, targeting MPs and their constituents, urging policymakers to ‘chuck chequers’ — a reference to the UK prime minister’s current Brexit proposal to the EU — which are badged as being paid for by an organization called ‘Mainstream Network’, without it being clear who on earth is behind that…

https://t.co/XDZwvfNfwt Who's behind the Mainstream Network which has spent £250k on ""chuck Chequers"" Facebook ads? No clue on their website — Rory Cellan-Jones (@ruskin147) October 20, 2018

“We are investigating those matters and will be looking at whether or not there was a contravention of the GDPR by that organization in sending out those communications,” Denham told the committee.

But wider concerns about how Facebook’s ad platform operates have now been handed over to the Irish DPC to investigate — a far smaller, less well resourced watchdog than the ICO; the largest such agency in Europe.

Any future audit of Facebook’s platform — as has been recently called for by the EU parliament — would also be led by Ireland, Denham confirmed to the committee.

She was asked whether she had any concerns about the smaller regulator being able to handle its burgeoning caseload. “We can work with,” she replied, noting the ICO likely has greater capacity to conduct technical audits. “We certainly can support them and work with them.”

She noted too that the newly established European Data Protection Board — which is responsible for ensuring consistency in the application of the GDPR — is working on “a more holistic way” to co-ordinate regulating social media platforms across Europe.

“[It] is looking at… what we need to do as a community with Facebook and other social media platforms,” she told the committee, adding that under the GDPR the Irish DPC is the “lead authority on Facebook because that’s where Facebook is based in Europe so they would the lead on an audit that’s going forward in the future”.

“Regulators need to look at the effectiveness of their processes,” she added. “That’s really at the heart of this — and there’s a fundamental tension between the advertising business model of Facebook and fundamental rights like protection of privacy. And that’s where we’re at right now.

“It’s a very big job both for the regulators but for the policymakers to ensure that the right requirements and oversight and sanctions are in place.”",Social Media,TechCrunch,https://techcrunch.com/2018/11/07/facebook-is-facing-an-eu-data-probe-over-fake-ads/,"The UK's privacy watchdog has raised concerns about the use of Facebook's ad platform, including how it is being used to target and spread fake ads in an effort to manipulate voters. The issue has been referred to the Irish Data Protection Commission, given that Facebook's international HQ is based in Ireland. The regulator will look into the data protection concerns",Security & Privacy
52,UK report highlights changing gadget habits — and our need for an online fix – TechCrunch,"UK report highlights changing gadget habits — and our need for an online fix

A look back at the past decade of consumer technology use in the UK has shone a light on changing gadget habits, underlining how Brits have gone from being smartphone dabblers back in 2008 when a top-of-the-range smartphone cost ~£500 to true addicts in today’s £1k+ premium smartphone era.

The report also highlights what seems to be, at times, a conflicted relationship between Brits and the Internet.

While nine in ten people in the UK have home access to the Internet, here in 2018, some web users report feeling being online is a time-sink or a constraint on their freedom.

But even more said they feel lost or bored without it.

Over the past decade the Internet looks to have consolidated its grip on the spacetime that boredom occupied for the less connected generations that came before.

The overview comes via regulator Ofcom’s 2018 Communications Market report. The full report commenting on key market developments in the country’s communications sector is a meaty, stat and chart-filled read.

The regulator has also produced a 30-slide interactive version this year.

Commenting on the report findings in a statement, Ian Macrae, Ofcom’s director of market intelligence, said: “Over the last decade, people’s lives have been transformed by the rise of the smartphone, together with better access to the Internet and new services. Whether it’s working flexibly, keeping up with current affairs or shopping online, we can do more on the move than ever before.

“But while people appreciate their smartphone as their constant companion, some are finding themselves feeling overloaded when online, or frustrated when they’re not.”

We’ve pulled out some highlights from the report below…

Less than a fifth (17%) of UK citizens owned a smartphone a decade ago; the figure now stands at 78% — and a full 95% of 16-24 year-olds . So, yeah, kids don’t get called digital natives for nothin’

. So, yeah, kids don’t get called digital natives for nothin’ People in the UK check their smartphones, on average, every 12 minutes of the waking day . (‘Digital wellbeing’ tools clearly have their work cut out to kick against this grain… )

. (‘Digital wellbeing’ tools clearly have their work cut out to kick against this grain… ) Ofcom found that two in five adults (40%) first look at their phone within five minutes of waking up (rising to 65% of the under 35s). While around a third (37%) of adults check their phones five minutes before lights out (again rising to 60% of under-35s). Shame it didn’t also ask how well people are sleeping

(rising to 65% of the under 35s). While around a third (again rising to 60% of under-35s). Shame it didn’t also ask how well people are sleeping Contrary to a decade ago, most UK citizens say they need and expect a constant Internet connection wherever they go. Two thirds of adults (64%) say it’s an essential part of their life. One in five adults (19%) say they spend more than 40 hours a week online , up from 5% just over ten years ago

, up from 5% just over ten years ago Three quarters (74%) of people say being online keeps them close to friends and family. Two fifths (41%) say it enables them to work more flexibly

Smartphone screen addicts, much?

Seventy-two per cent of adults say their smartphone is their most important device for accessing the Internet; 71% say they never turn off their phone ; and 78% say they could not live without it

; and Ofcom found the amount of time Brits spend making phone calls from mobiles has fallen for the first time — using a mobile for phone calls is only considered important by 75% of smartphone users vs 92% who consider web browsing on a smartphone to be important (and indeed the proportion of people accessing the Internet on their mobile has increased from 20% almost a decade ago to 72% in 2018)

— using a mobile for phone calls is only considered important by 75% of smartphone users vs (and indeed the proportion of people accessing the Internet on their mobile has increased from 20% almost a decade ago to 72% in 2018) The average amount of time spent online on a smartphone is 2 hours 28 minutes per day. This rises to 3 hours 14 minutes among 18-24s

Social and emotional friction, plus the generation gap…

On the irritation front, three quarters of people (76%) find it annoying when someone is listening to music, watching videos or playing games loudly on public transport; while an impressive 81% object to people using their phone during meal times

TV is another matter though. The majority (53%) of adults say they are usually on their phone while watching TV with others . There’s a generation gap related to social acceptance of this though: With a majority (62%) of people over the age of 55 thinking it’s unacceptable — dropping to just two in ten (21%) among those aged 18-34

. There’s a generation gap related to social acceptance of this though: With a majority (62%) of people over the age of 55 thinking it’s unacceptable — dropping to just two in ten (21%) among those aged 18-34 Ofcom also found that significant numbers of people saying the online experience has negative effects. Fifteen per cent agree it makes them feel they are always at work, and more than half (54%) admit that connected devices interrupt face-to-face conversations with friends and family — which does offer a useful counterpoint to social media giant’s shiny marketing claims that their platforms ‘connect people’ (the truth is more they both connect & disconnect). While more than two in five (43%) also admit to spending too much time online

— which does offer a useful counterpoint to social media giant’s shiny marketing claims that their platforms ‘connect people’ (the truth is more they both connect & disconnect). While more than two in five Around a third of people say they feel either cut off (34%) or lost (29%) without the Internet, and if they can’t get online, 17% say they find it stressful. Half of all UK adults (50%) say their life would be boring if they could not access the Internet

On the flip side, a smaller proportion of UK citizens view a lack of Internet access in a positive light. One in ten says they feel more productive offline (interestingly this rises to 15% for 18-34 year-olds); while 10% say they find it liberating; and 16% feel less distracted

The impact of (multifaceted and increasingly powerful and capable) smartphones can also be seen on some other types of gadgets. Though TV screens continue to compel Brits (possibly because they feel it’s okay to keep using their smartphones while sitting in front of a bigger screen… )

Ofcom says ownership of tablets (58% of UK households) and games consoles (44% of UK adults) has plateaued in the last three years

and has plateaued in the last three years Desktop PC ownership has declined majorly over the past decade — from a large majority (69%) of households with access in 2008 to less than a third (28%) in 2018

over the past decade — from a large majority (69%) of households with access in 2008 to less than a third (28%) in 2018 As of 2017, smart TVs were in 42% of households — up from just 5% in 2012

— up from just 5% in 2012 Smart speakers weren’t around in 2008 but they’ve now carved out a space in 13% of UK households

One in five households (20%) report having some wearable tech (smart watches, fitness trackers). So smart speakers look to be fast catching up with fitness bands

BBC mightier than Amazon …

BBC website visitor numbers overtook those of Amazon in the UK in 2018. Ofcom found the BBC had the third-highest number of users after Google and Facebook

Ofcom also found that six in ten people have used next-day delivery for online purchases, but only three in ten have used same-day delivery in 2018. So most Brits are, seemingly, content to wait until tomorrow for ecommerce purchases — rather than demanding their stuff right now

What else are UK citizens getting up to online? More of a spread of stuff than ever, it would appear…

Less general browsing/surfing than last year, though it’s still the most popular reported use for Internet activity (69% saying they’ve done this in the past week vs 80% who reported the same in 2017)

though it’s still the most popular reported use for Internet activity (69% saying they’ve done this in the past week vs 80% who reported the same in 2017) Sending and receiving email is also still a big deal — but also on the slide (66% reporting doing this in the past week vs 76% in 2017)

— but also on the slide (66% reporting doing this in the past week vs 76% in 2017) Social media use is another popular but slightly less so use-case than last year (50% in 2017 down to 45% in 2018). (Though Twitter bucks the trend with a percentage point usage bump (13% -> 14%) though it’s far less popular overall)

than last year (50% in 2017 down to 45% in 2018). (Though Twitter bucks the trend with a percentage point usage bump (13% -> 14%) though it’s far less popular overall) Instant messaging frequency also dropped a bit (46% -> 41%)

As did TV/video viewing online (40% -> 36%), including for watching short video clips (31% to 28%)

Online shopping has also dropped a bit in frequency (48% -> 44%)

But accessing news has remained constant (36%)

(36%) Finding health information has seen marginal slight growth (22% -> 23%); ditto has finding/downloading information for work/college (32% -> 33%); using local council/government services (21% -> 23%); and playing games online/interactively (17% -> 18%)

Streaming audio services have got a bit more popular (podcasts, we must presume), with 15% reporting using them in the past week in 2017 up to 19% in 2018. Listening to the radio online is also up (13% -> 15%)

(podcasts, we must presume), with 15% reporting using them in the past week in 2017 up to 19% in 2018. (13% -> 15%) However uploading/adding content to the Internet has got a bit less popular, though (17% to 15%)

One more thing: Women in the UK are bigger Internet fans than men.

Perhaps contrary to some people’s expectations, women in the UK spend more time online on average than men across almost all age groups, with the sole exception being the over 55s (where the time difference is pretty marginal)…",Social Media,TechCrunch,https://techcrunch.com/2018/08/02/uk-report-highlights-changing-gadget-habits-and-our-need-for-an-online-fix/,"The UK's Ofcom report highlights the changing gadget habits of British citizens over the past decade, and points to the sometimes conflicted relationship between Brits and the Internet. It finds that smartphones have become essential for many, with people checking their phones on average every 12 minutes of the waking day, and 15% of people reporting that the online experience",User Experience & Entertainment
53,Changes to Facebook Graph Search leaves online investigators in a lurch – TechCrunch,"When Facebook Graph Search launched six years ago, it was meant to help users discover content across public posts on the platform. Since then, the feature stayed relatively low-profile for many users (its last major announcement was in 2014 when a mobile version was rolled out), but became a valuable tool for many online investigators who used it to collect evidence of human rights abuses, war crimes and human trafficking. Last week, however, many of them discovered that Graph Search features had suddenly been turned off, reports Vice.

Graph Search let users search in plain language (i.e. sentences written the way people talk, not just keywords), but more importantly, it also let them filter search results by very specific criteria. For example, users could find who had liked a page or photo, when someone had visited a city or if they had been in the same place at the same time with another person. Despite the obvious potential for privacy issues, Graph Search was also an important resource for organizations like Bellingcat, an investigative journalism website that used it to document Saudi-led airstrikes in Yemen for its Yemen Project.

Other investigators also used Graph Search to build tools like StalkScan, but the removal of Graph Search means they have had to suspend their services or offer them in a very limited capacity. For example, StalkScan’s website now has a notice that says:

“As of June 6th, you can scan only your own profile with this tool. After two years and 28M+ StalkScan sessions, Facebook decided to make the Graph Search less transparent. As usual, they did this without any communication or dialogue with activists and journalists that used it for legitimate purposes.The creepy graph search itself still exists, but is now less accessible and more difficult to use. Make sure to check yourself with this tool, since your data is still out there!”

Facebook may be trying to take a more cautious stance because it is still dealing with the fall out from several major security lapses, including the Cambridge Analytica data scandal, as well as the revelation earlier this year that it had stored hundreds of millions of passwords in plain text.

In a statement to Vice, a Facebook spokesperson said “The vast majority of people on Facebook search using keywords, a factor which led us to pause some aspects of graph search and focus more on improving keyword search. We are working closely with researchers to make sure they have the tools they need to use our platform.” But one of Vice’s sources, a current employee at Facebook, said within the company there is “lots of internal and external struggle between giving access to info so people can find friends or research things (like Bellingcat), and protecting it.”

TechCrunch has contacted Facebook for more information.",Social Media,TechCrunch,https://techcrunch.com/2019/06/10/changes-to-facebook-graph-search-leaves-online-investigators-in-a-lurch/,"The removal of Graph Search from Facebook has had a major impact on online investigators who used it to uncover evidence of human rights abuses, war crimes and human trafficking, as well as services like StalkScan, which have now had to suspend or limit their services.",Equality & Justice
54,Facebook urged to offer an API for political ad transparency research – TechCrunch,"Facebook has been called upon to provide good faith researchers with an API to enable them to study how political ads are spreading and being amplified on its platform.

A coalition of European academics, technologists and human and digital rights groups, led by Mozilla, has signed an open letter to the company demanding far greater transparency about how Facebook’s platform distributes and amplifies political ads ahead of elections to the European parliament which will take place in May.

We’ve reached out to Facebook for a reaction to the open letter.

The company had already announced it will launch some of its self-styled ‘election security’ measures in the EU before then — specifically an authorization and transparency system for political ads.

Last month its new global comms guy — former European politician and one time UK deputy prime minister, Nick Clegg — also announced that, from next month, it will have human-staffed operations centers up and running to monitor how localised political news gets distributed on its platform, with one of the centers located within the EU, in Dublin, Ireland.

But signatories to the letter argue the company’s heavily PR’ed political ad transparency measures don’t go far enough.

They also point out that some of the steps Facebook has taken have blocked independent efforts to monitor its political ad transparency claims.

Last month the Guardian reported on changes Facebook had made to its platform that restricted the ability of an external political transparency campaign group, called WhoTargetsMe, to monitor and track the flow of political ads on its platform.

The UK-based campaign group is one of more than 30 groups that have signed the open letter — calling for Facebook to stop what they couch as “harassment of good faith researchers who are building tools to provide greater transparency into the advertising on your platform”.

Other signatories include the Center for Democracy and Technology, the Open Data Institute and Reporters Without Borders.

“By restricting access to advertising transparency tools available to Facebook users, you are undermining transparency, eliminating the choice of your users to install tools that help them analyse political ads, and wielding control over good faith researchers who try to review data on the platform,” they write.

“Your alternative to these third party tools provides simple keyword search functionality and does not provide the level of data access necessary for meaningful transparency.”

The letter calls on Facebook to roll out “a functional, open Ad Archive API that enables advanced research and development of tools that analyse political ads served to Facebook users in the EU” — and do so by April 1, to enable external developers to have enough time to build transparency tools before the EU elections.

Signatories also urge the company to ensure that all political ads are “clearly distinguished from other content”, as well as being accompanied by “key targeting criteria such as sponsor identity and amount spent on the platform in all EU countries”.

Last year UK policymakers investigating the democratic impacts of online disinformation pressed Facebook on the issue of what the information it provides users about the targeting criteria for political ads. They also asked the company why it doesn’t offer users a complete opt-out from receiving political ads. Facebook’s CTO Mike Schroepfer was unable — or unwilling — to provide clear answers, instead choosing to deflect questions by reiterating the tidbits of data that Facebook has decided it will provide.

Close to a year later and Facebook users in the majority of European markets are still waiting for even a basic layer of political transparency, as the company has been allowed to continue self regulating at its own pace and — crucially — by getting to define what ‘transparency’ means (and therefore how much of the stuff users get).

Facebook launched some of these self-styled political ad transparency measures in the UK last fall — adding ‘paid for by’ disclaimers, and saying ads would be retained in an archive for seven years. (Though its verification checks had to be revised after they were quickly shown to be trivially easy to circumvent.)

Earlier in the year it also briefly suspended accepting ads paid for by foreign entities during a referendum on abortion in Ireland.

However other European elections — such as regional elections — have taken place without Facebook users getting access to any information about the political ads they’re seeing or who’s paying for them.

The EU’s executive body has its eye on the issue. Late last month the European Commission published the first batch of monthly ‘progress reports’ from platforms and ad companies that signed up to a voluntary code of conduct on political disinformation that was announced last December — saying all signatories need to do a lot more and fast.

On Facebook specifically, the Commission said it needs to provide “greater clarity” on how it will deploy consumer empowerment tools, and also boost its cooperation with fact-checkers and the research community across the whole EU — with commissioner Julian King singling the company out for failing to provide independent researchers with access to its data.

Today’s open letter from academics and researchers backs up the Commission’s assessment of feeble first efforts from Facebook and offers further fuel to feed its next monthly assessment.

The Commission has continued to warn it could legislate on the issue if platforms fail to step up their efforts to tackle political disinformation voluntarily.

Pressuring platforms to self-regulate has its own critics too, of course — who point out that it does nothing to tackle the core underlying problem of platforms having too much power in the first place…

The problem isn’t Zuckerberg makes poor decisions on behalf of the public. It’s that he’s making them in the first place. — Matt Stoller (@matthewstoller) February 11, 2019

Update: In a statement responding to the open letter, attributed to Rob Leathern, its director of product management, Facebook said:",Social Media,TechCrunch,https://techcrunch.com/2019/02/11/facebook-urged-to-offer-an-api-for-political-ad-transparency-research/,"The open letter calls attention to the undesirable consequences of social media, such as the lack of transparency in political advertising and the power of platforms to spread disinformation. It highlights the need for greater transparency, independent research, and consumer empowerment tools to help hold social media companies accountable.",Politics
55,Why I’m Deleting All My Old Tweets,"While I gave birth to my first child in 2015, my brother sat across the street from the hospital in a bar, live tweeting his experience of waiting to meet his nephew. As the hours of my long labor wore on, my brother got drunker and his jokes more off the wall. When my son was finally born and I went to send an email birth announcement, I found that everyone already knew. Emails had flooded my inbox already congratulating me. My colleagues had all been following along with my brother’s live tweets. The second my son was born, word had reached them—and thousands of other people.

Unless you, too, were following my brother on Twitter that day, you’d have no way to verify THAT this story happened. I assure you it did, but you can’t go back and look at his tweets about it because they’ve long since been deleted. Like many other prolific tweeters, my brother---the person who first introduced me to Twitter, who is the most devout Twitter user I know, and whose entire living is made running the social media accounts for a major media company---has joined the ranks of the tweet deleters.

In fact, over the past few months, many of the journalists I follow have casually mentioned that they have timers on their accounts set to delete their old tweets. “Why wouldn’t you delete them?” tweeted one, in a tweet I can no longer find because it has since been deleted.

At first, I was aghast. If something happens on Twitter but then gets deleted, did it even happen? Deleting it is an affront to history! Isn’t Twitter a sacred record of our [checks notes] … inane thoughts and bad jokes? Oh wait, maybe I do get it.

There are practical reasons to delete your tweets. Increasingly, old tweets are being used as ammunition to get their owners fired or ruin their reputation by people with an ax to grind. We’ve seen this recently with the film director James Gunn. After Gunn criticized President Trump and Republicans online earlier this month, far-right figures and outlets like the Daily Caller dug up old posts he wrote that mentioned pedophilia and rape. The director apologized for his “offensive” and “shocking” jokes, and though the tweets were a decade old, Disney found them objectionable enough to fire him.

It’s hardly a new trend. The playbook used to take down Gunn was honed years ago during Gamergate, with its attacks against women gaming professionals and media companies. Last year, far-right trolls took a satirical tweet from journalist Sam Seder and used it to get him fired from MSNBC; the network later rehired him when they understood he’d been the target of a coordinated campaign. Using Donald Trump's old tweets as proof of his hypocrisy (or his penchant for accusing others of the things he himself may be guilty of) is practically liberals' favorite pastime.

Twitter is a reaction to stimulus. Once that stimulus is gone, though, the tweets linger, like a too-loud laugh at a joke no one else heard.

That kind of bad-faith trolling is not to be confused with the work of people like CNN's Andrew Kaczynski, who has mined the old social media posts of public officials to shed light on beliefs that are pertinent to their jobs. Most recently, he reported on a Veteran Affairs official who spread birther conspiracy theories and made anti-Muslim comments; he has written about Democrats, too.

It's especially easy for trolls to weaponize old tweets and use them against their enemies because of the nature of the platform itself: Once tweets have been sent, they exist out of context. There’s no way to easily tell, when looking back at someone’s timeline from years ago, what jokes were trending, what the national mood was like, what everyone was faux-outraged by. Twitter is a reaction to stimulus. Once that stimulus is gone, though, the tweets linger, like a too-loud laugh at a joke no one else heard. What was funny? Who knows. We’re all going to die anyway.",Social Media,WIRED,https://www.wired.com/story/im-deleting-all-my-old-tweets/,Social media can be weaponized against users when trolls use old tweets out of context to get someone fired or ruin their reputation. This is a risk that users have to consider when deciding whether or not to delete their old tweets.,Security & Privacy
56,WhatsApp reportedly testing anti-chain letter spam warning – TechCrunch,"WhatsApp appears to be testing a new feature aimed at trying to limit the spread of chain letter style spam messages and tackle viral fakes spreading on its platform.

Two WhatsApp blog sites, Whatsappen.nl and WABetaInfo, report spotting notification messages in the app which appear intended to warns users that they are at risk of forwarding some spam — by displaying a notice that the message “has been forwarded many times”.

A similar warning is also shown displayed on a frequently forwarded message when it’s received, although the feature is described as still in development and it’s not clear whether WhatsApp intends to officially roll it out or not (WhatsApp owner Facebook frequently experiments with feature tweaks on its messaging products, especially in international markets; and — safe to say — not all its tests end up officially baked into the product).

At the time of writing the company had not responded to our requests for confirmation that it’s testing the anti-spam feature. Update: A WhatsApp spokesman declined to comment.

Chain letter messages that contain — at best — nonsense claims, and which urge other WhatApp users to forward the message to all their contacts to keep amplifying the spread are a staple sight on the platform (and indeed also on Facebook where users can get asked to copy-paste and repost a spam status message to spread it).

Such as the below example — which was circulating on the WhatsApp of a UK TechCrunch editor’s child — which creatively reimagines the CEO of the company as an individual called ‘Jim Balsamic’…

[gallery ids=""1587040,1587039""]

While these sorts of chain letters might seem like a pretty trivial form of fake news to worry about, the problem of false rumors spreading like lightning across WhatsApp’s platform has been linked to far more serious consequences.

In India, for example, fake rumors have reportedly triggered mob attacks that resulted in deaths and injuries.

Religious and caste tensions in India and Burma have also been stoked by fake posts spread via social media, as the Washington Post reported recently.

Other fake rumors spread via WhatsApp apparently thwarted a local government immunization drive.

Last year Facebook took out full page adverts in Indian newspapers telling people how to spot fake news (something it also did in Europe ahead of elections).

The problem is the people most likely to be convinced by false information are unlikely to be the same people buying and reading newspapers.

So stepping up technical anti-spam measures on its platforms to try to slow the spread of viral fakes seems like the responsible thing for the company to do given the very real risks attached to how its technology is accelerating misinformation.",Social Media,TechCrunch,https://techcrunch.com/2018/01/16/whatsapp-reportedly-testing-anti-chain-letter-spam-warning/,"Social media has been linked to serious consequences such as mob attacks, religious and caste tensions, and thwarting of local government immunization drives.",Security & Privacy
57,Study ties Facebook engagement to attacks on refugees – TechCrunch,"A study of circumstances and demographics attendant on attacks against refugees and immigrants in Germany has shown that Facebook use appears to be deeply linked with the frequency of violent acts. Far from being mere trolling or isolated expressions of controversial political opinions, spikes in anti-refugee posts were predictive of violent crimes against those groups.

The study was conducted by Karsten Müller and Carlo Schwarz of the University of Warwick. Their theory was that if country-wide waves of “right wing anti-refugee sentiment” result in subsequent waves of actual crime, these waves would travel the way any others do, via TV, word of mouth, radio and, of course, social media.

Now, if the anti-refugee rhetoric spreads via social media, then we can expect more crimes to occur in areas where there is more social media use, right? And specifically, areas where there is more activity among anti-refugee groups would see the most.

To test this theory, Müller and Schwarz used activity on a pair of major Facebook pages in Germany to measure social media use in general and specific to right-wing groups. For right-wing activity they looked at the page of the “Alternative for Germany” party, the most popular anti-immigration political faction in the country and one that does not attempt to control the conduct on its threads. As a measure of overall Facebook use, they used Nutella’s popular public German page.

With hundreds of thousands of posts and comments broken down by area, the researchers were able to identify overall patterns of social media use, and then isolate anti-refugee sentiment within that. Their findings are unambiguous:

Using these measures, we find that anti-refugee hate crimes increase disproportionally in areas with higher Facebook usage during periods of high anti-refugee sentiment online. This effect is especially pronounced for violent incidents against refugees, such as arson and assault. Taken at face value, this suggests a role for social media in the transmission of Germany-wide anti-refugee sentiment.

A quick estimate on their part suggests that the social media activity may have increased attacks by 13 percent or so — not a number to be quoted as definitive, but rather an indicator that we are not quibbling over half a percent here and there but meaningful numbers.

But the researchers are also careful both to carefully define the scope of those findings:

We do not claim that social media itself causes crimes against refugees out of thin air. In fact, hate crimes are likely to have many fundamental drivers; local differences in xenophobic ideology or a higher salience of immigrants are only two obvious examples. Rather, our argument is that social media can act as a propagating mechanism for the flare-up of hateful sentiments. Taken together, the evidence we present suggests that quasi-random shifts in the local population’s exposure to such sentiments on social media can magnify their effect on refugee attacks.

…and to account for the many confounding variables that may invisibly affect the data.

Correlation versus causation

No doubt many readers will be skeptical of any study like this one; after all, these are very complex issues with many moving parts, and correlations may appear between things regardless of whether those things directly cause or effect one another. Fortunately, the researchers foresaw this objection and were circumspect in their delineation of the link between social media use and attacks.

There are a handful of prominent possible alternative explanations, which the paper deals with in various ways.

First is the possibility that attacks are just more likely in areas where there is heavier social media use. This was my first thought: where are conflicts likely to occur? In places with dense and diverse populations, which seem likely to also have more internet and social media use.

This is dispatched in several ways. In the first place, the study looks at changes in violence levels within an area, not across the whole of Germany. In other words, the pattern of anti-immigrant posts preceding anti-immigrant violence is seen whether it takes place in a smaller town with low levels of social media engagement, or in larger cities where Facebook use is much more frequent.

Next, the Nutella control group provides a measure of social media activity independent of political issues — so patterns of use for a broad swath of users associated with seasons, weekly rhythms, holidays and so on can be identified down to the level of the county. When a population deviates from that pattern, you can be reasonably sure that something about that population is driving that deviation.

Something they couldn’t exactly control but is nonetheless useful is seeing how various internet and Facebook outages affect the patterns. It turns out that internet disruptions completely eliminate the increases in violence normally seen during a country-wide wave of anti-immigrant sentiment. Furthermore, they write, “the effect of refugee posts on hate crimes essentially vanishes in weeks of major Facebook outages.”

Spikes in activity expressing negative feelings toward other frequently targeted groups, for instance Jews, were not associated with increases in refugee-related violence, so it isn’t just that people lash out when they are feeling especially hateful.

Lastly, the researchers showed that other coverage of refugee-related issues, like that by major news outlets, drives local engagement in the form of protests, but does not seem to predict violent acts.

As the researchers say, Facebook isn’t just plain causing violence to happen. The places where it happens are often historically right-wing places that have had higher incidence of violence and hate crimes in the past. But it seems inescapable that Facebook is nevertheless an important way that refugee-related hatred and vitriol in particular is spread, as evidenced by the lack of increases in violence when the social network is unavailable.

The connection seems clear: Hateful content spreads via Facebook and where it is engaged with the most, there you find the most violence. On its face this doesn’t seem like something Facebook can moderate away — it’s a natural consequence of how the fundamental social media ecosystem pioneered by Facebook works. Having it repeatedly and systematically connected with increases in violence isn’t a good look.",Social Media,TechCrunch,https://techcrunch.com/2018/08/21/study-ties-facebook-engagement-to-attacks-on-refugees/,"This study has shown that Facebook use is deeply linked with the frequency of violent acts against refugees and immigrants in Germany, with spikes in anti-refugee posts being predictive of subsequent acts of hate crimes in the same area. This suggests that social media can act as a propagating mechanism for the flare-up of hateful sentiments, and may",Equality & Justice
58,Facebook-WhatsApp data sharing now on pause in UK at regulator’s request — and across Europe – TechCrunch,"A controversial decision this summer by Facebook-owned messaging giant WhatsApp to share data on its users with its parent company — including for advertising purposes — continues to attract the ire of European regulators.

Now Facebook has agree to pause data sharing in the UK, following an investigation by data protection watchdog, the ICO. Although TechCrunch understands this pause only applies to sharing user information for products/ads purposes; WhatsApp user data is still being shared with Facebook for fighting spam and other business intelligence purposes (such as deduplicating the number of users across different Facebook-owned services).

Update: We also understand the data-sharing pause applies across all 28 European Union Member States at this point.

In a strongly worded blog post detailing how its probe has been progressing, UK information commissioner Elizabeth Denham writes: “I had concerns that consumers weren’t being properly protected, and it’s fair to say the enquiries my team have made haven’t changed that view. I don’t think users have been given enough information about what Facebook plans to do with their information, and I don’t think WhatsApp has got valid consent from users to share the information. I also believe users should be given ongoing control over how their information is used, not just a 30 day window.”

“We’ve set out the law clearly to Facebook, and we’re pleased that they’ve agreed to pause using data from UK WhatsApp users for advertisements or product improvement purposes,” she adds.

Denham also hits out at “vague terms of service” for generally failing to give consumers “the protection we need”.

The updated WhatsApp privacy policy offered users an opt out of sharing their data with Facebook but default opted them in — unless they clicked to read the terms more closely and turned the sharing option off, having understood what the toggle represented. Users were also given a 30-day window to revoke consent via the settings in the app — after which they would be unable to withdraw consent. None of which has pleased the ICO.

In the blog post Denham says the ICO has asked Facebook and WhatsApp to sign an undertaking committing to “better explaining to customers how their data will be used, and to giving users ongoing control over that information”, and goes on to warn the company may face enforcement action if it does not alter its approach.

“We also want individuals to have the opportunity to be given an unambiguous choice before Facebook start using that information and to be given the opportunity to change that decision at any point in the future. We think consumers deserve a greater level of information and protection, but so far Facebook and WhatsApp haven’t agreed. If Facebook starts using the data without valid consent, it may face enforcement action from my office,” she continues.

In a statement on the ICO’s action, a Facebook spokesperson rejected criticism it is not being clear enough with WhatsApp users about the data-sharing arrangement, telling TechCrunch: “WhatsApp designed its privacy policy and terms update to give users a clear and simple explanation of how the service works, as well as choice over how their data is used. These updates comply with applicable law, and follow the latest guidance from the UK Information Commissioner’s Office.”

Facebook’s spokesperson added: “We hope to continue our detailed conversations with the ICO and other data protection officials, and we remain open to working collaboratively to address their questions.”

TechCrunch understands the company has had multiple meetings with the ICO about the matter, as well as also receiving questions from several other European regulators who also have concerns about the arrangement. WhatsApp-Facebook data-sharing is on pause in all 28 European Union Member States at this point.

In September Facebook and WhatsApp were ordered by a local data protection regulator in Germany to stop sharing data on users. Facebook said at the time it would be appealing that order.

The Spanish DPA has also stated publicly that it intends to investigate the data-sharing arrangement.

The data transfer arrangement is now being probed by the European Union’s data protection watchdog group, which is made up of representatives from all EU Member State DPAs. The Article 29 Working Party said in October its members (such as the ICO) would be acting “in a coordinated way” to target any problems they identify, and to co-ordinate any enforcement action they may deem necessary.

Denham said the ICO intends to keep “pushing”, along with its fellow European DPAs, for WhatsApp users to be provided with more information about how data shared with Facebook will be used; as well as for a clearer description of the choice they have to share or not to share; and for users to have an ongoing choice to revoke consent.

In her blog post she also flags a wider data protection concern related to companies triangulating user data via acquisition — a concern that has also been voiced by the EU’s competition commissioner this year. And raised most recently as an objection to the Microsoft-LinkedIn acquisition.

“It’s a particular concern when company mergers mean that vast amounts of customers’ personal data become an asset to be bought and sold,” writes Denham. “We’re seeing situations where companies are being bought primarily for this data, and when it is combined with information the purchasing company already holds, there’s a danger that consumers will have little control as datasets are matched and intrusive details revealed.”

She says the regulator is planning to publish a report on this early next year.

“It’s a problem that is broader than data protection, and we’re speaking with industry, competition regulators and consumer groups to see how we can make people clearer on the law. We’ll be publishing a report on this in the new year, outlining our concerns and discussing solutions.”

This post was updated to clarify that only data sharing for ads/product purposes has been paused at this stage, and that the pause applies to all EU Member States",Social Media,TechCrunch,https://techcrunch.com/2016/11/08/facebook-whatsapp-data-sharing-now-on-pause-in-uk-at-regulators-request/,"This article discusses the issues that arise from Facebook and WhatsApp's controversial decision to share user data for advertising purposes, which has been met with disapproval from European regulators. Facebook has agreed to pause data sharing in the UK, but this does not address the underlying issues of users having limited control over their data and companies buying and selling vast amounts of data",Security & Privacy
59,Twitter today starts enforcing new rules around violence and hate – TechCrunch,"Twitter today says it will begin to enforce new rules related to how it handles hateful conduct and abusive behavior taking place on its platform. The changes are a part of the company’s broader agenda to craft new policies focused on reducing the amount of abuse, hate speech, violence and harassment.

Specifically, Twitter explains that in addition to threatening violence or physical harm, it will also look for accounts affiliated with groups that promote violence against citizens to further their causes. This includes any group that promotes violence either on or off Twitter’s platform, but doesn’t apply to military or government entities. Twitter also says it will consider exceptions for groups that are engaged in peaceful resolution.

Meanwhile, any content that glorifies violence or the perpetrators of a violent act will also be in violation of Twitter’s new policies. That means someone like Jason Kessler, the organizer of the white supremacist rally in Charlottesville, Virginia, could be banned for tweets like the one he posted about Heather Heyer, the protester killed at the event, which essentially supported the violence that led to her murder.

Twitter further details that its policies will include celebrating “any violent act in a manner that may inspire others to replicate it or any violence where people were targeted because of their membership in a protected group.”

A single tweet won’t result in an immediate expulsion from the service, however. Instead, Twitter will initially require offending tweets to be removed. It will consider permanent suspension only for repeat violations.

In addition, Twitter says it’s broadening its hateful conduct policy and rules against abusive behavior to include those accounts that abuse or threaten others through their profile information, like their username, display name, or profile bio.

That means users can’t hide their slurs, epithets, and racist or sexist tropes in their bio without a penalty. Twitter will also look in profile information for violent threats, any statements meant to incite fear, or anything else that reduces someone to “less than human.”

These accounts will be permanently suspended, and the company plans to develop internal tools to help it identify accounts in violation to supplement user reports.

Rules around hateful imagery will also now be enforced.

Twitter had previously explained that would consider hateful imagery and hate symbols “sensitive media,” a category that also includes adult content and graphic violence. Today, it’s defining hateful imagery as “logos, symbols, or images whose purpose is to promote hostility and malice against others based on their race, religion, disability, sexual orientation, or ethnicity/national origin.”

In this case, Twitter will accept profile-level reports and require the account owners to remove the violating media.

Twitter’s lack of policy and consistent enforcement over the years has led to a surge in hate speech on its platform, particularly from neo-Nazis and other members of the alt-right. But with Twitter’s promised crackdown in the works, many have since fled to the alt-right’s version of Twitter, Gab. That network may benefit again with an influx of exiting alt-righters following today’s changes. (The alt-right, by the way, has built its own set of alternatives to mainstream social media. But many of the sites are plagued with bugs, a recent report found, and they can struggle to raise funds from traditional venture capital and angel investors.)

Though Twitter’s new policies seem like a decent starting point, many users don’t believe the company will actually enforce the rules it has laid out. Twitter’s history in this area is not great, after all. For example, despite its claims that it was taking abuse more seriously, a BuzzFeed investigation from earlier this year found a number of egregious examples of abuse and threats that slipped through the cracks.

While Twitter says that it will begin its policy enforcement today, it noted that it may still make some mistakes. The company added that it’s also working on a “robust” appeals process for those who are flagged.",Social Media,TechCrunch,https://techcrunch.com/2017/12/18/twitter-today-starts-enforcing-new-rules-around-violence-and-hate/,"Twitter's lax enforcement of its policies has led to a surge in hate speech on its platform, and alt-right members have fled to alternative networks like Gab.",Equality & Justice
60,Here’s how to kick nazis off your Twitter right now – TechCrunch,"While you wait for Twitter to roll out “more aggressive” rules regarding hate speech, which CEO Jack Dorsey promised are coming within “weeks” as of late Friday, here’s a quick workaround to kick nazis off of your Twitter feed right now: Go to the ‘Settings and privacy’ page and under the ‘Content’ section set the country to Germany (or France).

This switches on Twitter’s per country nazi-blocking filter which the company built, all the way back in 2012, to comply with specific European hate speech laws that prohibit pro-Nazi content because, y’know, World War II.

Switching the country in your Twitter settings doesn’t change the language, just the legal jurisdiction. So, basically, you get the same Twitter experience, just without so many of the Swastika wielding nazis.

In Germany incitement to hatred is deemed a criminal offense against public order, and nazi imagery, expressions of antisemitism, Holocaust denial and so on are effectively banned in the country.

Free speech is protected in the German constitution but the line is drawn at outlawed speech — which, as programmer and blogger Kevin Marks has noted, is actually a result of the post-war political settlement applied by the triumphant allied forces — led by, er, the U.S…

It's not something that Twitter advertise, but they do have a built-in Nazi-blocking filter: https://t.co/3ZkYKadn8b — Kevin Marks (@kevinmarks) October 13, 2017

In a further irony, Twitter’s nazi blocking filter gained viral attention on Twitter last week when a Twitter user creatively couched it: “One Weird Trick to get nazi imagery off Twitter”. At the time of writing her tweet has racked up 16,000 likes and 6.6k retweets:

https://twitter.com/christapeterso/status/918517320338644992

Dorsey’s pledge of effective action against hate tweets followed yet another storm of criticism about how Twitter continues to enable harassment and abuse via its platform. Which in turn led to a spontaneous 24 hour boycott on Friday. Just before Dorsey tweet stormed to say the company would be rolling out new rules around “unwanted sexual advances, non-consensual nudity, hate symbols, violent groups, and tweets that glorifies violence”.

(i.e. the stuff women and other victims of online harassment have been telling Twitter to do for years and years.)

Yet in 2012, when Twitter announced the rollout of per country content blocking, it was absolutely sticking to its free speech guns that the “tweets still must flow” — i.e. even nazi hate speech tweets, just in all other markets where this kind of hateful content is not literally illegal.

Indeed, Twitter said then that its rational for developing per country blocking was to minimize the strictures on free speech across its entire platform. Meaning that censured content (such as nazi hate tweets) would only be blocked for the smallest possible number of Twitter users.

“Starting today, we give ourselves the ability to reactively withhold content from users in a specific country — while keeping it available in the rest of the world. We have also built in a way to communicate transparently to users when content is withheld, and why,” the company wrote in 2012, saying also that it would “evaluate each request [to withhold content] before taking any action”.

So Twitter’s nazi filter was certainly not designed to be pro-active about blocking hate speech — but merely to react to specific, verified legal complaints.

“One of our core values as a company is to defend and respect each user’s voice. We try to keep content up wherever and whenever we can, and we will be transparent with users when we can’t. The Tweets must continue to flow,” it wrote then.

“We’ve been working to reduce the scope of withholding, while increasing transparency, for a while,” it went on to say, explaining the timing of the move. “We have users all over the world and wanted to find a way to deal with requests in the least restrictive way.”

More than five years on from Twitter’s restated conviction that “tweets still must flow”, tech platforms are increasingly under attack for failing to take responsibility for pro-actively moderating content on their platforms across a wide range of issues, from abuse and hate speech; to extremist propaganda and other illegal content; to economically incentivized misinformation; to politically incentivized disinformation.

It’s fair to say that the political climate around online content has shifted as the usage and power of the platforms have grown, and as they have displaced and eroded the position of traditional media.

To the point where a phrase like “the tweets must flow” now carries the unmistakable whiff of effluent. Because social media is in the spotlight as a feeder of anti-social, anti-civic impacts, and public opinion about the societal benefits of these platforms appears to be skewing towards the negative.

So perhaps Twitter’s management really has finally arrived at the realization that if, as a content distribution platform, you allow hateful ideas to go unchallenged on your platform then your platform will become synonymous with the hateful content it is distributing — and will be perceived, by large swathes of your user-base, as a hateful place to be exactly because you are allowing and enabling abuse to take place under the banner of an ill-thought-through notion that the “tweets must flow”.

Yesterday Dorsey claimed Twitter has been working on trying to “counteract” the problem of voices of abuse victims being silenced on its platform for (he said) the past two years. So presumably that dates from about the time former CEO Dick Costolo sent that memo — admitting Twitter ‘sucks at dealing with abuse’.

Although that was actually February 2015. Ergo, more than two years ago. So the question of why it’s taken Twitter so very long to figure out that enabling abuse also really sucks as a business strategy is still in need of a definitive answer.

“We prioritized this in 2016. We updated our policies and increased the size of our teams. It wasn’t enough,” Dorsey tweeted on Friday. “In 2017 we made it our top priority and made a lot of progress.

“Today we saw voices silencing themselves and voices speaking out because we’re still not doing enough.”

He did not offer any deeper, structural explanation of why Twitter might be failing at dealing with abuse. Rather he seems to be saying Twitter just hasn’t yet found the right ‘volume setting’ to give to the voices of victims of abuse — i.e. to fix the problem of their voices being drowned out by online abuse.

Which would basically be the ‘treat bad speech with more speech’ argument that really only makes sense if you’re already speaking from a position of privilege and/or power.

When in fact the key point that Twitter needs to grasp is that hate speech itself suppresses free speech. And that victims of abuse shouldn’t have to spend their time and energy trying to shout down their abusers. Indeed, they just won’t. They’ve leave your platform because it’s turned into a hateful place.

In a response to Dorsey’s tweet storm, Twitter user Eric Markowitz also pointed out that by providing verification status to prominent nazis Twitter is effectively validating their hate speech — going on on to suggest the company could “fairly simply develop better criteria around verifying people who espouse hate and genocide”.

Dorsey responded that: “We’re reconsidering our verification policies. Not as high a priority as enforcement, but it’s up there.”

“Enforcing according to our rules comes first. Will get to it as soon as we can, but we have limited resources and need to strictly prioritize,” he added.

At this point — with phrases like “limited resources” being dropped — I’d say you shouldn’t get your hopes up of a root and branch reformation of Twitter’s policy towards purveyors of hate. It’s entirely possible the company is just going to end up offering yet another set of ineffective anti-troll tools.

Thing is, having invited the hate-filled voices in, and allowed so many trolls to feel privileged to speak out, Twitter is faced with a philosophical U-turn in extricating its product from the unpleasantness its platform has become synonymous with.

And really, given its terrible extant record on dealing with abuse, it’s not at all clear whether the current management team is capable of the paradigm shift in perspective needed to tackle hate speech. Or whether we’ll just get another fudge and fiddle focused on preserving a definition of free speech that has, for so long, allowed hateful tweets to flow over and drown out other speech.

As I wrote this week, Twitter’s abuse problem is absolutely a failure of leadership. And we’re still seeing only on-the-back-foot responses from the CEO when users point out long standing, structural problems with its approach.

This doesn’t bode well for Twitter being able to fix a crisis of its own flawed conviction.",Social Media,TechCrunch,https://techcrunch.com/2017/10/14/heres-how-to-kick-nazis-off-your-twitter-right-now/,"Social media has become a platform for hateful speech, which has resulted in users being silenced and abused. This is a failure of leadership on the part of social media companies, and it is unclear whether they can effect the paradigm shift needed to tackle hate speech.",Equality & Justice
61,Google's Search Algorithm Could Steal the Presidency,"Imagine an election---a close one. You’re undecided. So you type the name of one of the candidates into your search engine of choice. (Actually, let’s not be coy here. In most of the world, one search engine dominates; in Europe and North America, it’s Google.) And Google coughs up, in fractions of a second, articles and facts about that candidate. Great! Now you are an informed voter, right? But a study published this week says that the order of those results, the ranking of positive or negative stories on the screen, can have an enormous influence on the way you vote. And if the election is close enough, the effect could be profound enough to change the outcome.

In other words: Google’s ranking algorithm for search results could accidentally steal the presidency. ""We estimate, based on win margins in national elections around the world,"" says Robert Epstein, a psychologist at the American Institute for Behavioral Research and Technology and one of the study’s authors, ""that Google could determine the outcome of upwards of 25 percent of all national elections.""

Epstein’s paper combines a few years’ worth of experiments in which Epstein and his colleague Ronald Robertson gave people access to information about the race for prime minister in Australia in 2010, two years prior, and then let the mock-voters learn about the candidates via a simulated search engine that displayed real articles.

One group saw positive articles about one candidate first; the other saw positive articles about the other candidate. (A control group saw a random assortment.) The result: Whichever side people saw the positive results for, they were more likely to vote for---by more than 48 percent. The team calls that number the ""vote manipulation power,"" or VMP. The effect held---strengthened, even---when the researchers swapped in a single negative story into the number-four and number-three spots. Apparently it made the results seem even more neutral and therefore more trustworthy.

But of course that was all artificial---in the lab. So the researchers packed up and went to India in advance of the 2014 Lok Sabha elections, a national campaign with 800 million eligible voters. (Eventually 430 million people voted over the weeks of the actual election.) ""I thought this time we’d be lucky if we got 2 or 3 percent, and my gut said we’re gonna get nothing,"" Epstein says, ""because this is an intense, intense election environment."" Voters get exposed, heavily, to lots of other information besides a mock search engine result.

The team 2,150 found undecided voters and performed a version of the same experiment. And again, VMP was off the charts. Even taking into account some sloppiness in the data-gathering and a tougher time assessing articles for their positive or negative valence, they got an overall VMP of 24 percent. ""In some demographic groups in India we had as high as about 72 percent.""

The effect doesn’t have to be enormous to have an enormous effect.

The fact that media, including whatever search and social deliver, can affect decision-making isn’t exactly news. The ""Fox News Effect"" says that towns that got the conservative-leaning cable channel tended to become more conservative in their voting in the 2000 election. A well-known effect called recency means that people make decisions based on the last thing they heard. Placement on a list also has a known effect. And all that stuff might be too transient to make it all the way to a voting booth, or get swamped by exposure to other media. So in real life VMP is probably much less pronounced.

But the effect doesn’t have to be enormous to have an enormous effect. The Australian election that Epstein and Robertson used in their experiments came down to a margin of less than 1 percent. Half the presidential elections in US history came down to a margin of less than 8 percent. And presidential elections are really 50 separate state-by-state knife fights, with the focus of campaigns not on poll-tested winners or losers but purple “swing states” with razor-thin margins.",Social Media,WIRED,https://www.wired.com/2015/08/googles-search-algorithm-steal-presidency/,The study demonstrates that Google's ranking algorithm for search results has the potential to sway undecided voters and could even change the outcome of an election if the margin of victory is close enough. This could result in a situation where an election is stolen due to an algorithm's influence on the decision-making process.,"Information, Discourse & Governance"
62,"Six Reasons Why I'm Not On Facebook, By Wired UK's Editor","""David, you're sounding like an old dude!"" Matt Flannery, who runs social-lending website Kiva, couldn't understand when I explained that, no, I wouldn't be keeping in touch with him via Facebook. ""What are you worried about?"" he teased in a break at the PINC conference in Holland. ""Only old guys get worked up about privacy.""

Well, Matt, I admit I'm the wrong side of 30, and that I still avoid using emoticons in formal correspondence. But let me explain why I'm not active on Facebook, nor sharing my credit-card purchases on Blippy, nor allowing Google Buzz to mine my contacts list, nor even publishing my DNA on 23andMe.com. My cautious use of the social networks has nothing to do with paranoia about privacy; and yes, I celebrate the unprecedented transparency and connectivity that these services can empower. But what's increasingly bothering me is the wider social and political cost of our ever-greater enmeshment in these proprietary networks. Here are half a dozen reasons why.

1) Private companies aren't motivated by your best interests

Facebook and Google exist to make money, by selling advertisers the means to target you with ever greater precision. That explains the endless series of ""privacy"" headlines, as these unregulated businesses push boundaries to make it easier for paying third parties to access your likes, interests, photos, social connections and purchasing intentions. That's why Facebook has made it harder for users to understand exactly what they're giving away -- why, for instance, its privacy policy has grown from 1,004 words in 2005 to 5,830 words today (by comparison, as the New York Times has pointed out, the U.S. Constitution is 4,543). Founder Mark Zuckerberg once joked dismissively about the ""dumb fucks"" who ""trust me"". I admire the business Zuckerberg's built; but I don't trust him.

2) They make it harder to reinvent yourself

""When you’re young, you make mistakes and you do some stupid stuff,"" President Obama warned high-school students in Virginia last September. ""Be careful about what you post on Facebook, because in the YouTube age whatever you do will be pulled up later somewhere in your life."" He's right: anything posted online might come to haunt you permanently, yet all of us need space to grow. As the writer Jaron Lanier said in a recent lecture, if Robert Zimmerman, of small-town Hibbing, Minnesota, had had a Facebook profile, could he really have re-created himself as the New York beatnik Bob Dylan

3) Information you supply for one purpose will invariably be used for another …

Phone up to buy a pizza, and the order-taker's computer gives her access to your voting record, employment history, library loans -- all ""just wired into the system"" for your convenience. She'll suggest a tofu pizza as she knows about your 42-inch waist, she'll add a delivery surcharge because a nearby robbery yesterday puts you in ""an orange zone"" -- and she'll be on her guard because you've checked out the library book Dealing With Depression. This is where the American Council for Civil Liberties sees consumerism going -- watch its pizza video online -- and it's not to hard to believe. Already surveys suggest that 35 percent of firms are rejecting applicants because of information found on social networks. What makes you think you can control what happens to your personal data?",Social Media,WIRED,https://www.wired.com/2010/09/six-reasons-why-wired-uks-editor-isnt-on-facebook/,"Social media companies exist to make money, and their lack of regulation and disregard for user privacy can lead to individuals not being able to reinvent themselves, their personal data being used for unintended purposes, and them even being rejected for job applications due to information found on social networks.",Security & Privacy
63,Facebook faces another moderation scandal over migrant torture videos – TechCrunch,"Facebook is faced with yet another content-related scandal, after The Times newspaper reported that people traffickers and slave traders are using its platform to broadcast videos of migrants being tortured to try to extort money from their families.

According to the newspaper’s report, footage showing Libyan gangmasters threatening the lives of migrants had remained on the social network for months.

It reports that harrowing footage shared on Facebook showed emaciated and injured migrants, mostly Somalis and Ethiopians, huddled in a concrete cellar describing the abuse they have suffered and pleading for their lives.

The Times quotes a United Nations migration agency criticizing the company for allowing people traffickers to use its site to “advertise their services, entice vulnerable people on the move and then exploit them and their families”.

Mohammed Abdiker, of the International Organisation for Migration, said: “It is irresponsible for tech companies like Facebook to ignore this issue. It’s hard to believe that the tech giants cannot put some real effort into stopping these smugglers from using their platforms for racketeering.”

In one instance the newspaper says a video that had been posted on June 9 was still on the site until yesterday.

Facebook confirmed to TechCrunch it had removed some of the content after the Times reported it. In a statement, a spokesperson told us: “Offering services to take part in, support or promote people smuggling on Facebook, violates our Community Standards.”

In an earlier statement to the Times, it added: “People smuggling is illegal and any posts, pages or groups that co-ordinate this activity are not allowed on Facebook. We encourage people to keep using our reporting tools to flag this kind of behaviour so it can be reviewed and swiftly removed by our global team of experts, who work with law enforcement agencies around the world.”

A series of public outcries over content moderation has cranked up the political pressure on Facebook in recent times — ranging from suicides and murders being broadcast via its live streaming service, to extremist propaganda and child abuse content.

It also faces ongoing pressure to speed up hate speech take downs — especially in Europe, where legislators are eyeing fines to enforce action.

In May, faced with rising political pressure on multiple fronts, Facebook announced it would be adding 3,000 additional staff to its content moderation team — expanding it to 7,500. Although for a platform with two billion users globally that’s clearly a drop in the ocean.

CEO Mark Zuckerberg has also said it’s hope is that developments in AI technology will enable it to automate content moderation at scale in time. Though he also warned such a scenario is likely years out — meanwhile the scandals keep coming. Including, now, an accusation that it is not doing enough to stop people traffickers utilizing its platform to profit from human misery.

The company argues it seeks to balance raising awareness of controversial issues, including from war zones, with taking down content that may be disturbing to users.

A spokesman told us that in this instance, for example, while it has removed abuse videos reported to it by the Times, it has not removed a video report by a Somali journalist covering people smuggling, although it has added a warning to flag the disturbing nature of the content.

“We also believe it is important that Facebook continues to be a place where people can raise awareness of important, and sometimes controversial issues. This specific video was posted to condemn the content, so we would not consider it a violation of our policies. However, the content is alarming, and we have marked the video as disturbing. This means there will be a warning screen and the video’s distribution will be limited to those aged 18 and over,” the spokesperson added.",Social Media,TechCrunch,https://techcrunch.com/2017/08/25/facebook-faces-another-moderation-scandal-over-migrant-torture-videos/,"Facebook is facing yet another content-related scandal due to reports that people traffickers and slave traders are using its platform to broadcast videos of migrants being tortured. The UN has criticized the company for allowing such videos to remain on the platform for months, highlighting the potential for social media to be used for exploitation and extortion.",Equality & Justice
64,YouTube tests hiding dislike counts on videos – TechCrunch,"YouTube announced today it will begin testing what could end up being a significant change to its video platform: It’s going to try hiding the dislike count on videos from public view. The company says it will run a “small experiment” where it will try out a few different designs where dislike counts are no longer shown, however none will see the “dislike” button itself removed entirely.

The company announced the tests on Twitter, but then explains further in a community forum post that the goal is not to remove the ability for users to signal they disliked a video — creators will still have access to the video’s like and dislike count from YouTube Studio and dislikes will still help power YouTube’s recommendation algorithms.

Instead, YouTube says that the idea to try hiding dislikes is based on creator feedback.

“We’ve heard from creators that the public dislike counts can impact their well-being and may motivate a targeted campaign of dislikes on a creator’s video,” the announcement reads. “So, we’re testing designs that don’t include the visible like or dislike count in an effort to balance improving the creator experience, while still making sure viewer feedback is accounted for and shared with the creator.”

Of course, there can be a sort of mob mentality that accompanies the use of the Like and Dislike buttons on YouTube. But seeing the dislike count can also help to signal to others when videos are clickbait, spam or misleading, which can be helpful.

Creators, you'll still be able to see the exact number of likes and dislikes in YouTube Studio. For viewers, if you're in the experiment, you can still like or dislike a video to share feedback with creators and help tune the recommendations you see on YouTube. — YouTube (@YouTube) March 30, 2021

YouTube showed off one potential design being tested that simply shows the same button layout but instead of a number of dislikes, the word “Dislike” appears underneath the thumbs down icon.

There will be no way to opt out of the test if you see the changes appear when you’re logged into YouTube — you’ll only be able to share feedback, the company notes.

To be clear, however, YouTube isn’t yet committed to removing the dislike count for everyone at this time. The feedback from this test will help inform YouTube as to if, when or how it will release designs like this more broadly.

YouTube wouldn’t be the first to experiment with removing metrics from a social app. Instagram has also been testing removing the number of positive engagements (Likes), in order to make the experience feel more authentic and less about chasing clout. And Facebook this year removed the “Like” button from Facebook Pages, in favor of the more accurate “Followers” measurement. However, in the case of removing just the dislike count and not the likes, viewers may misunderstand a video’s true popularity.

The company told TechCrunch the tests will run globally over the next few weeks on Android and iOS while it gathers feedback from a handful of designs.

Updated 3/30/21, 6 pm et to add details about launch.",Social Media,TechCrunch,https://techcrunch.com/2021/03/30/youtube-tests-hiding-dislike-counts-on-videos/,"YouTube is testing a design that hides the dislike count from public view in an effort to balance improving the creator experience, while still allowing viewers to signal their feedback. This could have an unintended effect of viewers misunderstanding a video's true popularity.",User Experience & Entertainment
65,Facebook's Latest Fix for Fake News: Ask Users What They Trust,"Mark Zuckerberg promised to spend 2018 fixing Facebook. Last week, he addressed Facebook making you feel bad. Now he’s onto fake news.

Late Friday, Facebook buried another major announcement at the end of the week: How to make sure that users see high-quality news on Facebook. Facebook’s solution? Let its users decide what to trust. On the difficult problem of fixing fake news, Zuckerberg took the path with the least responsibility for Facebook, but described it as the most objective.

“We could try to make that decision ourselves, but that's not something we're comfortable with,” Zuckerberg wrote on his Facebook page. “We considered asking outside experts, which would take the decision out of our hands but would likely not solve the objectivity problem. We decided that having the community determine which sources are broadly trusted would be most objective.”

The vetting process will happen through Facebook’s ongoing quality surveys — the same surveys it uses to ask whether Facebook is a force for good in the world and whether the company seems to care about its users. Now, Facebook will ask users if they are familiar with a news source and, if so, whether they trust the source.

According to Zuckerberg, these surveys will help the truth about trustworthiness rise to the top: “The idea is that some news organizations are only trusted by their readers or watchers, and others are broadly trusted across society even by those who don't follow them directly.”

It’s tempting to read a lot into Zuckerberg’s words, especially when the missive was so short on details. The perils are evident: Bad actors can game the survey! This only increases filter bubbles! After the year Facebook just had, how can you possibly think the masses can be objective?

Relying on users “lets them sidestep allegations of bias and take steps to fix it without directly becoming the dreaded 'arbiter of truth,'"" says researcher Renee DiResta, a technologist who has been studying the manipulation of social-media platforms.

Facebook did not immediately return a request for comment. There’s a good chance the new policy could cause as many problems as it solves. For the best known media brands, the survey could be a leg up. But what about niche publications that have narrow, but credible readerships? Does this mean that National Review or Slate are deemed untrustworthy because they have definitive points of view? Do they get put in the same bucket as Fox and MSNBC? What about BuzzFeed, where fun distractions and deep investigations all show up under the same URL?

Jason Kint, CEO of Digital Content Next, a trade association representing content companies, likes the idea of using brands as a proxy for trust. “But the details are really important,” he says. “What matters most is how this is being messaged. Facebook is clearly scrambling as the industry, Washington and the global community are losing trust in them. There is nothing worse to a company long-term.”",Social Media,WIRED,https://www.wired.com/story/facebooks-latest-fix-for-fake-news-ask-users-what-they-trust/,"Facebook's new policy of relying on user surveys to determine trustworthiness could lead to niche publications being deemed untrustworthy, create filter bubbles and messaging issues, and ultimately decrease trust in the platform.","Information, Discourse & Governance"
66,WhatsApp limits message forwarding in bid to reduce spam and misinformation – TechCrunch,"In a bid to cut down on the spread of false information and spam, WhatsApp recently added labels that indicate when a message has been forwarded. Now the company is sharpening that strategy by imposing limits on how many groups a message can be sent on to.

Originally, users could forward messages on to multiple groups, but a new trial will see that forwarding limited to 20 groups worldwide. In India, however, which is WhatsApp’s largest market with 200 million users, the limit will be just five. In addition, a ‘quick forward’ option that allowed users to pass on images and videos to others rapidly is being removed from India.

“We believe that these changes — which we’ll continue to evaluate — will help keep WhatsApp the way it was designed to be: a private messaging app,” the company said in a blog post.

The changes are designed to help reduce the amount of information that goes viral on the service, although clearly this isn’t a move that will end the problem altogether.

The change is in direct response to a series of incidents in India. The BBC recently wrote about an incident which saw one man dead and two others severely beaten after rumors of their efforts to abduct children from a village spread on WhatsApp. Reportedly 17 other people have been killed in the past year under similar circumstances, with police saying false rumors had spread via WhatsApp.

In response, WhatsApp — which is of course owned by Facebook — has bought full-page newspaper ads to warn about false information on its service.

Beyond concern about firing up vigilantes, the saga may also spill into India’s upcoming national general election next year. Times Internet today reports that Facebook and WhatsApp plan to introduce a fake news verification system that it used recently in Mexico to help combat spam messages and the spreading of incorrect news and information. The paper said that the companies have already held talks with India’s Election Commission.",Social Media,TechCrunch,https://techcrunch.com/2018/07/19/whatsapp-limits-message-forwarding/,"WhatsApp is trying to reduce the spread of false information and spam with limits on how many groups a message can be sent to, and by removing the 'quick forward' option in India. This is in response to incidents in which people have been killed due to false rumors spreading on the service. Facebook and WhatsApp are also in talks with",Security & Privacy
67,"Hoarding User Data, Facebook Works Around Google Block","Called out by its biggest rival in Silicon Valley last week for its greedy user-data policies, Facebook doubled down Monday on its policy of pulling in the world's data but only letting it out in ways that benefit the giant social network.

Last Friday, Google blocked Facebook from using its Contacts API to pull in Google contacts for a user who wants to build a network of friends -- a very common way of starting a profile or adding to it. Google justified the move, saying Facebook had no similar open API and wasn't playing fairly with user data.

Facebook made no public statements about the block.

Then, on Monday night, as first reported by Techcrunch, Facebook basically told Google to ""shove it,"" by giving users a way around that block: By linking deeply into Gmail's contact export system, it helps users download a CSV contact file and then re-upload it to Facebook.

Facebook did not respond last week when Wired.com inquired about its policy of sucking in data through other companies' open APIs, without making it possible to export your Facebook contacts, unless you are using a service from a company that is ""friends"" with Facebook, such as Yahoo or Microsoft.

Technically, Google could block Facebook's new hack, leading to a cat-and-mouse game over user data, but Google is opting for the high road -- at least for now.

""We're disappointed that Facebook didn't invest their time in making it possible for their users to get their contacts out of Facebook,"" a Google spokesman said by e-mail. ""As passionate believers that people should be able to control the data they create, we will continue to allow our users to export their Google contacts.""

While Google has long been one of the leaders in the data-portability movement, their motives here are not purely philosophical. The search and advertising giant has lagged behind Facebook in creating ""social"" online services, and the company is rumored to be launching a new effort, dubbed ""Google Me,"" in the near future. One key for that success will be getting new users to copy their contacts from other networks, most importantly Facebook, into Google's system.

While Facebook has said that it's ""complicated"" to figure out how to lets users export their friends' contact information, there seems to be no philosophical objection to the practice at the company. Users of Microsoft and Yahoo's e-mail services can pull that data out, and Android 2.2 mobile phone users can also painlessly merge their Facebook and Gmail contacts.

Instead, what we are seeing is an example of how Facebook is still, at times, a dorm-room operation, run by a defiant group of youngsters. They now seem to see Google as a substitute for the college authority figures that Facebook founder Mark Zuckerberg once thumbed his nose at.

But Zuckerberg and Co. aren't in college anymore, and there's nothing charming or rebellious about Facebook's data hypocrisy and their defensiveness about being called to the carpet about it.

A grownup company worthy of a multibillion dollar valuation and the trust of 500 million users would have simply acknowledged that Google had a point, said that export was complicated, that they were working on it and would have an open API in a couple of weeks.

But that's not Facebook's style and it's not clear it ever will be.

Facebook did not respond to a request for comment.

Follow us for disruptive tech news: Ryan Singel and Epicenter on Twitter.

See Also:",Social Media,WIRED,https://www.wired.com/2010/11/facebook-google-showdown/,"Facebook's unwillingness to acknowledge Google's valid point and make their user data more portable is an example of how the company is still operating with a ""dorm-room"" mentality, and its refusal to make user data more accessible is concerning and likely to lead to a loss of trust from its users.",Security & Privacy
68,Facebook refuses to disclose ‘chuck Chequers’ Brexit advertiser to UK parliament – TechCrunch,"Facebook has refused to provide the British parliament with the names of individuals behind a shadowy network backing an extreme ‘no deal’ Brexit outcome over a government-negotiated compromise.

Since the June 2016 EU referendum vote, politics in the UK has been consumed by the question of how to implement a close vote to leave.

And last year the UK’s Electoral Commission confirmed the vote was tarnished by in influx of dark money ploughed into social media ads — with platforms such as Facebook offering an unregulated route for circumventing democratic norms.

Nor have the Brexit ads stopped since the referendum.

An unknown group, called ‘Mainstream Network’, ran a series of political ads on Facebook’s platform last year which targeted voters in key leave voting constituencies urging them to pressure on their member of parliament not to support the prime minister’s approach to seek a withdrawal deal from the EU.

Such a deal would allow the UK to leave the bloc more smoothly, with more contingencies in place to cover the exit. But legally, if no deal (and/or no extension to Article 50) is agreed before the end of this month the UK could just ‘crash out’ of the EU without any such safety net.

Unknown entities have been using Facebook’s platform to push for exactly that to happen — by paying Facebook to target leave voters with anti-Brexit-deal ads (which included the line “chuck Chequers”; a reference to the prime minister’s Brexit deal).

Last year research commissioned by a UK parliamentary committee as part of an enquiry into political advertising online spotlit the existence of Mainstream Network, estimating the unknown Facebook advertiser had spent ~£257,000 in just over 10 months.

Its Facebook pages were said to have reached between 10M and 11M people on the platform.

Mainstream Network also operated a ‘news’ website, where whoever was behind it curated pro-Brexit content and advocated for no deal being “better than partition or permanent vassalage”.

Last November Facebook policy VP Richard Allan faced questions from the DCMS committee about who is behind the ‘Mainstream Network’ Brexit ads running on Facebook.

DCMS chair Damian Collins asked for Facebook to provide details of the accounts behind Mainstream Network or if it would not to provide a reason for not disclosing the information.

Yesterday the committee published Facebook’s refusal to provide the information to the DCMS committee. Though it said it has passed some information to the UK’s data watchdog, the Information Commissioner’s Office (ICO).

“The Committee asked about an advertiser on our platform called Mainstream Network. As I noted at the time, in the event that Facebook receives a request for personal data from an entity which can legally require such information, Facebook will provide information in line with normal procedures. You will appreciate that it would be inappropriate to provide personal data of our users to any third party absent a lawful basis for such disclosure,” writes Allan.

He goes on to say that Facebook has provided “information” about Mainstream Network to the UK’s data watchdog “on a private and confidential basis”.

The ICO is investigating the advertiser as part of a wider probe into the use of social media for political campaigning.

“It is now a matter for ICO (acting in accordance with its statutory duties) to determine what they will do with the data provided to them,” Allan adds.

We reached out to the ICO to ask whether it intends to disclose the names.

“We received a response from Facebook to an Information Notice issued by the ICO. The information is under review and forms part of our ongoing investigation into the use of data analytics for political purposes,” a spokeswoman told us.

Last summer information commissioner Elizabeth Denham called for an ethical pause of the use of social media tools for political ads — saying she was concerned about the lack of transparency and the knock-on impact that could have on democracy.

In recent years Facebook has been busy making loud crisis PR noises about how it’s ‘increasing the transparency’ around advertisers on its platform — ever since the 2016 US presidential election disinformation scandal blew up, and it emerged quite how many Roubles Facebook had been accepting to allow divisive Kremlin ads to target US voters.

The company launched ‘political ad transparency’ measures in the U.S. initially, including a requirement for election advertisers to verify they are US-based.

It has also since rolled out some similar measures in some international markets — including in the U.K. where it introduced a system for disclosing political ads last fall. (Though it quickly had to rework the system after it was shown being trivially easy to spoof.)

Allan appears to intend to reference the latter measures in the concluding portion of his letter, albeit he gets the date wrong by a full year.

“I further note that as of 29 November 2019 [sic], we have required political advertisers to consent to the publication of additional information in the form of a disclaimer that they create when they go through the authorisation process. All political advertisements along with these disclaimers are made available to the public in our Ad Library,” he writes, without making it clear why that should mean Facebook can’t disclose the identities behind Mainstream Network.

The company has claimed to be working towards having a “global system” for political ad transparency.

But the reality on the ground remains highly variable, piecemeal and very far from perfect full transparency.

Nor will Facebook even come clean with the public when specifically asked to do so by policymakers, as its refusal to the DCMS shows.

Responding to the company’s letter in a series of tweets, Collins writes: “I believe there is a strong public interest in understanding who is behind the Mainstream Network, and that this information should be published. People have a right to know how is targeting them with political advertisements and why.”

2/3 I believe there is a strong public interest in understanding who is behind the Mainstream Network, and that this information should be published. People should have a right to know who is targeting them with political advertisements and why. — Damian Collins (@DamianCollins) March 5, 2019

It remains to be seen whether the ICO will release the information Facebook has provided it.

The watchdog has also so far declined to disclose the identities of several senior Facebook executives who knew about another political ad scandal — the Cambridge Analytica data breach — earlier than the company had publicly claimed it knew.

The DCMS committee published its final report into online disinformation last month, setting out a laundry list of recommendations for cleaning up political campaigning in the digital era.

The report also includes the tidbit about the trio of senior Facebook managers who knew about the Cambridge Analytica breach sooner than Zuckerberg himself (but apparently did not think to share the incident with the CEO), as well as singling out Facebook for “disingenuous” and “bad faith” responses to democratic concerns about the misuse of people’s data.

The committee’s report also calls for privacy and antitrust regulators to investigate the company.",Social Media,TechCrunch,https://techcrunch.com/2019/03/06/facebook-refuses-to-disclose-chuck-chequers-brexit-advertiser-to-uk-parliament/,"Facebook has refused to disclose the identities behind a network backing an extreme 'no deal' Brexit outcome, illustrating the lack of transparency and unregulated power of social media to influence political campaigns - something the UK's parliamentary committee has called for regulators to investigate.",Politics
69,Google Adds Some Snark to Facebook Open-Data Battle,"Why this fight now?

Well, e-mail addresses aren't just a way to write to a person you know. They serve as a unique identifier, not unlike a Social Security number or driver's license number. Social sites use e-mail addresses to connect you to other users. Facebook is pretending that it's looking out for its users, when in fact, it is acting like the dominant market player that it is. It's using its advantage as the biggest social network to force others to play by the rules it sets.

Given that Google is readying a new social application to rectify its earlier failures to build a sticky social service, the company desperately wants people to be able to port over their contacts and their unique identifiers over to that new service.

Google knows, just as Facebook does, that individuals have spent hours building up their web of connections on Facebook. Google says it's the right thing -- for users and other services -- for that so-called ""social graph"" to belong to users. For its part, Facebook is willing to let other services rely on its so-called ""Open Graph"" services, but Facebook, not its users, retains ownership and control.

Openness has always been a key tool for challengers, and Google is using that tool well here. But Facebook is the Ma Bell of social networking, and it's made it clear that it's willing to live with the charge of being a data hypocrite -- at least until after Google debuts its social network. Then if that seems to fail -- in no small part from being starved of data from Facebook -- Facebook can announce with a flourish a more-open contact-export policy.

If Facebook has taught us anything, it's that our identities are inextricable from our connections to other people. Building relentlessly on that idea put Facebook at the center of the net, allowing it to become the company people rely on: to communicate with friends, craft an online persona and log them in around the net. That's a much bigger deal than most people think.

Which leads to the inevitable conclusion: If Facebook won't let you export your connections to whomever you like, it is claiming ownership over your identity.

Even if it has a nice way for you to export your photos.

Follow us for disruptive tech news: Ryan Singel and Epicenter on Twitter.

See Also:",Social Media,WIRED,https://www.wired.com/2010/11/google-trap-my-data-snark/,"The fight over e-mail addresses is important because if Facebook won't let users export their connections to whomever they like, the company is claiming ownership over their identity, which has become inextricably linked to their connections on the platform.",Security & Privacy
70,Amnesty International used machine-learning to quantify the scale of abuse against women on Twitter – TechCrunch,"Update: Twitter’s response has been added to the end of this post.

A new study by Amnesty International and Element AI attempts to put numbers to a problem many women already know about: that Twitter is a cesspool of harassment and abuse. Conducted with the help of 6,500 volunteers, the study, billed by Amnesty International as “the largest ever” into online abuse against women, used machine-learning software from Element AI to analyze tweets sent to a sample of 778 women politicians and journalists during 2017. It found that 7.1 percent, or 1.1 million, of those tweets were either “problematic” or “abusive,” which Amnesty International said amounts to one abusive tweet sent every 30 seconds.

On an interactive website breaking down the study’s methodology and results, the human rights advocacy group said many women either censor what they post, limit their interactions on Twitter or just quit the platform altogether: “At a watershed moment when women around the world are using their collective power to amplify their voices through social media platforms, Twitter’s failure to consistently and transparently enforce its own community standards to tackle violence and abuse means that women are being pushed backwards towards a culture of silence.”

Amnesty International, which has been researching abuse against women on Twitter for the past two years, signed up 6,500 volunteers for what it refers to as the “Troll Patrol” after releasing a report earlier this year that described Twitter as a “toxic” place for women.

In total, the volunteers analyzed 288,000 tweets sent between January and December 2017 to the 778 women studied, who included politicians and journalists across the political spectrum from the United Kingdom and United States. Politicians included members of the U.K. Parliament and the U.S. Congress, while journalists represented a diverse group of publications, including The Daily Mail, The New York Times, Guardian, The Sun, gal-dem, Pink News and Breitbart.

The Troll Patrol’s volunteers, who come from 150 countries and range in age from 18 to 70 years old, received training about what constitutes a problematic or abusive tweet. Then they were shown anonymized tweets mentioning one of the 778 women and asked if the tweets were problematic or abusive. Each tweet was shown to several volunteers. In addition, Amnesty International said “three experts on violence and abuse against women” also categorized a sample of 1,000 tweets to “ensure we were able to assess the quality of the tweets labelled by our digital volunteers.”

The study defined “problematic” as tweets “that contain hurtful or hostile content, especially if repeated to an individual on multiple occasions, but do not necessarily meet the threshold of abuse,” while “abusive” meant tweets “that violate Twitter’s own rules and include content that promote violence against or threats of people based on their race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, disability, or serious disease.”

Then a subset of the labelled tweets was processed using Element AI’s machine-learning software to extrapolate the analysis to the total 14.5 million tweets that mentioned the 778 women during 2017. (Because tweets weren’t collected for the study until March 2018, Amnesty International notes that the scale of abuse was likely even higher because some abusive tweets may have been deleted or made by accounts that were suspended or disabled.) Element AI’s extrapolation produced the finding that 7.1 percent of tweets sent to the women were problematic or abusive, amounting to 1.1 million tweets in 2017.

Black, Asian, Latinx, and mixed race women were 34 percent more likely to be mentioned in problematic or abusive tweets than white women. Black women in particular were especially vulnerable: they were 84 percent more likely than white women to be mentioned in problematic or abusive tweets. One in 10 tweets mentioning black women in the study sample was problematic or abusive, compared to one in 15 for white women.

“We found that, although abuse is targeted at women across the political spectrum, women of color were much more likely to be impacted, and black women are disproportionately targeted. Twitter’s failure to crack down on this problem means it is contributing to the silencing of already marginalized voices,” said Milena Marin, Amnesty International’s senior advisor for tactical research, in the statement.

Breaking down the results by profession, the study found that 7 percent of tweets that mentioned the 454 journalists in the study were either problematic or abusive. The 324 politicians surveyed were targeted at a similar rate, with 7.12 percent of tweets that mentioned them problematic or abusive.

Of course, findings from a sample of 778 journalists and politicians in the U.K. and U.S. is difficult to extrapolate to other professions, countries or the general population. The study’s findings are important, however, because many politicians and journalists need to use social media in order to do their jobs effectively. Women, and especially women of color, are underrepresented in both professions, and many stay on Twitter simply to make a statement about visibility, even though it means dealing with constant harassment and abuse. Furthermore, Twitter’s API changes means many third-party anti-bullying tools no longer work, as technology journalist Sarah Jeong noted on her own Twitter profile, and the platform has yet to come up with tools that replicate their functionality.

For a long time I used blocktogether to automatically block accounts younger than 7 days and accounts with fewer than 15 followers. After Twitter’s API changes, that option is no longer available to me. — sarah jeong (@sarahjeong) December 18, 2018

A friend coded up a way for me to automatically mute people who tweeted certain trigger words for me. (Like, say, “gook.”) This is also no longer available to me because of API changes. — sarah jeong (@sarahjeong) December 18, 2018

Amnesty International’s other research about abusive behavior toward women on Twitter includes a 2017 online poll of women in 8 countries, and an analysis of abuse faced by female members of Parliament before the U.K.’s 2017 snap election. The organization said the Troll Patrol isn’t about “policing Twitter or forcing it to remove content.” Instead, the organization wants the platform to be more transparent, especially about how the machine-learning algorithms it uses to detect abuse.

Because the largest social media platforms now rely on machine learning to scale their anti-abuse monitoring, Element AI also used the study’s data to develop a machine-learning model that automatically detects abusive tweets. For the next three weeks, the model will be available to test on Amnesty International’s website in order to “demonstrate the potential and current limitations of AI technology.” These limitations mean social media platforms need to fine-tune their algorithms very carefully in order to detect abusive content without also flagging legitimate speech.

“These trade-offs are value-based judgements with serious implications for freedom of expression and other human rights online,” the organization said, adding that “as it stands, automation may have a useful role to play in assessing trends or flagging content for human review, but it should, at best, be used to assist trained moderators, and certainly should not replace them.”

TechCrunch has contacted Twitter for comment. Twitter replied with several quotes from a formal response issued to Amnesty International on December 12, Vijaya Gadde, Twitter’s legal, policy and trust and safety global lead.",Social Media,TechCrunch,https://techcrunch.com/2018/12/18/amnesty-international-used-machine-learning-to-quantify-the-scale-of-abuse-against-women-on-twitter/,"This study by Amnesty International and Element AI found that Twitter is a cesspool of harassment and abuse, with 7.1 percent of tweets sent to a sample of 778 women politicians and journalists in 2017 being either “problematic” or “abusive.” This has led many women to censor what they post, limit",Equality & Justice
71,Instagram says growth hackers are behind spate of fake Stories views – TechCrunch,"If you use Instagram and have noticed a bunch of strangers watching your Stories in recent months — accounts that don’t follow you and seem to be Russian — well, you’re not alone.

Nor are you being primed for a Russian disinformation campaign. At least, probably not. But you’re right to smell a fake.

TechCrunch’s very own director of events, Leslie Hitchcock, flagged the issue to us — complaining of “eerie” views on her Instagram Stories in the last couple of months from random Russian accounts, some seemingly genuine (such as artists with several thousand followers) and others simply “weird” looking.

A thread on Reddit also poses the existential question: “Why do Russian Models (that don’t follow me) keep watching my Instagram stories?” (The answer to which is: Not for the reason you hope.)

Instagram told us it is aware of the issue and is working on a fix.

It also said this inauthentic activity is not related to misinformation campaigns but is rather a new growth hacking tactic — which involves accounts paying third parties to try to boost their profile via the medium of fake likes, followers and comments (in this case by generating inauthentic activity by watching the Instagram Stories of people they have no real interest in in the hopes that’ll help them pass off as real and net them more followers).

Eerie is spot on. Some of these growth hackers probably have banks of phones set up where Instagram Stories are ‘watched’ without being watched. (Which obviously isn’t going to please any advertisers paying to inject ads into Stories… )

A UK social media agency called Hydrogen also noticed the issue back in June — blogging then that: “Mass viewing of Instagram Stories is the new buying followers of 2019”, i.e. as a consequence of the Facebook-owned social network cracking down on bots and paid-for followers on the platform.

So, tl;dr, squashing fakes is a perpetual game of whack-a-mole. Let’s call it Zuckerberg’s bane.

“Our research has found that several small social media agencies are using this as a technique to seem like they are interacting with the public,” Hydrogen also wrote, before going on to offer sage advice that: “This is not a good way to build a community, and we believe that Instagram will begin cracking down on this soon.”

Instagram confirmed to us it is attempting to crack down — saying it’s working to try to get rid of this latest eyeball-faking flavor of inauthentic activity. (We paraphrase.)

It also said that, in the coming months, it will introduce new measures to reduce such activity — specifically from Stories — but without saying exactly what these will be.

We also asked about the Russian element but Instagram was unable to provide any intelligence on why a big proportion of the fake Stories views seem to be coming from Russia (without any love). So that remains a bit of a mystery.

What can you do right now to prevent your Instagram Stories from being repurposed as a virtue-less signalling machine for sucking up naive eyeballs?

Switching your profile to private is the only way to thwart the growth hackers, for now.

Albeit, that means you’re limiting who you can reach on the Instagram platform as well as who can reach you.

When we suggested to Hitchcock she switch her account to private she responded with a shrug, saying: “I like to engage with brands.”",Social Media,TechCrunch,https://techcrunch.com/2019/08/15/instagram-says-growth-hackers-are-behind-spate-of-fake-stories-views/,"Growth hackers are using fake accounts to watch Instagram Stories in order to boost their profile, but this activity is not related to any disinformation campaigns. Inauthentic activity such as this can be reduced by switching to a private Instagram profile.",User Experience & Entertainment
72,Jack Dorsey records podcast with fitness writer who claimed ‘vaccines do indeed cause autism’ – TechCrunch,"Jack Dorsey, known for making tone-deaf statements on the platform he co-founded, is in the middle of another controversy. This time it is for plugging a podcast he recorded with fitness writer Ben Greenfield. Greenfield has espoused anti-vaccination views on Twitter and other platforms, continuing to do so despite measles outbreaks in the United States.

Dorsey retweeted Greenfield’s tweet about their podcast interview, commenting “Great conversation, and appreciate all you do to simplify the mountain of research focused on increasing one’s healthspan!”

Thanks Ben! Great conversation, and appreciate all you do to simplify the mountain of research focused on increasing one’s healthspan! Grateful for you https://t.co/EDmhE3OKnk — jack (@jack) March 13, 2019

Greenfield recently doubled down on the disproven claim that vaccines cause autism and has repeatedly included anti-vaccine propaganda on his podcast and social media pages.

Vaccines do indeed cause autism (and for Pete’s sake, don’t trust @snopes for your news on this matter or any other alternative health news): https://t.co/9AFQd6lekk — Ben Greenfield (@bengreenfield) February 11, 2019

A Twitter spokesperson told TechCrunch that Dorsey, who has done a string of podcast appearances recently, was not aware of Greenfield’s stance on vaccines, his participation in the interview was not an endorsement of them and that the topic was not discussed during their conversation.

Dorsey’s appearance on Greenfield’s show is especially striking considering that other tech companies, including YouTube and Facebook, are currently clamping down on anti-vaccination content. For example, YouTube recently announced it will demonetize anti-vaccination videos, while Facebook is down-ranking vaccine misinformation on its News Feed and hiding it on Instagram. Pinterest, which has prohibited anti-vaccination content in its terms for years, also recently said it will stop returning any search result related to vaccines.

Dorsey recently generated backlash for a tweet thread about his meditation retreat in Myanmar that neglected to mention the Rohingya genocide and declaring that Elon Musk is his favorite Twitter user, despite the fact that Musk’s tweets have landed him in legal trouble, including with the Securities and Exchange Commission.",Social Media,TechCrunch,https://techcrunch.com/2019/03/13/jack-dorsey-records-podcast-with-fitness-writer-who-claimed-vaccines-do-indeed-cause-autism/,"Jack Dorsey's recent podcast appearance with anti-vaccination advocate Ben Greenfield has reignited criticism of his tone-deafness, especially in light of recent measles outbreaks in the US and other tech companies clamping down on anti-vaccination content. Dorsey's actions are just another example of the risks of irresponsible statements on social",Social Norms & Relationships
73,Social media and telco companies urged to preserve evidence from Capitol attack,"Sen. Mark Warner (D-Virginia), incoming chair of the Senate Intelligence Committee, is asking mobile carriers and social media platforms to preserve “content and associated metadata” that may be connected to the attack on the US Capitol. Warner said in a statement Saturday that he contacted the CEOs of AT&T, T-Mobile, Verizon, Apple, Facebook, Gab, Google, Parler, Signal, Telegram, and Twitter.

“The United States Capitol is now a crime scene,” Warner wrote in his letters. “The FBI and other law enforcement agencies are currently investigating the events of that day, and trying to piece together what happened and the perpetrators involved. The prospect of litigation on behalf of the victims of the mayhem also is highly likely. Messaging data to and from your subscribers that may have participated in, or assisted, those engaged in this insurrection – and associated subscriber information – are critical evidence in helping to bring these rioters to justice.”

Warner noted that many of those participating in the January 6th attack that left five people dead posted images to social media, or shared them via text and mobile messaging platforms while the riot was underway and afterward, “to celebrate their disdain for our democratic process.”

All the named companies routinely comply with court-issued preservation orders, issued during criminal investigations. But Warner’s letter is an informal request, without the legal force of a criminal preservation order, and it is unclear how the companies will respond. Court-issued preservation orders are often issued under seal so it is also possible that one of the many agencies investigating the Capitol raid has already issued such an order.

We’ve reached out to the companies to ask how and if they plan to comply with Warner’s request and will update if we hear back from them.

A Facebook spokesperson said in an email to The Verge that the company was “continuing our ongoing, proactive outreach to law enforcement and have worked to quickly provide responses to valid legal requests. We are removing content, disabling accounts, and working with law enforcement to protect against direct threats to public safety.” T-Mobile acknowledged it had received Warner’s letter. “As always, we will fully cooperate with requests from law enforcement,” a spokesperson said in an email to The Verge.

To reiterate, however, Warner’s request doesn’t carry quite the same legal weight as a subpoena or other formal request from a law enforcement agency.

Warner told Politico that following Wednesday’s attack, Congress would “come back with a vengeance” against social media platforms that were unable to rein in violent content and threats on their platforms. “This is going to come back and bite ‘em,” he said.

Update January 9th 1:15PM ET: Adds comment from Facebook

Update January 9th 3:43PM ET: Adds comment from T-Mobile",Social Media,Verge,https://www.theverge.com/2021/1/9/22222200/social-media-telco-urged-preserve-evidence-capitol-attack-twitter-facebook-google-verizon-apple,"Sen. Mark Warner has warned that Congress will take action against social media platforms for their inability to control violent content and threats on their platforms, and has asked mobile carriers and social media platforms to preserve content and metadata related to the US Capitol attack.",Security & Privacy
74,Clearly Snapchat Doesn't Get What's Wrong With Yellowface,"Snapchat’s at it again. As first reported by The Verge and Mic, the ephemeral snapping app adored by millennials yesterday released a filter that superimposes stereotypical Asian features onto peoples’ faces, including slanted shut eyes. Many people, predictably, found this to be quite offensive. (The cartoon figure also looks to be carrying a bayonet...for some reason.)

Needless to say, the outrage spilled over on Twitter. Snapchat, which just four months ago was called out for another filter that celebrated 420 by superimposing Bob Marley dreadlocks and a digital blackface on photos, quickly took the “yellowface” filter down and apologized. According to a company spokesperson, the lens was meant to pay tribute to the popularity of anime, and won't be circulated again. But what Snapchat just proved is that it blatantly doesn’t get what the problem is with yellowface. Just like it didn't get what the problem was with blackface a few months ago. And that reveals how short-sighted the company is on these issues.

For Snapchat, experimenting with filters is good for business. They prove the company’s value for advertisers as a creative way to let brands insert themselves in conversations that don’t alienate teens. The more filters people use, the more likely they'll be to download a lens sponsored by a brand, like the Target-branded filter that's live right now. OK. So far so good. But this filter wasn't about a brand. It serves no discernible purpose—but it does normalize racial prejudice.

And the fact that the company targeted Asians is particularly tone-deaf, given the growing discussion in America about the insidiousness of Asian racism in mainstream media. (Check out the #myyellowfacestory hashtag on Twitter if you haven't already.) Many advocates have been highlighting this issue, including the comedian Aziz Ansari, who recently sought to shed light on how many Asian roles are played by white people in Hollywood—something that is itself a form of yellowface. Dating back to the beginning of cinema, Asian roles have been disproportionately played by white actors (the most high-profile example of this, of course, is Mickey Rooney playing a Japanese character in Breakfast at Tiffany's) but the trend continues to this day, with films like last year's Aloha, which cast Emma Stone as an Asian, to the upcoming film The Great Wall, which will star Matt Damon. And others in Hollywood, it seems, are still comfortable making jokes about Asians that they would not for other races or ethnicities. Two prominent examples of this were Chris Rock's jokes at the Oscars this year, and Amy Schumer's questionable use of stereotypical sound effects (video below). ""Yellowface has almost always been derogatory and demeaning, usually intentionally,"" says Frank H. Wu, chairman of the Committee of 100, an organization that focuses on issues important to the Chinese-American community.

Content View Iframe URL

The problem of racism against Asian Americans is nuanced. ""It’s hard for Asian immigrants and Asian Americans to raise issues of civil rights and diversity because we’re perceived as perpetual foreigners, not real Americans, or we’re the 'model minority,'"" says Wu. ""We don’t have any 'real' problems or issues to complain about."" Because of this reputation, Asian Americans are often considered safe targets for jokes (which usually flick at how “overly upstanding"" or ""so smart"" they are), because the jokes point out traits that are considered ""good."" Even in the tech industry, where diversity numbers are all but a joke, Asians are an over-represented group, contributing the notion that it's OK to stereotype them. Snapchat hasn't released numbers on its employee diversity, and CEO Evan Spiegel has said that's partially because the company doesn't ""think about diversity in terms of numbers that way.""

""I'm sure [Snapchat has] lots of Asian employees,"" says entrepreneur Anil Dash. ""The question is about why they're either not comfortable speaking up or couldn't anticipate this issue. Put another way: we haven't been effective in holding companies accountable.""

Here's what someone—anyone—should have pointed out in the pitch meeting for this filter: it's easy to tell if something is racist. Does it paint any race in broad strokes or stereotypes? Ding ding ding. This kind of thing projects specific (and often unrealistic) expectations onto groups of people—when we should be giving all the dignity of being an individual—which limits those folks' potential in social and economic mobility. There is nothing playful about it, as Snapchat has suggested after getting hit with backlash today. Yes, there's a gradation of racial images and remarks, going from subtle to not-so-subtle—but either way, these pave the way and make it easier for people to be bigoted. ""It's not about intent, but effect,"" says Wu. Nor is this any real kind of homage to anime. Anime history and culture is vibrant and deep, and can't be reduced to a single image of closed eyes, a narrow mouth, and rosy cheeks.

What Snapchat did, unwittingly or not, by releasing this yellowface filter is join a chorus of voices that have made racism against Asians in particular seem acceptable. Snapchat is not alone in this. But it wields specific influence with the most impressionable members of our society: young people, who count the app as among their most used. American culture is waking up, if slowly, to the harm in yellowface and blackface and the racist history and demeaning value judgments they imply. Snapchat runs the risk of being the last one to get that message.

Update on 08/10/2016 6:30PM ET: Updated with additional comments from Frank Wu.",Social Media,WIRED,https://www.wired.com/2016/08/snapchat-anime-filter-yellowface/,"Snapchat's recent filter that superimposed stereotypical Asian features onto faces has sparked outrage on social media and revealed the company's short-sightedness on the issue of racism, normalizing racial prejudice and perpetuating the notion that its acceptable to stereotype Asian Americans.",Equality & Justice
75,White House shares manipulated Infowars video to justify CNN press ban – TechCrunch,"Read this slowly: The White House’s press secretary has tweeted a manipulated video shared by the editor-at-large of conspiracy theorist outlet Infowars to attempt to justify its decision to suspend the press credentials of CNN’s chief white house correspondent.

CNN’s Jim Acosta had his press pass pulled by the White House earlier today after press secretary Sarah Sanders claimed he had “plac[ed] his hands on a young woman just trying to do her job”.

Acosta disputes this.

This is a lie. https://t.co/FastFfWych — Jim Acosta (@Acosta) November 8, 2018

The journalist had being trying to continue asking president Trump questions during a contentious exchange at a White House press briefing.

During this exchange Trump cut over him verbally — saying “that’s enough” — at which point a female White House intern moved towards Acosta and attempted to take the microphone out of his hands.

The journalist dodged and then blocked several attempts to take the microphone by using his arm and the side of his hand against the intern’s arm, addressing her with “pardon me ma’am” as he did so, and indicating that he was trying to ask Trump another question.

You can see what happened in a video shared by NBC News which captured footage of the incident (below). In the footage the intern can be seen stopping trying to remove the mic after Acosta speaks to her. He goes on to ask Trump if he is “worried about indictments coming down in [the Russia] investigation”.

Trump does not answer, repeating “that’s enough” and “put down the mic”.

Getting no answers, Acosta does then relinquish the mic.

BREAKING: White House aide grabs and tries to physically remove a microphone from CNN Correspondent Jim Acosta during a contentious exchange with President Trump at a news conference. pic.twitter.com/fFm7wclFw2 — NBC News (@NBCNews) November 7, 2018

Far right conspiracy theorist outlet Infowars quickly spun into action after this episode — publishing a couple of posts on its website couching Acosta’s actions as a “physical confrontation with female White House staffer”, and asking in a lengthy video post whether Acosta “assault[ed] a woman?”.

In the video Infowars editor-at-large Paul Joseph Watson can be seen following the modern political disinformation playbook — avoiding personally claiming the incident constituted an assault while repeatedly showing manipulated, slowed down footage, stripped of its audio, to make it look like an assault — all the while suggestively reframing what happened to whip up hyperpartisan sentiment (‘what if this had been a conservative reporter ranting at Obama’ etc) in order to manipulate his audience to side with the president against CNN.

Watson was also active on social media, seeding a further doctored version of footage on Twitter — which includes a repeat close crop that zooms in on the CNN reporter’s hand against the intern’s arm, making it look as if Acosta is giving her a karate chop.

Yes the incident clearly did happen. Acosta placed his hands on a woman. Do you think we're all stupid? pic.twitter.com/lbYOXtgXJx — Paul Joseph Watson (@PrisonPlanet) November 8, 2018

This is very clearly not what the unedited video shows.

In the unedited footage Acosta can be seen essentially brushing off the intern’s attempt to grab the mic — and addressing her politely at the crucial moment, when the side of his hand is resting on her arm. She responds to his polite “pardon me ma’am” by stepped back and stopping trying to take the mic away.

Acosta then asks Trump more questions which Trump does not answer.

Now Infowars conspiracy theorists creating doctored videos to try to spin hyperpartisan junk news is not new or news. Their business model is based on manipulating viewers’ emotions to flog them, er, junk supplements.

But what is new is that three hours after Sanders issued her series of tweets accusing Acosta of inappropriately placing his hands on a young woman, the White House press secretary tweeted again — this time appearing to share the exact same doctored video that had been shared earlier by Watson, as he worked to put the Infowars’ divisive alternative spin on reality.

Sanders referred directly to the video in her tweet, claiming that “inappropriate behaviour” had been “clearly documented in this video”:

We stand by our decision to revoke this individual’s hard pass. We will not tolerate the inappropriate behavior clearly documented in this video. pic.twitter.com/T8X1Ng912y — Sarah Sanders (@PressSec) November 8, 2018

So the White House is using video footage that’s been manipulated through a conspiracy theorist lens to justify a free speech-chilling ban on an actual journalist.

I’ll say that again: The White House is using a manipulated video shared by conspiracy theorists to justify suspending the press credentials of CNN’s chief white house correspondent.

And once more: The White House is using lies to justify pulling the press credentials of a genuine journalist.

turns out, as we probably all knew it would, that we didn't need deepfakes for nation states to start pushing lies using video https://t.co/rghBqSNcgO — hal (@halhod) November 8, 2018

Trump has made no secret of his hatred for CNN — repeatedly badging the cable news network ‘fake news’ in myriad vitriolic tweets since taking office.

Now his administration has gone a step further in seeking to stamp out reality by using manipulated video to bar a genuine news outlet from presidential press briefings. A news outlet that the president especially hates.

Let that sink in.

It’s not just conspiracy theorists who use this kind of information manipulation playbook of course. Authoritarian regimes, terrorists, criminals, racists… the list goes on.

Now you can add the White House press secretary to that ignominious list.

We’ve reached out to the White House to ask why Sanders chose to share the Infowars video — rather than sharing unedited footage of the incident. We’ll update this post with any response.

Meanwhile, on the list of people being allowed into White House press briefings these days…",Social Media,TechCrunch,https://techcrunch.com/2018/11/08/white-house-shares-manipulated-infowars-video-to-justify-cnn-press-ban/,The White House press secretary's decision to tweet a manipulated video shared by the editor-at-large of conspiracy theorist outlet Infowars in an attempt to justify banning CNN's chief White House correspondent from press briefings is a dangerous example of the power of social media to spread lies and manipulate public opinion.,"Information, Discourse & Governance"
76,Twitter Is Adding New Filtering Tools in an Effort to Curb Abuse,"Over the next few days, Twitter will add new features intended to help curb abuse on the platform. Users will be able filter out certain keywords, phrases, user names, and hashtags in their mentions, Twitter says. You'll also have the option to mute threads. The company is also revamping its abuse reporting system so that bystanders can report harassment and hate speech directly rather than leaving that option solely to the person on the receiving end.

And it's about time. From celebrities like Ghostbusters star Leslie Jones and Robin Williams's daughter Zelda Williams to average folks, users have complained for years that the company fails to protect them from onslaughts of abuse. Harassment has become such a big problem for Twitter that it was reportedly one of the main reasons business software company Salesforce decided not try to acquire the company. You can bet it's a big part of why no other company has stepped forward with a bid yet either.

One of the biggest problems with Twitter is that up until now, dealing with harassment has been a largely reactive process. You can report abuse, but Twitter offered few tools to avoid seeing harassing tweets in the first place. And in the instance of coordinated harassment campaigns like the one faced by Jones, new accounts constantly spring up to take the place of those that Twitter has blocked.

In August, Twitter added a feature called ""quality filter,"" which tried to help cut down on harassment and spam by blocking automated tweets and accounts that the company has somehow identified as potentially dubious. But the approach had some problems. If the filter accidentally caught tweets you wanted to see, you couldn't whitelist them. All you could do was choose to miss those tweets, or turn the quality filter off. The new features announced today will give users more control.

For example, keyword filtering could help those who routinely receive notifications containing gendered or ethnic slurs avoid seeing those tweets entirely. It could also help people who are being targeted by harassers using multiple different accounts. Instead of having to block or mute each account that sends you abuse after the fact, you could preemptively block tweets that contain keywords or hashtags frequently used by harassers.

According to a blog post announcing the new features, Twitter is also retraining its support staff regarding how they handle harassment cases and expanding the number of internal tools for dealing with abuse incidents. All of which sound like good steps—the question is whether even Twitter can really stem an onslaught that has grown to global proportions.",Social Media,WIRED,https://www.wired.com/2016/11/twitter-adding-new-filtering-tools-effort-curb-abuse/,"Twitter has long been plagued by abuse, with celebrities like Leslie Jones and Zelda Williams being victims of coordinated harassment campaigns. To address this, Twitter is introducing new features like keyword filtering and improved abuse reporting systems.",Equality & Justice
77,UK asks competition watchdog to put adtech market review top of its to-do list – TechCrunch,"The U.K. government has written to the country’s competition authority to ask the watchdog to respond to concerns about the lack of transparency in the digital advertising market and carry out a formal market study “as soon as possible.”

In a letter to the Competition and Markets Authority, chancellor Philip Hammond writes that the online ad sector “has been widely described as lacking transparency.”

“A Market Study would provide greater understanding of the existence, nature and potential solutions to any problems within the digital advertising market, and would further develop understanding of the operation of platform markets which rely on digital advertising for revenue,” he continues. “It would also enhance the CMA’s ability to detect and assess digital mergers when these may be of concern.”

The government’s move follows the publication of an independent review of competition policy this week, which recommended ministers ask the CMA to examine the market.

The government-commissioned Furman review also called for wider policy changes to respond to competition and consumer problems created by “winner takes all” tech platforms.

Hammond’s letter goes on to note that several U.K. parliamentary subcommittees have also called for regulatory scrutiny of online adtech practices in recent months, including the Digital Culture Media and Sport (DCMS) select committee, which called for the CMA to probe Facebook’s business practices.

Last year the U.K.’s data watchdog also called for an ethical pause of online political advertising — warning of risks to democratic debate and trust.

Reached for a response to the government’s call for it to prioritize a market review of online advertising, a spokesperson for the CMA pointed us to its response to the Furman review yesterday — in which it says it has also been considering whether to undertake work in the digital advertising market.

Though it warns that its ability to launch new projects is “heavily dependent on the outcome of EU Exit negotiations” — a reference to the ongoing Brexit process in the U.K. following the country’s 2016 referendum vote to leave the European Union.

In his letter, Hammond accepts that anything other than “an orderly exit” from the EU might derail the watchdog’s ability to prioritize a review of the online ad market, as he would like it to.

“I wish to be clear that I recognise the potential challenges on CMA resourcing associated with scenarios relating to the UK’s departure from the European Union other than an orderly exit,” he writes. “For these reasons I am today writing to ask whether the CMA Board would prioritise a decision on whether to take forward a market study into digital advertising market, as soon as you consider it possible to do so, and come forward with recommendations.”

Neither the CMA nor the government make mention of how social media targeted ads might have impacted the Brexit vote itself in their respective statements of concern about the online ad market.

Yet, last year, the U.K.’s Electoral Commission found that the official Leave campaign had breached election campaign spending limits — with illegal spend going on targeting pro-Brexit ads at voters on social media, principally via Facebook.

Last month’s DCMS committee report was also especially trenchant in its criticism of Facebook’s business practices — with MPs singling out the company for what it dubbed “disingenuous” and “bad faith” responses to genuine democratic concerns about the misuse of people’s data.

The other digital adtech elephant in the room is of course Google — which has been accused of essentially running its own market given its hold on various key links in the digital adtech chain.

The key question, which any future CMA review would surely probe, is how Google’s dominance affects other players in the online ad market and the ecosystem as a whole?

Anyone studying the duopoly control of Facebook/Google should check out Arete Research's (Richard Kramer) 15 pages of wisdom. One example. pic.twitter.com/jxbsMIdugV — Jason Kint (@jason_kint) May 26, 2017

We reached out to Google and Facebook for a response to Hammond’s request that the CMA prioritize carrying out a formal market review of online advertising.

Facebook told us it’s not commenting on the Furman review — though that’s not actually what we asked it — saying it’s still reviewing the report itself. It added that it had valued the opportunity to contribute to the process.

In a follow-up response, Facebook’s spokeswoman told us she wasn’t sure whether it would have anything further to add vis-à-vis the government pushing forward with asking the CMA to review the ad market.

At the time of writing, Google had not responded to our request for comment.

The company later pointed us to a statement made by local tech industry member association, Tech UK — which counts Google UK as a member — responding in a statement yesterday to the chancellor’s Spring Speech announcement instructing the CMA to prioritize an ad market review by saying it and its members would “closely engage in this review into the market.”

“Many businesses across the economy depend upon a well-functioning online advertising market,” said Tech UK deputy CEO Antony Walker. “Any intervention must be carefully thought through and targeted. Most importantly, the CMA must ensure it has the necessary technical experts available to conduct a review. This will require staff who truly understand the complexities of the adtech market and the role these platforms play in the wider digital ecosystem.”

This report was updated with additional comment.",Social Media,TechCrunch,https://techcrunch.com/2019/03/14/uk-asks-competition-watchdog-to-put-adtech-market-review-top-of-its-to-do-list/,"The U.K. government has called on the country’s competition authority to investigate the lack of transparency in the digital advertising market, following reports of the misuse of people's data and illegal spending on pro-Brexit ads on social media.",Security & Privacy
78,New Study Shows Bay Area Residents Increasingly Distrust Tech Companies,"In conservative circles, the pitchforks have been out for tech since at least the 2016 election season, with far-right media organizations like Breitbart and Project Veritas accusing the industry and its leaders of silencing Republican voices, advocating for open borders, and bankrolling Democratic campaigns. And yet, a new survey suggests that the tech backlash festering on the far-right fringes has also escalated on the industry's largely liberal home turf.

According to Edelman’s annual Trust Barometer survey of California, there has been a steep incline in the number of Californians and Bay Area residents calling for stricter regulation of the tech industry over the last year. While tech is still the most trusted industry in the state and around the globe, there is a growing feeling in California and the Bay Area that the tremendous success these companies have had is not helping the average citizen. The outlook is especially grim for social media companies, which Bay Area respondents viewed as the most untrustworthy industry of all, faring even worse than often vilified sectors like big pharma, telecommunications, and financial services.

“This idea in political campaigns that all politics is local couldn’t be any more relevant for the tech industry right now,” says Stacey Zolt Hara, managing director corporate and public affairs at Edelman Bay Area.

Whereas 62 percent of Californians surveyed expressed trust in tech, that figure dropped to just 37 percent when it comes to social media companies. In the Bay Area, it was only 35 percent. That disparity between tech and social media has a lot to do with more than three-quarters of respondents blaming social media companies for the spread of fake news.

These results also run counter to surveys taken last fall showing the majority of Americans still view social media giants like Facebook favorably. Since that time, though, Facebook, Twitter, and Google have all walked Congress through the gory details of how misinformation agents used and abused their platforms during the election. Edelman conducted its survey in January.

'The class divide you see in the rest of the country is exacerbated in California.' Stacey Zolt Hara, Edelman Bay Area

As tech companies work to fend off regulation in Washington, they’re unlikely to find many allies locally. More than two-thirds of Californians say the industry is under-regulated, a six point increase from the year before. Specifically, a whopping 87 percent of respondents, both Republican and Democrat, say tech companies ought to be financially liable for data breaches. That’s about the same as the number of people who said political ads on social media should be held to the same transparency and disclosure standards as television ads.

The list goes on. The majority of respondents support policies that would fine social media companies for publishing fake news, increase their taxes if they move manufacturing overseas, and reduce the number of skilled workers they can bring in from other countries, among other things.

Even as the industry bears the blame for global issues like the spread of fake news and propaganda, long simmering resentments between rich tech workers and the rest of the community persist. Just 38 percent of respondents in the Bay Area said they’d benefitted from the tech industry’s growth, a single point drop from the year before. More than three-quarters of Bay Area residents say tech companies need to address their outsized impact on traffic and housing costs. Meanwhile, the number of California respondents who said the tech industry further enriches the wealthy without helping the rest of the state rose five points to 64 percent.",Social Media,WIRED,https://www.wired.com/story/trust-in-tech-declines-bay-area/,"The Trust Barometer survey suggests that there is growing resentment towards Social Media companies in California, with the majority of respondents blaming them for spreading fake news, increasing traffic and housing costs, and not benefiting the rest of the state.",Equality & Justice
79,"Twitter starts testing its own version of Stories, called ‘Fleets,’ which disappear after 24 hours – TechCrunch","Twitter is testing its own version of Stories. The company announced today it will begin to trial a new sharing format called “Fleets,” starting in Brazil, which will let users post ephemeral content to its social network for the first time. Unlike Tweets, Twitter’s new Fleets can’t receive Likes, Replies or Retweets. And they’ll disappear entirely after 24 hours.

Fleets aren’t non-public, to be clear; they’re just a little less accessible. You could visit someone’s public Twitter profile and tap to view their Fleets even if you don’t follow them. But their Fleet won’t circulate Twitter’s network, show up in Search or Moments, and it can’t be embedded on an external website.

Twitter is one of the last major social platforms to test out a Stories format. First popularized by Snapchat, you can now find a version of Stories across Instagram, Facebook, WhatsApp, YouTube, and others. Spotify also recently announced a test of a Stories-like feature and even Microsoft’s Skype gave it a go at one time, as did Match and Bumble.

In Twitter’s case, Fleets are meant to address one of the primary reasons why users don’t tweet: they feel uncomfortable with Twitter’s public nature. On this front, Twitter said at CES in January it would soon test new controls for determining the audience for your Tweets — like public, followers only, and so on. But those tests haven’t yet begun, we understand.

Fleets, meanwhile, represent a simpler and more familiar solution.

In Brazil, testers will see rounded profile icons right at the top of their Timeline on Twitter’s mobile app. This will be immediately recognizable as a Stories feature. The first icon is actually a little thought bubble displaying your own profile photo. Users will simply click on the “+” button to compose their Fleet.

The composer interface is more bare-bones than what you’d find on rival social networking sites. Twitter says that’s to reflect its product’s text-centric nature. However, users are able to add photos, GIFs and videos to a Fleet, even if fancy editing tools are not available.

At launch, consumers will be able to post videos up to 2 minutes and 20 seconds in length (or 512MB). Whitelisted publishers will be able to publish videos up to 10 minutes in length.

Users can also post multiple Fleets, which viewers will move through using gestures.

This is where Twitter’s version of Stories is a little different and potentially cumbersome. To view the multiple Fleets a user has posted, you swipe down instead of advancing through the Fleets horizontally with taps on the sides of the screen. Meanwhile, to move to the next person’s Fleet, you swipe to the left.

But these gestures could change based on user feedback, Twitter says.

Though Fleets don’t move through Twitter’s network the way that Tweets can, viewers can interact with them, in a way. If the poster allows DMs (direct messages), you can reply to the Fleet privately. You’ll also be able to react to a Fleet with an emoji, similar to how Stories work on other social apps.

One of Twitter’s bigger challenges with its take on Stories is figuring out which Fleets will be displayed first on your home screen. On networks like Snapchat, Instagram, and Facebook, users typically follow their friends and a varying range of public figures and brands. But on Twitter, it’s fairly common to find users who follow hundreds and even thousands of other users.

To make a Stories feature compelling on Twitter, the lineup of Fleets will have to be highly personalized to the end user, perhaps by allowing users to designate their “close friends” at some point. (Twitter won’t have any such option at Fleets’ launch, however.)

For now, Twitter says it determines which Fleets to display first based on recency and mutual follows.

Twitter’s test arrives shortly after activist investor Elliott Management Corp. took a stake in Twitter to push for changes at the social network. The investment firm believes Twitter isn’t living up to its potential and its CEO Jack Dorsey — who the firm wants replaced — is distracted by his side projects and by his other CEO job at Square. Twitter is also seen as having lagged behind on innovations. While other social networks have adopted popular features like Stories, Twitter has remained focused only on its core product.

The company says it will use the Brazil test to better understand if Fleets help users become more comfortable sharing on Twitter, a perennial problem for the post-in-public social network. (Last year, Twitter even invented a new metric — mDAUs, or Monetizable Daily Active Users — in order to make its user numbers look more attractive to Wall Street investors, who have been disappointed with Twitter’s slow user growth.)

The public nature of Tweets isn’t Twitter’s real problem, of course. Its that Twitter has allowed online abuse over the years to run rampant on the platform. Twitter today feels like a minefield, not a safe space to share your thoughts.

In addition, Twitter has become associated with a form of aggressive wokeness dubbed “call-out culture” or “cancel culture.” This can sometimes involve adversaries digging through a user’s older tweets in order to hold them accountable for offensive remarks or inappropriate behavior they posted online years ago. Whether warranted or not, cancel culture’s mere existence has made users more hesitant to Tweet and more likely to use a third-party app or service to auto-delete their Tweets if they do.

Of course, users’ hesitancy to post is bad for Twitter’s growth and bad for advertisers, who need a steady stream of user-generated content into which they can insert their marketing messages.

Twitter says Fleets will begin to roll out starting today to Brazilian users on both iOS and Android, following the app’s update. The test will run for a few months before Twitter decides to roll out it out to other global markets.",Social Media,TechCrunch,https://techcrunch.com/2020/03/04/twitter-starts-testing-its-own-version-of-stories-called-fleets-which-disappear-after-24-hours/,"Twitter's new ""Fleets"" feature is being tested in Brazil to address users' hesitancy to post due to the platform's associated ""call-out culture"" or ""cancel culture,"" which involves users being held accountable for past offensive remarks or behavior.",Social Norms & Relationships
80,Facebook’s ad system shown failing to enforce its own anti-discriminatory policy – TechCrunch,"Can Facebook be trusted to abide by even its own stated standards? In the case of Internet political advertising the social giant wants to be allowed to continue to self regulate — despite the scandal of Russian bought socially divisive ads which (we now know) were tainting democratic discussion during the 2016 US presidential election (and beyond).

‘Don’t regulate us, we can regulate ourselves — honest!‘ is shaping up to be CEO Mark Zuckerberg’s massively moonshot new year project for 2018.

But results from a new ProPublica investigation suggest the tech giant is failing at even simple self-policing — undermining any claims it can responsibly manage the bad and even out-and-out illegal outcomes that are being enabled via its platform, and bolstering the case for more formal regulation.

Case in point: A year ago Facebook said it would disable ethnic affinity ad targeting for housing, employment and credit-related ads, following a ProPublica investigation that had suggested the platform’s ad-targeting capabilities could be used for discriminatory advertising — particularly in housing and employment, where such practices are illegal.

This month ProPublica checked in again, to see how Facebook is doing — by purchasing dozens of rental housing ads and asking that Facebook’s ad platform exclude groups that are protected from discrimination under the US Federal Fair Housing Act — such as African Americans and Jews.

Its test ads promoted a fictional apartment for rent, targeted at people aged 18 to 65 who were living in New York, house hunting and likely to move — with ProPublica narrowing the audience by excluding certain “Behaviors”, listed in a section Facebook now calls “Multicultural Affinity”, including “Hispanic”, “African American” and “Asian American”.

However instead of the platform blocking the potentially discriminatory ad buys, ProPublica reports that all its ads were approved by Facebook “within minutes” — including an ad that sought to exclude potential renters “interested in Islam, Sunni Islam and Shia Islam”. It says that ad took the longest to approve of all its buys (22 minutes) — but that all the rest were approved within three minutes.

It also successfully bought ads that it judged Facebook’s system should at least flag for self-certification because they were seeking to exclude other members of protected categories. But the platform just accepted housing ads blocked from being shown to categories including ‘soccer moms’, people interested in American sign language, gay men and people interested in wheelchair ramps.

Yet, back in February, Facebook announced new “stronger” anti-discriminatory ad polices, saying it was deploying machine learning tech tools to help it identify ads in the categories of concern.

“We’ve updated our policies to make our existing prohibition against discrimination even stronger. We make it clear that advertisers may not discriminate against people based on personal attributes such as race, ethnicity, color, national origin, religion, age, sex, sexual orientation, gender identity, family status, disability, medical or genetic condition,” it wrote then.

Of the new tech tools, Facebook said: “This will allow us to more quickly provide notices and educational information to advertisers — and more quickly respond to violations of our policy.”

Explaining how the new system would work, Facebook said advertisers who attempt to show “an ad that we identify as offering a housing, employment or credit opportunity” and which “either includes or excludes our multicultural advertising segments — which consist of people interested in seeing content related to the African American, Asian American and US Hispanic communities” will find the platform disapproves the ad.

The new system would also require all advertisers that attempt to buy targeted advertising in the categories of concern to self-certify they are complying with Facebook’s anti-discrimination policies and with “applicable anti-discrimination laws”.

ProPublica says it never even encountered these self-certification screens, as well as never having any of its ad buys blocked.

“Under its own policies, Facebook should have flagged these ads, and prevented the posting of some of them. Its failure to do so revives questions about whether the company is in compliance with federal fair housing rules, as well as about its ability and commitment to police discriminatory advertising on the world’s largest social network,” it writes.

Responding to ProPublica’s findings, Facebook sent a statement attributed to Ami Vora, VP of product management, in which she concedes its system failed in this instance. “This was a failure in our enforcement and we’re disappointed that we fell short of our commitments. The rental housing ads purchased by ProPublica should have but did not trigger the extra review and certifications we put in place due to a technical failure,” said Vora.

She went on to claim Facebook’s anti-discrimination system had “successfully flagged millions of ads” in the credit, employment and housing categories — but also said Facebook will now begin requiring self-certification for ads in all categories that choose to exclude an audience segment.

“Our systems continue to improve but we can do better,” she added.

The latter phrase is now a very familiar refrain from Facebook where content review and moderation is concerned. Aside from socially divisive political disinformation, it has faced growing criticism this year for enabling the spread of content such as extremist propaganda and child exploitation, as well as for multiple incidents of its tools being used to broadcast suicides and murders.

The wider question for governments and regulators is at what point will Facebook’s attempts to ‘do better’ be deemed just not good enough?

Commenting on ProPublica’s findings in a statement, Rachel Goodman, an attorney with the ACLU‘s Racial Justice Program, said: “We’re very, very disappointed to see these significant failures in Facebook’s system for identifying and preventing discrimination in advertisements for rental housing. We and other advocates spent many hours helping Facebook move toward solving the egregious discrimination problem built into its ad targeting business — that advertisers could exclude people from seeing ads based on race, gender, and religion, including in ads for housing, credit, and employment. Facebook’s representations to us indicated that this problem had been substantially solved, but it now seems clear that was not the case.

“While we appreciate that Facebook continues to express a desire to get it right on this important civil rights issue, this story highlights the need for greater transparency and accountability. Had outside researchers been able to see and the system Facebook created to catch these ads, those researchers could have spotted this problem and ended the mechanism for discrimination sooner.”

This story was updated with additional comment from the ACLU",Social Media,TechCrunch,https://techcrunch.com/2017/11/22/facebooks-ad-system-shown-failing-to-enforce-its-own-anti-discriminatory-policy/,"Social media platforms such as Facebook have been found to be failing to adequately self-regulate, enabling the spread of discriminatory and illegal content, as well as extremist propaganda, child exploitation, and even streaming of suicides and murders. This has raised questions about the need for greater regulation of the platforms.",Equality & Justice
81,Bad Algorithms Didn't Break Democracy,"But the reason these companies—Facebook in particular—talk about free speech is not simply to conceal their economic stake in the reproduction of misinformation; it’s also a polite way for them to suggest that the real culpability for what pullulates on their platforms lies with their users. Facebook has always presented itself, in contrast to legacy gatekeepers, as a neutral bit of infrastructure; people may post what they like and access what they fancy. When Zuckerberg talks about “free expression,” he is describing the sanctity of a market­place where supply is liberated to seek the level of demand. What he is saying, by implication, is that the affliction of partisan propaganda reflects not a problem of supply but of demand—a deep and transparent expression of popular desire.

This might be a maddening defense, but it is not a trivial argument to counter. Over the past few years, the idea that Facebook, YouTube, and Twitter somehow created the conditions of our rancor—and, by extension, the proposal that new regulations or algorithmic reforms might restore some arcadian era of “evidential argument”—has not stood up well to scrutiny. Immediately after the 2016 election, the phenomenon of “fake news” spread by Macedonian teenagers and Russia’s Internet Research Agency became shorthand for social media’s wholesale perversion of democracy; a year later, researchers at Harvard University’s Berkman Klein Center concluded that the circulation of abjectly fake news “seems to have played a relatively small role in the overall scheme of things.” A recent study by academics in Canada, France, and the US indicates that online media use actually decreases support for right-wing populism in the US. Another study examined some 330,000 recent YouTube videos, many associated with the far right, and found little evidence for the strong “algorithmic radicalization” theory, which holds YouTube’s recommendation engine responsible for the delivery of increasingly extreme content.

Regardless of how one study or another breaks, tech companies have reason to prefer abstract arguments about the values of untrammeled expression. They have chosen to adopt the language of classical liberalism precisely because it puts their liberal critics in an uncomfortable position: It’s unacceptably patronizing to claim that some subset of our neighbors have to be protected from their own demands. It’s even worse to question the authenticity of those demands in the first place—to suggest that the desires of our neighbors are not really their own. Critics must rely on such potted ideas as “astroturfing” to explain how it might be that good people come to demand bad things.

The case for corporate blame is, at any rate, probably more expedient than it is empirical. It’s much easier to imagine how we might exercise leverage over a handful of companies than it is to address the preferences of billions of users. It’s always tempting to search for our keys where the light is better. A better solution would require tech’s critics to take what people demand as seriously as the corporations do, even if that means looking into the dark.

The first step toward an honest reckoning with the reality of demand is to admit that political polarization long predates the rise of social media. By the time Facebook opened its walled orchard to everyone in 2006, the US had already spent 40 years sorting itself into two broad camps, as Ezra Klein points out in his new book Why We’re Polarized. At the beginning of the 1960s, the Democratic and Republican parties both contained self-described liberals and conservatives. Then the passage of civil rights legislation and Richard Nixon’s Southern strategy set in motion the coalescence of each party around a consensus set of “correct” views. Race was the original fault line, and has remained salient. But the constellations of other views often shifted and were increasingly secondary to the simpler matter of group affiliation.

Where many technology critics see the rise of social media, some 15 years ago, as a vast shift that ushered in the era of “filter bubbles” and tribal sorting, Klein describes it as less the original cause than an accelerant—especially insofar as it encouraged individuals to see all their beliefs and preferences, if only in brief but powerful moments of perceived threat, as potential expressions of a single underlying political identity. Facebook and Twitter allotted each user one persona, with a profile, a history, and a signaling apparatus of unprecedented reach. Users faced new and acute kinds of public pressure—to be coherent, for one thing—and could only look to other members of their communities for clues to what might viably constitute coherence.",Social Media,WIRED,https://www.wired.com/story/polarization-politics-misinformation-social-media/,"Social media has accelerated political polarization, by encouraging users to see all their beliefs and preferences as part of a single underlying political identity, leading to acute public pressure to be coherent, and increasing reliance on other members of the same community for cues on how to be coherent.",Politics
82,Zuckerberg again snubs UK parliament over call to testify – TechCrunch,"Facebook has once again eschewed a direct request from the UK parliament for its CEO, Mark Zuckerberg, to testify to a committee investigating online disinformation — without rustling up so much as a fig-leaf-sized excuse to explain why the founder of one of the world’s most used technology platforms can’t squeeze a video call into his busy schedule and spare UK politicians’ blushes.

Which tells you pretty much all you need to know about where the balance of power lies in the global game of (essentially unregulated) U.S. tech platforms giants vs (essentially powerless) foreign political jurisdictions.

At the end of an 18-page letter sent to the DCMS committee yesterday — in which Facebook’s UK head of public policy, Rebecca Stimson, provides a point-by-point response to the almost 40 questions the committee said had not been adequately addressed by CTO Mike Schroepfer in a prior hearing last month — Facebook professes itself disappointed that the CTO’s grilling was not deemed sufficient by the committee.

“While Mark Zuckerberg has no plans to meet with the Committee or travel to the UK at the present time, we fully recognize the seriousness of these issues and remain committed to providing any additional information required for their enquiry into fake news,” she adds.

So, in other words, Facebook has served up another big fat ‘no’ to the renewed request for Zuckerberg to testify — after also denying a request for him to appear before it in March, when it instead sent Schroepfer to claim to be unable to answer MPs’ questions.

At the start of this month committee chair Damian Collins wrote to Facebook saying he hoped Zuckerberg would voluntarily agree to answer questions. But the MP also took the unprecedented step of warning that if the Facebook founder did not do so the committee would issue a formal summons for him to appear the next time Zuckerberg steps foot in the UK.

Hence, presumably, that addendum line in Stimson’s letter — saying the Facebook CEO has no plans to travel to the UK “at the present time”.

The committee of course has zero powers to comply testimony from a non-UK national who is resident outside the UK — even though the platform he controls does plenty of business within the UK.

Last month Schroepfer faced five hours of close and at times angry questions from the committee, with members accusing his employer of lacking integrity and displaying a pattern of intentionally deceptive behavior.

The committee has been specifically asking Facebook to provide it with information related to the UK’s 2016 EU referendum for months — and complaining the company has narrowly interpreted its requests to sidestep a thorough investigation.

More recently research carried out by the Tow Center unearthed Russian-bought UK targeted immigration ads relevant to the Brexit referendum among a cache Facebook had provided to Congress — which the company had not disclosed to the UK committee.

MPs say Facebook has misled Parliament. Again. It told MPs only $1 of Russian ads in UK. But @d1gi has now found nearly £1,000 worth. This is ads paid for in roubles inciting hate for refugees & immigrants in months before Brexit. https://t.co/0kxbP0b62A — Carole Cadwalladr (@carolecadwalla) May 13, 2018

At the end of the CTO’s evidence session last month the committee expressed immediate dissatisfaction — claiming there were almost 40 outstanding questions the CTO had failed to answer, and calling again for Zuckerberg to testify.

It possibly overplayed its hand slightly, though, giving Facebook the chance to serve up a detailed (if not entirely comprehensive) point-by-point reply now — and use that to sidestep the latest request for its CEO to testify.

Still, Collins expressed fresh dissatisfaction today, saying Facebook’s answers “do not fully answer each point with sufficient detail or data evidence”, and adding the committee would be writing to the company in the coming days to ask it to address “significant gaps” in its answers. So this game of political question and self-serving answer is set to continue.

In a statement, Collins also criticized Facebook’s response at length, writing:

It is disappointing that a company with the resources of Facebook chooses not to provide a sufficient level of detail and transparency on various points including on Cambridge Analytica, dark ads, Facebook Connect, the amount spent by Russia on UK ads on the platform, data collection across the web, budgets for investigations, and that shows general discrepancies between Schroepfer and Zuckerberg’s respective testimonies. Given that these were follow up questions to questions Mr Schroepfer previously failed to answer, we expected both detail and data, and in a number of cases got excuses. If Mark Zuckerberg truly recognises the ‘seriousness’ of these issues as they say they do, we would expect that he would want to appear in front of the Committee and answer questions that are of concern not only to Parliament, but Facebook’s tens of millions of users in this country. Although Facebook says Mr Zuckerberg has no plans to travel to the UK, we would also be open to taking his evidence by video link, if that would be the only way to do this during the period of our inquiry. For too long these companies have gone unchallenged in their business practices, and only under public pressure from this Committee and others have they begun to fully cooperate with our requests. We plan to write to Facebook in the coming days with further follow up questions.

In terms of the answers Facebook provides to the committee in its letter (plus some supporting documents related to the Cambridge Analytica data misuse scandal) there’s certainly plenty of padding on show. And deploying self-serving PR to fuzz the signal is a strategy Facebook has mastered in recent more challenging political times (just look at its ‘Hard Questions’ series to see this tactic at work).

At times Facebook’s response to political attacks certainly looks like an attempt to drown out critical points by deploying self-serving but selective data points — so, for instance, it talks at length in the letter about the work it’s doing in Myanmar, where its platform has been accused by the UN of accelerating ethnic violence as a result of systematic content moderation failures, but declines to state how many fake accounts it’s identified and removed in the market; nor will it disclose how much revenue it generates from the market.

Asked by the committee what the average time to respond to content flagged for review in the region, Facebook also responds in the letter with the vaguest of generalized global data points — saying: “The vast majority of the content reported to us is reviewed within 24 hours.” Nor does it specify if that global average refers to human review — or just an AI parsing the content.

Another of the committee’s questions is: ‘Who was the person at Facebook responsible for the decision not to tell users affected in 2015 by the Cambridge Analytica data misuse scandal?’ On this Facebook provides three full paragraphs of response but does not provide a direct answer specifying who decided not to tell users at that point — so either the company is concealing the identity of the person responsible or there simply was no one in charge of that kind of consideration at that time because user privacy was so low a priority for the company that it had no responsibility structures in place to enforce it.

Another question — ‘who at Facebook heads up the investigation into Cambridge Analytica?’ — does get a straight and short response, with Facebook saying its legal team, led by general counsel Colin Stretch, is the lead there.

It also claims that Zuckerberg himself only become aware of the allegations that Cambridge Analytica may not have deleted Facebook user data in March 2018 following press reports.

Asked what data it holds on dark ads, Facebook provides some information but it’s also being a bit vague here too — saying: “In general, Facebook maintains for paid advertisers data such as name, address and banking details”, and: “We also maintain information about advertiser’s accounts on the Facebook platform and information about their ad campaigns (most advertising content, run dates, spend, etc).”

It does also confirms it can retain the aforementioned data even if a page has been deleted — responding to another of the committee’s questions about how the company would be able to audit advertisers who set up to target political ads during a campaign and immediately deleted their presence once the election was over.

Though, given it’s said it only generally retains data, we must assume there are instances where it might not retain data and the purveyors of dark ads are essentially untraceable via its platform — unless it puts in place a more robust and comprehensive advertiser audit framework.

The committee also asked Facebook’s CTO whether it retains money from fraudulent ads running on its platform, such as the ads at the center of a defamation lawsuit by consumer finance personality Martin Lewis. On this Facebook says it does not “generally” return money to an advertiser when it discovers a policy violation — claiming this “would seem perverse” given the attempt to deceive users. Instead it says it makes “investments in areas to improve security on Facebook and beyond”.

Asked by the committee for copies of the Brexit ads that a Cambridge Analytica linked data company, AIQ, ran on its platform, Facebook says it’s in the process of compiling the content and notifying the advertisers that the committee wants to see the content.

Though it does break out AIQ ad spending related to different vote leave campaigns, and says the individual campaigns would have had to grant the Canadian company admin access to their pages in order for AIQ to run ads on their behalf.

The full letter containing all Facebook’s responses can be read here.",Social Media,TechCrunch,https://techcrunch.com/2018/05/15/zuckerberg-again-snubs-uk-parliament-over-call-to-testify/,"The power imbalance between U.S. tech giants and political jurisdictions outside of the U.S., such as the UK, is evident in the fact that Facebook has refused to comply with the UK Parliament's request that its CEO, Mark Zuckerberg, testify. This refusal highlights the lack of regulation of these tech giants and the subsequent lack of accountability","Information, Discourse & Governance"
83,Cambridge Analytica raided by UK data watchdog – TechCrunch,"The UK’s data watchdog, the ICO, finally obtained a warrant to enter and search the offices of Cambridge Analytica late Friday — carrying out an evidence gathering sweep of the company into the small hours of Saturday morning.

Cambridge Analytica is at the centre of a data misuse storm that’s wiped billions off the value of Facebook since newspaper revelations late last week revealed the extent of data swiped by the UK political consultancy which intended to use the information for the Trump campaign.

In a statement on its website today, the ICO said:

The warrant to inspect the premises of Cambridge Analytica was executed at 20.00 on 23 March 2018. Our investigators left the premises at about 03.00. We will now need to assess and consider the evidence before deciding the next steps and coming to any conclusions. This is one part of a larger investigation by the ICO into the use of personal data and analytics by political campaigns, parties, social media companies and other commercial actors.

ICO commissioner Elizabeth Denham has confirmed previously that complaints related to Cambridge Analytica’s use of Facebook data for political ad targeting form part of that larger, ongoing investigation.

It opened its formal investigation into the use of data analytics for political purposes in May 2017.

This week, after saying it had failed to obtain information it had requested from CA, the regulator applied for a warrant to enter and search CA’s offices — in the wake of reports by the New York Times and Observer of London based on interviews with former CA employee, Chris Wylie.

According to Wylie, CA used a survey app created by a Cambridge University academic to suck up data on 50M Facebook users. The intent was to use the information for political targeting purposes for the benefit of the Trump campaign.

Earlier this month Denham told a UK parliamentary committee that’s also running an investigation into fake news that she hoped to publish her review of digital political ad targeting this May. Although that was before the ICO decided it needed to apply for a warrant to raid CA’s offices.

The regulator has now said it will need time to sift through the evidence it gathered overnight — so it remains to be seen whether its wider review of political ad targeting will be delayed as a result.

ICO granted warrant: We’re pleased with the decision of the judge and we plan to execute the warrant shortly. This is just one part of a larger investigation into the use of personal data for political purposes and we will now need time to collect and consider the evidence — ICO (@ICOnews) March 23, 2018

In an online statement published yesterday, the acting CEO of CA, Alexander Taylor, apologized for Facebook “data and derivatives” having been obtained without consent “from most respondents”.

But he also claimed the company believed it was acting within Facebook’s policies and UK data protection law when it licensed the data from professor Aleksandr Kogan whose survey app was the Trojan horse used to gather 270,000 Facebook users’ data and their friends’ data — resulting in some 50M profiles being harvested in all.

“[CA] believed that the data had been obtained in line with Facebook’s terms of service and data protection laws,” writes Taylor.

He also claims that in October 2015 CA deleted the data from its file server after Facebook asked it to. But he seems less certain whether other copies might not have existed as he says the company “began the process of searching for and removing any of its derivatives in our system”.

“When Facebook sought further assurances a year ago, we carried out an internal audit to make sure that all the data, all derivatives and backups had been deleted, and gave Facebook a certificate to this effect. Please can I be absolutely clear: we did not use any GSR data in the work we did in the 2016 US presidential election,” he further claims.

“We are now undertaking an independent third-party audit to verify that we do not hold any GSR data,” Taylor adds. “We have been in touch with the UK Information Commissioner’s Office (ICO) since February 2017, when ​we hosted its team in our London office to provide total transparency on the data we hold, how we process it, and the legal basis for us processing it. I want to make sure we remain committed to helping the ICO in their investigations.”",Social Media,TechCrunch,https://techcrunch.com/2018/03/24/cambridge-analytica-raided-by-uk-data-watchdog/,"The UK's data watchdog, the ICO, has recently obtained a warrant to enter and search the offices of Cambridge Analytica, a company at the centre of a data misuse storm that has had a negative impact on Social Media companies, including Facebook. The ICO's investigation is part of a larger probe into the use of personal data for political",Security & Privacy
84,"UK Online Safety Bill, coming next year, will propose fines of up to 10% of annual turnover for breaching duty of care rules – TechCrunch","UK Online Safety Bill, coming next year, will propose fines of up to 10% of annual turnover for breaching duty of care rules

The U.K. is moving ahead with a populist but controversial plan to regulate a wide range of illegal and/or harmful content almost anywhere online such stuff might pose a risk to children. The government has set out its final response to the consultation it kicked off back in April 2019 — committing to introduce an Online Safety Bill next year.

“Tech platforms will need to do far more to protect children from being exposed to harmful content or activity such as grooming, bullying and pornography. This will help make sure future generations enjoy the full benefits of the internet with better protections in place to reduce the risk of harm,” it said today.

In an earlier partial response to the consultation on its Online Harms white paper ministers confirmed the U.K.’s media regulator, Ofcom, as its pick for enforcing the forthcoming rules.

Under the plans announced today, the government said Ofcom will be able to levy fines of up to 10% of a company’s annual global turnover (or £18 million, whichever is higher) on those that are deemed to have failed in their duty of care to protect impression eyeballs from being exposed to illegal material — such as child sexual abuse, terrorist material or suicide-promoting content.

Ofcom will also have the power to block non-compliant services from being accessed in the U.K. — although it’s not clear how exactly that will be achieved (or whether the legislation will seek to prevent VPNs being used by Brits to access blocked internet services).

The regulator’s running costs will be paid by companies that fall under the scope of the law, above a threshold based on global annual revenue, per the government, although it’s not yet clear where that pay-bar will kick in (nor how much tech giants and others will have to stump up for the cost of the oversight).

The online safety “duty of care” rules are intended to cover not just social media giants like Facebook but a wide range of internet services — from dating apps and search engines to online marketplaces, video sharing platforms and instant messaging tools, as well as consumer cloud storage and even video games that allow relevant user interaction.

P2P services, online forums and pornography websites will also fall under the scope of the laws, as will quasi-private messaging services, according to a government press release.

That raises troubling questions about whether the legal requirements could put pressure on companies not to use end-to-end encryption (i.e. if they face being penalized for not being able to monitor robustly encrypted content for illegal material).

“The new regulations will apply to any company in the world hosting user-generated content online accessible by people in the UK or enabling them to privately or publicly interact with others online,” the government writes in a press release.

The rules will include different categories of responsibility for content and activity — with a top tier (category 1) only applying to companies with “the largest online presences and high-risk features,” which the government said is likely to include Facebook, TikTok, Instagram and Twitter.

“These companies will need to assess the risk of legal content or activity on their services with ‘a reasonably foreseeable risk of causing significant physical or psychological harm to adults’. They will then need to make clear what type of ‘legal but harmful’ content is acceptable on their platforms in their terms and conditions and enforce this transparently and consistently,” it said.

Category 1 companies will also have a legal requirement to publish transparency reports about the steps they are taking to tackle online harms, per the government’s PR.

While all companies that fall under the scope of the law will be required to have mechanisms so people can easily report harmful content or activity while also being able to appeal the takedown of content, it added.

The government believes that less than 3% of U.K. businesses will fall within the scope of the legislation — adding that “the vast majority” will be Category 2 services.

Protections for free speech are also slated as being baked in — with the government saying the laws will not affect articles and comments sections on news websites, for example.

The legislation will contain provisions to impose criminal sanctions on senior managers (introduced by parliament via secondary legislation). On this the government added that it will not hesitate to use the power if companies fail to take the new rules seriously (such as by not responding “fully, accurately and in a timely manner” to information requests from Ofcom).

Commenting on the plans in a statement, digital secretary Oliver Dowden said: “I’m unashamedly pro tech but that can’t mean a tech free for all. Today Britain is setting the global standard for safety online with the most comprehensive approach yet to online regulation. We are entering a new age of accountability for tech to protect children and vulnerable users, to restore trust in this industry, and to enshrine in law safeguards for free speech.

“This proportionate new framework will ensure we don’t put unnecessary burdens on small businesses but give large digital businesses robust rules of the road to follow so we can seize the brilliance of modern technology to improve our lives.”

In another supporting statement, home secretary Priti Patel added: “Tech companies must put public safety first or face the consequences.”

Also commenting, Ofcom CEO Dame Melanie Dawes welcomed its new broader oversight remit, adding in a statement that: “Being online brings huge benefits, but four in five people have concerns about it. That shows the need for sensible, balanced rules that protect users from serious harm, but also recognise the great things about online, including free expression. We’re gearing up for the task by acquiring new technology and data skills, and we’ll work with Parliament as it finalises the plans.”

The government has said it will publish Interim Codes of Practice today to provide guidance for companies on tackling terrorist activity and online child sexual exploitation prior to the introduction of legislation — which is unlikely to make it into law before late 2021 at the earliest to allow adequate time for parliamentary debate and scrutiny.

And while a noisy political push to “protect kids” online can expect to enjoy plenty of tabloid-level support, the wide-ranging application of the duty of care rules the government is envisaging — with large swathes of the U.K.’s tech sector set to be impacted — means ministers can expect to attract plenty of homegrown criticism too, from business groups, entrepreneurs and investors and legal and policy experts, including over specific concerns about knock-on impacts on privacy and security.

Its plan to push ahead with an Online Safety Bill that will impact scores of smaller digital businesses, instead of zeroing in on the handful of platform giants that are responsible for generating high volumes of harms, has already attracted criticism from the tech sector.

Coadec, a digital policy group that advocates for startups and the U.K. tech sector, branded the plan “a confusing minefield” for entrepreneurs — arguing it will do the opposite of fostering digital competition, counteracting other measures recently announced by the government in response to concerns about market concentration in the digital advertising sphere.

“Last week the Government announced a new unit within the CMA [Competition and Markets Authority] to promote greater competition within digital markets. Days later they have announced regulatory measures that risk having the opposite effect,” said Dom Hallas, Coadec’s executive director in a statement. “86% of UK investors say that regulation aiming to tackle big tech could lead to poor outcomes that damage tech startups and limit competition — these plans risk being a confusing minefield that will have a disproportionate impact on competitors and benefit big companies with the resources to comply.”

“British startups want a safer internet. But it’s not clear how these proposals, which still cover a huge range of services that are nowhere near social media from ecommerce to the sharing economy, are better targeted than the last time government published proposals nearly a year and a half ago,” he added. “Until the Government starts to work collaboratively instead of consistently threatening startup founders with jail time it’s not clear how we’re going to deliver proposals that work.”

One gap in the government’s proposal is financial harms — with issues such as fraud and the sale of unsafe goods explicitly excluded from the framework (as it says it wants the regulations to be “clear and manageable” for businesses and to avoid the risk of duplicating existing rules).

Some “lower-risk” services may also be exempt from the duty of care requirement, per the government, to avoid the law being overly burdensome.

Email services will also not be in scope, it confirmed.

And while it says some types of advertising will be in scope (such as influencer ads posted on social media) ads placed on an in-scope service via a direct contract between an advertiser and an advertising service (such as Facebook or Google Ads) will be exempt because “this is covered by existing regulation” — which looks set to let the adtech duopoly off the harmful ads hook without good clear reason.

After all, existing U.K. regulations do not seem to have done much to stem the tide of crypto scam ads running on Facebook (or served via Google’s ad tools) in recent years — which led to a campaign by a consumer advice personality to get Facebook and other companies to clean up their act, for example.

Consumer group Which? has criticized the lack of government attention to financial scams in the Online Safety Bill. In a response statement, Rocio Concha, its director of policy and advocacy, said: “It’s positive that the government is recognising the responsibility of online platforms to protect users, but it would be a big missed opportunity if online scams were not dealt with through the upcoming bill. Our research has shown the financial and emotional toll of scams and that social media firms such as Facebook and search engines like Google need to do much more to protect users.

“We look forward to the detail and hope to see a clear plan to give online platforms greater responsibility for fraudulent content on their sites, including having in place better controls to prevent fake adverts from appearing, so that all users can be confident that they will truly be safe online.”

European Union lawmakers are due to unveil their own pan-EU policy package to regulate illegal and harmful content later today — but the Digital Services Act will tackle the sale of illegal goods online as well as proposing to harmonize rules for reporting troublesome content on online services.",Social Media,TechCrunch,https://techcrunch.com/2020/12/14/uk-online-harms-bill-coming-next-year-will-propose-fines-of-up-to-10-of-annual-turnover-for-breaching-duty-of-care-rules/,"The UK Government's proposed Online Safety Bill will impose fines of up to 10% of annual turnover for companies who breach duty of care rules, which could have a disproportionate impact on smaller tech businesses, and does not address financial scams.",Security & Privacy
85,"Even more US adults now getting news from social media, says Pew – TechCrunch","Even more US adults now getting news from social media, says Pew

New research by Pew suggests there has been another increase in the proportion of U.S. adults getting news via social media platforms.

In May last year the researcher reported that 62 per cent of American adults were obtaining news from tech platforms, saying 18 per cent were doing so often. Now, in it’s latest survey, it says two-thirds (67%) of U.S. adults are reporting getting at least some of their news on social media. While a fifth (20%) report doing so “often”.

And while it’s not a huge increase, it is nonetheless a rise (Pew terms it a “modest” increase).

And a concerning one, given that the main social media purveyor of news — Facebook — has a demonstrable disinterest in and/or incapacity to distinguish fact from nonsensical fiction on its platform.

Indeed, as many have already pointed out Facebook’s business benefits from increased user engagement, and made-up stories that play to people’s prejudices and/or contain wild, socially divisive claims have been shown to be able to clock up far more Facebook views than factual reports of actual news.

So any rise in news consumption on social media should give pause for thought — especially as Facebook (and Google, principally) continues to suck ad revenue away from traditional media outlets, threatening the sustainability of businesses that have traditionally played a key role in a functioning democracy.

Pew’s survey is based on responses from 4,971 U.S. adults who are members of the Center’s nationally representative American Trends Panel. The research was carried out between August 8-21, 2017.

It said it found that growth in news consumption across social media platforms is being driven by increases among Americans who are “older, less educated and non-white” — noting that for the first time in Pew Research Center surveys more than half (55%) of Americans aged 50 or older report getting news on social media sites — up 10 percentage points from 2016.

Pew found that three of the social media platforms it asked about in 2017 — Twitter, YouTube and Snapchat — had an increase in the share of their audience that gets news on each site.

About three-quarters (74%) of Twitter users reported getting news on the site, up 15 percentage points from early 2016.

While about a third of YouTube’s users (32%) now get news from the site, up from 21% in early 2016 — so a rise of 11 percentage points.

And consuming news also rose among Snapchat’s user base — with 29% currently saying they are doing this, up from 17% in early 2016, so an increase of 12 percentage points.

Still, Facebook remains the primary social media platform for sourcing news for the U.S. population as a whole — with just under half (45%) of all U.S. adults reporting they get news on the site (aka a large majority — 68% — of Facebook’s user base).

How times change. Just a year ago Facebook was pooh-poohing the notion that the social mega-platform is playing the role of a media company. ‘We are mere tech platform’ was the refrain in September 2016, despite how its algorithms select and order news-related content for billions of users.

By December, and following the fake news backlash after the US election result, that line was approaching the breaking point of credibility, and CEO Mark Zuckerberg conceded Facebook might indeed be a media company — though he suggested it’s not a ‘traditional’ one.

And that is surely true. No other media entity on Earth has enjoyed such vast reach and power.

Little wonder Russian agents spied potential to sew social division across the U.S. population as the country was headed into the salient point of an election cycle by purchasing, targeting and distributing politically charged ads via the platform — as Facebook this week revealed had been the case.

It reported that an in-house investigation found pro-Kremlin entities appear to have purchased around $100,000 in political marketing on the platform between 2015 and 2016.

The company has so far resisted pressure to publicly reveal the ads purchased by Russian entities.

This is complete nonsense. First, the Russians who violated law and policy have no privacy rights on their ads. https://t.co/xkqvTtbZIP — Pierre Omidyar (@pierre) September 8, 2017

Returning to Pew’s report, its survey also found news consumption growing among YouTube’s (also growing) user-base — saying the Google/Alphabet-owned user generated video platform is now the second most common social media site for news, with roughly two-in-ten (18%) of all U.S. adults getting news there.

For Twitter, Pew said that while a very large share of its users (74 per cent) obtain news via the site, given that its user base is also significantly smaller than Facebook’s or YouTube’s this results in a smaller overall reach for news: With just 11% of U.S. adults get news via Twitter.

The researcher concluded that Americans are now more likely than ever to report getting news from multiple social media sites — with around a quarter of all U.S. adults (26%) getting news from two or more sites, up from 18% in 2016.

In additional research it also said its data shows the Internet is closing in on television as a source of news. As of August 2017, 43% of Americans report often getting news online, compared with 50% who often get news on television — so just a seven-percentage-point gap. While in early 2016 the gap between the two news platforms was 19 points.

So, in short, the Internet’s social platform giants are busily consuming broadcast/TV news media, not just print.",Social Media,TechCrunch,https://techcrunch.com/2017/09/09/even-more-us-adults-now-getting-news-from-social-media-says-pew/,"Social media's capacity to spread fake news, combined with its ability to suck away ad revenue from traditional media outlets, is a concerning development with potential to damage the functioning of democracy.","Information, Discourse & Governance"
86,"Thousands of Twitter accounts that spread fake news during the 2016 election are still active today, say researchers – TechCrunch","Thousands of Twitter accounts that spread fake news during the 2016 election are still active today, say researchers

Fake news and misinformation was a key tactic used by the Russians during the 2016 presidential election to try to sway voters against candidates and sow mistruths and mistrust.

Now, with just weeks before the 2018 midterm elections, researchers say things are almost as bad.

Research out Thursday by the Knight Foundation found that more than 80 percent of the Twitter accounts that repeatedly spread false information during the 2016 election “are still active,” and are in some cases pushing more than a million tweets a day.

The foundation examined more than 10 million tweets from 700,000 accounts that were found linking to over 600 sites associated with misinformation and conspiracy theories. The researchers found 6.6 million of those tweets linked directly to fake news during the month before the 2016 election, and another four million tweets spreading fake news six months after the election.

Although that’s a decline, it’s still more fake news than anyone would want.

“Our democracy relies on access to news and information we can trust,” said Sam Gill, Knight Foundation vice president for communities and impact. “Right now, the discussion about misinformation online is based on anxiety and conventional wisdom.”

“That’s not good enough,” he said.

Twitter has spent the past year trying to clean up after the 2016 election, after which lawmakers attributed blame to the social media company for failing to do more to protect voters from misinformation. The company, with 330 million users, has stamped out tens of millions of accounts in the past year for spreading misinformation and other suspicious content.

But even chief executive Jack Dorsey admitted that Twitter has “not figured out” how to stamp out fake news on the site.

When reached, Del Harvey, Twitter’s global vice president of trust & safety, disputed the findings:

“Firstly, this study was built using our public API and therefore does not take into account any of the actions we take to remove automated or spammy content and accounts from being viewed by people on Twitter. We do this proactively and at scale, every single day. Secondly, as a uniquely open service, Twitter is a vital source of real-time antidote to day-to-day falsehoods. We are proud of this use case and work diligently to ensure we are showing people context and a diverse range of perspectives as they engage in civic debate and conversations on our service.”

The Knight Foundation criticized Twitter for claiming it cracked down on automated and “spammy” accounts, and said that “so many easily identified abusive accounts is difficult to square with any effective crackdown.”

According to their findings, the researchers said that the accounts were densely interlinked by following each other, described as a “disinformation supercluster.” The accounts participate in “coordinated campaigns to push fake news” by tweeting links to only a handful of fake news sites.

“The core of this network remains highly active as this report goes to press,” said the researchers. “Both before and after the election, most Twitter links to fake news are concentrated on a few dozen sites, and those top fake and conspiracy sites are largely stable.”

“Reducing the social media audience of just the dozen most linked fake and conspiracy sites could dramatically reduce fake news on Twitter,” they concluded.",Social Media,TechCrunch,https://techcrunch.com/2018/10/04/thousands-of-twitter-accounts-that-spread-fake-news-during-the-2016-election-are-still-active-today-say-researchers/,"Research has found that fake news and misinformation is still prevalent on social media platforms, such as Twitter, with many of the same accounts that spread false information during the 2016 election still active today. This could have damaging effects on democracy and the upcoming midterm elections if steps are not taken to reduce the spread of fake news.","Information, Discourse & Governance"
87,A Campus Murder Tests Facebook Clicks as Evidence of Hate,"Investigators say they still don't know why Sean Urbanski, a 22-year-old University of Maryland student, walked up to 23-year-old Richard Collins III, a US Army lieutenant just days shy of college graduation, and fatally stabbed him at a campus bus stop this weekend. What they do say they know is that Collins, who was visiting a friend at UMD and did not appear to know Urbanski, was black, and that Urbanski belonged to a Facebook group called Alt-Reich: Nation, a haven of white supremacist content.

""Suffice to say that it’s despicable,"" UMD police chief David Mitchell said, at a press conference, of the now deleted Alt-Reich: Nation group. ""It shows extreme bias against women, Latinos, members of the Jewish faith, and especially African Americans.""

'These are not questions the law has had to answer before.' Neil Richards, Washington University School of Law

In addition to the local police department's ongoing homicide investigation, the FBI is looking into whether Collins' murder also amounts to a hate crime. The judges and jury of the internet have quickly reached a guilty verdict, but law enforcement is less sure. ""We need something probably more than just a Facebook posting,"" said Angela Alsobrooks, prosecutor for Prince George's County, Maryland, during a press conference Monday.

Digital breadcrumbs have become key pieces of evidence for investigators in the age of social media, but they've also put a unique strain on the legal system, forcing courts to grapple with new questions about the relative significance of a Facebook post, a ""Like,"" a follow, a tweet. It's natural for the public to want to level the harshest punishment on a person who could kill a stranger in cold blood, particularly when that killer lurked in the internet's darkest corners and may have been motivated by racial hatred. But in Urbanski's case, investigators, and eventually the courts, will have to carefully decide how much weight they can really put on a person's online allegiances and whether mere membership in such a hateful online group constitutes evidence of intent to commit a hate crime.

""These are not questions the law has had to answer before,"" says Neil Richards, a professor of First Amendment and privacy law at Washington University School of Law. ""We don’t want to permit a system in which merely reading something or associating with other people can be used as strong evidence that you hold the views of the people you hang out with or the things you read.""

Critical Evidence

So far, investigators have revealed little about Urbanski's relationship to the Alt-Reich: Nation page. John Erzen, a spokesperson for the Office of the State Attorney for Prince George's County, declined to tell WIRED whether Urbanski had ever posted suspicious content in the group. ""It's one of many aspects of the investigation right now,"" he said.

Meanwhile, Matthew Goodman, one of the founders of Alt-Reich: Nation, told The New York Times that he ""never saw [Urbanski] comment or like anything” on the page. He also denied that the group had any ties to white supremacy, despite its collection of overtly racist memes.

Barring more evidence revealing some racial animus on Urbanski's part, legal experts say Facebook group membership alone won't be enough to bring a hate crime case against him. ""From an investigator’s point of view, it's a hot lead,"" says Dan Rhynhart, chair of commercial litigation at Blank Rome, who has used social media evidence in his cases. ""But without more, you'd have a hard time getting that into evidence.""

In fact, Rhynhart anticipates such evidence might lead to a pretrial hearing to determine whether it should be considered prejudicial, and therefore withheld from the trial altogether. ""That could be the critical piece of evidence that everyone argues about,"" Rhynhart says.",Social Media,WIRED,https://www.wired.com/2017/05/campus-murder-tests-facebook-clicks-evidence-hate/,"The use of social media as evidence in legal proceedings poses unique challenges for the courts, as it forces them to grapple with difficult questions about the relative weight of Facebook posts or ""Likes"" in determining intent. Without more evidence, it can be difficult to prove that a person's online affiliations amount to anything more than just a passing interest",Equality & Justice
88,Killing the Blue Whale Challenge,"On July 18, I opened a browser, pulled up various social media sites, and searched for the #bluewhalechallenge.

The Blue Whale Challenge is a nefarious internet meme that encourages those who embrace it to hurt themselves. Over the course of 50 days, an anonymous administrator is said to assign players escalating dares that involve self harm. The final task is suicide. Interested people find these administrators through hashtags and images on social media. It could well be a threat to public safety. The trouble is, like many things on the internet, no one is really sure what the truth is.

Rumors of the game’s prevalence have existed for several months, but in early July, they escalated. For one, a baby-faced Texas teen killed himself—this really happened—and broadcast the suicide from his phone. His family found evidence he may have been participating in the challenge. Shortly after, a second American teen took her life, and her family also found evidence connecting her to the game. Then, on July 17, Siberian courts sentenced a 22-year-old Russian man to more than three years in prison for his part in launching the game, convicting him of inciting the deaths of two teenage Russian girls.

Jessi Hempel is Backchannel's editorial director. Sign up to get Backchannel's weekly newsletter.

As US television and newspaper reporters began to report on the alleged trend, warning parents and educators to keep an eye on their teens’ social media use, I decided to see how prominent social media sites were addressing search queries for the term. It may be impossible to tell exactly how many people are participating in the game, but social media sites have streams of data that reveals how many of their users are expressing interest in it.

The results varied broadly. A search on Tumblr produced a blue screen with a single headline: “Everything okay?” Beneath it, Tumblr users were encouraged to seek help from a variety of resources. A Tumblr spokesperson said the company launched this PSA after it was alerted to the challenge in May. Searches for the term spiked back in May at around 60,000.",Social Media,WIRED,https://www.wired.com/story/killing-the-blue-whale-challenge/,"The Blue Whale Challenge is a dangerous internet meme that encourages self-harm and suicide, and it's gaining traction on social media sites. This has caused serious concern among parents and educators, prompting some sites to launch campaigns to raise awareness of the dangers.",Security & Privacy
89,"After data incidents, Instagram expands its bug bounty – TechCrunch","Facebook is expanding its data abuse bug bounty to Instagram.

The social media giant, which owns Instagram, first rolled out its data abuse bounty in the wake of the Cambridge Analytica scandal, which saw tens of millions of Facebook profiles scraped to help swing undecided voters in favor of the Trump campaign during the U.S. presidential election in 2016.

The idea was that security researchers and platform users alike could report instances of third-party apps or companies that were scraping, collecting and selling Facebook data for other purposes, such as to create voter profiles or build vast marketing lists.

Even following the high-profile public relations disaster of Cambridge Analytica, Facebook still had apps illicitly collecting data on its users.

Instagram wasn’t immune either. Just this month Instagram booted a “trusted” marketing partner off its platform after it was caught scraping millions of users’ stories, locations and other data points on millions of users, forcing Instagram to make product changes to prevent future scraping efforts. That came after two other incidents earlier this year where a security researcher found 14 million scraped Instagram profiles sitting on an exposed database — without a password — for anyone to access. Another incident saw another company platform scrape the profile data — including email addresses and phone numbers — of Instagram influencers.

Last year Instagram also choked developers’ access as the company tried to rebuild its privacy image in the aftermath of the Cambridge Analytica scandal.

Dan Gurfinkel, security engineering manager at Instagram, said its new and expanded data abuse bug bounty aims to “encourage” security researchers to report potential abuse.

Instagram said it’s also inviting a select group of trusted security researchers to find flaws in its Checkout service ahead of its international rollout, who also will be eligible for bounty payouts.

Read more:",Social Media,TechCrunch,https://techcrunch.com/2019/08/19/instagram-data-abuse-bug-bounty/,"The Cambridge Analytica scandal exposed a concerning vulnerability of social media platforms, such as Facebook and Instagram, as third-party companies and apps were able to scrape and collect user data for malicious purposes. To help prevent such abuse, Facebook and Instagram have rolled out data abuse bug bounty programs to incentivize security researchers to report any potential abuse.",Security & Privacy
90,The Internet Is Now Part Of The Crime Scene – TechCrunch,"Elliot Rodger, a young man with some horrible ideas and serious mental problems, killed at least six people in Santa Barbara. He left pages of digital photos and hours of video detailing his pain and his envy. In the voice of an entitled boy not given what he wants, he talks about being alone while others are together. He became a misogynist through his own twisted self-reflection. Now his efforts to reach out to seemingly like-minded groups on the Internet make him look like a monster created by the Internet himself.

This is wrong.

A decade ago a crime scene was a photo and a report. Now it is a sea of interconnected tracings, the murderer bobbing loosely in social media and the forums. We can watch him make his way through these straits, we can watch the madness growing, and we can watch his terrible end, all through murk of media. We are quick to judge and we are quick to look at his wake and say, definitively, that he was this or he was that. He was frustrated. The frustration grew. He went to a place he thought would help. It didn’t.

There are two ways to see this young man. On one hand he absorbed the self-serving tactics of the Pick Up Artist crowd and the narcissism of the body builders and they made him what he was. On the other hand, he was sick and his efforts to find himself led him down terrible paths. But we can’t expect the Internet to help a man like Elliot Rodger. He wanted human contact and he got no solace there. He wanted advice and the Internet did what it does best: it reflected his own sickness back at him. Forum posters can catch a whiff of madness and react with sarcasm but who there can help? Hashtags don’t prevent murders. Therapists and gun locks do.

We are not qualified to look at this man’s life and judge him. He did a horrible thing. He was terribly sick. He had access to all the tools necessary for this to go either way: he had a gun to kill and he had expensive therapists to help him. He took the way of the gun.

We envy the perfect lives we see through the terrible lens of the web and we wish we were different. We envy every day and we covet. And we covet with our hearts. Most of us learn to temper this greed with love. He coveted until the end. That’s not the Internet’s fault, but we are all to blame for letting this man and others go as far as they did.",Social Media,TechCrunch,https://techcrunch.com/2014/05/25/envy/,"Social Media can be a dangerous platform, allowing people with mental health issues to be exposed to negative influences and fueling their thoughts of entitlement and envy, leading them to commit horrific acts of violence.",Security & Privacy
91,How K-pop fans became celebrated online vigilantes,"Online culture loves a vigilante in times of crisis, and many saw parallels with Anonymous, the leaderless hacktivist collective that has, in the past, allied itself with protest movements and doxxed KKK members. K-pop fans are now even making fan videos about Anonymous itself.

This narrative has found traction partly because it plays against stereotypes: K-pop fandom is often dismissed as a monolithic swarm of annoying, shallow screaming tweens who manipulate Twitter’s trending algorithms in order to establish which group or performer is the most worthy. Suddenly being shown evidence that stans are more complex, thoughtful, or socially aware than the stereotype is a surprise only for those who weren’t paying attention.

Repurposed tactics

“There’s a narrative that seems to persist with the general public and the media about K-pop fans, that mostly white, teen girls comprise the fan community,” says Keidra Chaney, a culture writer and publisher of The Learned Fangirl, a website that analyzes and critiques pop culture. “It’s very diverse, not just around race and ethnicity but age as well. The stereotype of ‘giggling teen girls’ does a lot to obscure the diversity of these fan communities and the more complex dynamics of how they interact.”

K-pop’s earliest American fan base was in the Asian diaspora, but then it “spread through communities of young people of color who are interested in other aspects of East Asian popular culture,” says Michelle Cho, an assistant professor of East Asian studies at the University of Toronto. She says fans often tell her that they found K-pop by seeking out non-Western pop culture as an alternative to an American mainstream “in which they feel they are not represented.”

Their ability to dominate online conversation is not an accident: learning how to get views on behalf of your favorite group is part of K-pop fandom. Fans learn tactics to help their groups explode in YouTube views and shoot up the charts whenever they release new material. Groups of stans stream new music videos and tracks on YouTube and Spotify for hours at a time, guided by fan-made tutorials. They make memes, like fancams—short, fan-produced videos focusing on a single performer—and share them widely. They’re so good at manipulating the metrics of social media that people who are new to watching K-pop in action can, on first glance, mistake the accounts for bots.

The genre’s fans have a tendency to prioritize harnessing their numbers for maximum visibility over using their social-media presence to make K-pop more accessible to outsiders, says Cho: “There’s a lot of retweeting, and making one’s influence known as part of an aggregate and not a single voice.”

This skill at redirecting online attention has translated into activism before. There is a long history of K-pop fans organizing around causes in the name of their favorite groups, and not just as a feel-good detour from streaming artists on Spotify: activism is part of participating in the fandom, where good deeds can become another metric. “K-pop fans often use their voices to uplift viral charity campaigns for global nonprofits, often done under the names of their favorite idols,” Chaney says.

In the past decade, this has included donating to create forests carrying the name of their favorite group or idol, creating donation drives, and elevating campaigns promoted by celebrities. “While altruism is definitely a motivation for these campaigns, it’s also an act of goodwill and positive publicity for fans’ favorite artists,” Chaney adds.

This all reached critical mass in the US with the emergence of protests over the killing of George Floyd in Minneapolis. Two days before K-pop stans took down the Dallas police app, fans of Blackpink, a South Korean girl group that regularly breaks streaming records, organized a campaign to stop a Twitter hashtag related to the group’s collaboration with Lady Gaga from trending, instead opting to amplify #blacklivesmatter.

Internal conflict

But some stans, and the academics who study them, say that while it’s great to see fans use these platforms for good, the rapid veneration is overshadowing the more complex dynamics underlying K-pop fandom. And, they say, the newfound reputation for anti-racist heroism largely ignores the voices of black K-pop fans, who have struggled with racism and harassment within the community.

“For a lot of black fans, including myself, to see white K-pop fans get praised and credited in the media for anti-racist activism, while black fans have faced (and will continue to face) anti-black harassment online for spearheading these conversations, feels like a punch in the gut—that we are being used for our social currency and then discarded,” Chaney says.

For example, as K-pop’s activism was attracting international news coverage, there was also a harassment campaign targeting fans who were calling out Suga, a member of BTS, an enormously popular K-pop group. They were concerned about a song on a new mixtape in which he sampled the voice of cult leader Jim Jones, whose victims were overwhelmingly black. A lot of black fans were expressing themselves on Twitter and, as a consequence, getting harassed and doxxed by other fans who didn’t want them to say negative things about their favorite artist, says Tamar Herman, a pop culture contributor for Billboard.

BTS itself also remained silent on Black Lives Matter, even as other groups were taking their fans’ lead and making public statements supporting the protests. Finally, on Thursday, the official BTS account tweeted about it.",Social Media,MIT,https://www.technologyreview.com/2020/06/05/1002781/kpop-fans-and-black-lives-matter/,"The rise of K-pop stan activism on social media has been celebrated, but it has also overshadowed the complex dynamics and ignored the voices of black K-pop fans who have faced racism and harassment within the fandom. This has been highlighted by the recent harassment campaign targeting fans who called out Suga for sampling the voice of cult leader Jim",Equality & Justice
92,"Instagram to now flag potentially offensive captions, in addition to comments – TechCrunch","Earlier this year, Instagram launched a feature that would flag potentially offensive comments before they’re posted. Now, the social media platform is expanding this preemptive flagging system to Instagram’s captions, as well. The new feature will warn users after they’ve written a caption for a feed post that Instagram’s AI detects as being similar to those that have already been reported for bullying.

It will not, however, block users from publishing their hateful remarks. Instead, Instagram says these little nudges simply give users time to reconsider their words — something it found was helping to cut down on the bullying taking place in the comments section after the launch of the earlier feature.

Once live, when users write a caption that appears to be bullying, a notification will appear that says: “This caption looks similar to others that have been reported.” The notification allows users to edit the text, click a button to learn more or share their post anyway.

In addition to helping reduce online bullying, Instagram hopes the feature will also help to educate users about what sort of things aren’t allowed on Instagram and when accounts could be at risk of being disabled for rule violations.

While the addition doesn’t prevent someone from posting their negative or abusive captions, Instagram has rolled out a number of features to aid those impacted by online bullying. For example, users can turn off comments on individual posts, remove followers, filter comments, mute accounts and more. It also this year began downranking borderline content, giving those whose accounts step close to the line — but not enough to be banned — fewer reasons to post violent or hurtful content in search of Instagram fame.

Unfortunately for Instagram’s more than a billion users, these sorts of remediations have arrived years too late. It only today began globally fact checking posts for misinformation.

Like most of today’s social media platforms, Instagram wasn’t thoughtfully designed by those with an eye toward how its service could be abused by those who want to hurt others. In fact, the idea that tech could actually harm others was a later realization for Instagram and others, who are today still struggling to build workforces where a variety of viewpoints and experiences are considered.

While it’s certainly a welcome addition to flag remarks that appear to be bullying, it’s disappointing that such a feature arrived 10 years after Instagram’s founding. And while AI advances may have made such a tool more powerful and useful as it debuts sometime next year, a simpler form of this feature could have existed ages ago, with improvements added over time.

Instagram says the feature is also rolling out slowly, initially in select markets, then globally in the “coming months” — meaning 2020.",Social Media,TechCrunch,https://techcrunch.com/2019/12/16/instagram-to-now-flag-potentially-offensive-captions-in-addition-to-comments/,"Social media platforms such as Instagram have been found to be a breeding ground for online bullying, leading to its users feeling unsafe or attacked on the platforms. Instagram has been working to create features to help reduce this bullying, such as preemptive flagging systems, downranking of borderline content, and more.",Social Norms & Relationships
93,UPDATE: Brazilian Judge Shuts Down WhatsApp And Brazil’s Congress Wants To Shut Down The Social Web Next – TechCrunch,"UPDATE: Brazilian Judge Shuts Down WhatsApp And Brazil’s Congress Wants To Shut Down The Social Web Next

UPDATE: WhatsApp came back online 12 hours into a court-ordered 48-hour blackout across Brazil for refusing to turn over data in an investigation, after a second judge ruled that “it does not seem reasonable that millions of users are affected” and suggested a financial penalty instead. Overnight, messaging app Telegram gained 5 million new Brazilian users, and Twitter lit up with scenes from Castaway, imagery of people sitting alone on the moon, and people looking depressed in general. Trending hashtags included #eusemwhatsapp (me without WhatsApp) and #nessas48euvou (In these 48 hours I’m going to…).

If the anti-privacy legislation Brazilian Congress has on the table makes its way into law, companies like WhatsApp, Facebook and Google will have drastically increased obligations to collect, retain and share sensitive user data with the government.

A judge in Sao Paulo has ordered WhatsApp to shut down for 48 hours, starting at 9pm Eastern tonight.

WhatsApp is the single most used app in Brazil, with about 93 million users, or 93% of the country’s internet population. It’s a particularly useful service for Brazil’s youth and poor, many who cannot afford to pay the most expensive plans on the planet.

Brazilian telco’s have been lobbying for months to convince the government that WhatsApp’s voice service is unregulated and illegal (not entirely unlike the taxi industry’s posture on Uber), and have publicly blamed the “WhatsApp effect” for driving millions of Brazilians to abandon their cell phone lines.

A WhatsApp shut-down would be akin to taking half the country off the electricity grid because of an industry squabble over the impending threat of solar power.

It’s a particularly baffling move when you consider that Brazil is the Social Media Capital of the Universe: Brazilians are the #2 or #3 audience on every major global social platform, and on a per-user basis, Brazilians spend almost double the time on social media as Americans.

But a temporary WhatsApp shutdown is not even close to the craziest thing happening with the Brazilian internet right now.

If Brazil’s conservative Congress gets its way, they’re going to take down the entire social web as we know it, with bills circulating through the legislature to criminalize posting social media content and to allow the government to spy on its citizens.

It’s an about-face from last year, when President Dilma Rousseff approved Marco Civil, a groundbreaking Internet “Bill of Rights”, as a response to the Snowden revelations that the NSA was spying on Brazil. The landmark bill, Brazil’s first internet legislation, protects net neutrality, user privacy and freedom of speech.

Since then, Brazil’s economy has spiraled into crisis, triggered in large part by a wide-reaching corruption scandal at the state-owned Petrobras oil behemoth that is investigating heads of Brazil’s biggest construction firms, some 50 politicians who are currently in office, and even ex-President Lula.

If Brazil’s conservative Congress gets its way, they’re going to take down the entire social web as we know it, with bills circulating through the legislature to criminalize posting social media content and to allow the government to spy on its citizens.

Meanwhile, Dilma’s approval rating has stagnated in the single digits, and many are calling for her impeachment, including Eduardo Cunha, Brazil’s equivalent of the Speaker of the House. Cunha is under investigation himself for corruption and accused of laundering millions of dollars in a scandal involving the Brazilian oil company Petrobras.

Cunha, a former telco lobbyist, was one of the biggest opponents of Marco Civil (particularly its net neutrality clause) before the legislation made its way to Dilma’s desk and into law.

But a year later, he controls a Congress dominated by evangelical extremists and military dictatorship apologists, and is authoring or advocating on behalf of a slate of proposed laws that would not only dismantle Marco Civil’s provisions for consumer privacy and freedom of expression, but would also effectively criminalize the use of social media.

PL 215/15, which opponents are nicknaming the Big Spy (“O Espião”), is a surveillance law that would require Brazilians to enter theirtax ID, home address and phone number to access any website or app on the internet, and require companies like Facebook and Google to store that information for up to three years and provide access to police with a court order. An earlier draft said “competent authorities” could request the data without a court order.

Another part of the law, authored by Cunha, would allow politicians to censor social media practically at will.

It’s a twist on the European Union’s “right to be forgotten” legislation, which establishes a process for private citizens (but not public figures) to request some forms of sensitive content from their past to be de-indexed from search results (but not removed from the web).

The version Cunha authored would allow Brazilian politicians to not just request content they found defamatory, injurious, or simply out of date to be de-indexed, but actually order it to be taken down from the web (and with a court order, police could have the home address and tax ID of the person who published it on, say, Facebook).

Congress’ lower house approved PEC 215 in October. Now it goes to a Congressional vote before it would move on to the Senate, and ultimately Dilma’s vote.

“This is a very good example of how Congress thinks about the internet,” says Ronaldo Lemos, one of the instigators of Marco Civil, and current director of the Internet Technology Society in Rio de Janeiro. “All this effort and energy to criminalize the internet. Many politicians in Brazil feel that the internet is only used to say bad things about them. They hate the internet. It’s a threat.”

Lemos says the proposed legislation is the biggest threat to freedom of expression Brazil has seen in decades.

For those unfamiliar with Brazilian politics, free speech is but one of a rainbow of civil rights under Cunha’s attack.

This year alone he’s also pushed forward legislation for a “gay cure”; a law that allows 16 year-olds to be tried as adults in the criminal system; another one that bans the day-after pill and restricts rape victims’ access to abortion; and one called PEC 215 — not to be confused with PL 215 — that removes Indigenous Brazilians’ constitutional right to their land. And gives it to Congress.

It’s also worth noting that Cunha is not the only one behind the anti-internet legislation, and that the aforementioned bills are not the only ones on the table — or even the worst.

Many politicians in Brazil feel that the internet is only used to say bad things about them. They hate the internet. It’s a threat. Ronaldo Lemos, director of the Internet Technology Society

Among the other anti-internet bills that have been introduced within a year of Marco Civil’s passing — all authored by members of Congress’s evangelical bloc — is PL 1676, which may be voted on this week.

PL 1676 would make it a crime punishable by up to two years in jail for anyone to film, photograph or capture the voice of a person without their express authorization (making even selfies potentially criminal, if someone shows up in the background of the photo). The penalty jumps to up to six years if the footage is posted on the internet.

There’s more: PL 1547 and PL 1589, addendums to PL 215, would increase the penalty for cases of libel, slander and defamation on the web.

PL 2390 would create a centralized database of Brazilian internet users as a means to prohibit children and adolescents from accessing inappropriate content, but could just as easily be used to keep Brazilian youth — the fastest-growing and most active segment of internet users — from accessing major social platforms like YouTube and Twitter.

What’s at stake here is freedom of expression and the right to privacy in what may soon be the biggest social media market on the planet.

With less than half the number of Americans online, Brazil is already the #2 or #3 player behind the US on every major social platform globally — Facebook, Google, Twitter, you name it. And there are still another 100 million Brazilians who have not come online yet, including wide swaths of the youth, poor and rural demographics. It is entirely possible that in the next ten years Brazil will have the largest internet audience on the planet in terms of social media consumption, and that Millennial Brazilians will be the most socially active population on the planet.

What’s at stake here is freedom of expression and the right to privacy in what may soon be the biggest social media market on the planet.

To get a sense of just how disruptive internet adoption is in Brazil — and how big of a threat it is to the established order — consider that millennials are the only Brazilians alive today who have never lived under a military dictatorship (Brazil had two of them over the 20th century, the last one ending in 1985).

And as they come online, they are increasingly using the social web as a tool to speak out and organize. They used Facebook Events to bring millions to the streets in Brazil’s historic protests in 2013 ahead of the World Cup. Theycollaboratively drafted their own Marco Civil legislation, crowdsourcing 70% of the bill’s final text online. Independent media collectives like Midia Ninja and Papo Reto are attracting global attention for reporting on issues like rampant police violence against poor black youth.

In this context, the proposed anti-internet legislation is a direct reaction to this emerging digital empowerment, and a multi-pronged attack on social media — restricting access for Brazil’s poor and youth demographics to access the internet, criminalizing the posting of practically all video, photo and audio content, and censoring voices out of favor with the current government.

Industry experts agree that at least some combination of these proposed laws is likely to pass Congress. Eventually, however, the bills will stop at President Dilma’s desk.

Dilma pushed for Marco Civil last year, and her party has historically imposed these kinds of restrictions — although with the impending impeachment battle, Dilma’s days may be numbered. Politicians aside, we can look forward to seeing how Brazil’s digital generation decides to rally in support of the open web.",Social Media,TechCrunch,https://techcrunch.com/2015/12/16/brazils-congress-has-shut-down-whatsapp-tonight-and-the-rest-of-the-social-web-could-be-next/,"If Brazil's conservative Congress gets its way, they are threatening to take down the entire social web as we know it, with bills circulating through the legislature that would criminalize posting social media content and allow the government to spy on its citizens. This would be a major blow to freedom of expression and privacy rights, and stand to affect millions of",Security & Privacy
94,Feedless Takes the News Feed Out of Social Media,"If Ryan Orbuch’s high school had senior superlatives, he would have likely received “Most Likely to Start a Billion Dollar Company.” But Orbuch wasn’t around much his senior year of high school. At 16, he won Apple’s Design Award for his first startup, a productivity app called Finish. At 18, he founded his second company Volley, an AI-powered education tool. Now, at the ripe old age of 21, he's launching his third project—a new app, called Feedless.

Each of Orbuch’s projects have followed his stage in life. Finish was “the to-do list for procrastinators,” conceived during finals week of his sophomore year of high school. Volley, a learning tool for autodidacts, launched shortly after he decided to opt out of college. The App Store was created a month before Orbuch’s 12th birthday, and in many ways, he’s spent his entire life thinking about apps as the way to solve life's problems.

So it’s noteworthy that Feedless is something completely different. It’s an app, but one designed to save us from apps. It blocks the feed from social media websites like Facebook and Twitter on Safari for iOS, leaving just the bare bones of those apps. The goal, Orbuch says, “was to remove the most time-sucking feature of social media and leave all the useful stuff like messaging and events.”

Ryan Orbuch Ryan Orbuch

Feedless launches on the App Store today, just as the national conversation around social media’s dark side seems to be coming to a head. Researchers are exploring links between social media use and depression in young adults. A coalition of technologists called the Center for Humane Technology is leading an awareness campaign about how social media “hijacks our attention.” Not to mention Facebook’s fake news, Twitter’s fake followers, or Youtube’s fake actors. Social media has never had more dependents, or more detractors.

Feedless couldn’t have come at a better time. The app pushes back against phone addiction, without asking anyone to give up their phone (or even their Facebook account). It's Orbush’s attempt to not throw the baby out with the bathwater. How can you extract the good parts of social media without getting lost in a mindless scroll? After all, the problem with phones is often not what you come to them to do. It’s how much time you spend on them after that’s done.

Killing the Infinite Scroll

In 2005, researchers from Cornell conducted a now-famous study known as ""bottomless bowls."" Participants were given a bowl of soup and told to stop eating when they were full. Some participants ate from normal bowls; others ate from self-refilling bowls, which had tubes from the bottom that refilled automatically as participants slurped. Those who ate from the bottomless bowl consumed 73 percent more than those who ate from a normal bowl, even though the normal bowl was being consistently refilled by servers. Even more striking: Despite consuming 73 percent more, people with the bottomless bowls didn't believe they had consumed more, nor did they feel more sated.

Social media feeds are like those bottomless bowls. Some people argue that our consumption has become so mindless that we don’t even realize what we’ve taken in.

""The goal was to remove the most time-sucking feature of social media and leave all the useful stuff like messaging and events."" — Ryan Orbuch, creator of Feedless

If you take that stance, then fixing the problems with social media isn't just a matter of fixing the content on our feeds. Even without the FOMO-inducing highlight reels from your friends and the divisive propaganda from Russian bots—even as Facebook retools its newsfeed algorithm to optimize for meaningful content from family and friends—the infinite scroll persists. If too much content is the problem, then you have to eliminate the newsfeed altogether.

Solving App Addiction

Feedless follows a burgeoning market for technology designed to save us from our technology. From website blockers like Self Control and Freedom to accountability trackers like Moment and RescueTime to entirely new “mindful operating systems” like Siempo, the promise of tech that can help you put down your phone and get back to your life sure seems to be having a moment.

Apps like these “are small Band-Aids covering wounds left by the biggest companies in the world,” says Tristan Harris, who runs the Center for Humane Technology and has become the de facto leader of the movement to align tech with human values. “These [social media] companies have AIs that are essentially playing chess against your brain, trying to figure out the best way to get you hooked.”",Social Media,WIRED,https://www.wired.com/story/feedless-takes-the-news-feed-out-of-social-media/,"Social media is widely seen to be having negative effects on user's mental health, with researchers exploring links between its use and depression. Additionally, it has been called out for its fake news, fake followers, and fake actors, leading to a growing movement to align tech with human values. Apps like Feedless are attempting to help users put down",Social Norms & Relationships
95,YouTube is pulling Tide Pod Challenge videos – TechCrunch,"People doing stupid stuff on the Internet is hardly news. To wit: The Tide Pod Challenge, in which YouTubers have been filming themselves eating — or, we really hope, pretending to eat — laundry detergent pods.

Why? Uh, because they’re brightly colored?? We guess???????

Obviously this is Darwin Awards’ levels of idiocy — given that detergent is, y’know, not at all edible, toxic to biological life and a potent skin irritant. It would also literally taste of soap. Truly, one wonders what social historians will make of the 21st century.

But while eating Tide Pods appears to have started as a silly meme — which now has its own long and rich history — once YouTubers got hold of it, well, things started to turn from funny fantasy to toxic reality.

Funny that.

So now YouTube appears to be trying to get ahead of any wider societal outcry over (yet more) algorithmically accelerated idiocy on its platform — i.e. when sane people realize kids have been filming themselves eating detergent just to try to go viral on YouTube — and is removing Tide Pod Challenge videos.

At least when they have been reported.

A YouTube spokesperson sent us the following statement on this: “YouTube’s Community Guidelines prohibit content that’s intended to encourage dangerous activities that have an inherent risk of physical harm. We work to quickly remove flagged videos that violate our policies.”

Under YouTube’s policy channels that have a video removed on such grounds will get a strike — and if they get too many strikes could face having their channel suspended.

At the time of writing it’s still possible to find Tide Pod Challenge videos on YouTube, though most of the videos being surfaced seem to be denouncing the stupidity of the ‘challenge’ (even if they have clickbait-y titles that claim they’re going to eat the pods — hey, savvy YouTubers know a good viral backlash bandwagon to jump on when they see one!).

Other videos that we found — still critical of the challenge but which include actual footage of people biting into Tide Pods — require sign in for age verification and are also gated behind a warning message that the content “may be inappropriate for some users”.

As we understand it, videos that discuss the Tide Pod challenge in a news setting or educational/documentary fashion are still allowed — although it’s not clear where exactly YouTube moderators are drawing the tonal line. (For example this YouTube creator’s satirical video denouncing the stupidity of the Tide Pod Challenge was apparently removed on safety grounds.)

Fast Company reports that YouTube clamping down on Tide Pod Challenge videos is in response to pressure from the detergent brand’s parent company, Procter & Gamble — which has said it is working with “leading social media sites” to encourage the removal of videos that violate their polices.

Because, strangely enough, Procter & Gamble is not ecstatic that people have been trying to eat its laundry pods…

What should Tide PODs be used for? DOING LAUNDRY. Nothing else. Eating a Tide POD is a BAD IDEA, and we asked our friend @robgronkowski to help explain. pic.twitter.com/0JnFdhnsWZ — Tide (@tide) January 12, 2018

And while removal of videos that encourage dangerous activities is not a new policy on YouTube’s part, YouTube taking a more pro-active approach to enforcement of its own policies is clearly the name of the game for the platform these days.

That’s because a series of YouTube content scandals blew up last year — triggering advertisers to start pulling their dollars off of the platform, including after marketing messages were shown being displayed alongside hateful and/or obscene content.

YouTube responded to the ad boycott by saying it would give brands more control over where their ads appeared. It also started demonitizing certain types of videos.

There was also a spike in concern last year about the kinds of videos children were being exposed to on YouTube — and indeed the kinds of activities YouTubers were exposing their children to in their efforts to catch the algorithm’s eye — which also led the company to tighten its rules and enforcement.

YouTube is also increasingly in politicians’ crosshairs for algorithmically accelerating extremism — and it made a policy shift last year to also remove non-violent content made by listed terrorists.

It remains under rising political pressure to come up with technical solutions for limiting the spread of hate speech and other illegal content — with European Union lawmakers warning platforms last month they could look to legislate if tech giants don’t get better at moderating content themselves.

At the end of last year YouTube said it would be increasing its content moderation and other enforcement staff to 10,000 in 2018, as it sought to get on top of all the content criticism.

The long and short of all this is that user generated content is increasing under the spotlight and some of the things YouTubers have been showing and doing to gain views by ‘pleasing the algorithm’ have turned out to be rather less pleasing for YouTube the company.

As one YouTuber abruptly facing demonitization of his channel — which included videos of his children doing things like being terrified at flu jabs or crying over dead pets — told Buzzfeed last year: “The [YouTube] algorithm is the thing we had a relationship with since the beginning. That’s what got us out there and popular. We learned to fuel it and do whatever it took to please the algorithm.”

Another truly terrible example of the YouTuber quest for viral views occurred at the start of this year, when YouTube ‘star’, Logan Paul — whose influencer status had earned him a position in Google’s Preferred ad program — filmed himself laughing beside the dead body of a suicide victim in Japan.

It gets worse: This video had actually been manually approved by YouTube moderators, going on to rack up millions of views and appearing in the top trending section on the platform — before Paul himself took it down in the face of widespread outrage.

In response to that, earlier this week YouTube announced yet another tightening of its rules, around creator monetization and partnerships — saying content on its Preferred Program would be “the most vetted”.

Last month it also dropped Paul from the partner program.

Compared to that YouTube-specific scandal, the Tide Pod Challenge looks like a mere irritant.",Social Media,TechCrunch,https://techcrunch.com/2018/01/18/youtube-is-pulling-tide-pod-challenge-videos/,"YouTube is now removing Tide Pod Challenge videos that violate their policies, due to the potential for physical harm that comes with the challenge. This comes in response to pressure from the detergent brand's parent company, Procter & Gamble, and other scandals that have occurred on the platform, such as Logan Paul's video of a suicide victim.",Security & Privacy
96,What does a pandemic say about the tech we’ve built? – TechCrunch,"What does a pandemic say about the tech we’ve built? Dysfunctional is not the new normal

There’s a joke* being reshared on chat apps that takes the form of a multiple-choice question — asking who’s the leading force in workplace digital transformation? The red-lined punchline is not the CEO or CTO, but: C) COVID-19.

There’s likely more than a grain of truth underpinning the quip. The novel coronavirus is pushing a lot of metaphorical buttons right now. “Pause” buttons for people and industries, as large swathes of the world’s population face quarantine conditions that can resemble house arrest. The majority of offline social and economic activities are suddenly off limits.

Such major pauses in our modern lifestyle may even turn into a full reset, over time. The world as it was, where mobility of people has been all but taken for granted — regardless of the environmental costs of so much commuting and indulged wanderlust — may never return to “business as usual.”

If global leadership rises to the occasion, then the coronavirus crisis offers an opportunity to rethink how we structure our societies and economies — to make a shift toward lower carbon alternatives. After all, how many physical meetings do you really need when digital connectivity is accessible and reliable? As millions more office workers log onto the day job from home, that number suddenly seems vanishingly small.

COVID-19 is clearly strengthening the case for broadband to be a utility — as so much more activity is pushed online. Even social media seems to have a genuine community purpose during a moment of national crisis, when many people can only connect remotely, even with their nearest neighbours.

Hence the reports of people stuck at home flocking back to Facebook to sound off in the digital town square. Now that the actual high street is off limits, the vintage social network is experiencing a late second wind.

Facebook understands this sort of higher societal purpose already, of course. Which is why it’s been so proactive about building features that nudge users to “mark yourself safe” during extraordinary events like natural disasters, major accidents and terrorist attacks. (Or indeed, why it encouraged politicians to get into bed with its data platform in the first place — no matter the cost to democracy.)

In less fraught times, Facebook’s “purpose” can be loosely summed to “killing time.” But with ever more sinkholes being drilled by the attention economy, that’s a function under ferocious and sustained attack.

Over the years the tech giant has responded by engineering ways to rise back to the top of the social heap — including spying on and buying up competition, or directly cloning rival products. It’s been pulling off this trick, by hook or by crook, for over a decade. Albeit, this time Facebook can’t take any credit for the traffic uptick; a pandemic is nature’s dark pattern design.

What’s most interesting about this virally disrupted moment is how much of the digital technology that’s been built out online over the past two decades could very well have been designed for living through just such a dystopia.

Seen through this lens, VR should be having a major moment. A face computer that swaps out the stuff your eyes can actually see with a choose-your-own-digital-adventure of virtual worlds to explore, all from the comfort of your living room? What problem are you fixing, VR? Well, the conceptual limits of human lockdown in the face of a pandemic quarantine right now, actually…

Virtual reality has never been a compelling proposition versus the rich and textured opportunity of real life, except within very narrow and niche bounds. Yet all of a sudden, here we all are — with our horizons drastically narrowed and real-life news that’s ceaselessly harrowing. So it might yet end up a wry punchline to another multiple choice joke: “My next vacation will be: A) Staycation, B) The spare room, C) VR escapism.”

It’s videoconferencing that’s actually having the big moment, though. Turns out even a pandemic can’t make VR go viral. Instead, long-lapsed friendships are being rekindled over Zoom group chats or Google Hangouts. And Houseparty — a video chat app — has seen surging downloads as barflies seek out alternative night life with their usual watering-holes shuttered.

Bored celebs are TikToking. Impromptu concerts are being live-streamed from living rooms via Instagram and Facebook Live. All sorts of folks are managing social distancing, and the stress of being stuck at home alone (or with family), by distant socializing: signing up to remote book clubs and discos; joining virtual dance parties and exercise sessions from bedrooms; taking a few classes together; the quiet pub night with friends has morphed seamlessly into a bring-your-own-bottle group video chat.

This is not normal — but nor is it surprising. We’re living in the most extraordinary time. And it seems a very human response to mass disruption and physical separation (not to mention the trauma of an ongoing public health emergency that’s killing thousands of people a day) to reach for even a moving pixel of human comfort. Contactless human contact is better than none at all.

Yet the fact all these tools are already out there, ready and waiting for us to log on and start streaming, should send a dehumanizing chill down society’s backbone.

It underlines quite how much consumer technology is being designed to reprogram how we connect with each other, individually and in groups, in order that uninvited third parties can cut a profit.

Back in the pre-COVID-19 era, a key concern being attached to social media was its ability to hook users and encourage passive feed consumption — replacing genuine human contact with voyeuristic screening of friends’ lives. Studies have linked the tech to loneliness and depression. Now that we’re literally unable to go out and meet friends, the loss of human contact is real and stark. So being popular online in a pandemic really isn’t any kind of success metric.

Houseparty, for example, self-describes as a “face to face social network” — yet it’s quite the literal opposite; you’re foregoing face-to-face contact if you’re getting virtually together in app-wrapped form.

The implication of Facebook’s COVID-19 traffic bump is that the company’s business model thrives on societal disruption and mainstream misery. Which, frankly, we knew already. Data-driven adtech is another way of saying it’s been engineered to spray you with ad-flavored dissatisfaction by spying on what you get up to. The coronavirus just hammers the point home.

The fact we have so many high-tech tools on tap for forging digital connections might feel like amazing serendipity in this crisis — a freemium bonanza for coping with terrible global trauma. But such bounty points to a horrible flip side: It’s the attention economy that’s infectious and insidious. Before “normal life” plunged off a cliff, all this sticky tech was labelled “everyday use;” not “break out in a global emergency.”

It’s never been clearer how these attention-hogging apps and services are designed to disrupt and monetize us; to embed themselves in our friendships and relationships in a way that’s subtly dehumanizing; re-routing emotion and connections; nudging us to swap in-person socializing for virtualized fuzz designed to be data-mined and monetized by the same middlemen who’ve inserted themselves unasked into our private and social lives.

Captured and recompiled in this way, human connection is reduced to a series of dilute and/or meaningless transactions; the platforms deploying armies of engineers to knob-twiddle and pull strings to maximize ad opportunities, no matter the personal cost.

It’s also no accident we’re seeing more of the vast and intrusive underpinnings of surveillance capitalism emerge, as the COVID-19 emergency rolls back some of the obfuscation that’s used to shield these business models from mainstream view in more normal times. The trackers are rushing to seize and colonize an opportunistic purpose.

Tech and ad giants are falling over themselves to get involved with offering data or apps for COVID-19 tracking. They’re already in the mass surveillance business, so there’s likely never felt like a better moment than the present pandemic for the big data lobby to press the lie that individuals don’t care about privacy, as governments cry out for tools and resources to help save lives.

First the people-tracking platforms dressed up attacks on human agency as “relevant ads.” Now the data industrial complex is spinning police-state levels of mass surveillance as pandemic-busting corporate social responsibility. How quick the wheel turns.

But platforms should be careful what they wish for. Populations that find themselves under house arrest with their phones playing snitch might be just as quick to round on high-tech gaolers as they’ve been to sign up for a friendly video chat in these strange and unprecedented times.

Oh, and Zoom (and others) — more people might actually read your “privacy policy” now they’ve got so much time to mess about online. And that really is a risk.

Every day there's a fresh Zoom privacy/security horror story. Why now, all at once? It's simple: the problems aren't new but suddenly everyone is forced to use Zoom. That means more people discovering problems and also more frustration because opting out isn't an option. https://t.co/O9h8SHerok — Arvind Narayanan (@random_walker) March 31, 2020

*Source is a private Twitter account called @MBA_ish",Social Media,TechCrunch,https://techcrunch.com/2020/03/31/what-does-a-pandemic-say-about-the-tech-weve-built/,"Social media platforms have been designed to disrupt our connections and monetize our emotions, reducing human connection to a series of dilute and/or meaningless transactions. With the COVID-19 pandemic, these underlying issues are becoming more apparent, as people flock to video apps for connection and surveillance capitalism looks to capitalize on the crisis.",Security & Privacy
97,Facebook and Instagram boot close Trump ally Roger Stone for network of fake accounts – TechCrunch,"Facebook released today its latest report detailing disinformation campaigns operating on its massive social network, and this one came with a few surprises. In the new report, Facebook disclosed that it had removed a network of accounts linked to close Trump ally and former campaign advisor Roger Stone for “inauthentic” activity and coordinated fake accounts around the time of the 2016 presidential election. Facebook has since removed Stone’s own accounts from both Facebook and Instagram.

The accounts linked to Stone, who is set to go to prison next week, posted on a number of topics, mostly from 2015 to 2017, including Florida politics, WikiLeaks’ release of hacked Democratic National Committee emails, the 2016 races and Stone himself, “praising his political acumen, defending him against criminal charges.” Facebook removed 54 Facebook accounts, 50 Facebook pages and four Instagram accounts linked to Stone and his close associates. The network ran related accounts on Twitter and YouTube, as well.

Researchers at the social analytics firm Graphika dive into considerable detail in their own report on the newly unearthed campaign, which was discovered in connection with newly public search warrants from Special Counsel Robert Mueller’s investigation.

Facebook also noted that some of the pages it removed had links to the Proud Boys, the extremist group Facebook eventually banned in 2018 after the group leveraged the platform to recruit and grow its ranks for months if not years. It began looking into the network of accounts through suspected activity by Proud Boys members seeking to return to the platform, uncovering the broader network after the search warrants came to light in April.

Last November, Stone was found guilty of seven felony charges, including making false statements to Congress, obstructing Congress and witness tampering. President Trump has hinted that he plans to pardon his longtime associate. Trump’s Attorney General William Barr created a firestorm of scrutiny earlier this year when he took the highly unusual step of intervening in order to reduce Stone’s sentence, presumably due to the president’s closeness with Stone.

Stone wasn’t the only high-profile political figure to be caught manipulating the social network. A parallel Facebook investigation into fake account networks in Brazil uncovered a cluster of accounts linked to the office of Brazil’s president Jair Bolsonaro and his sons, who had previously been investigated for running “a criminal fake news racket.”",Social Media,TechCrunch,https://techcrunch.com/2020/07/08/facebook-roger-stone-fake-account-network/,"Facebook has recently uncovered a network of accounts linked to a close Trump ally and former campaign advisor, Roger Stone, which were used to spread false information around the time of the 2016 presidential election. In addition, Facebook has also uncovered a similar network in Brazil connected to the office of its President Jair Bolsonaro. This demonstrates the misuse of","Information, Discourse & Governance"
98,Why You Should Think Twice Before Shaming Anyone on Social Media,"Illustration: Yuko Shimizu

Earlier this year, at a tech conference called PyCon, the consultant Adria Richards overheard some indelicate puns — involving the terms ""dongles"" and ""forking"" — from a couple of male attendees sitting behind her. The jokes made Richards uncomfortable, so in the heat of the moment she decided to register her displeasure by tweeting a picture of the two guys, calling their behavior ""not cool.""

More from this issue

In the context of a tech culture that often fails to make women feel welcome, it's easy to see why Richards, sitting there in the (roughly 80 percent male) PyCon audience, felt like she wasn't the one with the power in that room.

But online it was a different story. The two men were social-media nobodies, whereas Richards had more than 9,000 Twitter followers, some highly connected in the tech world. Her grievance quickly received more than 100 retweets and press coverage that stretched from The Washington Post to MSNBC.

PyCon soon responded — sympathetically — to her complaint, but the damage was done. One of the men was recognized by his employer and lost his job. The backlash against his firing then triggered a massive onslaught of online abuse against Richards, who also got fired. No one emerged happy. ""I have three kids, and I really liked that job,"" wrote the newly unemployed jokester. ""Let this serve as a message to everyone, our actions and words, big or small, can have a serious impact."" Later, Richards made a similar assessment: ""I don't think anyone who was part of what happened at PyCon that day could possibly have imagined how this issue would have exploded into the public consciousness ... I certainly did not, and now ... the severest of consequences have manifested.""

Shaming, it seems, has become a core competency of the Internet, and it's one that can destroy both lives and livelihoods. But the question of who's responsible for the destruction — the person engaging in the behavior or the person revealing it — depends on whom you ask. At its best, social media has given a voice to the disenfranchised, allowing them to bypass the gatekeepers of power and publicize injustices that might otherwise remain invisible. At its worst, it's a weapon of mass reputation destruction, capable of amplifying slander, bullying, and casual idiocy on a scale never before possible.

at its best, social media has given a voice to the disenfranchised. at its worst, it's a weapon of mass reputation destruction.

The fundamental problem is that many shamers, like Richards, don't fully grasp the power of the medium. It's a problem that lots of us need to reckon with: There are millions of Twitter accounts with more than 1,000 followers, and millions on Facebook with more than 500 friends. The owners of those accounts might think they're just regular people, whispering to a small social circle. But in fact they're talking through megaphones that can easily be turned up to a volume the entire world can hear.",Social Media,WIRED,https://www.wired.com/2013/07/ap-argshaming/,"Social media can be used to amplify slander, bullying, and other forms of casual idiocy to a scale never before possible, leading to potentially devastating consequences for those exposed to the public.",Social Norms & Relationships
99,"Following backlash, WhatsApp to roll out in-app banner to better explain its privacy update – TechCrunch","Last month, Facebook-owned WhatsApp announced it would delay enforcement of its new privacy terms, following a backlash from confused users which later led to a legal challenge in India and various regulatory investigations. WhatsApp users had misinterpreted the privacy updates as an indication that the app would begin sharing more data — including their private messages — with Facebook. Today, the company is sharing the next steps it’s taking to try to rectify the issue and clarify that’s not the case.

The mishandling of the privacy update on WhatsApp’s part led to widespread confusion and misinformation. In reality, WhatsApp had been sharing some information about its users with Facebook since 2016, following its acquisition by Facebook.

But the backlash is a solid indication of much user trust Facebook has since squandered. People immediately suspected the worst, and millions fled to alternative messaging apps, like Signal and Telegram, as a result.

Following the outcry, WhatsApp attempted to explain that the privacy update was actually focused on optional business features on the app, which allow a business to see the content of messages between it and the end user, and give the businesses permission to use that information for its own marketing purposes, including advertising on Facebook. WhatsApp also said it labels conversations with businesses that are using hosting services from Facebook to manage their chats with customers, so users were aware.

In the weeks since the debacle, WhatsApp says it spent time gathering user feedback and listening to concerns from people in various countries. The company found that users wanted assurance that WhatsApp was not reading their private messages or listening to their conversations, and that their communications were end-to-end encrypted. Users also said they wanted to know that WhatsApp wasn’t keeping logs of who they were messaging or sharing contact lists with Facebook.

These latter concerns seem valid, given that Facebook recently made its messaging systems across Facebook, Messenger and Instagram interoperable. One has to wonder when similar integrations will make their way to WhatsApp.

Today, WhatsApp says it will roll out new communications to users about the privacy update, which follows the Status update it offered back in January aimed at clarifying points of confusion (see below).

In a few weeks, WhatsApp will begin to roll out a small, in-app banner that will ask users to re-review the privacy policies — a change the company said users have shown to prefer over the pop-up, full-screen alert it displayed before.

When users click on “to review,” they’ll be shown a deeper summary of the changes, including added details about how WhatsApp works with Facebook. The changes stress that WhatsApp’s updates don’t impact the privacy of users’ conversations, and reiterate the information about the optional business features.

Eventually, WhatsApp will begin to remind users to review and accept its updates to keep using WhatsApp. According to its prior announcement, it won’t be enforcing the new policy until May 15.

Users will still need to be aware that their communications with businesses are not as secure as their private messages. This impacts a growing number of WhatsApp users, 175 million of whom now communicate with businesses on the app, WhatsApp said in October.

In today’s blog post about the changes, WhatsApp also took a big swipe at rival messaging apps that used the confusion over the privacy update to draw in WhatsApp’s fleeing users by touting their own app’s privacy.

“We’ve seen some of our competitors try to get away with claiming they can’t see people’s messages – if an app doesn’t offer end-to-end encryption by default that means they can read your messages,” WhatsApp’s blog post read.

This seems to be a comment directed specifically toward Telegram, which often touts its “heavily encrypted” messaging app as a more private alternative. But Telegram doesn’t offer end-to-end encryption by default, as apps like WhatsApp and Signal do. It uses “transport layer” encryption that protects the connection from the user to the server, a Wired article citing cybersecurity professionals explained in January. When users want an end-to-end encrypted experience for their one-on-one chats, they can enable the “secret chats” feature instead. (And this feature isn’t even available for group chats.)

In addition, WhatsApp fought back against the characterization that it’s somehow less safe because it has some limited data on users.

“Other apps say they’re better because they know even less information than WhatsApp. We believe people are looking for apps to be both reliable and safe, even if that requires WhatsApp having some limited data,” the post read. “We strive to be thoughtful on the decisions we make and we’ll continue to develop new ways of meeting these responsibilities with less information, not more,” it noted.",Social Media,TechCrunch,https://techcrunch.com/2021/02/18/following-backlash-whatsapp-to-roll-out-in-app-banner-to-better-explain-its-privacy-update/,"The mishandling of the privacy update by WhatsApp led to a widespread lack of trust for Facebook, with millions of users fleeing to alternative messaging apps. This indicates the dangers of social media, and the possible implications of misusing user data.",Security & Privacy
100,Brexit backer’s insurance firm and leave campaign fined £120k by data watchdog – TechCrunch,"The UK’s data protection watchdog has issued fines against a pro-Brexit campaign, Leave.EU, and an insurance company owned by the largest individual donor to the leave cause, Arron Banks’ Eldon Insurance.

The penalties have been handed down for what the Information Commissioner’s Office (ICO) dubs “serious breaches of electronic marketing laws” during the 2016 referendum on the UK’s European Union membership.

The fines — served under the Privacy and Electronic Communications Regulations 2003, which governs electronic marketing — total £120,000 (~$157k); with Leave.EU fined a total of £60k (covering two incidents) and Eldon Insurance £60k.

The ICO’s investigation found the two entities were closely linked and it says systems for segregating the personal data of insurance customers’ from that of political subscribers’ were “ineffective”.

Leave.EU used Eldon Insurance customers’ details unlawfully to send almost 300,000 political marketing messages, according to the ICO’s probe.

Eldon Insurance was also found to have carried out two unlawful direct marketing campaigns which involved the sending of more than a million emails to Leave.EU subscribers without “sufficient consent”.

The ICO says it will now review how both entities are complying with data protection laws by carrying out audits — to observe how personal data is processed; what policies and procedures are in place; and look at the types of training made available for staff.

Key employees across both organisations will also be interviewed, including directors, staff and their data protection officers.

The ICO adds that it will publish its findings when it concludes the audits.

Commenting in a statement, information commissioner Elizabeth Denham, said: “It is deeply concerning that sensitive personal data gathered for political purposes was later used for insurance purposes; and vice versa. It should never have happened. We have been told both organisations have made improvements and learned from these events. But the ICO will now audit the organisations to determine how they are using customers’ personal information.”

We have issued fines totalling £120,000 LeaveEU and Eldon Insurance for serious breaches of electronic marketing laws and will be reviewing how they are using customers’ personal information. https://t.co/ESZBNHeQXb — ICO (@ICOnews) February 1, 2019

The ICO issued a preliminary enforcement notice and three notices of intent to fine Leave.EU and Eldon Insurance trading as Go Skippy Insurance, last November, as part of a wide-ranging investigation into data analytics for political purposes.

“After considering the companies’ representations, the ICO has issued the fines, confirming a change to one amount, with the other two remaining unchanged,” it writes today. “The regulator has also issued two assessment notices to Leave.EU and Eldon Insurance to inform both organisations that they will be audited.”

Banks and associates connected to his unofficial leave campaign remain under investigation by the UK’s National Crime Agency. Last November the NCA announced an investigation into the source of £8M in funding Banks provided to the Leave.EU campaign — after an Electoral Commission investigation found there were reasonable grounds to suspect he was “not the true source” of the money.

The UK introduced legislation back in the year 2000 to outlaw foreign donations, with donors of even a few thousand pounds needing to be both British citizens and on the UK electoral roll for the donations to be legal.

However since then the rise of social media platforms has provided an unregulated workaround for election spending rules by offering a free-for-all conduit for political ads by the backdoor.

And it’s only since major scandals over election interference, such as Kremlin propaganda targeting the 2016 US presidential election, that tech giants have started to pay attention to the problem and introduce some checks on who can run political ads.

Facebook, for example, recently announced it will set up human-staffed operations centers to monitor political news.

In a few markets it’s also launched tools that offer a degree of transparency around who is buying certain types of political ads. But such measures clearly come far too late for Brexit.

And remember! @arron_banks probably has your data whether you’re an Eldon insurance customer or not. He got mine from Moneysupermarket. Does he have yours?https://t.co/Gg0sCqXxtj — Carole Cadwalladr (@carolecadwalla) February 1, 2019

A UK parliamentary committee which spend months investigating the issue of online political disinformation — and slammed Facebook for dodging its questions — came out with a laundry list of recommendations for changes to the law in a preliminary report last year, including calling for a levy on social media firms to defend democracy from disinformation.

Although the government rejected the levy, and most of the committee’s recommendations — preferring a ‘wait and see’ approach. (It has previously committed to legislate around social media and safety, though.)

Last year the UK’s election oversight body issued a series of fines for other leave-backed Brexit referendum campaigns — after finding the official Vote Leave campaign had breached election campaign spending limits by undeclared joint working with a youth-focused Brexit campaign, BeLeave.

Almost half a million pounds in illegal overspending was channeled via a Canadian data firm, AggregateIQ, to use for targeting political advertising pushing pro-Brexit ads on Facebook’s platform.

Facebook later released some of the ads that had been used by Brexit campaigns, which included fake claims and dogwhistle racism being used by leave campaigns to stir up fear among voters about foreigners coming to the UK.

The Facebook Cambridge Analytica data misuse scandal which snowballed into a major global scandal last year, also triggered a major ICO investigation into the use of personal data for political campaigning, parts of which remain ongoing.

The watchdog issued a £500,000 fine on Facebook last year, as part of that probe — saying the company had “failed to sufficiently protect the privacy of its users before, during and after the unlawful processing” by Cambridge Analytica.

Though Facebook has filed an appeal, arguing the ICO did not find evidence that any UK users’ data was processed by CA.

Last year information commissioner Elizabeth Denham also called for an “ethical pause” around the use of microtargeting ad tools for political campaigning — saying there was “a risk of developing a system of voter surveillance by default”.

In the case of Facebook, the platform has generally preferred to continue accepting money for political ads, while it works on expanding self-styled “election security” measures.

Although it did temporarily suspend foreign-funded ads during a referendum in Ireland last year on whether to repeal or retain a constitutional ban on abortion — acting after concerns had been raised. It also fast tracked the launch of an ad transparency tool in the market ahead of the vote.",Social Media,TechCrunch,https://techcrunch.com/2019/02/01/brexit-backers-insurance-firm-and-leave-campaign-fined-120k-by-data-watchdog/,"The UK's data protection watchdog has issued fines to Leave.EU and Arron Banks' Eldon Insurance for breaking electronic marketing laws during the 2016 EU referendum. This has drawn attention to the problem of unregulated political advertising on Social Media, which can lead to disinformation campaigns, foreign interference and voter surveillance.",Security & Privacy
101,"Ringing alarm bells, Biden campaign calls Facebook ‘foremost propagator’ of voting disinformation – TechCrunch","In a new letter to its chief executive on the eve of the first presidential debate, the Biden campaign slammed Facebook for its failure to act on false claims about voting in the U.S. election.

In the scathing letter, published by Axios, Biden Campaign Manager Jen O’Malley Dillon specifically singled out a troubling video post the Trump campaign shared to Facebook and Twitter last week.

Over the course of that video, the president’s son claims that his father’s political opponents “plan to add millions of fraudulent ballots that can cancel your vote and overturn the election” and calls on supporters to “enlist now” in an “army for Trump election security operation.” Those false claims appear to have inspired some Trump supporters, who plan to guard ballot drop-off sites and polling places — a form of voter intimidation that would likely constitute a federal crime.

When the Biden campaign (along with many others) flagged the video to Facebook, the company apparently said that the content would not be removed, pointing to its small, unobtrusive voting info labels that appear alongside all posts related to the 2020 U.S. election. The video remains up on Twitter with a similar label.

“We were assured that the label affixed to the video, buried on the top right corner of the screen where many viewers will miss it, should allay any concerns,” O’Malley Dillon wrote in the letter, addressed to Mark Zuckerberg.

“No company that considers itself a force for good in democracy, and that purports to take voter suppression seriously, would allow this dangerous claptrap to be spread to millions of people. Removing this video should have been the easiest of easy calls under your policies, yet it remains up today.”

In the letter, O’Malley Dillon also cites the president’s own repeated attempts to undermine national confidence in the 2020 election with unsubstantiated lies about the voting process, which is already under unique strain this year from the pandemic.

Rather than taking a strong approach to limit the reach of election-related disinformation from the president and his supporters, Facebook has largely remained hands-off. The platform is more comfortable touting its get out the vote campaign and other politically neutral efforts to inform and mobilize voters. Facebook clearly hopes those measures will offset its current role disseminating domestic disinformation from the president himself, but given the scope of what’s happening — and its lingering failures from 2016 — that doesn’t look likely.

“As you say, ‘voting is voice.’ Facebook has committed to not allow that voice to be drowned out by a storm of disinformation, but has failed at every opportunity to follow through on that commitment,” O’Malley Dillon wrote, adding that the Biden campaign would “be calling out those failures” over the course of the remaining 36 days until the election.",Social Media,TechCrunch,https://techcrunch.com/2020/09/29/biden-campaign-facebook-letter-trump-jr-army-for-trump/,"The Biden campaign has criticized Facebook for its failure to act on false claims about voting in the U.S. election, citing a troubling video post from the Trump campaign that could constitute voter intimidation. Facebook has been slow to limit the reach of election-related disinformation from the president and his supporters, undermining the trust in the 2020 election.","Information, Discourse & Governance"
102,Twitter has a big bot problem – TechCrunch,"Twitter bots – robots that interact with humans – have a long history. The Twitter API is fairly easy to use (I made a bot that plays Zork with a friend two years ago) and there is little protection against creating new accounts automatically. This ease of use used to be great for programmers but now Twitter has a huge bot problem.

Bots have expanded beyond the traditional tactics. One older site recommended creating a bot network to “get a database together of all your competitors.”

“Have a rebuttal promoting your brand over their brands as messages. Whenever someone in your niche talks about the competition, send a tweet or even A DM (Direct Message), with your message of being the better product/service, and to give you a try.”

The new system pales in comparison.

In the latest example, a reporter for the Daily Beast, Joseph Cox, was kicked off of Twitter for being followed too quickly by an army of Twitter bots.

Cox can’t talk about why he was banned – his conversation with Twitter was off the record – but after writing his post he saw a strange pattern. A series of bots followed him in the same order they followed other major folks who have been writing about the Russian bot phenomenon. In short, the bots “read” news stories and then followed their authors, one after the other, in lockstep.

Once this happens, Twitter’s anti-bot tools spring into action and… ban the original poster.

“Let that sink in for a moment: A huge collection of botted accounts — the vast majority of which should be easily detectable as such — may be able to abuse Twitter’s anti-abuse tools to temporarily shutter the accounts of real people suspected of being bots!” wrote security researcher Brian Krebs. “The botnet or botnets appear to be targeting people who are exposing the extent to which sock puppet and bot accounts on social media platforms can be used to influence public opinion.”

The ordeal started when Atlantic Council’s Digital Forensic Research Lab was investigating pro-Kremlin accounts after the events in Charlottesville and found itself quickly attacked by the same bot armies that actively spread misinformation and pro-Trump politics. After writing about bots and intimidation tactics, hackers quickly targeted the DFR for attack resulting in tweets and follows from “tens of thousands of automated accounts […] that was apparently meant as a show of force.”

A fascinating Planet Money episode further exposes the strange connection between media, bots, and Russian astroturfing.

Further, the botnet essentially created a denial of service attack on the DFR’s twitter accounts, sending hundreds of fake retweets to the account and essentially making it useless if standard Twitter notification settings are used. As you can see, the retweets exploded for a few hours and then slowed down. “Thus the massive retweeting did not spread to genuine Twitter users. Instead, the main effect was to bombard the Twitter feeds of the accounts mentioned in the post with an endless series of notifications,” wrote Ben Nimmo.

Cox wrote a story based on DFR’s research and quickly found himself a target of intimidation.

“It’s hilarious that someone would even bother using bots on my account, but if bots were behind the temporary suspension, then Twitter may have some more issues around policing its platform,” said Cox when I asked him about his experience. His account is back online. However, thanks to Twitter’s policies, messages and memes are spreading in ways probably never envisioned by the folks who made it easy to activate a Tweetbot.",Social Media,TechCrunch,https://techcrunch.com/2017/09/01/twitter-has-a-big-bot-problem/,"Twitter bots are being used to spread misinformation, pro-Trump politics, and to intimidate, resulting in a denial of service attack on Twitter accounts. This has caused Twitter to suspend people's accounts, which has raised questions about their anti-bot policies.",Security & Privacy
103,YouTube declares war on US election misinformation… a month late – TechCrunch,"As Twitter and Facebook scrambled to institute new policies for the 2020 election, YouTube was… mostly quiet. The platform didn’t make any flashy announcements about a crackdown on election-related misinformation, nor did it really fully grapple with its massive role in distributing information during what was widely regarded as an extremely volatile time for American democracy.

Former Vice President Joe Biden won the presidential election on November 7, but YouTube decided to wait until the “safe harbor” deadline, when audits and recounts must be wrapped up at the state level, to enforce a set of rules against election misinformation.

In a new blog post out Wednesday, the world’s second-biggest social network explained itself — sort of:

Yesterday was the safe harbor deadline for the U.S. Presidential election and enough states have certified their election results to determine a President-elect. Given that, we will start removing any piece of content uploaded today (or anytime after) that misleads people by alleging that widespread fraud or errors changed the outcome of the 2020 U.S. Presidential election, in line with our approach towards historical U.S. Presidential elections. For example, we will remove videos claiming that a Presidential candidate won the election due to widespread software glitches or counting errors. We will begin enforcing this policy today, and will ramp up in the weeks to come.

YouTube clarified that while its users were allowed to spread misinformation about an undecided election, content claiming that “widespread fraud or errors” influenced the result of a past election will not be allowed. And from YouTube’s perspective, which accommodated the Trump administration’s many empty challenges to the results, the election was only decided yesterday.

The four days between November 3 and November 7 were fraught, plagued by false victory claims from President Trump and his supporters and concerns about political violence as online misinformation, already a pervasive threat, kicked into overdrive. Rather than wading into all that as Twitter and even the ever-reluctant-to-act Facebook did, YouTube mostly opted to sit back and wait for history to take its course. The company was more comfortable universally pointing users toward real information than making tough calls and actively purging false claims from its platform.

YouTube doesn’t go to great lengths to explain itself these days, much less make real-time platform policy decisions in a transparent way. Twitter has pioneered that approach, and while its choices aren’t always clear or decisive, its transparency and open communication is admirable. If Twitter doesn’t always get it right, YouTube fails to even step up to the plate, making few real efforts to adapt to the rapidly mutating threats posed by misinformation online.

YouTube’s opaque decision-making process is compounded by the also opaque nature of online video, which is vastly more difficult for journalists to search and index than text-based platforms. The result is that YouTube had largely gotten away with relatively little scrutiny compared to its stature in the social media world. It’s bizarre to see Mark Zuckerberg and Jack Dorsey called before the Senate Judiciary Committee without even a passing thought to bringing in YouTube CEO Susan Wojcicki as well. In spite of its massive influence and two billion users, the social video behemoth is barely on the radar for lawmakers.

If YouTube’s strategy is that communicating less attracts less attention, unfortunately it appears to be working. The company is bound to be anxious about getting dragged into federal and state-level antitrust investigations, particularly with state lawsuits that could try to force Facebook and Instagram apart.

The Justice Department is already targeting Google with a historic antitrust suit focused on its search business, but that doesn’t preclude other antitrust actions from taking aim at YouTube. Keeping its head down may have worked for YouTube during four years of Trump, but President-elect Biden is more interested in inoculating people against misinformation rather than super-spreading it.",Social Media,TechCrunch,https://techcrunch.com/2020/12/09/youtube-declares-war-on-us-election-misinformation-a-month-late/,"YouTube's lack of transparency on its decision-making process concerning election-related misinformation has allowed it to largely evade scrutiny, even though its role in spreading false information has been significant. This has created an environment of misinformation that has been difficult for both journalists and lawmakers to counter.","Information, Discourse & Governance"
104,Facebook is hiring a director of human rights policy to work on “conflict prevention” and “peace-building” – TechCrunch,"Facebook is advertising for a human rights policy director to join its business, located either at its Menlo Park HQ or in Washington DC — with “conflict prevention” and “peace-building” among the listed responsibilities.

In the job ad, Facebook writes that as the reach and impact of its various products continues to grow “so does the responsibility we have to respect the individual and human rights of the members of our diverse global community”, saying it’s:

… looking for a Director of Human Rights Policy to coordinate our company-wide effort to address human rights abuses, including by both state and non-state actors. This role will be responsible for: (1) Working with product teams to ensure that Facebook is a positive force for human rights and apply the lessons we learn from our investigations, (2) representing Facebook with key stakeholders in civil society, government, international institutions, and industry, (3) driving our investigations into and disruptions of human rights abusers on our platforms, and (4) crafting policies to counteract bad actors and help us ensure that we continue to operate our platforms consistent with human rights principles.

Among the minimum requirements for the role, Facebook lists experience “working in developing nations and with governments and civil society organizations around the world”.

It adds that “global travel to support our international teams is expected”.

The company has faced fierce criticism in recent years over its failure to take greater responsibility for the spread of disinformation and hate speech on its platform. Especially in international markets it has targeted for business growth via its Internet.org initiative which seeks to get more people ‘connected’ to the Internet (and thus to Facebook).

More connections means more users for Facebook’s business and growth for its shareholders. But the costs of that growth have been cast into sharp relief over the past several years as the human impact of handing millions of people lacking in digital literacy some very powerful social sharing tools — without a commensurately large investment in local education programs (or even in moderating and policing Facebook’s own platform) — has become all too clear.

In Myanmar Facebook’s tools have been used to spread hate and accelerate ethic cleansing and/or the targeting of political critics of authoritarian governments — earning the company widespread condemnation, including a rebuke from the UN earlier this year which blamed the platform for accelerating ethnic violence against Myanmar’s Muslim minority.

In the Philippines Facebook also played a pivotal role in the election of president Rodrigo Duterte — who now stands accused of plunging the country into its worst human rights crisis since the dictatorship of Ferdinand Marcos in the 1970s and 80s.

While in India the popularity of the Facebook-owned WhatsApp messaging platform has been blamed for accelerating the spread of misinformation — leading to mob violence and the deaths of several people.

Facebook famously failed even to spot mass manipulation campaigns going on in its own backyard — when in 2016 Kremlin-backed disinformation agents injected masses of anti-Clinton, pro-Trump propaganda into its platform and garnered hundreds of millions of American voters’ eyeballs at a bargain basement price.

So it’s hardly surprising the company has been equally naive in markets it understands far less. Though also hardly excusable — given all the signals it has access to.

In Myanmar, for example, local organizations that are sensitive to the cultural context repeatedly complained to Facebook that it lacked Burmese-speaking staff — complaints that apparently fell on deaf ears for the longest time.

The cost to American society of social media enabled political manipulation and increased social division is certainly very high. The costs of the weaponization of digital information in markets such as Myanmar looks incalculable.

In the Philippines Facebook also indirectly has blood on its hands — having provided services to the Duterte government to help it make more effective use of its tools. This same government is now waging a bloody ‘war on drugs’ that Human Rights Watch says has claimed the lives of around 12,000 people, including children.

Facebook’s job ad for a human rights policy director includes the pledge that “we’re just getting started” — referring to its stated mission of helping people “build stronger communities”.

But when you consider the impact its business decisions have already had in certain corners of the world it’s hard not to read that line with a shudder.

Citing the UN Guiding Principles on Business and Human Rights (and “our commitments as a member of the Global Network Initiative”), Facebook writes that its product policy team is dedicated to “understanding the human rights impacts of our platform and to crafting policies that allow us both to act against those who would use Facebook to enable harm, stifle expression, and undermine human rights, and to support those who seek to advance rights, promote peace, and build strong communities”.

Clearly it has an awful lot of “understanding” to do on this front. And hopefully it will now move fast to understand the impact of its own platform, circa fifteen years into its great ‘society reshaping experience’, and prevent Facebook from being repeatedly used to trash human rights.

As well as representing the company in meetings with politicians, policymakers, NGOs and civil society groups, Facebook says the new human rights director will work on formulating internal policies governing user, advertiser, and developer behavior on Facebook. “This includes policies to encourage responsible online activity as well as policies that deter or mitigate the risk of human rights violations or the escalation of targeted violence,” it notes.

The director will also work with internal public policy, community ops and security teams to try to spot and disrupt “actors that seek to misuse our platforms and target our users” — while also working to support “those using our platforms to foster peace-building and enable transitional justice”.

So you have to wonder how, for example, Holocaust denial continuing to be being protected speech on Facebook will square with that stated mission for the human rights policy director.

At the same time, Facebook is currently hiring for a public policy manager in Francophone, Africa — who it writes can “combine a passion for technology’s potential to create opportunity and to make Africa more open and connected, with deep knowledge of the political and regulatory dynamics across key Francophone countries in Africa”.

That job ad does not explicitly reference human rights — talking only about “interesting public policy challenges… including privacy, safety and security, freedom of expression, Internet shutdowns, the impact of the Internet on economic growth, and new opportunities for democratic engagement”.

As well as “new opportunities for democratic engagement”, among the role’s other listed responsibilities is working with Facebook’s Politics & Government team to “promote the use of Facebook as a platform for citizen and voter engagement to policymakers and NGOs and other political influencers”.

So here, in a second policy job, Facebook looks to be continuing its ‘business as usual’ strategy of pushing for more political activity to take place on Facebook.

And if Facebook wants an accelerated understanding of human rights issues around the world it might be better advised to take a more joined up approach to human rights across its own policy staff board, and at least include it among the listed responsibilities of all the policy shapers it’s looking to hire.",Social Media,TechCrunch,https://techcrunch.com/2018/09/16/facebook-is-hiring-a-director-of-human-rights-policy-to-work-on-conflict-prevention-and-peace-building/,"Facebook's tools have been used to spread hate, accelerate ethnic cleansing, target political critics of authoritarian governments, and spread misinformation, leading to mob violence and numerous deaths. This has resulted in Facebook facing fierce criticism and condemnation, and necessitates the company taking greater responsibility for its products and investing in local education programs.",Equality & Justice
105,The scramble to archive Capitol insurrection footage before it disappears,"Coleman says that Anonymous’s efforts were once considered extreme, but with each passing protest, doxxing has become more mainstream. “Of course, you’ve also got groups like Bellingcat who are like amateur professionals when it comes to open-source intelligence formalized into an organization,” Coleman says. “But you’re continuing to see masses of people come together online [and doxx].”

That creates ethical quandaries. The data now being archived could haunt people in the photos for years to come, even if they later renounce or pay criminal penalties for their actions. On r/DataHoarder, for instance, someone asked, “Do you think it’s ethical to preserve content that features someone who now wants the content to no longer be public?”

I asked Lynch whether it was hypocritical for someone working to expose members of the mob to ask a reporter for anonymity.

“I believe people have the right to protest and share their voice,” was the response. “If they [mob members] wanted to protect their identity, they could have easily worn a mask or not livestreamed. But they didn’t wear a ski mask—not even a covid mask.”

“I think certainly a lot of this is context dependent,” Coleman says. “If you are engaging in an activity that is meant to call attention to the activity itself and don’t take precautions to hide your identity, it’s understandable how there will be people who will take that information and make it public.”

Lynch, who plans to ultimately submit the data to the Library of Congress, believes this activity is preserving history, saying: “We can only hoard what the world gives us. We’re just librarians.”

Correction: An earlier version of this story incorrectly stated that Reddit moderators had shut down the thread on r/DataHoarder, rather than Mega shutting down the upload link.",Social Media,MIT,https://www.technologyreview.com/2021/01/08/1015929/archive-capitol-insurrection-trump-maga-footage/,"The rising use of doxxing and other forms of data archiving by Anonymous and other organizations has led to ethical quandaries, as the data being archived can haunt people in the photos for years to come, even if they later renounce or pay criminal penalties for their actions.",Security & Privacy
106,Facebook removes hundreds of accounts linked to fake news group in Indonesia – TechCrunch,"Facebook said today it has removed hundreds of Facebook and Instagram accounts with links to an organization that peddled fake news.

The world’s fourth largest country with a population of more than 260 million, Indonesia is in an election year alongside Southeast Asia neighbors Thailand and the Philippines. Facebook said this week it has set up an “election integrity” team in Singapore, its APAC HQ, as it tries to prevent its social network being misused in the lead-up to voting as happened in the U.S.

This Indonesia bust is the first move announced since that task force was put in place, and it sees 207 Facebook Pages, 800 Facebook accounts, 546 Facebook Groups and 208 Instagram accounts removed for “engaging in coordinated inauthentic behavior.”

“About 170,000 people followed at least one of these Facebook Pages, and more than 65,000 followed at least one of these Instagram accounts,” Facebook said of the reach of the removed accounts.

The groups and accounts are linked to Saracen Group, a digital media group that saw three of its members arrested by police in 2016 for spreading “incendiary material,” as Reuters reports.

Facebook isn’t saying too much about the removals other than: “we don’t want our services to be used to manipulate people.”

In January, the social network banned a fake news group in the Philippines in similar circumstances.

Despite the recent action, the U.S. company has struggled to manage the flow of false information that flows across its services in Asia. The most extreme examples come from Myanmar, where the UN has concluded that Facebook played a key role in escalating religious hatred and fueling violence. Facebook has also been criticized for allowing manipulation in Sri Lanka and the Philippines, among other places.",Social Media,TechCrunch,https://techcrunch.com/2019/01/31/facebook-fake-news-group-indonesia/,"Social media has been linked to the spread of false information and manipulation in various Asian countries, leading to religious hatred and violence in some cases. Facebook has taken steps to address this issue by setting up an ""election integrity"" team and removing hundreds of accounts in Indonesia, but it is still struggling to manage the flow of false information across its services","Information, Discourse & Governance"
107,Inside the Pricey War to Influence Your Instagram Feed,"When Sahara Lotti started her lash extensions company, Lashify, in 2017, she didn’t know what she was getting herself into. It wasn’t making and selling fake lashes that stumped her—she was more than prepared for that—but rather the bizarre and shadowy industry that seemed to envelop her.

The suggestions started early. Months before Lashify had officially launched, one of her investors, who had ties to the cosmetics industry, pulled her aside. He told her to prepare to pay influencers to speak positively about her lashes on YouTube and Instagram. She thought he was being dramatic. He wasn’t.

Lotti recalls the investor saying that if she wanted Lashify to succeed, quality didn’t matter, nor did customer satisfaction—only influencers. And they didn’t come cheap. She was told to expect to shell out $50,000 to $70,000 per influencer just to make her company’s name known, an insane amount for a new startup. There was no way around it; that’s just how things worked.

At the time, Lotti found the suggestion absurd, bordering on offensive. She thought paying some random person on the internet tens of thousands of dollars just because they had a lot of followers was beneath her, so she brushed off the suggestion. Looking back now, Lotti realizes how horribly naive she was. She may have avoided forking over cash, sure, but she ended up paying for her decision nonetheless.

Lotti found herself thrust into the wild world of influencer marketing, where prices and pressure are high, and hundreds of thousands of dollars change hands daily on murky terms, seeking sway over the posts in your feed. “It was literally like the mafia,” Lotti says. “[It] was a total nightmare because I didn’t understand the climate.”

Social media influencers ply their trade in realms far beyond fake lashes. Marketers of literature, wellness, fashion, entertainment, and other wares are all hooked on influencers. As brands have warmed to social-media advertising, influencer marketing has grown into a multibillion-dollar industry. Unlike traditional television or print ads, influencers have dedicated niche followings who take their word as gospel.

There’s another plus: Many users don’t view influencers as paid endorsers or salespeople—even though a significant percentage are—but as trusted experts, friends, and “real” people. This perceived authenticity is part of why brands shell out so much cash in exchange for a brief appearance in your Instagram feed.

Many influencers with substantial followings “are not promoting products without being compensated,” said Kevin James Bennett, a cosmetics developer and consultant who works with brands interested in influencer marketing. “That doesn’t make them bad people, it makes them salespersons—and you, the consumer, deserve to know when you’re being ‘sold’ something.”

The Federal Trade Commission agrees. As the practice has become more popular, the agency has adopted rules governing the disclosure of paid endorsements on social media. The text is long and complicated, but can be reduced to two essential concepts: If an influencer has received anything—be it cash, free products, or something else—that could affect how a viewer interprets their mention of a brand or product, they must disclose it; and the disclosure must be displayed prominently, and plainly, in the video, photo, or blog.

Ethical Concerns

In interviews, more than a dozen people involved in influencer marketing expressed concerns over the ethics of the burgeoning industry, where brands routinely shell out well over $60,000 in exchange for one video review—or upwards of $85,000 to publicly disparage a competitor’s product. The activity is not confined to reviews. Influencers with a sizable following rarely have to purchase products in their niche. Makeup, clothing, plants, books, you name it—all come free, often delivered to the influencer’s home or office in a highly Instagrammable box. That’s given rise to a new variation of the influencer game, similar to product placements in movies or television: Brands pay influencers to position products on their desks, behind them, or anywhere else they can subtly appear onscreen for a few seconds. Payouts increase if an influencer tags a brand in a post or includes a link to the company’s site, but silent endorsements are often preferred.

Sanders Kennedy, a popular YouTuber with over 200,000 subscribers and known for chronicling the drama within the influencer community, was once offered a couple of thousand dollars to leave a particular beverage on his desk while filming. He does not recall the brand but says a representative told him he only needed to make sure the drink appeared within the frame to get his paycheck. And he would not need to tell his audience that he was being paid for the placement, Kennedy says the rep told him.",Social Media,WIRED,https://www.wired.com/story/pricey-war-influence-your-instagram-feed/,"Social media influencer marketing has grown into a multibillion-dollar industry, often relying on unethical business practices such as paying influencers to give positive reviews without disclosure and providing free products in exchange for endorsements without mentioning the brand. These practices create an environment of deception for consumers, who are unaware of these hidden transactions and endorsements.",Equality & Justice
108,The LAPD built collecting social media info into its interview process for civilians,"The Los Angeles Police Department asked officers to gather social media information as a standard part of their interviews with civilians, whether or not the people were implicated in a crime. The practice, revealed in public records obtained by the Brennan Center for Justice, is part of a larger LAPD social media monitoring strategy — one that’s similar to other US police department policies but may go beyond them.

As the Brennan Center notes, a 2015 memo from LAPD Police Chief Charlie Beck told the department that “similar to a nickname or an alias, a person’s online persona or identity used for social media and communication can be highly beneficial to investigations and possibly even future outreach programs.” Beck said that officers should collect “social media and email account information” when filling out field interview report forms — which were updated to specifically ask about social media data.

Previous reporting has indicated that the LAPD aggregates field interview information into a database run by surveillance company Palantir. Officers are allowed — but not required — to enter field interview cards for anyone they come into contact with. Broadly collecting social media data could leave people who are stopped by police vulnerable to additional online surveillance regardless of whether they’re officially under investigation. An LAPD spokesperson told The Guardian that the interview forms were being “updated” but didn’t describe what the update includes, and the LAPD’s press office did not immediately respond to a request for clarification from The Verge.

The newly released documents also offer more details about how the LAPD has worked with online surveillance companies — a practice many police departments participate in. In 2016, the LAPD apparently employed Twitter analytics firm Dataminr (whose access Twitter later limited) to receive alerts around shooting and bomb threats, but also to track the movements of anti-Trump protesters on May Day. The department also used tracking service Geofeedia (which was cut off by Facebook, Twitter, and Instagram) to monitor protest-related keywords including #BlackLivesMatter and #FuckDonaldTrump. More recently, the LAPD has apparently entered a contract with Media Sonar — which pitches the ability to “automatically find digital footprints in the matter of seconds.”",Social Media,Verge,https://www.theverge.com/2021/9/8/22663261/lapd-social-media-information-public-records-document,"The LAPD's policy of collecting social media data during routine interviews with civilians can leave people vulnerable to additional online surveillance, regardless of whether they are officially under investigation.",Security & Privacy
109,Holy moly! Facebook test was nudging me to chat with a church – TechCrunch,"Facebook says it’s on a mission to de-bloat its Messenger app this year. At the same time, TechCrunch has discovered, the data-mining giant is running tests on its desktop platform to inject the Messenger “contact pages” of businesses directly into your chat sidebar.

And here it appears to be toying with the idea of spamming Facebookers with businesses they’ve neither used nor expressed any personal interest in using… So, er, so much for trimming its own fat then.

I know because earlier this month I found myself in one of these tests buckets, after I noticed some new “contact pages” icons appearing at the top of the Facebook chat sidebar…

To be clear, the sidebar is the messaging shortcut area at the right hand side of the Facebook News Feed (when you visit Facebook on the desktop). It usually contains a list of your friends so you can easily locate them to open up a chat.

But, this month, while I was in Facebook’s test bucket, I was seeing businesses’ contact pages appearing here too — above the ‘chat head’ icons of friends. So basically in the most prominent spot possible on the page.

At first glance only one of the three icons was familiar as a business I knew by sight (and had used). A second icon turned out to an online business of a Facebook friend of mine which I was also aware of (and had visited to test out).

The third was the Facebook page of a Methodist church. I had never come across this church before in my life…

What to make of this bizarre spiritual incursion by Facebook?!

The company was suddenly spamming me with a religious icon which, once I hovered over it, urged me to send a message to a place of worship I had never attended. The company was actively urging me to engage with a religious belief I do not in fact hold.

I’m really not sure that ‘religion’ is the kind of ‘business’ anyone might feel urged to contact just because they happen to have a Facebook friend who’s into that kind of thing. But here was Facebook testing exactly that possibility — offering me a suggestive messaging hotlink to, er, God.

While methodists might welcome Facebook’s Damascene conversion to (at least in this example) evangelizing their particular brand of Christianity, it doesn’t take much imagination to envisage the kind of ‘businesses’ Kremlin agents might seek to seed across Facebook to take advantage of algorithmic recommendations for “contact pages” and win prominent placing in users’ sidebars.

But I digress.

Returning to the test, there didn’t seem to be an obvious option to edit the contact pages. The only visible options were left and right arrows to scroll through some other (also unfamiliar) business pages. (Though later I noticed if you hover over each icon, which then pops up a glut more info about it — including a big button to message the page owner, a tiny grey cross also momentarily materializes next to the original icon, which presumably lets you edit it out of the selection.)

While I couldn’t (initially) see a way to delete the church — or indeed either of the other two businesses (which I also didn’t feel I would ever need to message via the Facebook homepage) — there was a (very visible) option to click to ‘see all’.

This opened a full spread of business contact pages (below) — which provided a hint at Facebook’s logic for surfacing those first three icons in the sidebar. Here it told me (in pale grey letters) that particular Facebook friends of mine had either liked each individual business or been geolocated as using it (via a Facebook ‘check in’).

In the church’s case its page had been liked by one of my Facebook friends. Hence, presumably, Facebook thinking I’d also like to chat to it. (Which seems more like an ‘artificial dumb idea’ than artificial intelligence to my eye.)

I can’t read a great deal of sense into why Facebook had settled on these particular friends to power these particular business suggestions either. They seem to be friends I have chatted with at least once via Facebook (though not necessarily that recently; but then I also don’t use Facebook for much messaging).

And/or friends that I had been geolocated with together at the same business by Facebook — at least in one instance (again not recently, and I know for a fact the friend has also only been to the business in question once). So shared location appears to be being very heavily weighted in Facebook’s business recommendation engine.

The rest of the contact pages (the second tier) were businesses that other users of Facebook had liked. None were businesses I had ever used (nor were familiar with). But there are ~2BN Facebook users these days which means there are, inevitably, billions and billions of ‘liked’ business pages on Facebook. Any of which, presumably, could be served into your sidebar by Facebook’s algorithms in future (assuming this test gets the go ahead).

These non-friend based suggestions appear to factor in where Facebook thinks you’re located. Beyond that, I’d guess they’re generally tailored to what Facebook thinks you’ll find interesting and relevant — based on the continuous data-mining the company performs of every user (and at least some non users) for its ad-targeting purposes.

Its assessment of my interests here — fitness, community projects, technology and pets — was certainly less ‘out there’ than nudging me towards a Methodist church. Or pointing me at a random fashion brand that a couple of guys I happen to know had apparently once liked (another of the friend-powered suggestions it had curated for me).

Though the actual businesses being suggested were still no less irrelevant in this context — i.e. in a personalized index with Facebook’s suggestive call to action being that I leap right in and start chatting.

Being quasi-relevant yet also useless made these suggestions feel even more creepy — highlighting the unstated fact of their being powered by intelligence Facebook had gleaned via background surveillance.

Would I ever decide to A) browse an index of businesses Facebook believes I would like and B) have the urge to ‘cold-call’ any of them via a Fb message? In a word, no.

If I’m looking for a particular type of business then I’ll do it under my own steam. I would certainly never think ‘I’ll go and see if Facebook has a suggestion’. That might be how algorithms ‘think’; but it’s not how I do.

I contacted Facebook to ask about what I was seeing and its intentions regarding the chat sidebar and businesses’ contact pages. And I was told multiples times, by multiple Facebook employees, that they were looking into it.

Meanwhile I had a look around online — and found several other Facebook users apparently stuck in similar test buckets (in a few different geographical locations, dating back at least as far back as November) — also complaining that the intrusion into their sidebar felt creepy and irrelevant…

And what has my Facebook Ticker now been replaced with? The option to 'Contact Pages'. None of those three pages is one I've liked, so I suppose if it's just if I'm in the mood to contact dog breeders or a bar in another state? pic.twitter.com/IhoitHCLLL — Trash Addict (@trash_addict) December 1, 2017

@facebook I don't want contact pages, I want you to restore the old situation, where I can see who of my contacts liked pages or commented so many minutes ago. W/o that you are close to finished for me. — Geor (@geor97) November 16, 2017

Is this Facebook ""contact pages"" widget new? Why on Earth would I need to contact a baby furniture outlet? pic.twitter.com/uos0sc7XxK — Randi Shaffer (@RandiMShaffer) November 6, 2017

Dear @facebook: Your ""contact pages"" on the right side of my screen is ill-thought of and just plain creepy. While I share my some of life online, don't pry deeper. Please stop being an eerie stalker. — Troy Alivio (@troy_alivio) January 11, 2018

Is this Facebook ""contact pages"" widget new? Why on Earth would I need to contact a baby furniture outlet? pic.twitter.com/uos0sc7XxK — Randi Shaffer (@RandiMShaffer) November 6, 2017

It’s been a week since I started asking Facebook about this, and I am still waiting for it to explain why its algorithms were nudging me to chat with a church.

While the company has not answered any questions nor even provided a general explanation of what’s going on, one of its comms spokespeople suggested the query might need to go to the Messenger team. (Albeit, when I contacted a member of the Messenger comms team they just rerouted my request back to Fb’s main press email saying they couldn’t help. So [shrug emoji].)

One development: I do appear to have been removed from the test bucket. So — for now at least — the chat sidebar has been returned to its pre-test state of being just a list of friends, not a filofax of irrelevant businesses thrust in my eyeline.

But it remains to be seen whether Facebook decides to roll it out in future. It does continuously run tests on its products. And many of these tweaks never end up getting actually baked in to the software.

But given that business messaging is a growing area of focus for the company — just last week it launched a business app for its WhatsApp messaging product, for example — and given its freshly stated desire to remove some of the non-personal ‘noise’ from News Feed, it at least seems unlikely it will abandon trying to find a more prominent placement for businesses’ contact pages on its desktop product.

It could be that now Facebook has decided the News Feed must include less content from the world outside your friend circle (aka your filter bubble), then it’s time for the chat sidebar to get a bit less personal (aka more spammy) — and that’s why it’s testing having it double up as a place where it can actively push users to chat with businesses as well as friends.

Even though mixing friends with algorithmically suggested business contact pages feels, well, horribly icky.

Having been in its test bucket, and having found myself spammed with some very random business contact details, I’d urge the company to think twice before it switches on yet another lead-footed algorithmic evangelizer — and risks wading into fresh controversies at a time when its platform is already bogged down on that front.

Alternatively, I’m left to wonder whether there was some kind of underlying spiritual coda being delivered when CEO Mark Zuckerberg talked about wanting Facebook to encourage “meaningful social interactions” and lead to more ‘time well spent’. What were you really trying to tell us, Mark? And what are you really trying to sell us?

Update: A Facebook spokesperson finally responded to say the goal of the test “is to help connect people with the businesses they care about on Facebook by offering suggestions to people for Pages they might want to message” — adding: “This is a Pages messaging test, not a Messenger product test or feature.”

She also clarified there is in fact an off switch for the test: “If you’d like to stop seeing these suggestions, you can opt out of this test by going to the bottom settings menu [on the chat sidebar] and clicking “Turn off Pages to Contact”.”",Social Media,TechCrunch,https://techcrunch.com/2018/01/23/holy-moly-facebook-test-was-nudging-me-to-chat-with-a-church/,"Facebook is running tests on its desktop platform to inject the Messenger “contact pages” of businesses directly into users' chat sidebars without their consent, potentially leading to users being spammed with irrelevant businesses and other unwanted contacts.",Social Norms & Relationships
110,Telegram gets 3M new signups during Facebook apps’ outage – TechCrunch,"Messaging platform Telegram claims to have had a surge in signups during a period of downtime for Facebook’s rival messaging services.

In a message sent to his Telegram channel, founder Pavel Durov wrote: “I see 3 million new users signed up for Telegram within the last 24 hours.”

It’s probably not a coincidence that Facebook and its related family of apps went down for most of Wednesday, as we reported earlier. At the time of writing, Instagram’s service has been officially confirmed restored. Unofficially, Facebook also appears to be back online, at least here in Europe.

Durov doesn’t offer an explicit explanation for Telegram’s sudden spike in signups, but he does take a thinly veiled swipe at social networking giant Facebook — whose founder recently claimed he now plans to pivot the ad platform to “privacy.”

“Good,” adds Durov on his channel, welcoming Telegram’s 3M newbies. “We have true privacy and unlimited space for everyone.”

A contact at Telegram confirmed to TechCrunch that the Facebook apps’ downtime is the likely cause of its latest signup spike, telling us: “These outages always drive new users.”

Though they also credited growth to “the mainstream overall increasing understanding about Facebook’s abusive attention harvesting practices.”

A year ago Telegram announced passing 200 million monthly active users. Though the platform has faced restrictions and/or blocks in some markets (principally Russia and Iran, as well as China) — apparently for refusing government requests for encryption keys and/or user information.

In Durov’s home country of Russia the government is also now moving to tighten internet restrictions via new legislation — and thousands of people took to the streets in Moscow and other Russian cities this weekend to protest growing internet censorship, per Reuters.

Such restrictions could increase demand for Telegram’s encrypted messaging service in the country as the app does appear to still be partially accessible there.

Durov, who famously left Russia in 2014 — stepping away from his home country and an earlier social network he founded (VK.com) because of his stance on free speech — has sought to thwart the Russian government’s Telegram blocks via legal and technical measures.

The Telegram messaging platform has of course also had its own issues with less political downtime too.

In a tweet last fall the company confirmed a server cluster had gone down, potentially affecting users in the Middle East, Africa and Europe, although in that case the downtime only lasted a few hours.",Social Media,TechCrunch,https://techcrunch.com/2019/03/14/telegram-gets-3m-new-signups-during-facebook-apps-outage/,"The recent surge in signups for Telegram, attributed to the downtime of Facebook and its related family of apps, highlights the increasing understanding of Facebook's abusive attention harvesting practices and the potential for growing demand for secure messaging platforms.",Security & Privacy
111,Yes Facebook is using your 2FA phone number to target you with ads – TechCrunch,"Yes Facebook is using your 2FA phone number to target you with ads

Facebook has confirmed it does in fact use phone numbers that users provided it for security purposes to also target them with ads.

Specifically a phone number handed over for two factor authentication (2FA) — a security technique that adds a second layer of authentication to help keep accounts secure.

Facebook’s confession follows a story Gizmodo ran a story yesterday, related to research work carried out by academics at two U.S. universities who ran a study in which they say they were able to demonstrate the company uses pieces of personal information that individuals did not explicitly provide it to, nonetheless, target them with ads.

While it’s been — if not clear, then at least evident — for a number of years that Facebook uses contact details of individuals who never personally provided their information for ad targeting purposes (harvesting people’s personal data by other means, such as other users’ mobile phone contact books which the Facebook app uploads), the revelation that numbers provided to Facebook by users in good faith, for the purpose of 2FA, are also, in its view, fair game for ads has not been so explicitly ‘fessed up to before.

Some months ago Facebook did say that users who were getting spammed with Facebook notifications to the number they provided for 2FA was a bug. “The last thing we want is for people to avoid helpful security features because they fear they will receive unrelated notifications,” Facebook then-CSO Alex Stamos wrote in a blog post at the time.

Apparently not thinking to mention the rather pertinent additional side-detail that it’s nonetheless happy to repurpose the same security feature for ad targeting.

Because $$$s, presumably.

We asked Facebook to confirm this is indeed what it’s doing — to make doubly doubly sure. Because, srsly wtaf. And it sent us a statement confirming that it repurposes digits handed to it by people wanting to secure their accounts to target them with marketing.

Here’s the statement, attributed to a Facebook spokesperson: “We use the information people provide to offer a better, more personalized experience on Facebook, including ads. We are clear about how we use the information we collect, including the contact information that people upload or add to their own accounts. You can manage and delete the contact information you’ve uploaded at any time.”

A spokesman also told us that users can opt out of this ad-based repurposing of their security digits by not using phone number based 2FA. (Albeit, the company only added the ability to do non-mobile phone based 2FA back in May, so anyone before then was all outta luck.)

On the ‘shadow profiles’ front — aka Facebook maintaining profiles of non-users based on the data it has been able to scrape about them from users and other data sources — the company has also been less than transparent.

Founder Mark Zuckerberg feigned confusion when questioned about the practice by US lawmakers earlier this year — claiming it only gathers data on non-users for “security purposes”.

Well it seems Facebook is also using the (valid) security concerns of actual users to extend its ability to target individuals with ads — by using numbers provided for 2FA to also carry out ad targeting.

Safe to say criticism of the company has been swift and sharp.

“At this point I consider Facebook a criminal enterprise. Maybe not legally, but morally” https://t.co/BrZ7Yeq5Jw — DHH (@dhh) September 27, 2018

The repurposing by Facebook of phone numbers, provided by users for SMS 2FA, to better target for advertising is utterly disgusting. I’m not easily shocked, but this… https://t.co/qFNgqMDFL1 — Elizatech (@FitzTechLawIE) September 26, 2018

Soon Facebook will also be using behind-the-scenes tech means to target ads at WhatsApp users — despite also providing a robust encrypted security wrapper around their actual messages.

Stamos — now Facebook’s ex-CSO — has also defended its actions on that front.

Related reading…",Social Media,TechCrunch,https://techcrunch.com/2018/09/27/yes-facebook-is-using-your-2fa-phone-number-to-target-you-with-ads/,"Facebook has been using phone numbers provided for two-factor authentication for ad targeting, a practice which has been widely criticized for its unethical and immoral nature. Additionally, Facebook is now planning to use behind-the-scenes tech means to target ads at WhatsApp users.",Security & Privacy
112,"Indonesia restricts WhatsApp, Facebook and Instagram usage following deadly riots – TechCrunch","Indonesia is the latest nation to hit the hammer on social media after the government restricted the use of WhatsApp and Instagram following deadly riots yesterday.

Numerous Indonesia-based users are today reporting difficulties sending multimedia messages via WhatsApp, which is one of the country’s most popular chat apps, and posting content to Facebook, while the hashtag #instagramdown is trending among the country’s Twitter users due to problems accessing the Facebook-owned photo app.

Wiranto, a coordinating minister for political, legal and security affairs, confirmed in a press conference that the government is limiting access to social media and “deactivating certain features” to maintain calm, according to a report from Coconuts.

Rudiantara, the communications minister of Indonesia and a critic of Facebook, explained that users “will experience lag on WhatsApp if you upload videos and photos.”

Facebook — which operates both WhatsApp and Instagram — didn’t explicitly confirm the blockages, but it did say it has been in communication with the Indonesian government.

“We are aware of the ongoing security situation in Jakarta and have been responsive to the Government of Indonesia. We are committed to maintaining all of our services for people who rely on them to communicate with their loved ones and access vital information,” a spokesperson told TechCrunch.

A number of Indonesia-based WhatsApp users confirmed to TechCrunch that they are unable to send photos, videos and voice messages through the service. Those restrictions are lifted when using Wi-Fi or mobile data services through a VPN, the people confirmed.

The restrictions come as Indonesia grapples with political tension following the release of the results of its presidential election on Tuesday. Defeated candidate Prabowo Subianto said he will challenge the result in the constitutional court.

Riots broke out in capital state Jakarta last night, killing at least six people and leaving more than 200 people injured. Following this, it is alleged that misleading information and hoaxes about the nature of riots and people who participated in them began to spread on social media services, according to local media reports.

For Facebook, seeing its services forcefully cut off in a region is no longer a rare incident. The company, which is grappling with the spread of false information in many markets, faced a similar restriction in Sri Lanka in April, when the service was completely banned for days amid terrorist strikes in the nation. India, which just this week concluded its general election, has expressed concerns over Facebook’s inability to contain the spread of false information on WhatsApp, which is its largest chat app with more than 200 million monthly users.

Indonesia’s Rudiantara expressed a similar concern earlier this month.

“Facebook can tell you, ‘We are in compliance with the government.’ I can tell you how much content we requested to be taken down and how much of it they took down. Facebook is the worst,” he told a House of Representatives Commission last week, according to The Jakarta Post.

Update 05/22 02:30 PDT: The original version of this post has been updated to reflect that usage of Facebook in Indonesia has also been impacted.",Social Media,TechCrunch,https://techcrunch.com/2019/05/22/indonesia-restricts-whatsapp-and-instagram/,"Social media platforms have been restricted in Indonesia, causing issues with users sending multimedia messages via WhatsApp, posting content to Facebook, and accessing Instagram. This follows deadly riots and the spread of false information which the government is attempting to prevent.","Information, Discourse & Governance"
113,Big Tech companies cannot be trusted to self-regulate: We need Congress to act – TechCrunch,"It’s been two months since Donald Trump was kicked off of social media following the violent insurrection on Capitol Hill in January. While the constant barrage of hate-fueled commentary and disinformation from the former president has come to a halt, we must stay vigilant.

Now is the time to think about how to prevent Trump, his allies and other bad actors from fomenting extremism in the future. It’s time to figure out how we as a society address the misinformation, conspiracy theories and lies that threaten our democracy by destroying our information infrastructure.

As vice president at Color Of Change, my team and I have had countless meetings with leaders of multi-billion-dollar tech companies like Facebook, Twitter and Google, where we had to consistently flag hateful, racist content and disinformation on their platforms. We’ve also raised demands supported by millions of our members to adequately address these systemic issues — calls that are too often met with a lack of urgency and sense of responsibility to keep users and Black communities safe.

The violent insurrection by white nationalists and far-right extremists in our nation’s capital was absolutely fueled and enabled by tech companies who had years to address hate speech and disinformation that proliferated on their social media platforms. Many social media companies relinquished their platforms to far-right extremists, white supremacists and domestic terrorists long ago, and it will take more than an attempted coup to hold them fully accountable for their complicity in the erosion of our democracy — and to ensure it can’t happen again.

To restore our systems of knowledge-sharing and eliminate white nationalist organizing online, Big Tech must move beyond its typical reactive and shallow approach to addressing the harm they cause to our communities and our democracy. But it’s more clear than ever that the federal government must step in to ensure tech giants act.

After six years leading corporate accountability campaigns and engaging with Big Tech leaders, I can definitively say it’s evident that social media companies do have the power, resources and tools to enforce policies that protect our democracy and our communities. However, leaders at these tech giants have demonstrated time and time again that they will choose not to implement and enforce adequate measures to stem the dangerous misinformation, targeted hate and white nationalist organizing on their platforms if it means sacrificing maximum profit and growth.

And they use their massive PR teams to create an illusion that they’re sufficiently addressing these issues. For example, social media companies like Facebook continue to follow a reactive formula of announcing disparate policy changes in response to whatever public relations disaster they’re fending off at the moment. Before the insurrection, the company’s leaders failed to heed the warnings of advocates like Color Of Change about the dangers of white supremacists, far-right conspiracists and racist militias using their platforms to organize, recruit and incite violence. They did not ban Trump, implement stronger content moderation policies or change algorithms to stop the spread of misinformation-superspreader Facebook groups — as we had been recommending for years.

These threats were apparent long before the attack on Capitol Hill. They were obvious as Color Of Change and our allies propelled the #StopHateForProfit campaign last summer, when over 1,000 advertisers pulled millions in ad revenues from the platform. They were obvious when Facebook finally agreed to conduct a civil rights audit in 2018 after pressure from our organization and our members. They were obvious even before the deadly white nationalist demonstration in Charlottesville in 2017.

Only after significant damage had already been done did social media companies take action and concede to some of our most pressing demands, including the call to ban Trump’s accounts, implement disclaimers on voter fraud claims, and move aggressively remove COVID misinformation as well as posts inciting violence at the polls amid the 2020 election. But even now, these companies continue to shirk full responsibility by, for example, using self-created entities like the Facebook Oversight Board — an illegitimate substitute for adequate policy enforcement — as PR cover while the fate of recent decisions, such as the suspension of Trump’s account, hang in the balance.

Facebook, Twitter, YouTube and many other Big Tech companies kick into action when their profits, self-interests and reputation are threatened, but always after the damage has been done because their business models are built solely around maximizing engagement. The more polarized content is, the more engagement it gets; the more comments it elicits or times it’s shared, the more of our attention they command and can sell to advertisers. Big Tech leaders have demonstrated they neither have the willpower nor the ability to proactively and successfully self-regulate, and that’s why Congress must immediately intervene.

Congress should enact and enforce federal regulations to reign in the outsized power of Big Tech behemoths, and our lawmakers must create policies that translate to real-life changes in our everyday lives — policies that protect Black and other marginalized communities both online and offline.

We need stronger antitrust enforcement laws to break up big tech monopolies that evade corporate accountability and impact Black businesses and workers; comprehensive privacy and algorithmic discrimination legislation to ensure that profits from our data aren’t being used to fuel our exploitation; expanded broadband access to close the digital divide for Black and low-income communities; restored net neutrality so that internet services providers can’t charge differently based on content or equipment; and disinformation and content moderation by making it clear that Section 230 does not exempt platforms from complying with civil rights laws.

We’ve already seen some progress following pressure from activists and advocacy groups including Color Of Change. Last year alone, Big Tech companies like Zoom hired chief diversity experts; Google took action to block the Proud Boys website and online store; and major social media platforms like TikTok adopted better, stronger policies on banning hateful content.

But we’re not going to applaud billion-dollar tech companies for doing what they should and could have already done to address the years of misinformation, hate and violence fueled by social media platforms. We’re not going to wait for the next PR stunt or blanket statement to come out or until Facebook decides whether or not to reinstate Trump’s accounts — and we’re not going to stand idly by until more lives are lost.

The federal government and regulatory powers need to hold Big Tech accountable to their commitments by immediately enacting policy change. Our nation’s leaders have a responsibility to protect us from the harms Big Tech is enabling on our democracy and our communities — to regulate social media platforms and change the dangerous incentives in the digital economy. Without federal intervention, tech companies are on pace to repeat history.",Social Media,TechCrunch,https://techcrunch.com/2021/03/12/big-tech-companies-cannot-be-trusted-to-self-regulate-we-need-congress-to-act/,"Without federal intervention, Big Tech companies are enabling harms to our democracy and communities, such as white nationalist organizing, targeted hate, and misinformation that can lead to violence. These platforms have too often been able to evade corporate accountability, and the only way to restore our systems of knowledge-sharing and eliminate these threats is through federal regulation.","Information, Discourse & Governance"
114,U.K. Social Media Users Get Legal Advice From On High On Avoiding Contempt Of Court – TechCrunch,"We’re all publishers now, and thanks to social media and digital broadcast networks like Twitter our idle musings can reach massive global audiences, hitting far more eyeballs than the graffiti scrawled on the proverbial toilet door ever could.

But the immediacy of social media apparently makes it easy for some users to forget how far their views can travel — causing a small number of them to end up in legal hot water over the things they have posted online. Or, from the establishment perspective, to threaten the judicial process by potentially prejudicing prosecutions.

The U.K. government’s chief legal advisor, Attorney General Dominic Grieve, whose remit includes trying to ensure fair trials can take place, has decided the time has come to provide free legal advice (well, he calls it “advisories“) to Twitterers and Facebookers to help educate them on the responsibilities of using a “tool of mass communication”.

From today, Grieve will be publishing court advisory notes that have previously only been available to mainstream media outlets. The notes will be published on the gov.uk website and via the Twitter feed of the Attorney General’s Office, @AGO_UK (which currently has less than 4,000 Twitter followers).

“I hope we will be able to keep it in simple language and terms,” said Grieve discussing the public advisories in a video interview with The Independent newspaper. “We may well give a slightly difference notice on the Twitter feed to the one we give to the print media, for example, where we sometimes say some matters in confidence. This will be a public document but I think it will have, I hope, an educational value.”

“People will be aware that this is an issue. And can also come to it as a reference point if they start thinking there’s a great deal of comment floating around about a particular case,” he added. “We have now an extraordinary tool of mass communication and it’s a wonderful thing. But I think that children particularly… need to be alerted at school that they have responsibilities.”

As well as trying to ensure social media users don’t trample over the ability of courts to conduct fair trials, Grieve noted the guidelines will aim to help people avoid saying things that might in themselves be a criminal offence.

Just last week, for instance, a man who flouted court directions by posting pictures purporting to be of Jon Venables, who murdered the toddler James Bulger in 1993 when he himself was also a child, was handed a 14-month suspended prison sentence.

Another recent example is Peaches Geldof, daughter of the singer Bob Geldof, who the Independent notes apologised this week for tweeting the names of two mothers whose babies were abused by the Welsh rock singer Ian Watkins.

There is also of course a libel risk attached to using social media to publicly discuss others — back in October Sally Bercow, wife of the House of Commons speaker agreed to pay £15,000 in damages to Lord McAlpine over a libellous tweet, in one example.

“I think there needs to be some education on this, because what you say to a group of two other people and what you say to thousands of people will be judged differently,” Grieve added.

Despite what is a historic move in the U.K. — to expand contempt of court advisories to any and all social media users — Grieve went on to stress that the problem posed to the criminal justice system by Twitterers et al is not a huge one.

“I don’t want to exaggerate the problem. The vast majority of criminal cases in this country — there’s some 18,000 Crown Court trials — take place without any difficulties whatsoever. It’s a very small minority which cause difficulties. Usually, those which are attracting a great deal of public attention — and those are the ones I want to try and tackle. Because if I can reduce it… most people in our experience who have tweeted wrongly, come back and say ‘I had no idea I’d done anything wrong’.

“So I think we just need to alert them, first of all, and try to make them understand why the issue matters.”

Getting the people who ‘tweet first, think later’ to have the prescience of mind to check the Attorney General’s Twitter feed and parse its contempt of court advice before letting off steam about so-and-so’s court case may prove rather challenging. But putting more information into the public domain on how to avoid crossing the legal line will, over time, allow it to be disseminated by the very networks that can cause bother in the first place — giving at least the chance for it to filter into more social media users’ bubbles.

The Attorney General’s move is aimed at updating laws that have lagged well behind mass uptake of social media. But there is also a suggestion in its own data that information being shared via social media is influencing mainstream media to publish more than it might otherwise. The Attorney General said he has issued 10 media advisories so far this year — the most ever issued by the AGO, which issues five on average annually.",Social Media,TechCrunch,https://techcrunch.com/2013/12/04/the-fine-blue-line/,"The use of Social Media can cause legal trouble for users, such as libel risk and contempt of court, as well as potentially prejudicing prosecutions. The UK government is now providing free legal advice to Twitterers and Facebookers in an attempt to help them educate themselves on their responsibilities when using a tool of mass communication.",Equality & Justice
115,WhatsApp adds a tip-line for gathering fakes ahead of India’s elections – TechCrunch,"Update: Proto, the startup working with WhatsApp on this election disinformation initiative (aka Project Checkpoint) has released an FAQ which makes it clear the effort is, first and foremost, a data-gathering exercise.

It writes that the tipline has been set up “only as a means to collect information that is otherwise inaccessible given the nature of private messaging”. So, as we suggested earlier, it’s an attempt to get a snapshot of stuff circulating invisibly on WhatsApp’s end-to-end encrypted platform.

“Checkpoint’s research is using a WhatsApp tipline to crowdsource data that would otherwise not be accessible,” confirms Proto.

Its FAQ also emphasizes that the tipline is not intended to act as a fully fledged fact-checking service, as local reports had suggested earlier.

“The Checkpoint tipline is primarily used to gather data for research, and is not a helpline that will be able to provide a response to every user,” it warns. “The information provided by users helps us understand potential misinformation in a particular message, and when possible, we will send back a message to users. We would like to verify every rumor but we know that will not be possible given the diversity of information we will receive and the limitations of any verification research.”

Any verifications that are provided “will not be instant”, and may take up to 24 hours to receive, it adds.

“Over the next four months, we expect to aggregate these signals at scale, to better understand how misinformation during large events of public interest in India — such as the elections — spreads across languages, regions, even issues. Our dataset, findings and analyses will be compiled into a first of its kind report for India that will be shared with WhatsApp.”

Original story follows below…

Facebook-owned messaging platform WhatsApp has launched a fact-checking tipline for users in India ahead of elections in the country.

The fact-checking service consists of a phone number (+91-9643-000-888) where users can send dubious messages if they think they might not be true or otherwise want them verified.

The messaging giant is working with a local media skilling startup, Proto, to run the fact-checking service — in conjunction with digital strategy consultancy Dig Deeper Media and San Francisco-based Meedan, which builds tools for journalists, to provide the platform for verifying submitted content, per TNW.

We’ve reached out to Proto and WhatsApp with questions.

The Economic Times of India reports that the startup intends to use the submitted messages to build a database to help study misinformation during elections for a research project commissioned and supported by WhatsApp.

“The goal of this project is to study the misinformation phenomenon at scale. As more data flows in, we will be able to identify the most susceptible or affected issues, locations, languages, regions, and more,” said Proto’s co-founders Ritvvij Parrikh and Nasr ul Hadi in a statement quoted by Reuters.

WhatsApp also told the news agency: “The challenge of viral misinformation requires more collaborative efforts and cannot be solved by any one organisation alone.”

According to local press reports, suspicious messages can be shared to the WhatsApp tipline in four regional languages, with the fact-checking service covering videos and pictures, as well as text. The submitter is also to confirm they want a fact-check and, on doing so, will get a subsequent response indicating if the shared message is classified as true, false, misleading, disputed or out of scope.

Other related information may also be provided, the Economic Times reports.

WhatsApp has faced major issues with fakes being spread on its end-to-end encrypted platform — a robust security technology that makes the presence of bogus and/or maliciously misleading content harder to spot and harder to manage since the platform itself does not have access to it.

The spread of fakes has become a huge problem for social media platforms generally. One that’s arguably most acute in markets where literacy (and digital literacy) rates can vary substantially. And in India WhatsApp fakes have led to some truly tragic outcomes — with multiple reports in recent years detailing how fast-spreading digital rumors sparked or fueled mob violence that’s led to death and injury.

India’s general election, which is due to take place in several phases starting later this month until mid next, presents a more clearly defined threat — with the risk of a democratic process and outcome being manipulated by weaponized political disinformation.

WhatsApp’s platform is squarely in the frame, given the app’s popularity in India.

It has also been accused of fueling damaging political fakes during elections in Brazil last year, with Reuters reporting that the platform was flooded with falsehoods and conspiracy theories.

An outsized presence on social media appears to have aided the election of right-winger Jair Bolsonaro. While the left-wing candidate he beat in a presidential run-off later claimed businessmen backing Bolsonaro paid to flood WhatsApp with misleading propaganda.

In India, local press reports that politicians across the spectrum are being accused of seeking to manipulate the forthcoming elections by seeding fakes on the popular encrypted messaging platform.

It’s clear that WhatsApp offers a conduit for spreading unregulated and unaccountable propaganda at scale with even limited resources. So whether a tipline can offer a robust check against weaponized political disinformation very much remains to be seen.

There certainly look to be limitations to this approach. Though it could also be developed and enhanced — such as if it gets more fully baked into the platform.

For now it looks like WhatsApp is testing the water and trying to gather more data to shape a more robust response.

The most obvious issue with the tipline is it requires a message recipient to request a check — an active step that means the person must know about the fact-check service, have the number available in their contacts and trust the judgment of those running it.

Many WhatsApp users will fall outside those opt-in bounds.

It also doesn’t take much effort to imagine purveyors of malicious rumors spreading fresh fakes claiming the fact-checks/checkers are biased or manipulated to try to turn WhatsApp users against it.

This is likely why local grassroots political organizations are also being encouraged to submit any rumors they see circulating across the different regions during the election period. And why WhatsApp is talking about the need for collective action to combat the disinformation problem.

It will certainly need engagement across the political spectrum to counter any bias charges and plug gaps resulting from limited participation by WhatsApp users themselves.

How information on debunked fakes can be credibly and widely fed back to Indian voters in a way that broadly reaches the electorate is what’s really key though.

There’s no suggestion, here and now, that’s going to happen via WhatsApp itself — only those who request a check are set to get a response.

Although that could change in the future. But, equally, the company may be wary of being seen to accept a role in centralized distribution of (even fake) political propaganda. That way more accusations of bias likely lie.

In recent years Facebook has taken out adverts in traditional India media to warn about fakes. It has also experimented with other tactics to try to combat damaging WhatsApp rumors — such as using actors to role-play fakes in public to warn against false messages.

So the company looks to be hoping to develop a multi-stakeholder, multi-format information network off of its own platform to help get the message out about fakes spreading on WhatsApp.

Albeit, that’s clearly going to take time and effort. It’s also still not clear whether it will be effective versus an app that’s always on hand and capable of feeding in fresh fakes.

The tipline, inevitably, looks slow and painstaking beside the wildfire spread of digital fakes. And it’s not clear how much of a check on spread and amplification it can offer in this form.

Certainly initially — given the fact-checking process itself necessarily takes time.

While a startup, even one that’s being actively supported by WhatsApp — Proto confirmed to us that WhatsApp has provided funding to support the technology behind the tipline — is unlikely to have the resources to speedily fact-check the volume of fakes that will be distributed across such a large market, fueled by election interests.

Yet timely intervention is critical to prevent fakes going viral.

So, again, this initiative looks unlikely to stop the majority of bogus WhatsApp messages from being swallowed and shared. But the data set derived from the research project that underpins the tipline may help the company fashion a more responsive and proactive approach to contextualizing and debunking malicious rumors in the future.

Proto says it plans to submit its learnings to the International Center for Journalists to help other organizations learn from its efforts.

The Economic Times also quotes Fergus Bell, founder and CEO of Dig Deeper Media, suggesting the research will help create “global benchmarks” for those wishing to tackle misinformation in their own markets.

In the meantime, though, the votes go on.

This report was updated with comment and additional information from Proto",Social Media,TechCrunch,https://techcrunch.com/2019/04/02/whatsapp-adds-a-tip-line-for-checking-fakes-in-india-ahead-of-elections/,"WhatsApp's end-to-end encryption makes it difficult to spot malicious and false content on the platform, leading to tragic outcomes and potentially manipulating the democratic process in India ahead of the upcoming elections. To combat this, WhatsApp has launched a fact-checking tipline, however it is uncertain how effective this will be in combating the","Information, Discourse & Governance"
116,Now eight parliaments are demanding Zuckerberg answers for Facebook scandals – TechCrunch,"Facebook’s founder is facing pressure to accept an invite from eight international parliaments, with lawmakers wanting to question him about negative impacts his social network is having on democratic processes globally.

Last week Facebook declined an invitation from five of these parliaments.

The elected representatives of Facebook users want Mark Zuckerberg to answer questions in the wake of a string of data misuse and security scandals attached to his platform. The international parliaments have joined forces — forming a grand committee — to amp up the pressure on Facebook.

The U.K.-led grand committee said it would meet later this month, representing the interests of some 170 million Facebook users across Argentina, Australia, Canada, Ireland and the U.K. But Facebook snubbed that invite.

Today the request has been reissued with an additional three parliaments on board — Brazil, Latvia and Singapore.

In their latest invite letter they also make it clear that Facebook’s founder does not have to attend the hearing in person — which was the excuse the company used to decline the last request for Zuckerberg. (Which was just the latest in a long string of ‘nos’ Facebook’s founder has given the committee.)

“We note that while your letter states that you are ‘not able to be in London’ on 27th, it does not rule out giving evidence per se. Would you be amenable to giving evidence via video link instead?” the grand committee writes now.

We’ve asked Facebook whether Zuckerberg will be able to make time in his schedule to provide evidence remotely — and will update this report with any response. (A company spokesman suggested to us that it’s unlikely to do so.)

Of course Zuckerberg is very busy these days — given the fresh scandals slamming Facebook’s exec team. His political plate is truly heaped.

Last week a New York Times report painted an ugly and chaotic picture of Facebook’s leaders’ response to the political disinformation crisis — which included engaging an external public relations firm which used smear tactics against opponents. (Facebook has since severed ties with the firm.)

The grand committee references this controversy in its latest invitation letter, writing: “We believe that there are important issues to be discussed, and that you are the appropriate person to answer them. Yesterday’s New York Times article raises further questions about how recent data breaches were allegedly dealt with within Facebook.”

The U.K.’s DCMS committee, which has been spearheading efforts to hold Zuckerberg to account, has spent the best part of this year asking wide-ranging questions about the impact of online disinformation on democratic processes. But it has become increasingly damning in its criticism of Facebook — accusing the company of evasion, equivocation and worse as the months have gone on.

In a preliminary report this summer it also called on the government to act urgently, recommending a levy on social media and stronger laws to prevent social media tools being used to undermine democratic processes.

The U.K. government chose not to leap into action. But even there Facebook’s platform is implicated because Brexit — which was itself sold to voters via the medium of unregulated social media ads (with the Electoral Commission finding earlier this year that the official Vote Leave campaign used Facebook’s funnel to bypass electoral law) — is rather monopolizing ministerial attention these days…

One of the questions committee members are keen to get an answer to from Facebook is who at the company knew in the earliest incidence about the Cambridge Analytica data misuse scandal. In short they want to know where the buck stops. Who should be held accountable — for both the massive data breach and Facebook’s internal handling of it.

And it is very close to getting an answer to that after the U.K.’s data protection watchdog, the ICO, gave evidence earlier this month — saying it had obtained the distribution list for emails Facebook sent internally about the breach, saying it would pass the list on to the committee.

A spokeswoman for the DCMS committee told us it has yet to receive this information from the ICO.

An ICO spokesperson told us it will not be publishing the list — adding: “At this stage I’m not sure when it will be sent to the committee.”",Social Media,TechCrunch,https://techcrunch.com/2018/11/19/now-eight-parliaments-are-demanding-zuckerberg-answers-for-facebook-scandals/,"The international parliaments have joined forces to question Facebook's founder, Mark Zuckerberg, about the negative impacts his platform is having on democratic processes globally, such as the misuse of data and the spread of political disinformation.","Information, Discourse & Governance"
117,"Despite bans, Giphy still hosts self-harm, hate speech and child sex abuse content – TechCrunch","Despite bans, Giphy still hosts self-harm, hate speech and child sex abuse content Certain hashtags and keywords revealed the illegal and banned content

Image search engine Giphy bills itself as providing a “fun and safe way” to search and create animated GIFs. But despite its ban on illicit content, the site is littered with self-harm and child sex abuse imagery, TechCrunch has learned.

A new report from Israeli online child protection startup L1ght — previously AntiToxin Technologies — has uncovered a host of toxic content hiding within the popular GIF-sharing community, including illegal child abuse content, depictions of rape and other toxic imagery associated with topics like white supremacy and hate speech. The report, shared exclusively with TechCrunch, also showed content encouraging viewers into unhealthy weight loss and glamorizing eating disorders.

TechCrunch verified some of the company’s findings by searching the site using certain keywords. (We did not search for terms that may have returned child sex abuse content, as doing so would be illegal.) Although Giphy blocks many hashtags and search terms from returning results, search engines like Google and Bing still cache images with certain keywords.

When we tested using several words associated with illicit content, Giphy sometimes showed content from its own results. When it didn’t return any banned materials, search engines often returned a stream of would-be banned results.

L1ght develops advanced solutions to combat online toxicity. Through its tests, one search of illicit material returned 195 pictures on the first search page alone. L1ght’s team then followed tags from one item to the next, uncovering networks of illegal or toxic content along the way. The tags themselves were often innocuous in order to help users escape detection, but they served as a gateway to the toxic material.

Many of the more extreme content — including images of child sex abuse — are said to have been tagged using keywords associated with known child exploitation sites.

We are not publishing the hashtags, search terms or sites used to access the content, but we passed on the information to the National Center for Missing and Exploited Children, a national nonprofit established by Congress to fight child exploitation.

Simon Gibson, Giphy’s head of audience, told TechCrunch that content safety was of the “utmost importance” to the company and that it employs “extensive moderation protocols.” He said that when illegal content is identified, the company works with the authorities to report and remove it.

He also expressed frustration that L1ght had not contacted Giphy with the allegations first. L1ght said that Giphy is already aware of its content moderation problems.

Gibson said Giphy’s moderation system “leverages a combination of imaging technologies and human validation,” which involves users having to “apply for verification in order for their content to appear in our searchable index.” Content is “then reviewed by a crowdsourced group of human moderators,” he said. “If a consensus for rating among moderators is not met, or if there is low confidence in the moderator’s decision, the content is escalated to Giphy’s internal trust and safety team for additional review,” he said.

“Giphy also conducts proactive keyword searches, within and outside of our search index, in order to find and remove content that is against our policies,” said Gibson.

L1ght researchers used their proprietary artificial intelligence engine to uncover illegal and other offensive content. Using that platform, the researchers can find other related content, allowing them to find vast caches of illegal or banned content that would otherwise and for the most part go unseen.

This sort of toxic content plagues online platforms, but algorithms only play a part. More tech companies are finding human moderation is critical to keeping their sites clean. But much of the focus to date has been on the larger players in the space, like Facebook, Instagram, YouTube and Twitter.

Facebook, for example, has been routinely criticized for outsourcing moderation to teams of lowly paid contractors who often struggle to cope with the sorts of things they have to watch, even experiencing post-traumatic stress-like symptoms as a result of their work. Meanwhile, Google’s YouTube this year was found to have become a haven for online sex abuse rings, where criminals had used the comments section to guide one another to other videos to watch while making predatory remarks.

Giphy and other smaller platforms have largely stayed out of the limelight, during the past several years. But L1ght’s new findings indicate that no platform is immune to these sorts of problems.

L1ght says the Giphy users sharing this sort of content would make their accounts private so they wouldn’t be easily searchable by outsiders or the company itself. But even in the case of private accounts, the abusive content was being indexed by some search engines, like Google, Bing and Yandex, which made it easy to find. The firm also discovered that pedophiles were using Giphy as the means of spreading their materials online, including communicating with each other and exchanging materials. And they weren’t just using Giphy’s tagging system to communicate — they were also using more advanced techniques like tags placed on images through text overlays.

This same process was utilized in other communities, including those associated with white supremacy, bullying, child abuse and more.

This isn’t the first time Giphy has faced criticism for content on its site. Last year a report by The Verge described the company’s struggles to fend off illegal and banned content. Last year the company was booted from Instagram for letting through racist content.

Giphy is far from alone, but it is the latest example of companies not getting it right. Earlier this year and following a tip, TechCrunch commissioned then-AntiToxin to investigate the child sex abuse imagery problem on Microsoft’s search engine Bing. Under close supervision by the Israeli authorities, the company found dozens of illegal images in the results from searching certain keywords. When The New York Times followed up on TechCrunch’s report last week, its reporters found Bing had done little in the months that had passed to prevent child sex abuse content appearing in its search results.

It was a damning rebuke on the company’s efforts to combat child abuse in its search results, despite pioneering its PhotoDNA photo detection tool, which the software giant built a decade ago to identify illegal images based off a huge database of hashes of known child abuse content.

Giphy’s Gibson said the company was “recently approved” to use Microsoft’s PhotoDNA but did not say if it was currently in use.

Where some of the richest, largest and most-resourced tech companies are failing to preemptively limit their platforms’ exposure to illegal content, startups are filling in the content moderation gaps.

L1ght, which has a commercial interest in this space, was founded a year ago to help combat online predators, bullying, hate speech, scams and more.

The company was started by former Amobee chief executive Zohar Levkovitz and cybersecurity expert Ron Porat, previously the founder of ad-blocker Shine, after Porat’s own son experienced online abuse in the online game Minecraft. The company realized the problem with these platforms was something that had outgrown users’ own ability to protect themselves, and that technology needed to come to their aid.

L1ght’s business involves deploying its technology in similar ways as it has done here with Giphy — in order to identify, analyze and predict online toxicity with near real-time accuracy.",Social Media,TechCrunch,https://techcrunch.com/2019/11/15/giphy-illegal-content/,"Despite efforts by companies to combat it, social media platforms are plagued by toxic content including self-harm, hate speech, child sex abuse, and other illegal material. Startups are now helping to fill in the content moderation gaps, but more needs to be done to protect users from this harmful content.",Equality & Justice
118,Russia’s social media meddling could spell the end of online anonymity,"This week, representatives from Google, Facebook, and Twitter are appearing before House and Senate subcommittees to answer for their role in Russian manipulation during the 2016 election, and so far, the questioning has been brutal. Facebook has taken the bulk of the heat, being publicly called out by members of Congress for missing a wave of Russian activity until months after the election.

But one of the most interesting parts of yesterday’s proceedings actually came after the big companies had left the room, and a national security researcher named Clint Watts took the floor. Watts is one of the most respected figures in the nascent field of social media manipulation — and when it came time to diagnose root of Russia’s platform meddling, he put much of the blame on the decision to allow anonymous accounts. As long as Russian operatives can get on Twitter and Facebook without identifying themselves, Watts diagnosed, foreign actors will be able to quietly influence our politics:

“With features like account anonymity, unlimited audience access, low cost technology tools, plausible deniability – social media provides Russia an unprecedented opportunity to execute their dark arts of manipulation and subversion…Today, anonymous sites rife with conspiracy theories, such as 4Chan and Reddit, offer unlimited options for placement of digital forgeries that drive Kremlin narratives. Graphics, memes & documents litter these discussion boards providing ammunition for Kremlin narratives and kompromat. Anonymous posts of the Kremlin’s design or those generated by the target audiences power smear campaigns and falsehoods that tarnish confidence in America and trust in democratic institutions.”

The point is clear enough: if you’re fighting Russian interference on social media, anonymity is a big problem. In some ways, it’s the original sin, creating space for that first lie that lets trolls enter the conversation unnoticed. “Account anonymity in public provides some benefits to society, but social media companies must work immediately to confirm real humans operate accounts,” Watts told the committee. “The negative effects of social bots far outweigh any benefits.” It’s a common insight among bot-hunters, and one that’s become particularly popular amid this week’s hearings.

Thomas Rid expressed a similar idea this morning, making the case that Twitter had been more useful for Russian active measures than Facebook. “[Twitter’s] openness,” Rid writes, “particularly the openness for deletion, anonymity, and automation, has made the platform easy to exploit.” The Digital Forensics Research Lab, a longtime hub for bot-watchers, opened the hearing with four questions for social media companies, including a tricky one: What is the limit of anonymity on social media?

In each case, the writers stop short of asking for an outright ban on anonymous accounts. But measures like Facebook’s real name policy have been cast as useful tools in the fight against Russian influence — and often tools in need of stricter enforcement. The online pseudonym was once a guiding light of internet culture, a crucial protection for whistleblowers and communities with a legitimate fear of being exposed. Now, it’s increasingly seen as a threat. Worse, it seems more and more likely that platforms will respond to Russia concerns by tightening restrictions on online anonymity, and driving webgoers to live more and more of their online life under legal names.

Taking on the issue in the earlier panel, Facebook general counsel Colin Stretch cast the entire problem as one of identity. “It wasn’t so much the content,” Stretch told the committee. “The real problem with what we saw was its lack of authenticity.” It’s a useful line for Facebook, particularly as the criticism broadens from paid election ads (the target of the only recent bill to tackle the issue) to Russian posting in general. Reining in organic posts is much trickier than reining in ad spending, and it’s hard to imagine doing it without tighter identity controls.

“The real problem with what we saw was its lack of authenticity.”

Even though the idea has come to prominence on a wave of anger towards Facebook, Zuckerberg would probably suffer the least damage from a crackdown on anonymity. Facebook already has a real name policy, and they could easily tighten enforcement to include ID checks without altering their core product. There are plenty of services like NextDoor with stricter identity policies, and they don’t pose any significant technical problems. The problem is social. We’re used to anonymity on the internet, particularly on the services where it’s still available. It’s hard to know what an anonymity backlash would mean for services like Twitter, Reddit, and 4chan — all of which are named in Watts’ testimony as playing a role in Russian disinformation.

In the background, there’s an even harder question: is anonymity still worth saving? It’s foundational to many people’s idea of the internet, but amid widespread online harassment and Facebook itself, it’s come to mean less and less. Even without Russian influence campaigns, the web’s online spaces are largely associated with the ugliest parts of humanity. (4chan is a prime example.) With new pressure from Congress, bot analysts, and the public, online anonymity may not have any defenders left. In the face of that, Twitter, Reddit, and others might decide a real name policy is a small price to pay for forestalling federal regulation.

Maybe all that sounds like a straw man. I hope it is. The most likely path forward is still that Congress does nothing — or, failing that, sticks to the FEC regulations laid out in Warner-Klobuchar — and Russian disinformation continues to be a hazard of digital life. It’s a hard problem, and at this point, it’s reasonable to not trust Congress or Facebook management to solve it. Still, it’s worth considering what we want platforms to do about fake posts. Do we want stronger identity checks before a person is allowed to post online? Do we want an algorithm sniffing out activity that looks like an influence campaign, with all the inevitable false positives such a system would bring? Do we want intelligence services to actively collaborate with Facebook in sniffing out those campaigns? All those ideas make me nervous, but they don’t seem as implausible as they would have a year ago, or even a week ago.",Social Media,Verge,https://www.theverge.com/2017/11/1/16592374/russia-facebook-ads-clint-watts-social-media-anonymity-privacy,"The consequences of social media discussed here include the ability of anonymous users to spread disinformation, making it difficult to fight Russian interference. This has led to calls for tighter identity controls on platforms, which could have a negative impact on free expression online.",Security & Privacy
119,Facebook's Future Rests on Knowing You Even Better,"For the past few years, Facebook’s quarterly earnings calls have been something of a victory lap. Even at its massive scale---$40 billion in annual revenue and more than half of the world’s internet users, the company manages to grow consistently each quarter, even beating analyst expectations. Wall Street has rewarded the company with a stock price that can only go up, increasing 560 percent in five years and placing Facebook among the most valuable companies in the country.

But growing outrage over the social network’s role in Russian election interference, filter bubbles, and the spread of divisive disinformation has led the company to announce sweeping changes in the kinds of content it will show users. Facebook executives have acknowledged the changes may affect how much time people spend on their products, which would mean fewer opportunities to show them ads. Ad impressions, or the number of ads shown to people, only grew by 4 percent in the fourth quarter, and Facebook has warned not to expect much growth in “ad load,” or the number of ads it can show each person.

Less time spent on Facebook could deal a huge blow to Facebook’s once-ironclad business. But the company has a plan to counteract that: It is raising its prices. A lot.

Even as Facebook reported that users collectively spent 50 million fewer hours a day on the network in the fourth quarter, revenue during that period increased 47 percent to $13 billion. Facebook pulled this off by boosting the average price per ad by 43 percent, CFO David Wehner said on the company’s analyst call Thursday.

How does Facebook get away with jacking up its prices so dramatically? For one, it operates in an unregulated duopoly alongside Google. Together they control more than 60% of the digital-advertising market. But second, Facebook’s ability to target its users with highly tailored (and, in theory, highly effective) ads means marketers are willing to pay more.

On the analyst call, Wehner touted the company’s progress in delivering better targeting, better types of ads, and “driving better conversion,” as the reasons advertisers are willing to spend more on the platform. If Facebook can keep improving its targeting, it “will translate into higher effective prices for our business,” he said. COO Sheryl Sandberg frequently names targeting as the company’s big opportunity. “I think if you think about where the growth remains, it really is in increasing the relevance of the ads,” she told analysts in November.

That’s a lot of pressure for Facebook’s ads to be effective. Fortunately for investors, the company is well-positioned to do that. It has been collecting terabytes of data on all of our likes, friends, habits, and messages for over a decade. Facebook is so ingrained in our lives that it has the ability to manipulate our moods. Apply that data to advertising, and the company is so good at targeting us that it is subject to a persistent conspiracy theory that the Facebook app is somehow secretly listening to our conversations. Many people are staunchly convinced of it, no matter how many times the company denies it or how many news outlets (including WIRED) debunk it.",Social Media,WIRED,https://www.wired.com/story/facebooks-future-rests-on-knowing-you-even-better/,"Facebook's unchecked growth and targeting abilities has led to a duopoly in the digital advertising market and caused concern over the spread of divisive disinformation, filter bubbles, and even Russian election interference. As a result, their prices have drastically increased, putting pressure on the platform to prove its ads are effective.",Security & Privacy
120,Europe to push for one-hour takedown law for terrorist content – TechCrunch,"The European Union’s executive body is doubling down on its push for platforms to pre-filter the Internet, publishing a proposal today for all websites to monitor uploads in order to be able to quickly remove terrorist uploads.

The Commission handed platforms an informal one-hour rule for removing terrorist content back in March. It’s now proposing turning that into a law to prevent such content spreading its violent propaganda over the Internet.

For now the ‘rule of thumb’ regime continues to apply. But it’s putting meat on the bones of its thinking, fleshing out a more expansive proposal for a regulation aimed at “preventing the dissemination of terrorist content online”.

As per usual EU processes, the Commission’s proposal would need to gain the backing of Member States and the EU parliament before it could be cemented into law.

One major point to note here is that existing EU law does not allow Member States to impose a general obligation on hosting service providers to monitor the information that users transmit or store. But in the proposal the Commission argues that, given the “grave risks associated with the dissemination of terrorist content”, states could be allowed to “exceptionally derogate from this principle under an EU framework”.

So it’s essentially suggesting that Europeans’ fundamental rights might not, in fact, be so fundamental. (Albeit, European judges might well take a different view — and it’s very likely the proposals could face legal challenges should they be cast into law.)

What is being suggested would also apply to any hosting service provider that offers services in the EU — “regardless of their place of establishment or their size”. So, seemingly, not just large platforms, like Facebook or YouTube, but — for example — anyone hosting a blog that includes a free-to-post comment section.

Websites that fail to promptly take down terrorist content would face fines — with the level of penalties being determined by EU Member States (Germany has already legislated to enforce social media hate speech takedowns within 24 hours, setting the maximum fine at €50M).

Although, in a section on penalties, the Commission suggests systematic failure to comply should be subject to financial penalties of up to 4% of the hosting service provider’s global turnover for their last business year. (So for the big Internet platforms that would constitute a major deterrent — akin to what’s recently been baked into the EU’s privacy laws.)

“Penalties are necessary to ensure the effective implementation by hosting service providers of the obligations pursuant to this Regulation,” the Commission writes, envisaging the most severe penalties being reserved for systematic failures to remove terrorist material within one hour.

It adds: “When determining whether or not financial penalties should be imposed, due account should be taken of the financial resources of the provider.” So — for example — individuals with websites who fail to moderate their comment section fast enough might not be served the very largest fines, presumably.

The proposal also encourages platforms to develop “automated detection tools” so they can take what it terms “proactive measures proportionate to the level of risk and to remove terrorist material from their services”.

So the Commission’s continued push for Internet pre-filtering is clear. (This is also a feature of the its copyright reform — which is being voted on by MEPs later today.)

Albeit, it’s not alone on that front. Earlier this year the UK government went so far as to pay an AI company to develop a terrorist propaganda detection tool that used machine learning algorithms trained to automatically detect propaganda produced by the Islamic State terror group — with a claimed “extremely high degree of accuracy”. (At the time it said it had not ruled out forcing tech giants to use it.)

What is terrorist content for the purposes of this proposals? The Commission refers to an earlier EU directive on combating terrorism — which defines the material as “information which is used to incite and glorify the commission of terrorist offences, encouraging the contribution to and providing instructions for committing terrorist offences as well as promoting participation in terrorist groups”.

And on that front you do have to wonder whether, for example, some of U.S. president Donald Trump’s comments last year after the far right rally in Charlottesville where a counter protestor was murdered by a white supremacist — in which he suggested there were “fine people” among those same murderous and violent white supremacists might not fall under that ‘glorifying the commission of terrorist offences’ umbrella, should, say, someone repost them to a comment section that was viewable in the EU…

Safe to say, even terrorist propaganda can be subjective. And the proposed regime will inevitably encourage borderline content to be taken down — having a knock-on impact upon online freedom of expression.

The Commission also wants websites and platforms to share information with law enforcement and other relevant authorities and with each other — suggesting the use of “standardised templates”, “response forms” and “authenticated submission channels” to facilitate “cooperation and the exchange of information”.

It tackles the problem of what it refers to as “erroneous removal” — i.e. content that’s removed after being reported or erroneously identified as terrorist propaganda but which is subsequently, under requested review, determined not to be — by placing an obligation on providers to have “remedies and complaint mechanisms to ensure that users can challenge the removal of their content”.

So platforms and websites will be obligated to police and judge speech — which they already do, of course but the proposal doubles down on turning online content hosters into judges and arbiters of that same content.

The regulation also includes transparency obligations on the steps being taken against terrorist content by hosting service providers — which the Commission claims will ensure “accountability towards users, citizens and public authorities”.

Other perspectives are of course available…

There is no way a hosting provider (including your private website, if it includes comments section) can comply with these obligations without #UploadFilters. It’s not limited to large platforms. The @EU_Commission has ignored 100% of the #copyright discussion. #SaveYourInternet pic.twitter.com/WeV0GwDZVD — Julia Reda (@Senficon) September 12, 2018

The Commission envisages all taken down content being retained by the host for a period of six months so that it could be reinstated if required, i.e. after a valid complaint — to ensure what it couches as “the effectiveness of complaint and review procedures in view of protecting freedom of expression and information”.

It also sees the retention of takedowns helping law enforcement — meaning platforms and websites will continue to be co-opted into state law enforcement and intelligence regimes, getting further saddled with the burden and cost of having to safely store and protect all this sensitive data.

(On that the EC just says: “Hosting service providers need to put in place technical and organisational safeguards to ensure the data is not used for other purposes.”)

The Commission would also create a system for monitoring the monitoring it’s proposing platforms and websites undertake — thereby further extending the proposed bureaucracy, saying it would establish a “detailed programme for monitoring the outputs, results and impacts” within one year of the regulation being applied; and report on the implementation and the transparency elements within two years; evaluating the entire functioning of it four years after it’s coming into force.

The executive body says it consulted widely ahead of forming the proposals — including running an open public consultation, carrying out a survey of 33,500 EU residents, and talking to Member States’ authorities and hosting service providers.

“By and large, most stakeholders expressed that terrorist content online is a serious societal problem affecting internet users and business models of hosting service providers,” the Commission writes. “More generally, 65% of respondent to the Eurobarometer survey considered that the internet is not safe for its users and 90% of the respondents consider it important to limit the spread of illegal content online.

“Consultations with Member States revealed that while voluntary arrangements are producing results, many see the need for binding obligations on terrorist content, a sentiment echoed in the European Council Conclusions of June 2018. While overall, the hosting service providers were in favour of the continuation of voluntary measures, they noted the potential negative effects of emerging legal fragmentation in the Union.

“Many stakeholders also noted the need to ensure that any regulatory measures for removal of content, particularly proactive measures and strict timeframes, should be balanced with safeguards for fundamental rights, notably freedom of speech. Stakeholders noted a number of necessary measures relating to transparency, accountability as well as the need for human review in deploying automated tools.”",Social Media,TechCrunch,https://techcrunch.com/2018/09/12/europe-to-push-for-one-hour-takedown-law-for-terrorist-content/,"The proposed regulations from the European Commission would require hosting services to monitor and pre-filter the internet, which could have a negative impact on freedom of speech and expression.",Security & Privacy
121,Facebook named in suit alleging job ads on its platform unlawfully discriminated against women – TechCrunch,"Facebook’s ad platform is facing charges that it has enabled gender-based discrimination against millions of women in a class action suit filed on behalf of three female workers and backed by the American Civil Liberties Union (ACLU).

The legal action also names ten employers who are alleged to have used the social media giant’s platform to exclusively and unlawfully target job adverts at male Facebook users, thereby excluding women and non-binary users from receiving the ads.

The ACLU, law firm Outten & Golden LLP, and the Communications Workers of America have filed charges with the Equal Employment Opportunity Commission.

BREAKING: We've filed charges against @Facebook and 10 employers for using the platform to target their job ads — for positions in male-dominated fields — only to younger men. Facebook is violating federal civil rights law. Period. — ACLU (@ACLU) September 18, 2018

The 10 employers and employment agency advertisers named in the suit, which the charges allege ran discriminatory jobs in “mostly” male-dominated fields, include a police department, multiple retailers, a software development firm and various installation, repair and remodelling companies. (All ten named in the suit are listed in the ACLU’s press release.)

“I’ve heard stories about when people looked for jobs in the classified ads and big bold letters read ‘help wanted-male’ or ‘help wanted-female.’ I was shocked to find that this discrimination is still happening, just online instead of in newspapers,” said Bobbi Spees, a job-seeker and lead complainant in the case, commenting in a statement. “I shouldn’t be shut out of the chance to hear about a job opportunity just because I am a woman.”

“The internet did not erase our civil rights laws. It violates the law if an employer uses Facebook to deny job ads to women,” added Peter Romer-Friedman, an attorney at Outten & Golden, in another supporting statement. “The last time I checked, you don’t have to be a man to be a truck driver or a police officer. But Facebook and employers are acting like it’s the 1950s, before federal employment law banned sex discrimination.”

The charges allege that Facebook, via its platform, delivers job ads selectively based on age and sex categories that employers expressly choose, and that it earns revenue from placing job ads that exclude women and older workers from receiving the ads.

The ACLU notes that targeting job ads by sex is unlawful under federal, state, and local civil rights laws, including Title VII of the Civil Rights Act of 1964.

“Sex segregated job advertising has historically been used to shut women out of well-paying jobs and economic opportunities,” said Galen Sherwin, senior staff attorney at the ACLU Women’s Rights Project, in another supporting statement. “We can’t let gender-based ad targeting online give new life to a form of discrimination that should have been eradicated long ago.”

While online platforms are not as heavily regulated as publishing platforms the lawsuit argues that Facebook can be held legally responsible for:

creating and operating the system that allows and encourages employers to select the gender and age of the people who get their job ads, including providing employers with data on users’ gender and age for targeting purposes; delivering the gender- and age-based ads based on employers’ preferences; and acting as a recruiter connecting employers with prospective employees

We’ve reached out to Facebook for comment on the lawsuit. Update: A Facebook spokesperson told us: “There is no place for discrimination on Facebook; it’s strictly prohibited in our policies, and over the past year, we’ve strengthened our systems to further protect against misuse. We are reviewing the complaint and look forward to defending our practices.”

The company also told us that it will soon require all advertisers to consent that they will comply with its anti-discrimination policies and the law — in addition to prompts added last year when advertisers are creating campaigns that Facebook’s system identifies as offering housing, employment or credit ads.

This summer Facebook also announced it was whittling down the targeting categories advertisers can use — shaving more than 5,000 targeting options to “help prevent misuse”.

It’s by no means the first time the company has faced civil rights complaints related to its ad platform.

Back in 2016 ProPublica exposed how Facebook’s ad tools could be used to exclude users based on their “ethnic affinity” — including in protected categories such as housing, employment and credit opportunities which prohibit discriminatory advertising.

The company responded by saying it would build tools to prevent advertisers from applying ethnic affinity targeting in the protected categories. And also by rewording its ad policies to more clearly prohibit discrimination.

But the following year another ProPublica investigation showed it was still failing to block discriminatory ads — leaving Facebook to apologize for failing to effectively enforce its own policies (hmmm, now where else have we heard the company accused of that… ), and saying: “Our systems continue to improve but we can do better.”

Last year the company was also shown to have allowed ads that included hateful sentiments targeted at Jewish people.

Around about the same time that Facebook was facing renewed criticism over ethnic affinity targeting on its platform being used as a tool for racial discrimination, the company said it would also take a look at how advertisers are using exclusion targeting across other “sensitive segments” — such as those relating to members of the LGBTQ community and people with disabilities.

It’s not clear whether Facebook included gender-based discrimination in those 2017 self reviews too. (We’ve asked and will update this post with any response.)

Either way, it appears Facebook has failed to pick up on the potential for gender-based discrimination to be carried out via its ad platform.

And given all the attention its ad tools have attracted lately as a vector for discrimination and other types of abuse that looks careless to say the least.

Facebook’s ad platform has faced additional criticism in Europe for sensitive inferences it makes about users — given the platform allows advertisers to target people based on political and religious interests, meaning Facebook’s platform is quietly making sensitive inferences about individuals.

Privacy experts argue this modus operandi entails Facebook processing the sensitive personal data of individuals without explicitly asking people for their upfront consent (as would be required under EU law when you’re processing sensitive personal data such as political or religious affiliation).

An opinion on a person is still personal data of that person, they contend.

Facebook disagrees, disputing that the inferences its ad platform makes about users (based off of its tracking and data-mining of people) constitutes personal data. But it’s yet another bone of legal contention now being lobbed at the company.",Social Media,TechCrunch,https://techcrunch.com/2018/09/18/facebook-named-in-suit-alleging-job-ads-on-its-platform-unlawfully-discriminated-against-women/,"Facebook is being sued by the ACLU, on behalf of three female workers, for allegedly enabling gender-based discrimination via its ad platform. This follows other civil rights complaints related to ethnic affinity targeting and other sensitive segments, raising questions about the social media giant's enforcement of its policies.",Equality & Justice
122,"2018 really was more of a dumpster fire for online hate and harassment, ADL study finds – TechCrunch","Around 37 percent of Americans were subjected to severe hate and harassment online in 2018, according to a new study by the Anti-Defamation League, up from about 18 percent in 2017. And more than half of all Americans experienced some form of harassment, according to the ADL study.

Facebook users bore the brunt of online harassment on social networking sites according to the ADL study, with around 56 percent of survey respondents indicating that at least some of their harassment occurred on the platform — unsurprising, given Facebook’s status as the dominant social media platform in the U.S.

Around 19 percent of people said they experienced severe harassment on Twitter (only 19 percent? That seems low), while 17 percent reported harassment on YouTube, 16 percent on Instagram and 13 percent on WhatsApp.

In all, the blue-ribbon standards for odiousness went to Twitch, Reddit, Facebook and Discord, when the ADL confined their surveys to daily active users. nearly half of all daily users on Twitch have experienced harassment, the report indicated. Around 38 percent of Reddit users, 37 percent of daily Facebook users and 36 percent of daily Discord users reported being harassed.

“It’s deeply disturbing to see how prevalent online hate is, and how it affects so many Americans,” said ADL chief executive Jonathan A. Greenblatt. “Cyberhate is not limited to what’s solely behind a screen; it can have grave effects on the quality of everyday lives — both online and offline. People are experiencing hate and harassment online every day and some are even changing their habits to avoid contact with their harassers.”

And the survey respondents seem to think that online hate makes people more susceptible to committing hate crimes, according to the ADL.

The ADL also found that most Americans want policymakers to strengthen laws and improve resources for police around cyberbullying and cyberhate. Roughly 80 percent said they wanted to see more action from lawmakers.

Even more Americans, or around 84 percent, think that the technology platforms themselves need to do more work to curb the harassment, hate and hazing they see on social applications and websites.

As for the populations that were most at risk to harassment and hate online, members of the LGBTQ community were targeted most frequently, according to the study. Some 63 percent of people identifying as LGBTQ+ said they were targeted for online harassment because of their identity.

“More must be done in our society to lessen the prevalence of cyberhate,” said Greenblatt. “There are key actions every sector can take to help ensure more Americans are not subjected to this kind of behavior. The only way we can combat online hate is by working together, and that’s what ADL is dedicated to doing every day.”

The report also revealed that cyberbullying had real consequences on user behavior. Of the survey respondents, 38 percent stopped, reduced or changed online activities, and 15 percent took steps to reduce risks to their physical safety.

Interviews for the survey were conducted between December 17 to December 27, 2018 by the public opinion and data analysis company YouGov, and was conducted by the ADL’s Center for Technology and Society. The nonprofit admitted that it oversampled for respondents who identified as Jewish, Muslim, African American, Asian American or LGBTQ+ to “understand the experiences of individuals who may be especially targeted because of their group identity.”

The survey had a margin of error of plus or minus three percentage points, according to a statement from the ADL.",Social Media,TechCrunch,https://techcrunch.com/2019/02/13/2018-really-was-more-of-a-dumpster-fire-for-online-hate-and-harassment-adl-study-finds/,"The Anti-Defamation League recently released a study that showed that 37% of Americans experienced severe online hate and harassment in 2018. This has been linked to real life consequences, such as people reducing or changing their online activities and taking steps to reduce risks to their physical safety. The study also showed that hate crimes and cyberbullying have been",Security & Privacy
123,A Former Facebook VP Says Social Media Is Destroying Society. And He’s Right.,"Feedback Loop

Speaking at a recent event at the Stanford Graduate School of Business, Chamath Palihapitiya – a former vice president for user growth at Facebook – expressed a concern that social media platforms have become “tools that are ripping apart the social fabric of how society works.”

Palihapitiya brought up the example of a WhatsApp hoax campaign in India that led to a string of lynchings. However, new technology is also having more subtle effects on the way that we interact with one another.

“The short-term, dopamine-driven feedback loops we’ve created are destroying how society works,” said Palihapitiya, according to a report from The Verge. Interactions such as ‘liking’ a photograph or ‘favoriting’ a tweet are perhaps more about short-term gratification than the basis for meaningful communication and relationships, Palihapitiya suggested.

“I think that Pallhapitiya points out a very real issue regarding social media and immediate gratification,” said Lizbeth M. Kim, a doctoral candidate in social psychology and women’s, gender, and sexuality studies at Penn State whose research looks at social media, to Futurism. “I view his message as an important reminder of the part of the picture that we may often willfully ignore.”

Advertisement

Advertisement

She emphasized that today’s social media platforms allow for anyone’s message to be heard, amplified, and given credibility. This can allow for anything from small-scale cases of cyberbullying to much broader harassment campaigns to take root.

We’ve seen that terrorist groups have used the likes of Twitter, Facebook, and YouTube in an attempt to attract new recruits. Smartphone usage has been linked to teen depression and suicide, although there are now attempts to use artificial intelligence to watch out for users who might be at risk.

Like any kind of new technology, the impact of social media is a reflection of its user base. There’s always a capacity for people to misuse the tools that are available to them – but there are also very real benefits if used positively.

Social Hierarchy

“While there is increasing public attention placed on the ‘dark side’ of social media, in my work, I’m curious about what happens when people try to use social media platforms for the greater good,” said Kim.

Advertisement

Advertisement

Social media has all kind of benefits, like making useful information available to the general public, helping to build communities, and allowing for links between different social groups. However, people’s overt behavior on social media is only one part of the equation – the reactions to that behavior should also be taken into account.

Kim performed a study where subjects were shown a fictitious comment thread where one user confronted another for making a sexist comment. Responses to that thread proved to be different depending on whether the person doing the confronting was male or female, with the latter being perceived as less likable and more angry.

Image Credit: rawpixel/Pixabay

“People who saw the female confronter perceived the sexist perpetrator as more credible compared to when seeing the male confronter,” said Kim. “So, the same altruistic online behavior can be perceived differently based on subtle cues and stereotypes about identity and credibility.”

There are distinct differences between the way we interact with one another in person, and the way we communicate online. When we can disassociate the individual from their social media account, it’s easier to mistreat people, ranging from minor misdeeds to more serious transgressions.

Advertisement

Advertisement

“It is difficult to make a singular claim about the effects of social media on modern-day relationships,” said Kim. “As time goes on and more research is done to examine this, I think we will have a more nuanced idea about the specific features and activities built into websites like Facebook that are contributing to these positive and negative outcomes.”

It’s important to remember that social media is still a relatively new phenomenon, even though it’s become a huge part of everyday life for lots of people. We’re still learning about its effect on society at large – and it’s important that we consider both the good and bad that come from it.

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Social Media,Futurism,https://futurism.com/former-facebook-vp-social-media-destroying-society-hes-right,"Social media can lead to a range of negative consequences, from subtle effects such as short-term gratification from 'likes' and 'favorites' to more serious issues like cyberbullying and terrorist recruitment. These effects can be amplified by the way people perceive each other, with gender-based stereotypes impacting the way altruistic online behavior",Social Norms & Relationships
124,How to protect yourself online from misinformation right now,"There wasn’t a communications blackout in Washington, DC, on Sunday, but #dcblackout trended on Twitter anyway, thanks to some extremely distressing tweets telling people that, mysteriously, no messages were getting out from the nation’s capital. The tweets, Reddit posts, and Facebook messages about the “blackout” got thousands of shares, fueled by pleas to spread the information widely and ominous warnings about what would happen next to protesters.

But I can tell you that there wasn’t a blackout because I live in DC, and I had to assure worried friends that my internet was working as normal. Despite this, the hashtag stayed trending for hours on Monday, with some people questioning its claims, others dismissing attempts to debunk it, and no one clear on exactly how this rumor spread so far.

The logical response to seeing potentially harmful misinformation spread across the internet is to debunk, and to inform others on how they can avoid falling for it themselves. But it’s difficult to evaluate a river of information when you’re going through something traumatic—in the midst of a global pandemic, and with police escalating their use of force against people protesting police brutality.

“Nothing is okay, and we're going through the same motions that we go through every time there's a crisis,” says Whitney Phillips, an assistant professor of communication and rhetorical studies at Syracuse University. “We have a muscle memory of hitting retweet,” to share something that speaks to a personal experience, or to amplify the voices of others during a crisis. “It feels like it's helping.” But that same impulse can also lead to harm, especially when the content you’re sharing turns out to be misleading or false.

I asked Phillips, who has written about the intersection of toxic online misinformation and mental health, and Shireen Mitchell, the founder of Stop Online Violence Against Women, to give their advice on navigating online misinformation when everything is horrible.

Give yourself some credit

“People often think that because they’re not influencers, they're not politicians, they're not journalists, that what they do [online] doesn’t matter,” Phillips says. But trending hashtags are a good example of how volume, from big and small accounts alike, can drive attention to misinformation. Treating your online presence as if you’re inconsequential, no matter how few followers you have, can be dangerous.



“It doesn’t matter how well intentioned you are,” Phillips says. “By retweeting something that has #dcblackout in it, enough people can make it trend and send people into a panic.”

The good news is that your impulse to share injustice on the internet in order to make the world better can have an impact way beyond your immediate follower count. But it also means that if you share something that’s not true, you can cause more harm than you might think.

Hit pause

Misinformation about racist violence can be particularly difficult to examine as it passes in front of you, because the content itself is re-traumatizing, particularly for black Americans.

“For me, this is what happens with our community. People don’t believe us. So when something bad happens, you want people to share it,” Mitchell says. Misinformation targets this same impulse. The goal is to “evoke an emotion,” Mitchell says. “The minute it evokes an emotion, you have to hit pause.”

The danger is even more acute on the ground during a protest, Mitchell says. If a misleading or false rumor is spreading on social media, protesters have limited means to examine that information on the fly, particularly in an environment that might be unsafe.

Mitchell recommends stepping away from the center of a protest, if possible, when confronted with a distressing rumor to look into its source. “If you discover it’s not true, come back to the crowd,” she says. Let others know what you found.

Think laterally

Mitchell, like many experienced disinformation experts, has learned how to handle potential misinformation through years of practice. But there are ways to get better at it quickly. One of them is to learn to think laterally about a single piece of content—that is, open up some tabs and do your research before sharing something.

Mike Caulfield, a digital literacy expert, has developed what he calls the SIFT approach to looking at information: “Stop, Investigate the source, Find better coverage, and Trace claims, quotes, and media to the original context.” Caulfield has said that his method was adapted from a 2017 Stanford study on how professional fact checkers evaluate digital information. Many of the students and historians who participated in the study fell into the trap of trying to evaluate potential misinformation mainly by looking at it for clues to reliability. The fact checkers —including me—ran Google searches, read news coverage, and did research.

Mitchell’s method is similar. “Every time I go into a trending hashtag, I’m not trying to get the top-level conversation,” Mitchell says. “I’m digging through to find more about it.” And, crucially, she is still on pause.

For example, Mitchell saw a couple of videos that showed protesters acting violently toward bystanders. First, Mitchell looked at the source of the videos: Who posted them? Is this video original or an edited clip from something else? Is that source who they say they are?

Then she looked at where they were being shared; she looked for other videos with angles of the scene; she looked at whether the text accompanying the video accurately portrayed what was going on. It turned out The Intercept had a good rundown of how one of those videos had been edited to be misleading.

Understand that misinformation can still be “real”

Many of the most cited misinformation experts are white. When fact checking information about communities of color, these experts risk causing damage, no matter their intentions.

“Most white people do not believe our lived experience,” Mitchell says. By parachuting into a conversation to tell someone that they just shared a misleading video, you can also be implicitly “tell[ing] black people that their lived experience isn’t true.” That’s particularly problematic when you’re handling misinformation that is literally being shared with the intention of making the lived experience of black Americans more visible.

It’s also problematic to say nothing, Mitchell argues. However, if you are engaging with viral misinformation, don’t assume that your expertise should be immediately believed and heeded, or get defensive when your intentions are challenged. Everybody is worried about people’s motivations, especially when authoritative institutions have released inaccurate information or helped spread misinformation about protests.

Phillips says she tries to think about this in terms of “true” vs. “real” information. Something can be empirically untrue and still speak to something that is real. “There’s a way of affirming ‘This is a reality that people navigate,’ even if this specific video wasn’t taken yesterday,” Phillips says. That understanding should inform your approach to addressing misinformation in the middle of a trauma, whether you’re trying to debunk something that has been shared millions of times or you’re just trying to talk to your mom about one of her Facebook posts.

Consider logging off or stepping away

Examining misinformation can be hard work, and the work is harder when the content itself is traumatizing.

This is true even for experts and veterans. “I don't think that it can be emphasized more explicitly or firmly enough: we are being forced to navigate territory that is absolutely uncharted,” Phillips says. “Some of us have been doing this for years.” But even if you have the media literacy tools and deep emotional reserves, that’s not always enough.

“Maybe on paper some of us have resources we can pull from,” Phillips says. “But the fact is that none of us are prepared for this.”",Social Media,MIT,https://www.technologyreview.com/2020/06/02/1002505/black-lives-matter-protest-misinformation-advice/,"The spread of misinformation on social media can have dangerous consequences, particularly during times of crisis. Without proper media literacy tools, people can easily share false or misleading information that can trigger panic, cause harm, and further traumatize an already vulnerable population.","Information, Discourse & Governance"
125,Facebook warned over privacy risks of merging messaging platforms – TechCrunch,"Facebook’s lead data protection regulator in Europe has asked the company for an “urgent briefing” regarding plans to integrate the underlying infrastructure of its three social messaging platforms.

In a statement posted to its website late last week the Irish Data Protection Commission writes: “Previous proposals to share data between Facebook companies have given rise to significant data protection concerns and the Irish DPC will be seeking early assurances that all such concerns will be fully taken into account by Facebook in further developing this proposal.”

Last week the New York Times broke the news that Facebook intends to unify the backend infrastructure of its three separate products, couching it as Facebook founder Mark Zuckerberg asserting control over acquisitions whose founders have since left the building.

Instagram founders, Kevin Systrom and Mike Krieger, left Facebook last year, as a result of rising tensions over reduced independence, according to our sources.

While WhatsApp’s founders left Facebook earlier, with Brian Acton departing in late 2017 and Jan Koum sticking it out until spring 2018. The pair reportedly clashed with Facebook execs over user privacy and differences over how to monetize the end-to-end encrypted platform.

Acton later said Facebook had coached him to tell European regulators assessing whether to approve the 2014 merger that it would be “really difficult” for the company to combine WhatsApp and Facebook user data.

In the event, Facebook went on to link accounts across the two platforms just two years after the acquisition closed. It was later hit with a $122M penalty from the European Commission for providing “incorrect or misleading” information at the time of the merger. Though Facebook claimed it had made unintentional “errors” in the 2014 filing.

A further couple of years on and Facebook has now graduated to seeking full platform unification of separate messaging products.

“We want to build the best messaging experiences we can; and people want messaging to be fast, simple, reliable and private,” a spokesperson told us when we asked for a response to the NYT report. “We’re working on making more of our messaging products end-to-end encrypted and considering ways to make it easier to reach friends and family across networks.”

“As you would expect, there is a lot of discussion and debate as we begin the long process of figuring out all the details of how this will work,” the spokesperson added, confirming the substance of the NYT report.

There certainly would be a lot of detail to be worked out. Not least the feasibility of legally merging user data across distinct products in Europe, where a controversial 2016 privacy u-turn by WhatsApp — when it suddenly announced it would after all share user data with parent company Facebook (despite previously saying it would never do so), including sharing data for marketing purposes — triggered swift regulatory intervention.

Facebook was forced to suspend marketing-related data flows in Europe. Though it has continued sharing data between WhatsApp and Facebook for security and business intelligence purposes, leading to the French data watchdog to issue a formal notice at the end of 2017 warning the latter transfers also lack a legal basis.

A court in Hamburg, Germany, also officially banned Facebook from using WhatsApp user data for its own purposes.

Early last year, following an investigation into the data-sharing u-turn, the UK’s data watchdog obtained an undertaking from WhatsApp that it would not share personal data with Facebook until the two services could do so in a way that’s compliant with the region’s strict privacy framework, the General Data Protection Regulation (GDPR).

Facebook only avoided a fine from the UK regulator because it froze data flows after the regulatory intervention. But the company clearly remains on watch — and any fresh moves to further integrate the platforms would trigger instant scrutiny, evidenced by the shot across the bows from the DPC in Ireland (Facebook’s international HQ is based in the country).

The 2016 WhatsApp-Facebook privacy u-turn also occurred prior to Europe’s GDPR coming into force. And the updated privacy framework includes a regime of substantially larger maximum fines for any violations.

Under the regulation watchdogs also have the power to ban companies from processing data. Which, in the case of a revenue-rich data-mining giant like Facebook, could be a far more potent disincentive than even a billion dollar fine.

We’ve reached out to Facebook for comment on the Irish DPC’s statement and will update this report with any response.

Here’s the full statement from the Irish watchdog:

While we understand that Facebook’s proposal to integrate the Facebook, WhatsApp and Instagram platforms is at a very early conceptual stage of development, the Irish DPC has asked Facebook Ireland for an urgent briefing on what is being proposed. The Irish DPC will be very closely scrutinising Facebook’s plans as they develop, particularly insofar as they involve the sharing and merging of personal data between different Facebook companies. Previous proposals to share data between Facebook companies have given rise to significant data protection concerns and the Irish DPC will be seeking early assurances that all such concerns will be fully taken into account by Facebook in further developing this proposal. It must be emphasised that ultimately the proposed integration can only occur in the EU if it is capable of meeting all of the requirements of the GDPR.

Facebook may be hoping that extending end-to-end encryption to Instagram as part of its planned integration effort, per the NYT report, could offer a technical route to stop any privacy regulators’ hammers from falling.

Though use of e2e encryption still does not shield metadata from being harvested. And metadata offers a rich source of inferences about individuals which, under EU law, would certainly constitute personal data. So even with robust encryption across the board of Instagram, Facebook and WhatsApp the unified messaging platforms could still collectively leak plenty of personal data to their data-mining parent.

Facebook’s apps are also not open source. So even WhatsApp, which uses the respected Signal Protocol for its e2e encryption, remains under its control — with no ability for external audits to verify exactly what happens to data inside the app (such as checking what data gets sent back to Facebook). Users still have to trust Facebook’s implementation but regulators might demand actual proof of bona fide messaging privacy.

Nonetheless, the push by Facebook to integrate separate messaging products onto a single unified platform could be a defensive strategy — intended to throw dust in the face of antitrust regulators as political scrutiny of its market position and power continues to crank up. Though it would certainly be an aggressive defence to more tightly knit separate platforms together.

But if the risk Facebook is trying to shrink is being forced, by competition regulators, to sell off one or two of its messaging platforms it may feel it has nothing to lose by making it technically harder to break its business apart.

At the time of the acquisitions of Instagram and WhatsApp Facebook promised autonomy to their founders. Zuckerberg has since changed his view, according to the NYT — believing integrating all three will increase the utility of each and thus provide a disincentive for users to abandon each service.

It may also be a hedge against any one of the three messaging platforms decreasing in popularity by furnishing the business with internal levers it can throw to try to artifically juice activity across a less popular app by encouraging cross-platform usage.

And given the staggering size of the Facebook messaging empire, which globally sprawls to 2.5BN+ humans, user resistance to centralized manipulation via having their buttons pushed to increase cross-platform engagement across Facebook’s business may be futile without regulatory intervention.",Social Media,TechCrunch,https://techcrunch.com/2019/02/02/facebook-warned-over-privacy-risks-of-merging-messaging-platforms/,"Facebook's proposed integration of its three messaging platforms has raised significant data protection concerns, due to the potential for data to be illegally shared between the services. Additionally, the risk of user manipulation for marketing and other purposes remains, without sufficient regulatory intervention.",Security & Privacy
126,Gaming the System: How Marketers Rig the Social Media Machine,"Regular users of social networks generally collect friends and followers on a one-by-one basis, then use those connections to share their opinions and links to the latest ""Double Rainbow"" remix or whatever is making the rounds that day.

These systems are based on trust and loyalty, and as such, they present a massive opportunity to marketers who want to encourage those traits in their customers. People are more likely to trust people or companies that have lots of friends or YouTube views, so the incentive clearly exists to artificially inflate those counts -- and a cottage industry is emerging to help them do just that.

In addition, because real friends trust each other, marketers try to insert themselves into those conversations by offering product discounts or cash when people mention brands in messages to their friends. Suddenly, friendship is as ripe an area for product placement as music videos (and in some cases, music itself).

If clout is the new currency, in other words, it already has its share of counterfeiters.

To be clear, Twitter and Facebook are hardly overrun with this sort of thing, and I personally don't believe I have ever encountered a single paid-for message. And it's hard to say how many popular Twitizens may have paid for some percentage of their followers. Nonetheless, as the cat-and-mouse game between social networks and social networking marketers escalates, that could change.

Here are a few ways that self-promoters and marketers are already trying to game the social networking system.

Friends and followers for sale

Anyone who wants to look important, loved, influential or trusted might be tempted to purchase Twitter followers or Facebook friends by the thousand, and they have plenty of options when it comes to buying them.

Twitter spokesman Matt Graves pointed out that Twitter's rules forbid selling followers unless the seller has been ""specifically permitted to do so in a separate agreement with Twitter.""

""It's fairly simple for us to identify and suspend mass-created accounts with '10,000 followers,' so it's also a terrible purchase in general,"" he told Wired.com. ""They're never 'real' followers.""

Some clients of these services tell a different story. Affiliate marketer Jonathan Volk posted charts showing how his purchase of 1,000 Twitter followers from Followers for Sale using its recommended setting that adds followers ""very slowly"" so as to escape detection, resulted in real followers being added to his account on a steady basis.",Social Media,WIRED,https://www.wired.com/2010/07/gaming-the-system-how-marketers-rig-the-social-media-machine/,"the rise of artificial followers and friends, which can be purchased and used to game the social networking system. This can lead to a degradation of trust and loyalty between users, as well as a loss of the credibility of brands and influencers who use such tactics.",Social Norms & Relationships
127,"D-ID, the Israeli company that digitally de-identifies faces in videos and still images, raises $13.5 million – TechCrunch","D-ID, the Israeli company that digitally de-identifies faces in videos and still images, raises $13.5 million

If only Facebook had been using the kind of technology that TechCrunch Startup Battlefield alumnus D-ID was pitching, it could have avoided exposing all of our faces to privacy destroying software services like Clearview AI.

At least, that’s the pitch that D-ID’s founder and chief executive, Gil Perry, makes when he’s talking about the significance of his startup’s technology.

D-ID, which stands for de-identification, is a pretty straightforward service that’s masking some highly involved and very advanced technology to blur digital images so they can’t be cross-referenced to determine someone’s identity.

It’s a technology whose moment has come as governments and private companies around the world ramp up their use of surveillance technologies as the world adjusts to a new reality in the wake of the COVID-19 epidemic.

“Governments around the world and organizations have used this new reality basically as an excuse for mass surveillance,” says Perry. His own government has used a track and trace system that monitors interactions between Israeli citizens using cell phone location data to determine whether anyone had been in contact with a person who had COVID-19.

While awareness of the issue may be increasing among consumers and regulators alike, the damage has, in many cases, already been done. Social media companies have already had their troves of images scraped by companies like Clearview AI, ClearView, HighQ and NTechLabs, and much of our personal information is already circulating online.

D-ID is undeterred. Founded by Perry and two other members of the Israeli army’s cybersecurity and offensive cyber unit, 8200, Sella Blondheim and Eliran Kuta, D-ID thinks the need for anonymizing technologies will continue to expand — thanks to new privacy legislation in Europe and certain states in the U.S.

Meanwhile, the company is also exploring other applications for its technology. The services that D-ID uses to mask and blur faces can also be used to create deepfakes of images and video.

The market for these types of digital manipulations are still in their earliest days, according to Perry. Still, the company’s pitch managed to intrigue new lead investor AXA Ventures, which joined backers including Pitango, Y Combinator, AI Alliance, Hyundai, Omron, Maverick (U.S.) and Mindset, to participate in the company’s $13.5 million round.

D-ID already sees demand coming from automakers who want to use the technology to anonymize their driving monitoring systems — enabling them to record drivers’ reactions, but not any public identifying information. Security technologies that monitor for threats are another potential customer, according to the company. While closed circuit television monitors a physical space, it doesn’t need to collect the identifying information of people entering and exiting buildings.

“The convergence of increased surveillance and individual privacy protection places enterprises in a position where they must either anonymize their stored footage or risk violating privacy laws and face costly penalties.” said Blondheim.

The technical wizardry that D-ID has mastered is impressive — and a necessary defensive tool to ensure privacy in the modern world, according to its founders. Consumers are demanding it, according to D-ID’s chief executive.

“Privacy awareness and the importance of privacy enhancing technologies have increased,” Perry said.",Social Media,TechCrunch,https://techcrunch.com/2020/05/26/d-id-the-israeli-company-that-digitally-de-identifies-faces-in-videos-and-still-images-raises-13-million/,"Social media has led to privacy issues, as companies like Clearview AI have scraped troves of images and personal information, making it available to the public. D-ID, a TechCrunch Startup Battlefield alumnus, has developed technology to blur digital images and mask faces, providing a way to help protect individual privacy.",Security & Privacy
128,"Periscope cracks down on inauthentic behavior, including fake hearts, follows, chats and more – TechCrunch","Twitter’s video streaming app, Periscope, is cracking down on spam and, specifically, fake engagements. The company says it’s updating its policies around how it enforces its anti-spam rules, and is making improvements in terms of how those rules are enforced. This means users will see increased enforcement actions, Periscope notes — and these may even take place on “high-profile” accounts.

The app has struggled for some time to get a handle on spam and other bad behavior.

For example, in 2016 the live-streaming app rolled out real-time comment moderation — a much-needed change given the real-time nature of the live video and associated comments. Last year, it updated this system so broadcasters could assign their own chat moderators instead of relying on the crowd to handle the reporting and banning.

While these changes may have helped to address issues around trolling and abuse, spam is another matter — and particularly the inauthentic behavior around “fake engagements.”

This isn’t only a Periscope problem. On any platform where engagements — like hearts, favorites, follows and comments — are the currency of success, entire ecosystems pop up designed to help people cheat their way to the top.

With the updated spam policy, the company says it will now prohibit fake engagements — including any artificial hearts, chat, followers and views. It classifies these actions as “spam,” because they’re “deceptive” forms of activity.

Any selling or promoting of fake engagement will be prohibited, too.

In addition, the company says it will focus on proactive enforcement to help improve chat quality and will soon launch account-level spam-reporting options so others can report spammy users.

Fake engagement is not a new issue for the app.

For years there have been problems with fake followers and fake hearts as an attempt to manipulate the system. There are YouTube videos that detail how this works, tutorials on how to make these purchases, bots, and, of course, offers filed under “social media marketing” on sites like Fiverr — a marketplace where much of the fake internet is manufactured.

While Periscope may have turned a blind eye to the spam and fakery for some time, its decision to finally crack down on fake engagement arrives only a few months after Instagram did the same. In November, Instagram began fighting back against automated apps people used to leave spammy comments and to follow and unfollow users in hopes of growing their audience.

Social media platforms, as a whole, have actively ignored these sorts of attempts to manipulate their systems for most of their existence. After all, fake engagements like hearts and follows and comments make it look like their platforms are more active than they actually are. And if these fakery tools helped birth crowds of “influencers” who then, in turn, attracted more users to the platform, that could be even seen as a perk.

But fake accounts and activity aren’t always about people wanting a shortcut to online fame — inauthentic accounts are also the source for disinformation campaigns and attempts by foreign governments to hack our democracy. That’s shifted the scale in the other direction, and has forced social media platforms to finally stop ignoring the problem of inauthentic accounts and activity.

Periscope, however, told users it’s all about listening to their feedback.

“At Periscope, we value our community’s feedback to make our service better. Periscope is a place for instant engagement and we’ve heard your concerns about spammy accounts and chats,” the company said. “Whether you’re broadcasting or catching up with your favorite broadcaster, we are always looking for ways to make Periscope feel safer and more authentic for our community.”",Social Media,TechCrunch,https://techcrunch.com/2019/03/13/periscope-cracks-on-inauthentic-behavior-including-fake-hearts-follows-chats-and-more/,"Social media platforms have had to crack down on fake engagements such as hearts, follows, comments, and views, as they can be used to manipulate the system, create false influencers, and even be used to spread disinformation.",Security & Privacy
129,Facebook rolls out checks for UK political ads – TechCrunch,"Facebook has announced it rolled out a system of checks on political ads run on its platform in the UK which requires advertisers to verify their identity and location to try to make it harder for foreign actors to meddle in domestic elections and referenda.

This follows similar rollouts of political ad transparency tools in the U.S. and Brazil.

From today, Facebook said it will record and display information about who paid for political ads to run on its platform in the UK within an Ad library — including retaining the ad itself — for “up to seven years”.

It will also badge these ads with a “Paid for by” disclaimer.

So had the company had this system up and running during the UK’s 2016 Brexit referendum, the Canadian data firm AIQ would, presumably, have had to pass its political advertiser verification process, and display “Paid for by” Vote Leave/BeLeave/Veterans for Britain badges on scores of pro-Brexit ads… If it didn’t just get barred for not being based in the UK in the first place.

(How extensively Facebook will be checking up on political advertisers’ ‘paid for by’ claims is one pertinent question to ask, and we have asked; otherwise this looks mostly like a badging exercise — which requires other doing the work to check/police claims… ).

Ditto during Ireland’s referendum earlier this year, on overturning a constitutional ban on abortion. In that instance Facebook decided to suspend all foreign-funded ads a few weeks before the vote because it did not yet have a political ad check system in place.

In the UK, the new requirement on political advertisers applies to “all advertisers wanting to run ads in the UK that reference political figures, political parties, elections, legislation before Parliament and past referenda that are the subject of national debate”, Facebook said.

“We see this as an important part of ensuring electoral integrity and helping people understand who they are engaging with,” said Richard Allan, VP of global public policy, and Rob Leathern, director of product management in a blog post announcing the launch. “We recognise that this is going to be a significant change for people who use our service to publish this type of ad. While the vast majority of ads on Facebook are run by legitimate organisations, we know that there are bad actors that try to misuse our platform. By having people verify who they are, we believe it will help prevent abuse.”

UK lawmakers have been highly critical of Facebook’s response to their attempts to investigate how social media ads were used and mis-used during the UK’s 2016 EU referendum.

This summer the parliamentary committee that has been investigating online disinformation called for a levy on social media to ‘defend democracy’. And earlier this year Facebook told the same committee it would roll out an authentication process for political advertisers in time for the UK’s local elections, in May 2019 — with CTO Mike Schroepfer telling MPs the company believes “radical transparency” can fix concern about the societal and democratic impacts of divisive social media ads.

In response, MPs quizzed Schroepfer on whether Facebook’s political ad transparency tool would be so radical as to include “targeting data” in the disclosures — i.e. “will I understand not just who the advertiser was and what other adverts they’d run but why they’d chose to advertise to me”.

The Facebook CTO’s response in April suggested the company did not plan to go that far. And, indeed, Facebook says now that the details it will disclose in the Ad library are only: “A range of the ad’s budget and number of people reached, and the other ads that Page is running.”

So not, seemingly, any actual targeting data: Aka the specific reasons a particular user is seeing a particular political ad. Which could help Facebook users contextualize political ads and be wiser to attempts to manipulate their opinion, as well as generally better understand how their personal information is being used (and potentially misused).

It’s true that Facebook does already provide some data about broad-brush targeting, with a per-ad option users can click to get a response on ‘why am I seeing this?’. But the targeting categories the company serves via this feature are so broad and lacking in comprehensiveness as to be selectively uninformative and thus pretty useless at very best.

Indeed, the results have even been accused of being misleading.

If Facebook was required by law to rip away its adtech modesty curtain entirely there’s a risk, for its business model, that users would get horribly creeped out by the full bore view of the lidless eye in the digital wall spying on them to target ads.

So while Schroepfer teased UK MPs with “radical transparency” the reality, six months on, is something a whole lot more dilute and incremental.

Facebook itself appears to be conceding as much, and trying to manage expectations, when it writes: “We believe that increased transparency will lead to increased accountability and responsibility over time — not just for Facebook but for advertisers as well.”

So it remains to be seen whether UK lawmakers will be satisfied with this tidbit. Or call for blood, as they set themselves to the task of regulating social media.

The other issue is how comprehensively (or otherwise) Facebook will police its own political ad checks.

Its operational historical is replete with content identification and moderation failures. Which doesn’t exactly bode well for the company to robustly control malicious attempts to skew public opinion — especially when the advertisers in question are simultaneously trying to pour money into its coffers.

So it also remains to be seen how many divisive political ads will simply slip under its radar — i.e. via the non-political, non-verified standard route, and get distributed anyway. Not least because there is also the trickiness of identifying a political ad (vs a non-political ad).

Malicious political ads paid for by Kremlin-backed entities didn’t always look like malicious political ads. Some of the propaganda Russia was spreading via Facebook in the US targeted at voters included seemingly entirely apolitical and benign messages aimed at boosting support among certain identity-based groups, for example. And those sorts of ads would not appear to fit Facebook’s definition of a ‘political ad’ here.

In general, the company also looks to be relying on everyone else to do the grunt-work policing for it — as per its usual playbook.

“If you see an ad which you believe has political content and isn’t labeled, please report it by tapping the three dots at the top right-hand corner of the ad,” it writes. “We will review the ad, and if it falls under our political advertising policy, we’ll take it down and add it to the Ad Library. The advertiser will then be prevented from running ads related to politics until they complete our authorisation process and we’ll follow up to let you know what happened to the ad you reported.”",Social Media,TechCrunch,https://techcrunch.com/2018/10/16/facebook-rolls-out-checks-for-uk-political-ads/,"The main undesirable consequence of social media discussed here is the potential for foreign actors to meddle in domestic elections and referenda by using ads to manipulate public opinion. Facebook has implemented a system of checks to try and prevent this, though there are concerns that the company may not be able to effectively police the system and malicious ads may slip through the","Information, Discourse & Governance"
130,Disney CEO calls social media a ‘powerful marketing tool’ for extremism,"Disney CEO Bob Iger criticized social media platforms for allowing hate to spread, saying they enable the distribution of misinformation and the propagation of “vile ideology.”

Iger referred to social media as something Hitler “would have loved,” according to Variety, while accepting a humanitarian award earlier today from the Simon Wiesenthal Center, a human rights nonprofit that’s named after a Holocaust survivor. He added that social media is the “most powerful marketing tool an extremist could ever hope for.” Social media is designed to amplify “our deepest fears,” according to Iger, while also “constantly validating our convictions.”

“It creates a false sense that everyone shares the same opinion,” Iger said. “Social media allows evil to prey on troubled minds and lost souls and we all know that social news feeds can contain more fiction than fact, propagating vile ideology that has no place in a civil society that values human life.”

His comments come at a time when Facebook, Twitter, YouTube, Google, and Instagram are being accosted for allowing hateful ideologies to spread around the world and not doing enough to stop dangerous conspiracy theorists from gaming their algorithms. All of the aforementioned companies have admitted they can do better and have introduced a series of policy changes and platform shifts to try to combat the misuses of their products.

Iger isn’t the first CEO to acknowledge that more can be done to prevent hatred from spreading online. Apple CEO Tim Cook called out white supremacists while he was accepting his “Courage Against Hate” award from the Anti-Defamation League in December. Cook told white supremacists and “dangerous conspiracy theorists” that they don’t have a home on Apple’s services. His comments came not too long after Apple removed Alex Jones’ Infowars from the App Store. Iger sits on the board of Apple. Neither Apple nor Disney owns social media platforms.

“We only have one message for those who seek to push hate, division, or violence: You have no place on our platforms,” Cook said.

Iger also used his time onstage to ask people in the room to “once again renounce and reject hate in all forms.”",Social Media,Verge,https://www.theverge.com/2019/4/11/18306763/disney-ceo-bob-iger-social-media-hitler-extremism,"CEO Bob Iger and Apple's Tim Cook have both criticized social media platforms for enabling hate, misinformation, and dangerous conspiracy theories to spread, while also propagating “vile ideology”. Cook called out white supremacists and warned that they have no place on Apple's services. Iger urged people to renounce and reject hate in",Equality & Justice
131,Answers being sought from Facebook over latest data breach – TechCrunch,"Facebook’s lead data protection regulator in the European Union is seeking answers from the tech giant over a major data breach reported over the weekend.

The breach was reported by Business Insider on Saturday, which said personal data (including email addresses and mobile phone numbers) of more than 500 million Facebook accounts had been posted to a low-level hacking forum — making the personal information on hundreds of millions of Facebook users’ accounts freely available.

“The exposed data includes the personal information of over 533M Facebook users from 106 countries, including over 32M records on users in the US, 11M on users in the UK, and 6M on users in India,” Business Insider said, noting that the dump includes phone numbers, Facebook IDs, full names, locations, birthdates, bios and some email addresses.

Facebook responded to the report of the data dump by saying it related to a vulnerability in its platform it had “found and fixed” in August 2019 — dubbing the info “old data” which it also claimed had been reported in 2019. However as security experts were quick to point out, most people don’t change their mobile phone number often — so Facebook’s trigger reaction to downplay the breach looks like an ill-thought-through attempt to deflect blame.

It’s also not clear whether all the data is all “old”, as Facebook’s initial response suggests.

This is old data that was previously reported on in 2019. We found and fixed this issue in August 2019. https://t.co/mPCttLkjzE — Liz Bourgeois (@Liz_Shepherd) April 3, 2021

There’s plenty of reasons for Facebook to try to downplay yet another data scandal. Not least because, under European Union data protection rules, there are stiff penalties for companies that fail to promptly report significant breaches to relevant authorities. And indeed for breaches themselves — as the bloc’s General Data Protection Regulation (GDPR) bakes in an expectation of security by design and default.

By pushing the claim that the leaked data is “old” Facebook may be hoping to peddle the idea that it predates the GDPR coming into application (in May 2018).

However, the Irish Data Protection Commission (DPC), Facebook’s lead data supervisor in the EU, told TechCrunch that it’s not abundantly clear whether that’s the case at this point.

“The newly published dataset seems to comprise the original 2018 (pre-GDPR) dataset and combined with additional records, which may be from a later period,” the DPC’s deputy commissioner, Graham Doyle said in a statement.

“A significant number of the users are EU users. Much of the data appears to been data scraped some time ago from Facebook public profiles,” he also said.

“Previous datasets were published in 2019 and 2018 relating to a large-scale scraping of the Facebook website which at the time Facebook advised occurred between June 2017 and April 2018 when Facebook closed off a vulnerability in its phone lookup functionality. Because the scraping took place prior to GDPR, Facebook chose not to notify this as a personal data breach under GDPR.”

Doyle said the regulator sought to establish “the full facts” about the breach from Facebook over the weekend and is “continuing to do so” — making it clear that there’s an ongoing lack of clarity on the issue, despite the breach itself being claimed as “old” by Facebook.

The DPC also made it clear that it did not receive any proactive communication from Facebook on the issue — despite the GDPR putting the onus on companies to proactively inform regulators about significant data protection issues. Rather, the regulator had to approach Facebook — using a number of channels to try to obtain answers from the tech giant.

Through this approach the DPC said it learnt Facebook believes the information was scraped prior to the changes it made to its platform in 2018 and 2019 in light of vulnerabilities identified in the wake of the Cambridge Analytica data misuse scandal.

A huge database of Facebook phone numbers was found unprotected online back in September 2019.

Facebook had also earlier admitted to a vulnerability with a search tool it offered — revealing in April 2018 that somewhere between 1 billion and 2 billion users had had their public Facebook information scraped via a feature which allowed people to look up users by inputting a phone number or email — which is one potential source for the cache of personal data.

Last year Facebook also filed a lawsuit against two companies it accused of engaging in an international data scraping operation.

But the fallout from its poor security design choices continue to dog Facebook years after its ‘fix’.

More importantly, the fallout from the massive personal data spill continues to affect Facebook users whose information is now being openly offered for download on the internet — opening them up to the risk of spam and phishing attacks and other forms of social engineering (such as for attempted identity theft).

There are still more questions than there are answers about how this “old” cache of Facebook data came to be published online for free on a hacker forum.

The DPC said it was told by Facebook that “the data at issue appears to have been collated by third parties and potentially stems from multiple sources”.

The company also claimed the matter “requires extensive investigation to establish its provenance with a level of confidence sufficient to provide your Office and our users with additional information” — which is a long way of suggesting that Facebook has no idea either.

“Facebook assures the DPC it is giving highest priority to providing firm answers to the DPC,” Doyle also said. “A percentage of the records released on the hacker website contain phone numbers and email address of users.

“Risks arise for users who may be spammed for marketing purposes but equally users need to be vigilant in relation to any services they use that require authentication using a person’s phone number or email address in case third parties are attempting to gain access.”

“The DPC will communicate further facts as it receives information from Facebook,” he added.

At the time of writing Facebook had not responded to a request for comment about the breach.

Facebook users who are concerned whether their information is in the dump can run a search for their phone number or email address via the data breach advice site, haveibeenpwned.

According to haveibeenpwned’s Troy Hunt, this latest Facebook data dump contains far more mobile phone numbers than email addresses.

He writes that he was sent the data a few weeks ago — initially getting 370 million records and later “the larger corpus which is now in very broad circulation”.

“A lot of it is the same, but a lot of it is also different,” Hunt also notes, adding: “There is not one clear source of this data.”

Update: Facebook has now published a blog post with some additional details about the breach in which it writes that it believes the data in question was scraped from people’s Facebook profiles by “malicious actors” using a contact importer feature prior to September 2019 — before it made changes to the tool intended to prevent abuse by blocking the ability to upload a large set of phone numbers to find ones that matched profiles.

“Through the previous functionality, [users of Facebook’s contact importer tool] were able to query a set of user profiles and obtain a limited set of information about those users included in their public profiles,” Facebook writes, adding that the information obtained did not include financial information, health information or passwords.

However it does not specify what data could have been obtained by these malicious actors repurposing its tools. Or whether it has identified and sought to prosecute the actors in question.

Instead its PR segues into stating that such action is against its terms — as well as claiming it is “working to get this data set taken down”. It also says it will “continue to aggressively go after malicious actors who misuse our tools wherever possible” — again without offering any examples of instances where it has successfully identified and definitively barred an abuser from its service.

(Where, for instance, is the final report of an internal app audit that Facebook said it would carry out after the Cambridge Analytica scandal back in 2018? The UK’s data protection regulator said recently that a legal deal it has with Facebook prevents it from discussing the app audit in public. So Facebook certainly appears to have an aggressive approach when it comes to avoiding transparency on how it tackles misuse of its tools… )

“While we can’t always prevent data sets like these from recirculating or new ones from appearing, we have a dedicated team focused on this work,” Facebook adds in the PR, offering no guarantees to users that their data is safe with its service.

Instead it recommends users check the privacy settings it offers for accounts, including those which provide some controls over how others can find you on its service — a line which seeks to deflect from the latest Facebook data breach revelation via some suggestive blame-shifting; i.e. by implying that responsibility for data security is in the hands of Facebook users, rather than Facebook itself.

Of course that’s not the case. Not least because Facebook users are only offered partial controls over their data by Facebook, and entirely of Facebook’s design and devising (including setting privacy-hostile defaults).

Moreover, in Europe at least, the company has a legal responsibility to bake security into the design of its products. Failure to offer an adequate level of protection for personal data can attract major regulatory sanction — although the company continues to benefit from a GDPR enforcement bottleneck.

Facebook’s PR also suggests users enable two-factor authentication to improve account security.

That’s certainly a good idea but on the 2FA front it’s worth noting that Facebook does now offer support for security key and third-party authentication apps for 2FA — meaning you can add this extra layer of security without risking giving Facebook your mobile number. And since the service has demonstrably leaked users’ phone numbers at vast scale — while the business has also admitted to using 2FA digits for ad targeting — you really shouldn’t trust Facebook with your phone number.

Update 2: In additional background remarks Facebook said it will not be commenting on how it communicates with regulators.

It also said it has no plans to notify users individually about the breach — further specifying that owing to how the data was obtained (scraping) it cannot be entirely sure who would need to be notified.",Social Media,TechCrunch,https://techcrunch.com/2021/04/06/answers-being-sought-from-facebook-over-latest-data-breach/,"The European Union's Data Protection Commission is seeking answers from Facebook over a major data breach reported over the weekend, in which personal data of more than 500 million Facebook accounts was posted to a low-level hacking forum, making users vulnerable to spam, phishing attacks, and other forms of social engineering. Facebook has not taken responsibility for the breach",Security & Privacy
132,Sendit gains 3.5M downloads after Snapchat suspends top anonymous apps YOLO and LMK – TechCrunch,"In May of this year, Snap suspended two Snapchat-integrated apps that allowed users to send anonymous messages, Yolo and LMK, following a lawsuit filed on behalf of a mother whose son died by suicide after being bullied through messages on the apps for many months. In the wake of the suspensions, another anonymous messaging app called Sendit has been rising in the app stores’ charts, as Snapchat’s younger users sought a replacement for the apps the company blocked.

Since the news of the suspensions was first reported over 80 days ago, Sendit’s app has seen more than 3.5 million installs across iOS and Android, according to app intelligence firm Apptopia.

This is a rapid pace of installs compared with how quickly it grew while Yolo and LMK were active on the market. In the same period before the news was announced, Sendit had only seen seen 180,000 installs across iOS and Android, Apptopia says.

Sendit also received few user reviews before May 11, 2021. But in the days that followed the suspension, “yolo” has become the second-most-used keyword in Sendit’s user reviews, Apptopia told TechCrunch. Most of these reviews are positive, saying the app is like “Yolo but better,” for instance. In other words, Snap’s suspension hasn’t stamped out demand for anonymous Snapchat Q&A apps, it only crowned a new app as the market leader.

Sendit today is currently ranking No. 3 among Lifestyle apps on Apple’s U.S. App Store and has climbed to No. 57 on the App Store’s list of top free apps. It jumped three ranks overnight from Monday to Tuesday, in fact.

Like Yolo and LMK, Sendit also features a popular teen activity on Snapchat, anonymous Q&As. The app also includes other Lens games, like “Never Have I Ever,” “This or That,” “Kiss, Marry, Block” and others.

To be clear, none of these are official Snapchat applications. Instead, they integrate with a toolkit for third-party developers called Snap Kit, which allows them to create new product experiences that work with Snapchat’s best features, like Stories, Bitmoji, the Snapchat Camera and more.

Snap says its Snap Kit developers have to agree to its Terms of Service, which requires apps to prioritize user safety and take action on any reports of abuse. Those guidelines are meant to encompass any reports of bullying, harassment, hate speech or threats taking place on the third-party services. In addition, apps that offer friend finding, user-generated content and anonymous features are supposed to inform Snap of their moderation practices and customer support response times.

In practice, however — as the lawsuit highlighted — there appears to be an issue with how well those terms are enforced on Snap’s end. The company tells us that it’s continuing to review developers to ensure their compliance. It has yet to announce any policy changes as result of that investigation, but some child advocates would argue that anonymous apps should have no place in a teenager’s life at all.

Even before the Snap lawsuit, apps like Yolo and LMK had raised concerns among child advocates and parents alike. For example, nonprofit Common Sense Media, an independent source for media recommendations and advice for families, pointed out that “anonymity on social media can easily lead teens down a slippery slope of poor choices.” The organization said that while teens will be drawn to the excitement of responding anonymously — perhaps learning that someone might have a crush on them — “hiding behind anonymity can also bring out hatefulness and sexually explicit risk taking.”

Sendit’s App Store reviews (see photos) indicate that is, indeed, taking place. (Sendit didn’t respond to a request for more information about its app’s operations.)

The tech industry is littered with anonymous social apps that failed due to issues with cyberbullying. After numerous teen suicides related to Ask.fm’s anonymous platform, its owner IAC sold off the toxic property to an asset management firm. Other high-profile anonymous app failures include Secret, which became a home to cyberbullying; Sarahah, which was banned by the app stores and later pivoted; Yik Yak, whose founders left for Square after the app became plagued by cyberbullying; and After School, which also got kicked out of the App Store. To date, only anonymous platforms like Glassdoor and Blind, which focus on workplace chatter and career advice, have seemed to thrive.

The question for Snap to decide now is not just how it will enforce its terms on anonymous apps, but whether it’s worth allowing anonymous apps to operate given their documented dangers — and their potential tragic, as well as legal, consequences.

If you or someone you know is struggling with depression or has had thoughts of harming themselves or taking their own life, The National Suicide Prevention Lifeline (1-800-273-8255) provides 24/7, free, confidential support for people in distress, as well as best practices for professionals and resources to aid in prevention and crisis situations.",Social Media,TechCrunch,https://techcrunch.com/2021/08/03/anonymous-snapchat-app-sendit-surges-with-3-5m-installs-after-snap-bans-yolo-and-lmk/,"Anonymous social media apps have long been linked to cyberbullying and other negative behaviors, leading to tragic consequences like teen suicides. These dangers have led to several high-profile app failures, as well as a lawsuit against Snap for suspending two such apps.",Social Norms & Relationships
133,Social media firms given a month to fix consumer rights issues in Europe – TechCrunch,"Facebook, Twitter and Google are under more pressure in Europe to comply with regional rules. The latest issue they find themselves on the hook for relates to complaints pertaining to a variety of consumer rights that have been investigated by EU regulators since last year.

EU consumer authorities have been specifically looking into complaints about unfair terms and conditions, and looking for ways to tackle fraud and scams that mislead consumers when they are using the social networks — such as fake promotions to ‘win a smartphone for €1’ that also sign the user up to a hidden long term subscription for hundreds of euros.

Last November the three social media firms were sent letters by the EU regulators asking them to address the areas of concern. That was followed, earlier this month, by a meeting between the companies, regulators and the European Commission to discuss proposed solutions.

The companies have now been given a month to come up with fixes. If their proposals fail to pass muster they could face enforcement action in future, the EC said today.

Contacted for a response Facebook, Twitter and Google all declined to comment, saying they have nothing to share at this stage.

A variety of EU rules are involved in the matter, including the Unfair Commercial Practices Directive, the E-commerce Directive, the Consumer Rights Directive or the unfair contract terms Directive.

The latter directive, for example, can invalidate standard terms & conditions if they create a significant imbalance in parties’ rights and obligations to the detriment of the consumer — meaning the terms would be judged unfair.

The Directive also requires that T&Cs be drafted in “plain and intelligible language” — to ensure consumers are informed in a clear and understandable manner about their rights.

Some of the specific things the EC emphasizes today that social media companies cannot do under EU consumer rules (suggesting the three are being accused of, at least, some of these failings) are:

Social media networks cannot deprive consumers of their right to go to court in their Member State of residence;

Social media networks cannot require consumers to waive mandatory rights, such as their right to withdraw from an online purchase;

Terms of services cannot limit or totally exclude the liability of social media networks in connection with the performance of the service;

Sponsored content cannot be hidden but should be identifiable as such;

Social media networks cannot unilaterally change terms and conditions without clearly informing consumers about the justification and without given them the possibility to cancel the contract, with adequate notice;

Terms of services cannot confer unlimited and discretionary power to social media operators on the removal of content;

Termination of a contract by the social media operator should be governed by clear rules and not decided unilaterally without a reason;

T&Cs are something we’ve long criticized as a horrible problem in the tech space. And while Facebook, Twitter and Google are clearly not the only companies in the industry that could be accused of muddying their terms with opaque language, swingeing vagueness and impenetrable layers of complexity, the huge and growing societal power of social media platforms — and these three giants specifically — is bringing them into contact with regulators’ spotlights, more and more.

Earlier this week, for example, Facebook and Twitter were criticized for continued failings to promptly remove hate speech from their platforms in Germany. The government there has now proposed a new law aimed at standardizing social media platforms’ content moderation processes to ensure compliance in future.

Meanwhile growing awareness of how algorithms that social media firms use to distribute content on their platforms can encourage and amplify the spread of ‘fake news’ continues to ruffle feathers — and is starting to land on political agendas.

In recent years social media giants have also come under increased pressure to do more to help government agencies in the region combat terrorism. With great power, it would seem, comes increasing societal and regulatory responsibilities for social media platforms.

Mark Zuckerberg’s recent open letter — which sought to reframe some of the societal divisions that have been demonstrably exacerbated by social networking as a need to further embed social networking structures into human societies to harness even more of people’s activity — is unlikely to be the last public pronouncement the Facebook CEO feels moved to make as social networking platforms and political discourse are wound ever tighter.",Social Media,TechCrunch,https://techcrunch.com/2017/03/17/social-media-firms-given-a-month-to-fix-consumer-rights-issues-in-europe/,"Social Media giants such as Facebook, Twitter and Google have been placed under increasing pressure by EU regulators to address issues such as unfair terms and conditions, fraud and scams, and hidden long term subscription fees, or face potential enforcement action.",Equality & Justice
134,Child health advocates call for Facebook to shutter Messenger Kids app – TechCrunch,"The slings and arrows of outrage keep flying at Facebook. Today a coalition of child health advocates has published an open letter addressing CEO Mark Zuckerberg and calling for the company to shutter Messenger Kids: Aka the Snapchat-ish comms app it launched in the US last December — targeted at the under 13s.

At the time Facebook described Messenger Kids as an “easier and safer way” for kids to video chat and message with family and friends “when they can’t be together in person” — and said the product had been “co-developed with parents, kids and experts”.

The video chat and messaging app includes a child-friendly selection of augmented reality lenses, emoji, stickers and manually curated GIFs for spicing up family messaging.

At launch Facebook also emphasized there were “no ads” or paid content downloads inside the app, and also claimed: “Your child’s information isn’t used for ads.”

Though that particular message coming from a people-profiling ad giant whose business model entirely depends on encouraging usage of its products in order to harvest user data for ad targeting purposes can only hold so much water. And the company has been accused of trying to use Messenger Kids as, essentially, a ‘gateway drug’ to familiarize preschoolers with its products — to have a better chance of onboarding them into its ad-targeting mainstream product when they become teenagers.

A study conducted by UK media watchdog Ofcom last fall has suggested that use of social media by children younger than 13 is on the rise — despite social networks typically having an age limit of 13-years-old for signups. (In the EU, the incoming GDPR introduces a 13-years age-limit on kids being able to consent to use social media themselves, though Member States can choose to raise the limit to 16 years.)

In practice there’s little to stop kids who have access to a mobile device downloading and signing up for apps and services themselves — unless their parents are actively policing their device use. (Facebook says it closes the accounts of any underage Facebook users when it’s made aware of them.) And concern about the impact of social media pressures on children has been rising.

Earlier this month, for example, the UK government’s Children’s Commissioner for England called for parents to ban their kids from using the Snapchat messaging app — citing concerns over addictive features and cyber bullying.

With Messenger Kids Facebook may well be spying an opportunity to try to outmanoeuvre its teen-focused rival by winning over parents with a dedicated app that bakes in parental controls.

However this strategy of offering a sandboxed environment for kids to message with parentally approved contacts isn’t winning over everyone.

Spearheading a campaign against Facebook Messenger Kids, Boston-based not-for-profit the Campaign for a Commercial-Free Childhood has gathered together a coalition of around 100 child health advocates and groups to sign its open letter. It’s also running a public petition — under the slogan ‘no Facebook for five year olds’.

In the letter the group describes it as “particularly irresponsible” of Facebook to have launched an app targeting preschoolers at a time when they say there is “mounting concern about how social media use affects adolescents’ wellbeing”.

Last week, for example, a study conducted by researchers at San Diego State University found that teens who spent more time on social media, gaming, texting and video-chatting on their phones were not as happy as those who played sports, went outside and interacted with people face to face.

“Younger children are simply not ready to have social media accounts,” the coalition argues in the letter. “They are not old enough to navigate the complexities of online relationships, which often lead to misunderstandings and conflicts even among more mature users. They also do not have a fully developed understanding of privacy, including what’s appropriate to share with others and who has access to their conversations, pictures, and videos.”

They also argue that Facebook’s Messenger Kids app is likely to result in young kids spending more time using digital devices.

“Already, adolescents report difficulty moderating their own social media use,” they write. “Messenger Kids will exacerbate this problem, as the anticipation of friends’ responses will be a powerful incentive for children to check – and stay on – a phone or tablet.

“Encouraging kids to move their friendships online will interfere with and displace the face-to-face interactions and play that are crucial for building healthy developmental skills, including the ability to read human emotion, delay gratification, and engage with the physical world.”

The group goes on to rebut Facebook’s claims that Messenger Kids helps brings remote families closer — by pointing out that a dedicated Facebook app is not necessary for children to keep in touch with long distance relatives, and citing the plethora of alternative options that can be used for that (such as using a parents’ Facebook or Skype account or Apple’s FaceTime or just making an old fashioned telephone call) which do not require kids to have their own account on any app.

“[T]he app’s overall impact on families and society is likely to be negative, normalizing social media use among young children and creating peer pressure for kids to sign up for their first account,” they argue, adding: “Raising children in our new digital age is difficult enough. We ask that you do not use Facebook’s enormous reach and influence to make it even harder. Please make a strong statement that Facebook is committed to the wellbeing of children and society by pulling the plug on Messenger Kids.”

Asked for a response to the group’s call to close down Messenger Kids, a Facebook spokesperson sent us the following email statement — reiterating its messaging around the product at the time it launched:

Messenger Kids is a messaging app that helps parents and children to chat in a safer way, with parents always in control of their child’s contacts and interactions. Since we launched in December we’ve heard from parents around the country that Messenger Kids has helped them stay in touch with their children and has enabled their children to stay in touch with family members near and far. For example, we’ve heard stories of parents working night shifts being able read bedtime stories to their children, and mums who travel for work getting daily updates from their kids while they’re away. We worked to create Messenger Kids with an advisory committee of parenting and developmental experts, as well as with families themselves and in partnership with the PTA. We continue to be focused on making Messenger Kids be the best experience it can be for families. We have been very clear that there is no advertising in Messenger Kids.

Discussing what evidence there is to support concerns over the development impact of digital devices on preschool children, John Oates, a senior lecturer in developmental psychology at the Open University who specializes in early childhood, told us: “The difficulty is that we have very high profile anecdotal cases [of social media concern, where the specific risk is tiny vs the total volume of chats being sent]… But, clearly the harm is potentially great — and the real issue is balancing risks and harm.”

“There is very little large scale evidence around actual developmental impacts on children. And I think that’s a problem — and it’s difficult to know quite how one would research that anyway. In, to isolate cause and effect in this area is really, really difficult. Because children differentially access and use these social media because of their differing profiles — let’s call them personality profiles.

“So some children are more likely to be drawn to use social media and then some of those children are more likely to use it in negative ways, and then some of those children are more likely to be then exposed, as a result, to risk. So the cause and effect chain that’s involved is very complex.”

Oates also points out that younger children, in the 6 to 12 years age range which Facebook Messenger Kids targets, are firstly not necessarily aware of the risks and potential harms, and secondly are also “not cognitively well able to analyze, rationally, the risks and make risk-free decisions”.

“So I think there is a difficulty around all social media in terms of children getting access to it when they’re not aware of the risks. Whether they could be educated better or not is a difficult question — because if they’re not cognitively able to make rational, risk-based judgements… it could be argued that no matter what parents do, and no matter what education does… this is still risky for children,” he said.

The other issue he raises as being a point of discussion and concern for child psychologists is the extent to which extensive use of social media might be a problem by taking children away from other activities that may be more valuable.

“There is a concern there, but there again it comes back to what differences in children predisposed them to get very involved in Snapchat and other social media, Facebook, etc,” he told TechCrunch. “And it seems that one of the main motivations for children is a social one, to feel that they are part of a social group that they can identify with.

“We know that children in this age range are very sensitive to peer approval and peer disapproval. So they’re often quite aware of negative messaging on social media — even if they haven’t experienced it, they’ll know about it. Because this is very salient to them. So it’s really the nature of their social nexus that they form that’s probably the formative element and the messaging within that.”

“It is a real challenge to unpick cause and effect in this area,” he added. “That’s why answering these questions… is extremely difficult. But I think what we can do, on the other hand, is we can draw tentative conclusions from what we do know about children’s development [such as the strong influence of peers].”

Oates also raises the potential of apps that enable children to form social networks digitally (vs only being able to do that face to face) as being a positive change — “for children seeing the world from a whole variety of perspectives and seeing bits of the world that they wouldn’t otherwise see; seeing other children’s points of views and so on and so forth”.

“There’s a lot of potential there and I wouldn’t be just simply negative about this. But recognize that with anything that opens up children’s worlds there are risks as well as benefits,” he added.

This article was updated to specify that the Campaign for a Commercial-Free Childhood is a not-for-profit that has been co-ordinating the campaign efforts against Messenger Kids",Social Media,TechCrunch,https://techcrunch.com/2018/01/30/child-health-advocates-call-for-facebook-to-shutter-messenger-kids-app/,"The Campaign for a Commercial-Free Childhood has expressed concern that Facebook's Messenger Kids app could lead to children spending more time on digital devices, potentially interfering with their development and creating peer pressure for them to get their own social media accounts.",Social Norms & Relationships
135,Tumblr confirms 84 accounts linked to Kremlin trolls – TechCrunch,"Tumblr has confirmed that Kremlin trolls were active on its platform during the 2016 US presidential elections.

In a blog post today the social platform writes that it is “taking steps to protect against future interference in our political conversation by state-sponsored propaganda campaigns”.

The company has also started emailing users who interacted with 84 accounts it now says it has linked to the Russian trollfarm, the Internet Research Agency (IRA).

In the blog post it says it identified the accounts last fall — and “notified law enforcement, terminated the accounts, and deleted their original posts”.

“Behind the scenes, we worked with the Department of Justice, and the information we provided helped indict 13 people who worked for the IRA,” it adds.

In an email sent to a user, which was passed to TechCrunch to review, the company informs the individual they “either followed one of [11] accounts linked to the IRA, or liked or reblogged one of their posts”.

“As part of our commitment to transparency, we want you to know that we uncovered and terminated 84 accounts linked to Internet Research Agency or IRA (a group closely tied to the the Russian government) posing as members of the Tumblr community,” the email begins.

“The IRA engages in electronic disinformation and propaganda campaigns around the world using phony social media accounts. When we uncovered these accounts, we notified law enforcement, terminated the accounts, and deleted their original posts.”

Last month Buzzfeed News — working with researcher, Jonathan Albright, from the Tow Center for Digital Journalism at Columbia University — claimed to have unearthed substantial Kremlin troll activity on Tumblr’s meme-laden platform — identifying what they dubbed as “a powerful, largely unrevealed network of Russian trolls focused on black issues and activism” which they said dated back to early 2015.

The trolls were reported to be using Tumblr to push anti-Clinton messages, including by actively promoting Democrat rival Bernie Sanders.

Decrying racial injustice and police violence in the US was another theme of the Russian-linked content.

Since then The Daily Beast has also reported on leaked data from the IRA which also implied agents at the trollfarm had used Tumblr — and also Reddit — to spread political propaganda to target the 2016 US election.

Those IRA leaks suggested the IRA had created at least 21 Tumblr accounts — and included names replete with slang terms, including some accounts listed in the user email we’ve reviewed.

Tumblr, which is owned by TechCrunch’s parent company Oath, did not respond to an email we sent to their press office last month asking about possible Kremlin activity on its platform.

In today’s public post, the company writes: “As far as we can tell, the IRA-linked accounts were only focused on spreading disinformation in the U.S., and they only posted organic content. We didn’t find any indication that they ran ads.”

As well as emailing affected users, Tumblr says it will be keeping a public record of usernames linked to the IRA or “other state-sponsored disinformation campaigns”.

The full list of 84 Kremlin accounts on its public page is as follows:

1-800-gloup 4mysquad previously known as: thomascestevez aaddictedtoblackk previously known as: kathyzunigaoverhere, soaddictedtoblackk artautumn awesomelizzyjenningsthings bastardpigeon previously known as: pigeonbastard belindadoingitgood bellaxiao previously known as: blogmadworldlove bellygangstaboo best-usa-today bestofninegag previously known as: apolicethepolicethings black-to-the-bones blackness-by-your-side previously known as: black-galaxy-magic, fullyfurrymiracle, u4guy, ufo-pilot-and-his-sexy-spouse blacknproud previously known as: in-the-mirror-world, indiebirdsblog, no-place-for-homophobia blacktolive bleepthepolice cartnsncreal previously known as: feelmydragonballs chantelreneeblm coldfirefan coolmagnificentbets cybercollectionnerdstuff delroyheater previously known as: delroyheater32, mastercucumber, scrumptiousluminarytimemachine, sexinstructor666, sexintructor destinyrush previously known as: delightfullyghostlysong down-to-venus previously known as: teenageflowerluminary eugeny-a-antonov everydaycutenesstherapy fedupwithlying previously known as: badgyalforyou funkycodex previously known as: craftykryptonitedelusion gentlexnoise previously known as: slakerglitch, superblydopepatrol getaninspiration gogomrbrown previously known as: go-mrbrown, infectedv0ice, todd-la-death gottimechillinaround previously known as: howardhenderson guns4l1fe previously known as: 4lifeme, guns4lifeme, unabashedpolicerunaway honestlyyoungpersona honeyjenna hustleinatrap previously known as: thenaturecanpost, tumblercube ineverstopexploringblog previously known as: charlenefletcher info-mix previously known as: americanstatistics, crazypolitician, girlsagainst, illegalmom, just-stat, rochelbarr itwontbeeasyme jamesjdelvecchio jenningsmiracle joycherryblossom previously known as: joyfullyhappycherryblossom justmarilynadventure previously known as: justmarylinthings kingbeardplayer previously known as: kirksgossplayer, placeofminimalthings kirstenbrownstories lagonegirl lamcbride loveboholiveboho massivelydopepolice massmedear previously known as: massblog021 melanin-diary previously known as: blackjourney2justice, dontshootus, jusslittlestoner missberrianstuff mooseblogtimes morningwoodz previously known as: 5cubes, bangbangempire, empireofweird, gifemprireohh, innerpicsempire, picsempire nadiquitegoodatit previously known as: runlovernad nevaehtyler previously known as: laserenita noteverythingiswhite previously known as: ashleyfsilver poligraphme previously known as: poligraphmy postingwhileblack previously known as: ghettablasta, heygeraldmartinjohanssen, honestinjun, nativewolveshere rebellloudwiththecrowd previously known as: massivelystrangetyrant ricordio rikki-ri sassydreamlandcloud scentedballoonpanda sergpolozov shoutoutworldwide previously known as: blackprideworldwide, krispymentalitycowboy shyshannonstuff skullofjustice previously known as: naughtykermit, ryanbutlersstuff, usnationaldebt skwad55 somestuff4ya previously known as: somestufffoya, unitedposters starling-all-black-all-day stopropaganda sumchckn previously known as: blondeinpolitics, blvckcommunity, classylgbthomie, hwuudoin, politixblondie summerinsomniac superblygun swagintherain previously known as: blacklivesmatterusa, carzwithgirlz the-real-eye-to-see thetrippytrip previously known as: matrixpath, themostpost thingstolovefor previously known as: the-inner-mirror this-truly-brutal-world previously known as: awesomewhitepearl, free-mind-and-soul voteforwest2020 previously known as: mrbadasscat weproudto2black wevanessastewartstuff yanbig Tumblr is urging users to be vigilant for ongoing troll activity — linking to a News Literacy Project checklist for spotting their tricks, and encouraging people to “be skeptical of things you read” and to use fact-checkers such as Snopes.

It also suggests users step in and “correct the record” when they see others spreading misinformation, regardless of whether they believe it’s being done intentionally or not.

Concluding its email to the user who had unwittingly engaged with 11 of the identified IRA accounts, Tumblr adds: “We deleted the accounts but decided to leave up any reblog chains so that you can curate your own Tumblr to reflect your own personal views and perspectives.

“Democracy requires transparency and an informed electorate and we take our disclosure responsibility very seriously. We’ll be aggressively watching for disinformation campaigns in the future, take the appropriate action, and make sure you know about it.”

Asked how he feels to learn Kremlin trolls had unknowingly infiltrated his Tumblr feeds, the user told us: “It’s unsettling, although maybe not surprising, that we legitimize and signal boost bad actors on social platforms by ‘liking’ or reposting content that doesn’t appear to have any political agenda at first glance.”",Social Media,TechCrunch,https://techcrunch.com/2018/03/23/tumblr-confirms-84-accounts-linked-to-kremlin-trolls/,"Kremlin trolls were active on Tumblr during the 2016 US presidential election, with 84 accounts linked to the IRA being identified and deleted. This has led to users being informed and warned against engaging with similar accounts in the future, as well as the need to be vigilant and skeptical of the information they consume online.","Information, Discourse & Governance"
136,"Facebook cuts off NYU researcher access, prompting rebuke from lawmakers – TechCrunch","Facebook shut down accounts belonging to two academic researchers late Tuesday, cutting off their ability to study political ads and misinformation on the world’s biggest social network.

The company accused the academics of engaging in “unauthorized scraping” and compromising user privacy on the platform, claims that Facebook’s many critics are slamming as a thin pretense for killing the transparency work.

The company took action against Laura Edelson and Damon McCoy, two well-known researchers affiliated with NYU’s Cybersecurity for Democracy project who have long sparred with the company. The move cuts off their access to Facebook’s Ad Library — one of the company’s only meaningful transparency efforts to date — and data on popular posts from the social media monitoring service CrowdTangle.

Facebook has a history with Edelson and McCoy. The company served the pair cease and desist letters just weeks before the 2020 election, calling on the team to disable an opt-in browser tool called Ad Observer and unpublish their findings. Ad Observer is a browser tool anyone can install that’s designed to give researchers a rare glimpse into how Facebook targets the ads that have transformed it into a trillion-dollar company.

“Over the last several years, we’ve used this access to uncover systemic flaws in the Facebook Ad Library, identify misinformation in political ads including many sowing distrust in our election system, and to study Facebook’s apparent amplification of partisan misinformation,” Edelson said on Twitter.

“By suspending our accounts, Facebook has effectively ended all this work. Facebook has also effectively cut off access to more than two dozen other researchers and journalists who get access to Facebook data through our project, including our work measuring vaccine misinformation with the Virality Project and many other partners who rely on our data.”

The incident set off a fresh round of criticism about the company’s preference for opacity over transparency when it comes to some of the more dangerous behavior that the platform incubates.

By Wednesday, Facebook’s actions had attracted the attention of some members of Congress. Sen. Ron Wyden (D-OR) criticized Facebook’s decision to punish the researchers under the pretense of protecting users in light of the company’s long history of invasive privacy practices. Wyden also called Facebook’s bluff over its claim that revoking researcher access is an effort to comply with a privacy order from the FTC that the company was issued for its previous user privacy violations.

After years of abusing users' privacy, it's rich for Facebook to use it as an excuse to crack down on researchers exposing its problems. I've asked the FTC to confirm that this excuse is as bogus as it sounds. https://t.co/eHuPiVYFe9 — Ron Wyden (@RonWyden) August 4, 2021

Sen. Mark Warner (D-VA) also weighed in on Facebook’s latest controversy, calling the decision “deeply concerning.” Warner praised independent researchers for “consistently [improving] the integrity and safety of social media platforms by exposing harmful and exploitative activity.”

“It’s past time for Congress to act to bring greater transparency to the shadowy world of online advertising, which continues to be a major vector for fraud and misconduct,” Warner said.

Firefox developer Mozilla came to the defense of Ad Observer on Wednesday, noting that the company “reviewed it twice, conducting both a code review and examining the consent flow” before recommending the browser extension through its storefront. In a blog post, Mozilla’s chief security officer stated that Facebook’s claims “simply do not hold water.”

A number of free press organizations, researchers and misinformation experts also condemned Facebook’s decision Wednesday. “Facebook’s cavalier approach to privacy enabled it to become so dominant,” The Markup’s Julia Angwin and Nabiha Syed wrote in a joint statement.

“But now, when independent researchers want to interrogate that platform and the influence it commands, Facebook is propping up user privacy as a shield to hide behind.”",Social Media,TechCrunch,https://techcrunch.com/2021/08/04/facebook-ad-observatory-nyu-researchers/,"Facebook's decision to shut down accounts belonging to two academic researchers has cut off their ability to study political ads and misinformation on the platform, and has been harshly criticized for killing the transparency work and compromising user privacy.",Security & Privacy
137,Scientists Say There’s a Clear Link Between Facebook and Depression,"Image by Futurism Mental Health

A growing body of scientific evidence suggests there may be a link between social media use and depression, NPR reports, with depression and suicide rates rising among teens for over a decade — and some experts believe Facebook in particular is attempting to obfuscate any correlation between poor mental health outcomes and social media use.

It’s a pertinent topic, since Facebook is trying to bring its products to a younger and younger demographic. Just last week, a group of 40 state attorneys general urged CEO Mark Zuckerberg to ditch plans to create a version of Instagram (which is owned by Facebook) aimed at under 13-year-olds, according to NBC.

Zuckerberg, however, called the research into question during a March congressional hearing that also included Twitter CEO Jack Dorsey and Google CEO Sundar Pichai.

“I don’t think that the research is conclusive on that,” he told representative Cathy McMorris Rodgers (R-WA) during the hearing, after she asked him to acknowledge the connection between children’s worsening mental health and social media use.

Advertisement

Advertisement

Researchers disagree with Zuckerberg’s assessment.

“The correlational evidence showing that there is a link between social media use and depression is pretty definitive at this point,” Diego State University psychology professor Jean Twenge told NPR.

“The largest and most well-conducted studies that we have all show that teens who spend more time on social media are more likely to be depressed or unhappy,” he added.

Still, the evidence is far from conclusive. Objective data is hard to come by, and funding is sparse.

Advertisement

Advertisement

And that’s frustrating lawmakers.

“You enjoy an outdated liability shield that incentivizes you to look the other way or take half-measures while you make billions at the expense of our kids, our health and the truth,” representative Kathy Castor (D-FL) said during the hearing, as quoted by NPR.

During the same hearing, Zuckerberg revealed that his company is internally researching the mental health effects of social media on children — but isn’t willing to share its findings.

“I believe that they have done the research,” McMorris Rodgers, who attended the March hearing and grilled Zuckerberg on the topic, told NPR. “They’re not being transparent.”

Advertisement

Advertisement

McMorris Rodgers is of the belief that Facebook is far more worried about its bottom line — and that includes its motivation to sell advertisements based on engagement.

The company’s profits are directly linked to how many people are engaged and how much they are engaged. And that often comes at the cost of not being aware of the mental health of its users.

“Basically all of the things that would contribute to these platforms being healthier for people to use, which is basically spend less time, don’t follow strangers, don’t spend time passively scrolling through this random feed that’s being suggested to you,” University of Pennsylvania psychology professor Melissa Hunt told NPR. “That completely undermines their whole business model.”

As of late, academics who were first contacted by Facebook have heard little about the company’s efforts to study the effects of its platforms on mental health.

Advertisement

Advertisement

We’re collectively spending far more time on social media than ever before, and the COVID-19 pandemic hasn’t helped.

It should be up to Facebook — the company that’s directly profiting — to lead the charge into investigating those effects transparently.

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Social Media,Futurism,https://futurism.com/neoscope/scientists-link-facebook-depression,"Research suggests a link between social media use and depression, yet Facebook CEO Mark Zuckerberg has questioned the evidence. Meanwhile, lawmakers are frustrated with the lack of transparency from the company, and the potential of its platforms to harm mental health is concerning given their reliance on engagement for profit.",Mental Health
138,"Facebook is being leaned on by US, UK, Australia to ditch its end-to-end encryption expansion plan – TechCrunch","Facebook is being leaned on by US, UK, Australia to ditch its end-to-end encryption expansion plan

Here we go again. Western governments are once again dialing up their attack on end-to-end encryption — calling for either no e2e encryption or backdoored e2e encryption so platforms can be commanded to serve state agents with messaging data in “a readable and usable format.”

U.S. Attorney General William Barr, acting U.S. Homeland Security Secretary Kevin McAleenan, U.K. Home Secretary Priti Patel and Australia’s minister for home affairs, Peter Dutton, have co-signed an open letter to Facebook calling on the company to halt its plan to roll out e2e encryption across its suite of messaging products. Unless the company can ensure what they describe as “no reduction to user safety and without including a means for lawful access to the content of communications to protect our citizens,” per a draft of the letter obtained by BuzzFeed ahead of publication later today.

If platforms have e2e encryption, a “means for lawful access” to the content of communications sums to a backdoor in the crypto — presumably along the lines of the “ghost protocol” that U.K. spooks have been pushing for the past year. AKA an “exceptional access mechanism” that would require platforms CC’ing a state/law enforcement agent as a silent listener to eavesdrop on a conversation on warranted request.

Facebook-owned WhatsApp was one of a number of tech giants joining an international coalition of civic society organizations, security and policy experts condemning the proposal as utter folly earlier this year.

The group warned that demanding a special security hole in encryption for law enforcement risks everyone’s security by creating a vulnerability which could be exploited by hackers. Or indeed, service providers themselves. But the age-old “there’s no such thing as a backdoor just for you” warning appears to have fallen on deaf ears.

In their open letter to Facebook, the officials write: “Companies should not deliberately design their systems to preclude any form of access to content, even for preventing or investigating the most serious crimes. This puts our citizens and societies at risk by severely eroding a company’s ability to detect and respond to illegal content and activity, such as child sexual exploitation and abuse, terrorism, and foreign adversaries’ attempts to undermine democratic values and institutions, preventing the prosecution of offenders and safeguarding of victims. It also impedes law enforcement’s ability to investigate these and other serious crimes.”

Of course, Facebook is not the only messaging company using e2e encryption, but it’s in the governments’ crosshairs now on account of a plan to expand its use of e2e crypto — announced earlier this year, as part of a claimed “pivot to privacy.” And, well, on account of it having two billion+ users.

The officials claim in the letter that “much” of the investigative activity, which is critical to protecting child safety and fighting terrorism, “will no longer be possible if Facebook implements its proposals as planned.”

“Risks to public safety from Facebook’s proposals are exacerbated in the context of a single platform that would combine inaccessible messaging services with open profiles, providing unique routes for prospective offenders to identify and groom our children,” they warn, noting that the Facebook founder expressed his own concerns about finding “the right ways to protect both privacy and safety.”

In March, Mark Zuckerberg also talked about building “the appropriate safety systems that stop bad actors as much as we possibly can within the limits of an encrypted service.”

Which could, if you’re cynically inclined, be read as Facebook dangling a carrot to governments — along the lines of: “We might be able to scratch your security itch, if your regulators don’t break up our business.”

Ironically enough, the high-profile intervention by officials risks derailing Facebook’s plan to unify the backends of its platforms — widely interpreted as a play to make it harder for regulators to act on competition concerns and break up Facebook’s business empire along messaging product lines: Facebook, WhatsApp, Instagram.

Or, well — alternative scenario — Facebook could choose to strip e2e crypto from WhatsApp, which is currently the odd one out in its messaging suite on account of having proper crypto. Governments would sure be happy if it did that. But it’s the opposite of what Zuckerberg has said he’s planning.

The government is demanding backdoor access to the private communications of 1.5 billion people using #WhatsApp. If @Facebook agrees, it may be the largest overnight violation of privacy in history. https://t.co/qkxO1pJuUh — Edward Snowden (@Snowden) October 3, 2019

Curiously, the draft letter makes no mention of platform metadata. Which is not shielded by even WhatsApp’s e2e encryption. And thus can be extracted — via a warrant — in a readable format for legit investigative purposes. And let’s not forget U.S. spooks are more than happy to kill people based on metadata.

Instead the officials write: “We must find a way to balance the need to secure data with public safety and the need for law enforcement to access the information they need to safeguard the public, investigate crimes, and prevent future criminal activity. Not doing so hinders our law enforcement agencies’ ability to stop criminals and abusers in their tracks.”

The debate is being framed by spooks and security ministers as all about content.

Yet a scrambled single Facebook backend would undoubtedly yield vastly more metadata, and higher-resolution metadata, on account of triangulation across the services. So it really is a curious omission.

We’ve reached out to Facebook for its reaction to the letter. BuzzFeed reports that it sent a statement in which it strongly opposes government attempts to build backdoors. So if Facebook holds firm to that stance it looks like another big crypto fight could well be brewing. À la Apple versus the FBI.

Update: Facebook has now sent us this statement:

We believe people have the right to have a private conversation online, wherever they are in the world. As the US and UK governments acknowledge, the CLOUD Act allows for companies to provide available information when they receive valid legal requests and does not require companies to build backdoors. We respect and support the role law enforcement has in keeping people safe. Ahead of our plans to bring more security and privacy to our messaging apps, we are consulting closely with child safety experts, governments and technology companies and devoting new teams and sophisticated technology so we can use all the information available to us to help keep people safe. End-to-end encryption already protects the messages of over a billion people every day. It is increasingly used across the communications industry and in many other important sectors of the economy. We strongly oppose government attempts to build backdoors because they would undermine the privacy and security of people everywhere.

Bilateral Data Access Agreement

In another announcement being made today, the U.K. and the U.S. have signed a “world first” Bilateral Data Access Agreement that’s intended to greatly speed up electronic data access requests by their respective law enforcement agencies.

The agreement is intended to replace the current process, which sees requests for communications data from law enforcement agencies submitted and approved by central governments via a process called Mutual Legal Assistance — which can take months or even years.

Once up and running, the claim is the new arrangement will see the process reduced to a matter of weeks or even days.

The agreement will work reciprocally with the U.K. getting data from U.S. tech firms, and the U.S. getting access from U.K. communication service providers (via a U.S. court order).

Any request for data must be made under an authorisation in accordance with the legislation of the country making the request and will be subject to independent oversight or review by a court, judge, magistrate or other independent authority, per the announcement.

The U.K. also says specifically that it has obtained “assurances” which are in line with the government’s continued opposition to the death penalty in all circumstances. Which is only mildly reassuring given the home secretary’s previous views on the topic.

The announcement also makes a point of noting the data access agreement does not change anything about how companies can use encryption — nor prevent them from encrypting data.

For interfering with proper encryption the plan among this trio of signals intelligence allies is, seemingly, to reach for the old PR lever and apply public pressure. So, yeah, here we go again.",Social Media,TechCrunch,https://techcrunch.com/2019/10/03/facebook-is-being-leant-on-by-us-uk-australia-to-ditch-its-end-to-end-encryption-expansion-plan/,"The US, UK, and Australia are pressuring Facebook to abandon the end-to-end encryption expansion plan, going so far as to create a Bilateral Data Access Agreement to speed up requests for communications data from law enforcement agencies. This could lead to the largest overnight violation of privacy in history and put risk public safety by eroding a company",Security & Privacy
139,Byte tops a million downloads amid spam issues and content concerns – TechCrunch,"New short-form video app Byte, heralded as Vine’s successor, is off to a strong start despite its issues. The app, built by Vine co-founder Dom Hofmann, brings back the six-second videos made popular by Vine which was shut down in late 2016 after Twitter’s acquisition of the popular video-sharing platform. According to new data from Sensor Tower, Byte’s launch has been well-received with over 1.3 million downloads during its first week alone. The U.S. delivered the bulk of these new installs, followed by Great Britain then Canada.

The U.S. contributed 912,000 downloads, or 70% of the installs, the report says. While Great Britain and Canada offered 7% and 6% of installs, respectively. The majority of Byte downloads were also on iOS, with 950,000 iOS downloads compared with 350,000 installs on Android.

App Annie’s numbers differed a bit, but also found that Byte topped 1 million total downloads on iOS and Android through Sunday, Feb. 2.

Sensor Tower’s new report compares Byte’s figures to Vine’s debut in January 2013, which only saw a total of 775,000 installs during its first week on iOS. However, that doesn’t mean Byte is soon to be a much more popular app than its predecessor.

For starters, the app market has grown over the years to include more users and more devices. In 2016, for example, only 2.5 billion users worldwide had smartphones. Now that number tops 3.5 billion. In addition, Vine launched as an unknown startup into a market that had yet to really embrace short-form. Byte, on the other hand, not only takes advantage of its association with Vine, it also arrives at a time when short-form video is now hugely popular thanks to Vine’s success and TikTok, the latter which became the No. 4 most-downloaded app of 2019.

Despite its solid launch numbers, Byte’s debut was not unmarred.

The app immediately saw massive comment spam as bots rushed to fill comment sections with follow requests (and follow for follows), including requests from pornbots. Byte’s early adopters also started snatching up coveted usernames — those belonging to real people, ranging from tech folks to celebs like Taylor Swift and other prominent figures like Trump, Bezos, Tiger Woods and others, Slate reported. The company quickly moved to acknowledge the problem and promised a cleanup was underway.

But that’s not Byte’s only issue. The app originally launched with a 12+ age rating, yet was immediately filled with adult humor alongside videos from obvious minors. Surfaced in Byte’s popular feed were videos with dick jokes and sexual humor, and problematic content including distasteful jokes about child abuse and coronavirus victims.

To give you a sense of Byte’s content, a perusal of the “Popular” feed on Friday surfaced a video featuring a teenaged-to-young adult boy joking “if you call me a slut in the comments one more time, I’m going to suck all your d***s.” Another teenaged-appearing boy joked about a prostate exam performed by his dad. A boy of a similar age asks if anyone had ever pooped into someone’s….and then the video cuts off.

It’s unclear if the boys in question are 18 or older, but seeing these — as well as so many other videos featuring dick jokes — followed by videos filmed by very young children was an uncomfortable experience.

The Popular feed also featured a video of a drone trying to fly a dildo into a sex doll. One video made light of child abuse, with a man viciously hitting the phone screen. The video is filmed from above, giving you the child’s perspective. The caption read: “when a child brings up a valid argument.”

Two other videos featured toddlers – one of a dad knocking the baby down, perhaps on purpose, as they played ball, only to later fall himself. Another depicted someone spraying a baby in the face with the kitchen sink nozzle, followed by the baby crying.

One video made fun of Chinese people dying from coronavirus. Another showed a teen smoking a joint, then hearing a siren and running.

Vine videos were strange and dumb in their own way, but the best weren’t typically crass or dirty. Think: duck army, eyebrows on fleek, hate blockers, what are those, Squidward hits the dab, and so on.

Given the amount of adult humor, Byte’s lack of an age-gate and the app’s 12+ rating was concerning. (Byte updated to 17+ over the weekend. The above videos aren’t surfacing now. We know Apple was taking a look at its content).

Another potential concern was that a lot of Byte’s content was recycled from elsewhere — there were clips from YouTube, FunnyorDie, TV shows, and even TikTok — logo and all. Users also reposted Snapchat videos and memes from around the web.

With the changes to the age rating, it seems Byte may have been alerted to some of its more problematic content. Byte now puts a curated Spotlight feed at the top of its discovery page, where videos curation is improved.

The company on Friday also published the initial details on its Partner Program, touting the potential for revenue other platforms don’t provide.

TikTok, by comparison, hasn’t quite figured out how to monetize — its app has seen 1.65 billion downloads to date, but only grossed $176.9 million in 2019. However, TikTok’s elite are making names for themselves that allow them to grow their brand in other ways, including by directing users to other social channels like YouTube and Instagram, and even doing meet-and-greets with fans.

Whether a whole new world of Byte stars emerges remains to be seen.",Social Media,TechCrunch,https://techcrunch.com/2020/02/03/byte-tops-a-million-downloads-amid-spam-issues-and-content-concerns/,"The launch of Byte saw massive comment spam, coveted usernames being taken, and inappropriate content with adult humor, distasteful jokes, and videos of children being exposed to a 12+ age-rated audience.",User Experience & Entertainment
140,British parliament presses Facebook on letting politicians lie in ads – TechCrunch,"In yet another letter seeking to pry accountability from Facebook, the chair of a British parliamentary committee has pressed the company over its decision to adopt a policy on political ad that supports flagrant lying.

In the letter Damian Collins, chair of the DCMS committee, asks the company to explain why it recently took the decision to change its policy regarding political ads — “given the heavy constraint this will place on Facebook’s ability to combat online disinformation in the run-up to elections around the world”.

Chair @DamianCollins has written to @facebook's Nick Clegg over changes to political advertising rules ahead of a potential General Election. Facebook have dropped a ban on “deceptive, false or misleading content” in political ads. Read more: https://t.co/1mA3d3uDnN pic.twitter.com/2L88mQMDb2 — Digital, Culture, Media and Sport Committee (@CommonsCMS) October 22, 2019

“The change in policy will absolve Facebook from the responsibility of identifying and tackling the widespread content of bad actors, such as Russia’s Internet Research Agency,” he warns, before going on to cite a recent tweet by the former chief of Facebook’s global efforts around political ads transparency and election integrity who has claimed that senior management ignored calls from lower down for ads to be scanned for misinformation.

“I also note that Facebook’s former head of global elections integrity ops, Yael Eisenstat, has described that when she advocated for the scanning of adverts to detect misinformation efforts, despite engineers’ enthusiasm she faced opposition from upper management,” writes Collins.

Facebook hired me to head Elections Integrity ops for political ads. I asked if we could scan ads for misinfo. Engineers had great ideas. Higher ups were silent. Free speech is b.s. answer when FB takes $ for ads. Time to regulate ads same as tv and print.https://t.co/eKJmH7Sa7r — Yael Eisenstat (@YaelEisenstat) October 9, 2019

In a further question, Collins asks what specific proposals Eisenstat’s team made; to what extent Facebook determined them to be feasible; and on what grounds were they not progressed.

He also asks what plans Facebook has to formalize a working relationship with fact-checkers over the long run.

A Facebook spokesperson declined to comment on the DCMS letter, saying the company would respond in due course.

In a naked display of its platform’s power and political muscle, Facebook deployed a former politician to endorse its ‘fake ads are fine’ position last month — when head of global policy and communication, Nick Clegg, who used to be the deputy prime minister of the UK, said: ” We do not submit speech by politicians to our independent fact-checkers, and we generally allow it on the platform even when it would otherwise breach our normal content rules.”

So, in other words, if you’re a politician you get a green light to run lying ads on Facebook.

Clegg was giving a speech on the company’s plans to prevent interference in the 2020 US presidential election. The only line he said Facebook would be willing to draw was if a politician’s speech “can lead to real world violence and harm”. But from a company that abjectly failed to prevent its platform from being misappropriated to accelerate genocide in Myanmar that’s the opposite of reassuring.

“At Facebook, our role is to make sure there is a level playing field, not to be a political participant ourselves,” said Clegg. “We have a responsibility to protect the platform from outside interference, and to make sure that when people pay us for political ads we make it as transparent as possible. But it is not our role to intervene when politicians speak.”

In truth Facebook roundly fails to protect its platform from outside interference too. Inauthentic behavior and fake content is a ceaseless firefight that Facebook is nowhere close to being on top of, let alone winning. But on political ads it’s not even going to try — giving politicians around the world carte blanche to use outrage-fuelling disinformation and racist dogwhistles as a low budget, broad reach campaign strategy.

We’ve seen this before on Facebook of course, during the UK’s Brexit referendum — when scores of dark ads sought to whip up anti-immigrant sentiment and drive a wedge between voters and the European Union.

And indeed Collins’ crusade against Facebook as a conduit for disinformation began in the wake of that 2016 EU referendum.

Since then the company has faced major political scrutiny over how it accelerates disinformation — and has responded by creating a degree of transparency on political ads, launching an archive where this type of advert can be searched. But that appears as far as Facebook is willing to go on tackling the malicious propaganda problem its platform accelerates.

In the US, senator Elizabeth Warren has been duking it out publicly with Facebook on the same point as Collins rather more directly — by running ads on Facebook saying it’s endorsing Trump by supporting his lies.

There’s no sign of Facebook backing down, though. On the contrary. A recent leak from an internal meeting saw founder Mark Zuckerberg attacking Warren as an “existential” threat to the company. While, this week, Bloomberg reports that Facebook’s executive has been quietly advising a Warren rival for the Democratic nomination, Pete Buttigieg, on campaign hires.

So a company that hires politicians to senior roles, advises high profile politicians on election campaigns, tweaks its policy on political ads after a closed door meeting with the current holder of the office of US president, Donald Trump, and ignores internal calls to robustly police political ads, is rapidly sloughing off any residual claims to be ‘just a technology company’. (Though, really, we knew that already.)

In the letter Collins also presses Facebook on its plan to rollout end-to-end encryption across its messaging app suite, asking why it can’t limit the tech to WhatsApp only — something the UK government has also been pressing it on this month.

He also raises questions about Facebook’s access to metadata — asking whether it will use inferences gleaned from the who, when and where of e2e encrypted comms (even though it can’t access the what) to target users with ads.

Facebook’s self-proclaimed ‘pivot to privacy‘ — when it announced earlier this year a plan to unify its separate messaging platforms onto a single e2e encrypted backend — has been widely interpreted as an attempt to make it harder for antitrust regulators to break up its business empire, as well as a strategy to shirk responsibility for content moderation by shielding itself from much of the substance that flows across its platform while retaining access to richer cross-platform metadata so it can continue to target users with ads…",Social Media,TechCrunch,https://techcrunch.com/2019/10/22/british-parliament-presses-facebook-on-letting-politicians-lie-in-ads/,"The main undesirable consequence of Social Media discussed here is the ability of politicians to spread disinformation with no consequences. Facebook has adopted a policy that allows politicians to get away with false and misleading ads, and this decision has been heavily criticized by the DCMS committee, a British Parliamentary committee, who is pushing for accountability. Facebook has also been accused of using","Information, Discourse & Governance"
141,Facebook and the endless string of worst-case scenarios – TechCrunch,"Facebook has naively put its faith in humanity and repeatedly been abused, exploited, and proven either negligent or complicit. The company routinely ignores or downplays the worst-case scenarios, idealistically building products without the necessary safeguards, and then drags its feet to admit the extent of the problems.

This approach, willful or not, has led to its latest scandal, where a previously available API for app developers was harnessed by Trump and Brexit Leave campaign technology provider Cambridge Analytica to pull not just the profile data of 270,000 app users who gave express permission, but of 50 million of those people’s unwitting friends.

Facebook famously changed its motto in 2014 from “Move fast and break things” to “Move fast with stable infra” aka ‘infrastructure’. But all that’s meant is that Facebook’s products function as coded even at enormous scale, not that they’re built any slower or with more caution for how they could be weaponized. Facebook’s platform iconography above captures how it only sees the wrench, then gets shocked by the lightning on the other end.

Sometimes the abuse is natural and emergent, as when people grow envious and insecure from following the highlights of their peers’ lives through the News Feed that was meant to bring people together. Sometimes the abuse is malicious and opportunistic, as it was when Cambridge Analytica used an API designed to help people recommend relevant job openings to friends to purposefully harvest data that populated psychographic profiles of voters so they could be swayed with targeted messaging.

Whether it doesn’t see the disasters coming, makes a calculated gamble that the growth or mission benefits of something will far outweigh the risks, or purposefully makes a dangerous decision while obscuring the consequences, Facebook is responsible for its significant shortcomings. The company has historically cut corners in pursuit of ubiquity that left it, potentially knowingly, vulnerable to exploitation.

And increasingly, Facebook is going to lengths to fight the news cycle surrounding its controversies instead of owning up early and getting to work. Facebook knew about Cambridge Analytica’s data policy violations since at least August 2016, but did nothing but send a legal notice to delete the information.It only suspended the Facebook accounts of Cambridge Analytica and other guilty parties and announced the move this week in hopes of muting forthcoming New York Times and Guardian articles about the issue (articles which it also tried to prevent from running via legal threats.) And since, representatives of the company have quibbled with reporters over Twitter, describing the data misuse as a “breach” instead of explaining why it didn’t inform the public about it for years.

“I have more fear in my life that we aren’t going to maximize the opportunity that we have than that we mess something up” Zuckerberg said at a Facebook’s Social Good Forum event in November. Perhaps it’s time for that fear to shift more towards ‘what could go wrong’, not just for Zuck, but the leaders of all of today’s tech titans.

An Abridged List Of Facebook’s Unforeseen Consequences

Here’s an incomplete list of the massive negative consequences and specific abuses that stem from Facebook’s idealistic product development process. [Thanks to user suggestions, we’ve added some more in an upate]:

Each time, Facebook built tools with rosy expectations, only to negligently leave the safety off and see worst-case scenarios arise. In October, Zuckerberg already asked for forgiveness, but the public wants change.

Trading Kool-Aid For Contrarians

The desire to avoid censorship or partisanship or inefficiency is no excuse. Perhaps people are so addicted to Facebook that no backlash will pry them their feeds. But Facebook can’t treat this as merely a PR problem, a distraction from the fun work of building new social features, unless its employees are ready to shoulder the blame for the erosion of society. Each scandal further proves it can’t police itself, inviting government regulation that could gum up its business. Members of congress are already calling on Zuckerberg to testify.

Yet even with all of the public backlash and calls for regulation, Facebook still seems to lack or ignore the cynics and diverse voices who might foresee how its products could be perverted or were conceptualized foolishly in the first place. Having more minorities and contrarians on the teams that conceive its products could nip troubles in the bud before they blossom.

“The saying goes that optimists tend to be successful and pessimists tend to be right” Zuckerberg explained at the November forum. “If you think something is going to be terrible and it is going to fail, then you are going to look for the data points that prove you right and you will find them. That is what pessimists do. But if you think that something is possible, then you are going to try to find a way to make it work. And even when you make mistakes along the way and even when people doubt you, you are going to keep pushing until you find a way to make it happen.”

That quote takes on new light given Facebook’s history. The company must promote a culture where pessimists can speak up without reprise. Where a seeking a raise, reaching milestones, avoiding culpability, or a desire to avoid rocking the Kool-Aid boat don’t stifle discussion of a product’s potential hazards. Facebook’s can-do hacker culture that codes with caution to the wind, that asks for forgiveness instead of permission, is failing to scale to the responsibility of being a two billion user communications institution.

And our species is failing to scale to that level of digital congregation too, stymied by our insecurity and greed. Whether someone is demeaning themselves for not having as glamorous of a vacation as their acquaintances, or seizing the world’s megaphone to spew lies in hopes of impeding democracy, we’ve proven incapable of safe social networking.

That’s why we’re relying on Facebook and the other social networks to change, and why it’s so catastrophic when they miss the festering problems, ignore the calls for reform, or try to hide their complicity. To connect the world, Facebook must foresee its ugliness and proactively rise against it.

For more on Facebook’s non-stop scandals, check out these TechCrunch feature pieces:",Social Media,TechCrunch,https://techcrunch.com/2018/03/18/move-fast-and-fake-things/,"People have abused Facebook's products to propagate misinformation, envy, and insecurity, creating a negative impact on society. This has led to calls for regulation, and has highlighted the need for Facebook to promote a culture of cynics and contrarians who can foresee how its products could be abused.","Information, Discourse & Governance"
142,Twitter users join 24hr boycott to protest online harassment – TechCrunch,"A number of Twitter users are joining in a 24 hour boycott of the platform today, organized around the hashtag #WomenBoycottTwitter, to draw attention to online harassment on social media, including how women’s voices are silenced by bullying and abuse.

If you can't join #WomenBoycottTwitter b/c of your job or other reasons, consider using the day to amplify women's voices. pic.twitter.com/GvBmkVqvF8 — Caroline O. (@RVAwonk) October 13, 2017

The boycott follows the temporary suspension of actress Rose McGowan’s Twitter account earlier this week after she had been tweeting about sexual violence against women, and specifically about the allegations coming out against Hollywood producer Harvey Weinstein.

Twitter yesterday claimed McGowan’s suspension was the result of her disclosing a phone number in one of her tweets.

However critics have pointed out that Twitter appears to apply its own policies selectively — including its policy of not disclosing information pertaining to individual accounts.

when will nuclear war violate your terms of service? https://t.co/72FiiyoZ59 — rose mcgowan (@rosemcgowan) October 12, 2017

Too bad twitter didn't suspend @realDonaldTrump when he tweeted Lindsay Graham's phone number. Guess @rosemcgowan deserved it though. — Audrey Wauchope (@audreyalison) October 12, 2017

And many Twitter users continue to criticize the company for repeatedly failing to ban abusive users and for allowing its platform to be used to carry out targeted harassment.

The 24 hour boycott was apparently spontaneously organized by a San Francisco based software engineer called Kelly Ellis, after McGowan’s account was suspended.

Individuals opting out doesn't seem to make a dent. What if #WomenBoycottTwitter for one day (along with men who stand with us?) — Kelly Ellis (@justkelly_ok) October 12, 2017

#WomenBoycottTwitter Friday, October 13th. In solidarity w @rosemcgowan and all the victims of hate and harassment Twitter fails to support. https://t.co/G0my9EyKpQ — Kelly Ellis (@justkelly_ok) October 12, 2017

It’s not clear how many Twitter users are joining in the boycott today. It’s not only women getting involved, though; some men have also said they are boycotting Twitter for 24 hours in solidarity with the cause.

Tomorrow I follow the Women. #WomenBoycottTwitter — Mark Ruffalo (@MarkRuffalo) October 13, 2017

Although some women have specified they are not joining in because they prefer not to self-silence — saying they believe it’s important to make women’s voices heard by continuing to speak out.

Big ups to those participating in #WomenBoycottTwitter but the foundation of my feminism is about NOT being silenced. — Danielle Henderson (@knottyyarn) October 13, 2017

Some women of color have also said they are not joining in — apparently to protest how certain feminist issues (affecting white woman) can demand support from all women, whereas other feminist issues (affecting women of color) are more likely to get overlooked.

There’s another hashtag for this protest: #WocAffirmation.

I'm not doing #WomenBoycottTwitter. I'll be right here talking 2 WoC in my DM's & emails making sure they're covered. #WocAffirmation — BAST (@ValerieComplex) October 13, 2017

Asked for comment about the boycott, a Twitter spokesperson pointed to the thread from the @TwitterSafety account yesterday, where the company set out its explanation for suspending McGowan’s account.

The spokesperson also reiterated a portion of the statement from that thread, saying: “Twitter is proud to empower and support the voices on our platform, especially those that speak truth to power. We stand with the brave women and men who use Twitter to share their stories, and will work hard every day to improve our processes to protect those voices.”

It’s certainly true that Twitter’s platform is used to amplify feminist voices — such as via the @EverydaySexism project’s account which regularly retweets women’s stories of sexism and harassment.

Equally, the company continues to provide a platform to misogynists, racists, neo-nazis and white supremacists — thereby enabling the spread and amplification of hate speech. (Whereas in Germany it does offer a nazi filter to comply with local hate speech laws. So the company is choosing to allow nazis to tweet elsewhere on account of prioritizing ‘free speech’ — to the detriment of individuals and groups who are singled out for hate speech.)

The company’s most recent transparency report suggests abusive behavior on its platform is the biggest political concern relating to Twitter — with the vast majority (98 per cent) of complaints it receives from governments pertaining to “abusive behavior”.

The report reveals Twitter received nearly 16,500 reports from government officials over this type of content between January and June this year.

It was not previously breaking out government reports into different categories so it’s not possible to compare how this has evolved over recent years — but it’s certainly one to watch going forward.

Twitter does not disclose how many reports of abusive behavior it receives from all its users.",Social Media,TechCrunch,https://techcrunch.com/2017/10/13/twitter-users-join-24-boycott-to-protest-online-harassment/,"The 24 hour #WomenBoycottTwitter boycott is drawing attention to how women's voices are silenced by bullying and abuse on social media, and how Twitter appears to apply its own policies selectively when dealing with such behavior. This is a major political concern for Twitter, as it received nearly 16,500 reports of abusive behavior from governments between January and",Politics
143,You Don't Have to React to Every Post and Text You See—Promise,"I often worry I'm an under-reactor.

It's not that things don't affect me, or that I'm needlessly stoic—born-and-bred Midwesterners like me tend to be level-headed. (Or at least as even-keeled as one can be in 2018.) This is a purely performative kind of reaction I'm talking about: I don't add the ""angry"" face to Facebook posts from old high-school classmates, don't ""heart"" nearly as many tweets as maybe I should. I probably don’t even double-tap enough sunset posts on Instagram. I am, at heart, a chronic un-enthusiast.

And I'm vastly outnumbered by the tap-happy hordes.

Somewhere in the last few years, it became necessary to respond to everything. This manifests itself in obvious ways, like commenting on Facebook posts or @ replying on Twitter, but also in the far more mundane ones, like Facebook emoji. And the requests for this input is constant. Every tweet, every Instagram post, every Snap—they all peer out from our phones like those twins in The Shining saying ""come play with us."" To reject them, to not interact, feels rude. Dismissive. Liking things, boosting them, is now part of the social contract. And fear of breaking that pact has now become a burden.

Larry Rosen, a research psychologist at California State University, Dominguez Hills, is well aware of this pressure. He studies the phone usage of young adults—mostly early-twentysomethings—and analyzes their appetites for interaction. The way he sees it, most of us, by virtue of using social media platforms, have submitted to their implicit obligations. ""We have created this social responsibility,"" he says—and now we feel beholden to it. We ourselves know the dopamine drip that comes from getting a stream of likes, hearts, and ""LOL""s, so we feel compelled to return that drip in kind to our friends and our ""friends.""

But that compulsion to respond is carrying over into other mediums. Ever since Apple introduced reactions on iMessage in 2016, they've become increasingly commonplace in group text threads, even when they're completely superfluous. I was in a movie once and felt my pocket vibrate so frequently I was sure President Trump had been impeached or Beyoncé had dropped an album. Neither were true—it was just a series of texts being haha'ed or !!'ed or ❤️'ed by everyone else in the conversation, with each micro-reponse triggering an alert. Sure, I may need to lock down my notifications, but this feels suspiciously like the essence of ""diminishing returns."" It's impossible to feel the love when it's clear someone is just acting out of a social obligation. ""People think they’re communicating and they’re really not,"" Rosen says. ""Communication is complex. It takes facial expressions, it takes body language. It can't be done through emojis and text.""

Every tweet, every Instagram post, every Snap—they all peer out from our phones like those twins in The Shining saying 'come play with us.' Liking things, boosting them, is now part of the social contract. And fear of breaking that pact has now become a burden.

This, perhaps, is taking it too seriously. Reactions on text threads and social media aren't that different from smiling or nodding while someone is talking. ""They are niceties that express that we’re listening, signal interest, and keep the conversation going,"" says Paul Dourish, Chancellor's Professor of Informatics at UC Irvine. That’s true, but there have to be limits. Remember when we used to send ""Goodnight, Twitter"" messages? No one says goodbye anymore; it's presumed the conversation will just pause and resume ad infinitum. It's OK for conversations to end, people. Launching yet another string of reactions, or adding ""!!"" to someone's party invite on a text thread when you've already said you're coming, just turns into the world's worst game of ""hang up""/""no, you hang up.""

Let me be clear, I’m not advocating that we stop reacting to all social interactions. What I want is for those reactions to mean something. Be scrupulous in your signs of affirmation and people will know you mean it. Nothing means less than getting an Instagram double-tap from someone who you know likes everything they see, so don't be that person. Tap with purpose, friends! Otherwise, we’re just marching a million ""Wow"" emojis off a cliff into meaninglessness.

We're also, it should be said, stressing ourselves out. According to Rosen, the constant stream of phone notifications triggers all kinds of emotions. They make us think we must engage. The only way to calm that feeling is to respond, and emojis and other reactions are the quickest way to do that—but they can also lead to burnout. ""Part of what we’re all feeling is overwhelmed and burdened by this social responsibility,"" Rosen says, ""by this seeming social contract that we’ve signed.""

Maybe it's time we tore that contract up.

More Great WIRED Stories",Social Media,WIRED,https://www.wired.com/story/group-text-reactions/,"The constant stream of notifications, emojis, and other reactions to posts on social media is creating a burden of social responsibility, leading to burnout and a sense of being overwhelmed.",Social Norms & Relationships
144,"WhatsApp founder, Brian Acton, says Facebook used him to get its acquisition past EU regulators – TechCrunch","WhatsApp founder, Brian Acton, says Facebook used him to get its acquisition past EU regulators

WhatsApp founder, Brian Acton, who left Facebook a year ago — before going on to publicly bite the hand that fed him, by voicing support for the #DeleteFacebook movement (and donating $50M to alternative encrypted messaging app, Signal) — has delved into the ethics clash behind his acrimonious departure in an interview with Forbes.

And for leaving a cool ~$850M in unvested stock on the table by not sticking it out a few more months inside Zuckerberg’s mothership, as co-founder Jan Koum did. (Collecting air cooled Porsches must be an expensive hobby, though.)

Acton has also suggested he was used by Facebook to help get its 2014 acquisition of WhatsApp past EU regulators who had been concerned it might be able to link accounts — as it subsequently did.

“You mean it won’t make as much money”

The WhatsApp founders’ departure from Facebook boils down to a disagreement over how to monetize their famously ‘anti-ads’ messaging platform from Menlo Park.

Though how the pair ever imagined their platform would be safe from ads in the clutches of, er, an ad giant like Facebook remains one of the tech world’s greatest unexplained brain-fails. Or else they were mostly just thinking of the billions Facebook was paying them.

Acton said he tried to push Facebook towards an alternative, less privacy hostile business model for WhatsApp — suggesting a metered-user model such as by charging a tenth of a penny after a certain large number of free messages were used up.

But that “very simple business” idea was rejected outright by Facebook COO Sheryl Sandberg, who he said told him “it won’t scale”.

“I called her out one time,” Acton also told Forbes. “I was like, ‘No, you don’t mean that it won’t scale. You mean it won’t make as much money as…,’ and she kind of hemmed and hawed a little. And we moved on. I think I made my point… They are businesspeople, they are good businesspeople. They just represent a set of business practices, principles and ethics, and policies that I don’t necessarily agree with.”

Still, it seems Acton and Koum had a pretty major inkling of the looming clash of business “principles and ethics” with Facebook’s management, given they had a clause written into their contract to allow them to immediately get all their stock if the company began “implementing monetization initiatives” without their consent.

So with his ideas being actively rejected, and with Facebook ramping up the monetization pressure on the “product group” (which is how Acton says Zuckerberg viewed WhatsApp), he thought he saw a route to both cash out and get out — by calling in the contract clause.

Facebook had other ideas, though. Company lawyers told him the clause didn’t yet apply because it had only been “exploring”, not yet implementing monetization. At a meeting over the issue he said Zuckerberg also told him: “This is probably the last time you’ll ever talk to me.” So presumably things got pretty chilly.

The original $19BN deal for Facebook to buy WhatsApp had been rushed through over a weekend in 2014, and Acton said there had been little time to examine what would turn out to be crucial details like the monetization clause.

But not doing the due diligence on that clearly cost him a second very sizeable personal fortune.

Regardless, faced with more uncomfortably chilly meetings, and a legal fight to get the unvested stock, Acton said he decided to just take the winnings he already had and leave.

He even rejected an alternative proposed settlement (without fleshing out exactly what it was) — saying Facebook management had wanted to put a nondisclosure agreement in it, and “that was part of the reason that I got sort of cold feet in terms of trying to settle with these guys”.

“At the end of the day, I sold my company. I am a sellout. I acknowledge that,” he also told Forbes, indicating that he’s not unaware that the prospect of a guy who got really, really wealthy by selling out his principles and his users then trying to claw out even more cash from the ad tech giant he sold to probably wouldn’t look so good.

At least this way he can say he took an $850M haircut to show he ‘cared’.

In August Facebook confirmed that from next year it will indeed begin injecting ads into WhatsApp statuses — which is where the multimedia montage Stories format it cloned from Snapchat has been bolted onto the platform.

So WhatsApp’s ~1.5BN+ monthly users can look forward to unwelcome intrusions as they try to go about their daily business of sending messages to their friends and family.

How exactly Facebook will ‘encourage’ WhatsApp users to eyeball the marketing noise it intends to monetize remains to be seen. But tweaks to make statues more prominent/unavoidable look likely. Facebook is a master of the dark pattern design, after all.

The company is also set to charge businesses for messages they receive from potential customers via the WhatsApp platform — of between a half a penny and 9 cents, depending on the country.

So, in a way, it’s picking up on Acton’s suggestion of a ‘metered model’ — just in a fashion that will “scale” the bottom line in Sandberg’s sought for ‘loadsamoney’ style.

Though of course neither Acton nor Koum will be around to cash in on the stock uplift as Facebook imposes its ad model onto a whole new unwilling platform.

“I think everyone was gambling… because enough time had passed”

In perhaps the most telling tidbit of the interview, Acton reveals that even before the WhatsApp acquisition had been cleared he was carefully coached by Facebook to tell European regulators it would be “really difficult” for it to combine WhatsApp and Facebook user data.

“I was coached to explain that it would be really difficult to merge or blend data between the two systems,” Acton said.

An ‘impossible conjoining’ that Facebook subsequently, miraculously went on to achieve, just two years later, which later earned it a $122M fine from the European Commission for providing incorrect or misleading information on the original filing. (Facebook has maintained that unintentional “errors” were to blame.)

After the acquisition had been cleared Acton said he later learned that elsewhere in Facebook there were indeed “plans and technologies to blend data” between the two services — and that specifically it could use the 128-bit string of numbers assigned to each phone to connect WhatsApp and Facebook user accounts.

Phone-number matching is another method used to link accounts — and sharing WhatsApp users’ phone numbers with the parent group was a change pushed onto users via the 2016 update to WhatsApp’s terms and conditions.

(Though Facebook’s linking of WhatsApp and Facebook accounts for ad targeting purposes remains suspended in Europe, after regulatory push-back.)

“I think everyone was gambling because they thought that the EU might have forgotten because enough time had passed,” he also said in reference to Facebook pushing ahead with account matching, despite having told European regulators it couldn’t be done.

Regulators did not forget. But a $122M fine is hardly a proportionate disincentive for a company as revenue-heavy as Facebook (which earned a whopping $13.23BN in Q2). And which can therefore swallow the penalty as another standard business cost.

Acton said Facebook also sought “broader rights” to WhatsApp users data under the new terms of service — and claims he and Koum pushed back and reached a compromise with Facebook management.

The ‘compromise’ being that the clause about ‘no ads’ would remain — but Facebook would get to link accounts to power friend suggestions on Facebook and to offer its advertising partners better targets for ads on Facebook. So really they just bought themselves (and their users) a bit more time.

Now, of course, with both founders out of the company Facebook is free to scrub the no ads clause and use the already linked accounts for ad targeting in both directions (not just at Facebook users).

And if Acton and Koum ever really thought they could prevent that adtech endgame they were horribly naive. Again, most probably, they just balanced the billions they got paid against that outcome and thought 2x [shrug emoji].

Facebook’s push to monetize WhatsApp faster than its founders were entirely comfortable with looks to be related to its own concerns about needing to please investors by being able to show continued growth.

Facebook’s most recent Q2 was not a stellar one, with its stock taking a hit on slowing user growth.

Three years after the WhatsApp acquisition, Acton said Zuckerberg was growing impatient — recounting how he told an all-hands meeting for WhatsApp staffers Facebook needed WhatsApp revenues to continue to show growth to Wall Street.

Internally, Acton said Facebook had targeted a $10 billion revenue run rate within five years of monetization of WhatsApp — numbers he thought sounded too high and which therefore must be reliant on ads.

And so within a year or so Acton was on his way out — not quite as personally mega-wealthy as he could have been. But definitely don’t cry for him. He’s doing fine.

At the Signal Foundation, where Acton now works, he says the goal is to make “private communication accessible and ubiquitous”.

Though the alternative e2e encrypted app has only unquantified “millions” of users to WhatsApp and Facebook’s multi billions. But at least it has $50M of Acton’s personal fortune behind it.",Social Media,TechCrunch,https://techcrunch.com/2018/09/26/whatsapp-founder-brian-acton-says-facebook-used-him-to-get-its-acquisition-past-eu-regulators/,"The main issue with Social Media discussed here is the use of it for monetization purposes, which goes against the principles and ethics of its founders and can lead to privacy intrusion and unwanted ads for users. This has been exemplified by Facebook's acquisition and subsequent monetization of WhatsApp, which Brian Acton, one of its founders, eventually had",Security & Privacy
145,Watch Workers Learn How to Filter Obscene and Violent Photos From Dating Sites,"Content View Iframe URL

For all the excitement about policing the web with image detection algorithms, machine learning, and other tools, the task of keeping the internet functioning and habitable still falls to people. Beneath the slick automation of companies like Google and Facebook hides a hidden army of manual laborers---many in countries like India and the Philippines. They perform the tedious, disturbing task that machines still can't, and that most Americans won't: Filtering social media sites for obscenities, abuse, and violence.

In their short documentary The Moderators, filmmakers Adrian Chen and Ciaran Cassidy go inside an Indian firm doing that work, capturing a week-long training session for new employees. Their film, which WIRED is premiering online, centers on a group of young Indians starting their first jobs for Bangalore-based Foiwe Info Global Solutions, whose clients include a handful of dating sites in the US, Europe, and India. They're told their mind-numbing quota: filtering 2,000 photos an hour. Over five days in a sterile office, they learn to recognize and moderate increasingly unnerving examples of forbidden online imagery, starting with nudity and culminating in child pornography and what seems to be graphic bodily harm---a kind of real-life, South Asian Clockwork Orange for the social media age.

Two years ago, Chen traveled to the Philippines to report a story on content moderation firms for WIRED. The piece won a Sidney Hillman Award for bringing to light the stories of employees traumatized by an endless parade of humanity's worst imagery. Now in The Moderators, he and co-director Cassidy return to that underworld to provide a fly-on-the-wall view of that grim work. Be thankful that after 19 minutes---unlike the moderators themselves---you can look away.",Social Media,WIRED,https://www.wired.com/2017/04/watch-people-learn-filter-awfulness-dating-sites/,"The Moderators documents the difficult reality of content moderation firms, with employees in countries like India and the Philippines having to sort through a barrage of disturbing images, ranging from nudity to child pornography, and even graphic violence. This mind-numbing job has a detrimental effect on the mental health of those who do it.",Equality & Justice
146,UK wants G7 to take collective action on online extremism – TechCrunch,"The UK Prime Minister is using the annual G7 summit of seven of the world’s major industrialized democracies to push for more to be done about online extremism, including co-ordinating on ways to force social media platforms to be more pro-active about removing and reporting extremist content to authorities.

Theresa May is chairing a counter-terrorism session at the G7 summit today in Sicily, meeting with the leaders of the US, Canada, Germany, France, Italy, Japan and representatives of the European Union.

It’s a drum her own Home Secretary has been banging at home in recent months. And just the latest instance of the political thumbscrews being applied to social media giants.

In Germany in April, for example, the government backed proposals to levy fines of up to €50 million on social media firms that fail to promptly remove illegal hate speech from their platforms.

Before leaving for the summit yesterday the BBC reported May planned to lead a discussion with her fellow world leaders on how to “work together to prevent the plotting of terrorist attacks online and to stop the spread of hateful extremist ideology on social media”.

According to The Guardian, she is expected to tell her G7 counterparts that the fight against ISIS is shifting from the “battlefield to the Internet”, and to urge them to co-operate to enforce stricter rules on social media companies.

Specifically, the newspaper said May will press for social media firms to:

develop tools that could automatically identify and remove harmful material based on what it contains and who posted it

tell the authorities when harmful material is identified so that action can be taken

revise conditions and industry guidelines to make them absolutely clear about what constitutes harmful material

The move follows another terror attack on UK soil after a suicide bomber blew himself up at a pop concert in Manchester on Monday evening, killing and wounding multiple people.

Although there have been no suggestions so far that social media platforms could have thwarted the attack by providing pre-emptive intelligence.

Indeed, the UK government’s own counterterrorism policies are facing the most uncomfortable questions in the wake of the attack, given the bomber had been repeatedly reported to police in the years prior. Yet was not, evidently, stopped from obtaining the know-how or materials to construct a bomb. Nor from successful executing an attack.

Other recent instances of terrorism on UK soil have included an attack in Westminster in March, when a lone attacker used a car and knives to attack pedestrians and police. The Westminster attacker apparently sent a WhatsApp message minutes before commencing the attack saying he was waging jihad in revenge for Western foreign policy.

While a homemade bomb planted on a London Underground Tube train in October last year, which failed to go off, had apparently been put together by the teenage perpetrator following instructions found online.

The problem for UK security services is they are under-resourced to meet the scale of the threat. As Muddassar Ahmed writes today in The Independent, there are around 3,000 people on UK terror watch lists — yet only 4,000 staff in MI5, the domestic intelligence agency. The agency simply does not have the manpower to closely monitor so many potential terrorists.

May has also faced specific criticism in the wake of the Manchester attack for cuts the government made to UK police numbers. She had apparently been warned two years ago that cuts to community policing in Manchester could threaten counter terrorism efforts in the city. The optics at this point look terrible.

Her G7 comments therefore risk looking like an attempt to shift both blame and responsibility — with May leaning out to apply pressure on social media firms in a bid to effectively outsource the responsibility for terrorism monitoring to tech platforms. At a time when her own government’s policy towards domestic policing and counterterrorism resourcing looks to be lacking.

Perhaps the most significant push she’s making at the G7 is for the countries to co-ordinate on revising industry guidelines on harmful material and on the conditions they place on tech firms. Although, according to Guardian sources, she’s not advocating for financing penalties at this stage (such as those already on the table in Germany).

The paper quotes a senior government source saying May wants the the G7 nations to “move towards a common approach focused on the need to defeat [Isis]. In particular she wants to use G7 to call for the members to adopt a collective approach when working with tech companies on this agenda, and she will say that the industry has a social responsibility to do more to remove harmful content from its networks”.

However, whether she will be able to convince President Trump to join a collective attempt to apply extra pressure on US tech giants is not clear.

Much of what she’s generally calling for social media firms to do is already arguably taking place. For example, Facebook has previously said it is working on tools and technologies to try to automate flagging up extremist content, including CEO Mark Zuckerberg publicly discussing the potential of AI to help with content review.

Facebook also emphasizes that it does already reach out to authorities if it finds evidence of an imminent threat of harm or terrorism. And does already use AI and image and video matching technology when it identifies terrorist content to try to unearth related content and accounts, for example.

That said, ongoing criticism of the effectiveness of Facebook’s content moderation processes underline the scale of the company’s own challenge on this front. And the 3,000 additional moderation staff announced by Zuckerberg in the wake of another content moderation scandal represents a tiny drop in the ocean for a platform with nearly two billion users.

With the additional moderator headcount Facebook will still only have 7,500 people employed to review all flagged content — so more than MI5’s headcount but relatively far fewer, given the staggering scale of its content moderation challenge, which runs the gamut from reviewing and making judgements on extremism/terrorist related content; to child abuse and other criminal content; to hate speech and racism; to violence and cruelty of all stripes. All the while aiming to balance ‘safety with openness’, as it puts it, i.e. preferring the sharing of controversial/disturbing content — provided it’s not illegal/sadistic. (A very tricky balancing act to pull off, evidently.)

In a statement today responding to May’s comments on extremist content, Facebook’s Monika Bickert, head of global policy management, said: “We want to provide a service where people feel safe. That means we do not allow groups or people that engage in terrorist activity, or posts that express support for terrorism. Using a combination of technology and human review, we work aggressively to remove terrorist content from our platform as soon as we become aware of it — and if there is an emergency involving imminent harm to someone’s safety, we notify law enforcement. Online extremism can only be tackled with strong partnerships. We have long collaborated with policymakers, civil society, and others in the tech industry, and we are committed to continuing this important work together.”

Twitter did not provide a statement when contacted for comment but a spokesman pointed to what he said are proactive technological steps the platform takes with regards countering violent extremism, such as using technological tools to surface accounts that are promoting terrorism. He noted, for example, that between July 1, 2016 through December 31, 2016, a total of 376,890 Twitter accounts were suspended for this reason — with 74 per cent of those having been surfaced by the platform’s tech tools rather than via user reports.

Update: In a joint statement after the meeting, The Guardian reports the G7 leaders backed May’s calls for social media firms to do more to tackle online extremism. “We agreed a range of steps the G7 could take to strengthen its work with tech companies on this vital agenda. We want companies to develop tools to identify and remove harmful materials automatically,” she said.",Social Media,TechCrunch,https://techcrunch.com/2017/05/26/uk-wants-g7-to-take-collective-action-on-online-extremism/,"The UK Prime Minister is using the G7 summit to push for more action from social media companies to remove and report extremist content, raising concerns about the potential for tech firms to take on responsibility for terrorism monitoring in an attempt to shift blame away from her own government's lack of resources for domestic policing and counter terrorism.",Security & Privacy
147,Twitter’s updated T&Cs look clearer — yet it still can’t say no to Nazis – TechCrunch,"Twitter’s updated T&Cs look clearer — yet it still can’t say no to Nazis

Twitter has taken a pair of shears to its user rules, shaving almost 2,000 words off of its T&Cs — with the stated aim of making it clearer for users what is not acceptable behaviour on its platform.

It says the rules have shrunk from 2,500 words to just 600 — with each of the reworded rules now encapsulated within a pithy tweet length (280 characters or less).

Though each tweet-length rule is still followed by plenty of supplementary detail — where Twitter explains the rationale behind it and provides examples of what not to do, and details of potential consequences. So the full rule-book is still way over 2,500 words.

“Everyone who uses Twitter should be able to easily understand what is and is not allowed on the service,” writes Twitter’s Del Harvey, VP of trust and safety, in a blog post announcing the changes. “As part of our continued push towards more transparency across every aspect of Twitter, we’re working to make sure every rule has its own help page with more detailed information and relevant resources, with abuse and harassment, hateful conduct, suicide or self-harm, and copyright being next on our list to update. Our focus remains on keeping everyone safe and supporting a healthier public conversation on Twitter.”

The newly reworded rules can be found at: twitter.com/rules

We’ve listed the tweet-sized rules below, without any of their qualifying clutter:

Notably the rules make no mention of fascist ideologies being unwelcome on Twitter’s platform. Although a logical person might be forgiven for thinking such hateful stuff would naturally be prohibited — based on the core usage principles Twitter is stating here (such as a ban on threatening and/or promoting violence against groups of people including on the basis of their race, ethnicity and so on).

But for Twitter nazi-ism remains, uh, ‘complicated’.

The company recently told Vice it’s working with researchers to consider whether or not it should ban Nazis. Which suggests its new ‘pithier’ rules are missing a few qualifying asterisks.

Here, we fixed one:

You may not threaten violence against an individual or a group of people*. We also prohibit the glorification of violence**. *unless you’re a Nazi **white supremacists totally get a pass while we mull the commercial implications of actually banning racist hate

Another abuse vector that continues to look like a blindspot in Twitter’s rule-book is sex.

While the company does include both ‘gender’ and ‘gender identity’ among the many categories it stipulates that users must not direct harassment, at or promote violence against, it does not offer the same shield based on a user’s sex. Which appears to have resulted in instances where Twitter has deemed tweets containing violent misogyny to not be in violation of its rules.

Last month a Twitter UK public policy rep told the parliamentary human rights committee, which had raised the issue of the violent sexist tweets, that it believed the inclusion of gender should be enough to protect against instances of violent misogyny, despite having demonstrably failed to do so in the selection of tweets the committee put to it.

We’ve asked Twitter about its continued decision not to prohibit harassment and threats of violence against users based on their sex, as well as its ongoing failure to ban Nazis and will update this report with any response. Update: A Twitter spokeswoman has now sent us this statement:

The Twitter Rules exist to help ensure everyone feels safe expressing their beliefs and we strive to enforce them with uniform consistency. Our Hateful Conduct Policy does not allow people to promote violence against or directly attack or threaten other people on the basis of certain protected categories such as sexual orientation, gender and gender identity among others. Meanwhile our Violent Extremism policy expressly states that a user may not make specific threats of violence or wish for the serious physical harm, death, or disease of an individual or group of people. This includes – but is not limited to threatening or promoting terrorism.

In addition to editing down the wording of its rules, Twitter says it has thematically organized them under three new categories — safety, privacy, and authenticity — to make it easier for users to find what they’re looking for.

Though it’s not quite as at-a-glance clear as that on the rules page — which also includes a general preamble; a note on wider content boundaries; a section dealing with spam and security; and an addendum on content visibility restrictions that Twitter may apply in cases where it suspects an account of abuses and is investigating.

But, as ever, algorithmically driven platforms are anything but simple.

Hideously wordy T&Cs have of course been a tech staple for years so it’s good to see Twitter paying greater attention to the acceptable conduct signals it gives users — and at least trying to boil down a clearer essence of what isn’t acceptable behavior, albeit tardily.

But, equally, refreshed wording of what’s unacceptable makes it plainer that Twitter retains stubborn blind-spots that allow its platform to be a conduct for targeted racial hatred.

Perhaps these blindspots are commercially motivated, in the case of far right ideologies. Or perhaps Twitter’s leadership is still so drunk on its own philosophical koolaid it really has fuzzed the lines between fascism and, er, humanity.

If that’s the case, no pithily written rules will save Twitter from itself.

Don’t forget, this is a company that has been promising to get a handle on its abuse problem for years. Including — just last year — making a grand stance about wanting to champion ‘conversational health‘.

Yet it still can’t screw its courage to the sticking place and say no Nazis.

Twitter’s multi-year struggles to respond to baked in hate might be farcical at this point — if the human impacts of amplifying racial and ethnic hatred weren’t a tragedy for all concerned.

And had it found a moral compass when it was first being warned about the rising tide of amplified abuse, it’s entirely possible one of its most high profile users might not be a geopolitical mega-bully known to retweet fascist propaganda.

Chew on that, Jack.",Social Media,TechCrunch,https://techcrunch.com/2019/06/06/twitters-updated-tcs-look-clearer-yet-it-still-cant-say-no-to-nazis/,"The main undesirable consequence of social media discussed here is that it has enabled the spread of hate and violence, particularly from far-right ideologies, without any proper repercussions from the platforms themselves. Despite Twitter's attempts to make their rules clearer, they still cannot bring themselves to explicitly ban Nazi ideology, further exacerbating the problem.",Equality & Justice
148,YouTube is not for kids – TechCrunch,"I’ve been thinking a lot about this important if long Medium post by James Bridle regarding the nonsensical and potentially damaging kids content on YouTube. In it he moves from video to video, finding stranger and stranger examples.

Further, an exodus of advertising is hitting the service as some disgusting comments were found below videos aimed at kids and of kids. Strangers with various motives are watching all of these videos for various reasons and that’s not OK. Even if one child enjoys one nursery rhyme, there is so much content that is both ridiculous and dangerous lurking under the surface of the service that you’re almost guaranteed to step on a content landmine.

In short, YouTube is not for kids.

As a parent, this is important and obvious. Chances are you’ve let your children watch something on YouTube just to keep them amused. If you haven’t done this then you have my admiration but YouTube has become a babysitter and playmate and TV channel.

It shouldn’t be.

It is trivial to snake through the YouTube rabbit hole and find content that is violent, weird, and insanely popular. Start here, with a search for Cars.

This is a search my eight-year-old son has figured out by himself. Next we pick a video (any video) that isn’t a toy unboxing (a genre of kids content that is also wildly popular) we find an animated short of trucks flying into space.

It got 1,379,902 views, presumably by kids simply playing it over and over again. Take it one more step and the cars begin exploding and Spider-Man is inexplicably involved.



We conclude our sojourn with an unboxing video featuring kids opening toys. It has 3,404,989 views.

Childhood education is a long and tricky process. It requires passing along body of knowledge that will allow a human being to exist among other human beings and, while the details change, the story is still the same: we want to ensure our are kids happy, healthy, and smart. Kid’s entertainment requires story and, hopefully, logic, lessons, and learning. This means Spider-Man and Elsa probably shouldn’t be losing their mouths in a wacky mixup video.

YouTube is a cesspool of garbage kids content created by what seems to be a sentient, angry AI bent on teaching our kids that collectible toys are the road to happiness. YouTube isn’t for kids. If you give it to kids they will find themselves watching something that is completely nonsensical or something violent or something sexual. It’s inevitable. YouTube can add kids channels, scanning services, and even human censors but trust me: your four year old is eventually going to hit stuff like this. This is not unsupervised play. This is a dangerous addiction.

I don’t want to spoil your brunch, fellow parents. Maybe there are great videos on YouTube that you’ve found and maybe your child doesn’t click into the black hole every time they sit with your phone. But there is a body of animated content out there with story, wonderful characters, and true messages of peace, love and understanding and it probably costs a few dollars to download. You wouldn’t feed them food given to you by some random person on the street. Why would you give them videos by someone with an eye on monetizing their eyeballs?",Social Media,TechCrunch,https://techcrunch.com/2017/11/29/youtube-is-not-for-kids/,"YouTube has become a go-to source of entertainment for kids, but much of the content that exists is nonsensical, violent, or otherwise inappropriate. Without proper supervision, children can easily find themselves watching videos that are damaging to their development, and parents should be wary of the implications of allowing their kids to watch YouTube without supervision.",User Experience & Entertainment
149,"In expanded crackdown, Facebook increases penalties for rule-breaking groups and their members – TechCrunch","Facebook this morning announced it will increase the penalties against its rule-breaking Facebook Groups and their members, alongside other changes designed to reduce the visibility of groups’ potentially harmful content. The company says it will now remove civic and political groups from its recommendations in markets outside the U.S., and will further restrict the reach of groups and members who continue to violate its rules.

The changes follow what has been a steady, but slow and sometimes ineffective crackdown on Facebook Groups that produce and share harmful, polarizing or even dangerous content.

Ahead of the U.S. elections, Facebook implemented a series of new rules designed to penalize those who violated its Community Standards or spread misinformation via Facebook Groups. These rules largely assigned more responsibility to Groups themselves, and penalized individuals who broke rules. Facebook also stopped recommending health groups, to push users to official sources for health information, including for information about Covid-19.

This January, Facebook made a more significant move against potentially dangerous groups. It announced it would remove civic and political groups, as well as newly created groups, from its recommendations in the U.S. following the insurrection at the U.S. Capitol on Jan. 6, 2021. (Previously, it had temporarily limited these groups ahead of the U.S. elections.)

As The WSJ reported when this policy became permanent, Facebook’s internal research had found that Facebook groups in the U.S. were polarizing users and inflaming the calls for violence that spread after the elections. The researchers said roughly 70% of the top 100 most active civic Facebook Groups in the U.S. had issues with hate, misinformation, bullying and harassment that should make them non-recommendable, leading to the January 2021 crackdown.

Today, that same policy is being rolled out to Facebook’s global user base, not just Facebook U.S. users.

That means in addition to health groups, users worldwide won’t be “recommended” civic or political groups when browsing Facebook. It’s important, however, to note that recommendations are only one of many ways users find Facebook Groups. Users can also find them in search, through links people post, through invites and friends’ private messages.

In addition, Facebook says groups that have gotten in trouble for violating Facebook’s rules will now be shown lower in recommendations — a sort of downranking penalty Facebook often uses to reduce the visibility of News Feed content.

The company will also increase the penalties against rule-violating groups and their individual members through a variety of other enforcement actions.

For example, users who attempt to join groups that have a history of breaking Facebook’s Community Standards will be alerted to the the group’s violations through a warning message (shown above), which may cause the user to reconsider joining.

The rule-violating groups will have their invite notifications limited, and current members will begin to see less of the groups’ content in their News Feed, as the content will be shown further down. These groups will also be demoted in Facebook’s recommendations.

When a group hosts a substantial number of members who have violated Facebook policies or participated in other groups that were shut down for Facebook Community Standards violations, the group itself will have to temporarily approve all members’ new posts. And if the admin or moderator repeatedly approves rule-breaking content, Facebook will then take the entire group down.

This rule aims to address problems around groups that re-form after being banned, only to restart their bad behavior unchecked.

The final change being announced today applies to group members.

When someone has repeated violations in Facebook Groups, they’ll be temporarily stopped from posting or commenting in any group, won’t be allowed to invite others to join groups, and won’t be able to create new groups. This measure aims to slow down the reach of bad actors, Facebook says.

The new policies give Facebook a way to more transparently document a group’s bad behavior that led to its final shutdown. This “paper trail,” of sorts, also helps Facebook duck accusations of bias when it comes to its enforcement actions — a charge often raised by Facebook critics on the right, who believe social networks are biased against conservatives.

But the problem with these policies is that they’re still ultimately hand slaps for those who break Facebook’s rules — not all that different from what users today jokingly refer to as “Facebook jail“. When individuals or Facebook Pages violate Facebook’s Community Standards, they’re temporarily prevented from interacting on the site or using specific features. Facebook is now trying to replicate that formula, with modifications, for Facebook Groups and their members.

There are other issues, as well. For one, these rules rely on Facebook to actually enforce them, and it’s unclear how well it will be able to do so. For another, they ignore one of the key means of group discovery: search. Facebook claims it downranks low-quality results here, but results of its efforts are decidedly mixed.

For example, though Facebook made sweeping statements about banning QAnon content across its platform in a misinformation crackdown last fall, it’s still possible to search for and find QAnon-adjacent content — like groups that aren’t titled QAnon but cater to QAnon-styled “patriots” and conspiracies).

Similarly, searches for terms like “antivax” or “covid hoax,” can also direct users to problematic groups — like the one for people who “aren’t anti-vax in general,” but are “just anti-RNA,” the group’s title explains; or the “parents against vaccines” group; or the “vaccine haters” group that proposes it’s spreading the “REAL vaccine information.” (We surfaced these on Tuesday, ahead of Facebook’s announcement.)

Cleary, these are not official health resources, and would not otherwise be recommended per Facebook policies — but are easy to surface through Facebook search. The company, however, takes stronger measures against Covid-19 and Covid vaccine misinformation — it says it will remove Pages, groups, and accounts that repeatedly shared debunked claims, and otherwise downranks them.

Facebook, to be clear, is fully capable of using stronger technical means of blocking access to content.

It banned “stop the steal” and other conspiracies following the U.S. elections, for example. And even today, a search for “stop the steal” groups simply returns a blank page saying no results were found.

So why should a search for a banned topic like “QAnon” return anything at all?

Why should “covid hoax?” (see below)

If Facebook wanted to broaden its list of problematic search terms, and return blank pages for other types of harmful content, it could. In fact, if it wanted to maintain a block list of URLs that are known to spread false information, it could do that, too. It could prevent users from re-sharing any post that included those links. It could make those posts default to non-public. It could flag users who violate its rules repeatedly, or some subset of those rules, as users who no longer get to set their posts to public…ever.

In other words, Facebook could do many, many things if it truly wanted to have a significant impact on the spread misinformation, toxicity, polarizing and otherwise harmful content on its platform. Instead, it continues inching forward with temporary punishments and those that are often only aimed at “repeated” violations, such as the ones announced today. These are, arguably, more penalties than it had before — but also maybe not enough.",Social Media,TechCrunch,https://techcrunch.com/2021/03/17/in-expanded-crackdown-facebook-increases-penalties-for-rule-breaking-groups-and-their-members/,"Facebook's latest changes to its groups, while a step in the right direction, are not enough to deal with the spread of misinformation, toxicity, polarization and other forms of harmful content. The company's enforcement actions often rely on temporary penalties and only target repeat offenders, and its search feature still allows users to access potentially dangerous content.","Information, Discourse & Governance"
150,Former Employee: We Made Facebook as Addictive as Cigarettes on Purpose,"""We took a page form Big Tobacco's playbook, working to make our offering addictive at the outset.""

Big Tobacco

Facebook’s former head of monetization, Tim Kendall, unloaded on the social media giant during a hearing about social media’s role in spreading extremist content — saying that his former employer, like big tobacco companies, worked to make its product as addictive as possible.

“We sought to mine as much attention as humanly possible,” he said. “We took a page from Big Tobacco’s playbook, working to make our offering addictive at the outset.”

Zuck Fix

Kendall, who worked for Facebook during the pivotal growth years from 2006 to 2010, said that the tech giant made upgrades to the site designed specifically to keep users coming back for more.

“Tobacco companies initially just sought to make nicotine more potent,” he said. “But eventually that wasn’t enough to grow the business as fast as they wanted. And so they added sugar and menthol to cigarettes so you could hold the smoke in your lungs for longer periods. At Facebook, we added status updates, photo tagging, and likes, which made status and reputation primary and laid the groundwork for a teenage mental health crisis.”

Advertisement

Advertisement

Civil War

Now, he said, the chickens have come home to roost, with the addictive platform spreading harmful content at scale.

“The social media services that I and others have built over the past 15 years have served to tear people apart with alarming speed and intensity,” Kendall said in his opening testimony (PDF). “At the very least, we have eroded our collective understanding — at worst, I fear we are pushing ourselves to the brink of a civil war.”

READ MORE: Former Facebook manager: “We took a page from Big Tobacco’s playbook” [Ars Technica]

More on Facebook: Report: Gene-Hacking Plants and Animals Could Fight Climate Change

Advertisement

Advertisement

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Social Media,Futurism,https://futurism.com/the-byte/former-facebook-employee-addictive-cigarettes,"Former head of monetization at Facebook, Tim Kendall, warns that social media has been tearing people apart at alarming speed and intensity, and that it could lead to civil war. He compared the company to Big Tobacco, saying that they sought to make their product addictive and erode collective understanding.",User Experience & Entertainment
151,When Instagram Influencing Isn't So Glamorous,"[Blogging is like] the fastest hamster wheel possible. You don’t ever get to get off of it. There is no rest. You are always on.

—Heather, mommy blogger

Over lunch at a British-style tearoom located on Philadelphia’s über-posh Main Line, Jessie, an effervescent twenty-something, recounted the circumstances that led her to launch a fashion and lifestyle blog four years earlier. Jessie had pursued a communications degree at a nearby liberal arts college while building impressive media credentials: experience writing for a local newspaper, where she had been freelancing since the age of 16; on-air radio training; and bylines in the features section of a respected regional magazine. Unfortunately, she graduated in the wake of a global economic recession when many businesses—including storied media publishers—were issuing layoffs. Such a turbulent employment market left newly minted graduates like Jessie hard-pressed, vying for positions that amounted to, in her words, “working a lot for free.” She recalled, “I got out of school, I left my internship at [the magazine] . . . I couldn’t get a job. I was interviewing everywhere, and I couldn’t even get back at my old internship because they were too full now.”

Adapted from (Not) Getting Paid to Do What You Love: Gender, Social Media, and Aspirational Work by Brooke Erin Duffy. Copyright © 2017 Brooke Erin Duffy. Reprinted by permission of Yale University Press.

Though she eventually landed a magazine internship, Jessie decided that her time and talent could be put to better use, so she rechanneled her creative energies into her then-nascent blog, Trend Hungry. Conjuring up her early forays into the blogosphere, she noted that her first year “was just a lot of cutting my teeth.” She added, “I made hardly any money off the blog at all, [and] I was pretty much full-time waitressing [to pay my bills].” But that was several years ago, and she had since generated enough income from her digitally created brand to go pro. It is perhaps not surprising, then, that the “about me” page on her blog includes the siren song of social media entrepreneurship: “Life is good when you do what you love!” Jessie was similarly upbeat in person; at one point she enthused, “I cannot differentiate work from life because I love what I do so much.” Yet over the course of our lunchtime interview, she pulled back the curtain on some of the less glamorous elements of the pro-blogger work culture: her incessant schedule of planning, styling, writing, and networking was taxing, and she lacked long-term stability. In fact, Jessie considered herself more of a “full-time freelancer,” given that Trend Hungry was only one of her revenue streams:

“I do it all. I do styling, I write for [my blog] . . . I write for the fashion spot on Philly.com, I do TV segments, QVC, I have a weekly syndicated radio segment, and I just started a vintage jewelry business. . . . Being an entrepreneur, nothing is the end-all, be-all; everything is like your launch pad to the next thing.”

At the same time, Jessie felt that many creative aspirants lacked a realistic sense of the time and commitment demanded of professional content producers in the digital age. Career hopefuls, she explained, “idealize [the blogger] life: they think that it’s going to be really glamorous. So they see other bloggers maybe working [for] brands or getting free things, and they only see . . . everything that’s through an Instagram filter that looks so fabulous.”

Jessie’s mention of the “Instagram filter” is a reference to the culture of vigilant self-monitoring on social media, particularly as individuals internalize directives to brand the self with resolve: We un-tag unflattering photos, we build credibility through “friend” and “follower” counts, and we harness our online personae to pithy self-descriptors that function as digital sound bites. For fashion bloggers, beauty vloggers, and other denizens of the feminine digital media economy, these activities are amplified; however, the work of such personal branding endeavors gets concealed behind a torrent of images and textual referents that ostensibly mask the labor required to earn a living doing what you love. Fashion bloggers and Instagrammers personify effortless glamour.",Social Media,WIRED,https://www.wired.com/story/when-instagram-influencing-isnt-so-glamorous/,"The rise of social media has led to an idealized view of ""the blogger life"" that fails to recognize the immense labor and commitment required of professional content producers in the digital age. This glamorization of the job hides the hard work behind it and can lead to unrealistic expectations of success.",User Experience & Entertainment
152,Social Networks May One Day Diagnose Disease - But at a Cost,"The world is becoming one big clinical trial. Humanity is generating streams of data from different sources every second. And this information, continuously flowing from social media, mobile GPS and wifi locations, search history, drugstore rewards cards, wearable devices, and much more, can provide insights into a person's health and well-being.

WIRED OPINION ABOUT Dr. Sam Volchenboum (@SamVolchenboum) is the director of the Center for Research Informatics at the University of Chicago, a board-certified pediatric hematologist and oncologist, and the cofounder of Litmus Health, a data science platform for early-stage clinical trials.

It’s now entirely conceivable that Facebook or Google—two of the biggest data platforms and predictive engines of our behavior—could tell someone they might have cancer before they even suspect it. Someone complaining about night sweats and weight loss on social media might not know these can be signs of lymphoma, or that their morning joint stiffness and propensity to sunburn could herald lupus. But it’s entirely feasible that bots trolling social network posts could pick up on these clues.

Sharing these insights and predictions could save lives and improve health, but there are good reasons why data platforms aren’t doing this today. The question is, then, do the risks outweigh the benefits?

A Thought Experiment

Although social media platforms get press for being useful in predicting, and possibly preventing, suicide, the possibility that those platforms could see into the future before a patient has even visited the doctor is, for now, hypothetical. But it’s not far-fetched.

Let’s say Facebook released a large set of de-identified data, such as members’ location, travel, likes and dislikes, post frequency, sentiment, browsing, and search habits. Based on these data, a researcher could build models that predict physical and emotional states.

For instance, a data set consisting of social media posts from tens of thousands of people will likely chronicle the journey that some had on their way to a diagnosis of cancer, depression, or inflammatory bowel disease. Using machine-learning techniques, a researcher could take those data and study the language, style, and content of those posts both before and after the diagnosis. They could devise models that, when fed new sets of users’ data, could predict who will likely go on to develop similar conditions.

And such a system would not need to look only for hard and fast symptoms like fevers or weight loss. Seemingly unimportant and unrelated data—like purchasing anti-nausea medicine or watching a documentary on insomnia—could end up fueling a set of predictive rules that indicate that a user might have a certain medical condition. The point is that our digital trail leaves many clues, both subtle and overt, to our overall health and well-being. How we use those data for good is another issue.

As a clinician, I support integrating data and putting the troves of information to use for society’s benefit. One of the reasons I cofounded Litmus Health, a data science company, was to help researchers better collect, organize, and analyze data from clinical trials, and in turn, use those data to improve health outcomes for society writ large. However, significant regulatory, ethical, technical, and societal considerations require caution.

From a regulatory perspective, all companies bear some responsibility to care for their users’ data, as defined in their terms of service. Unfortunately, what has been exposed in cases like a 2014 Facebook study and in research from Carnegie Mellon is that terms of service and/or privacy policies are overly complicated, no one reads them anyway, and users just blindly sign them.

Companies can demonstrate an ethical “do no harm” obligation to their users by having a straightforward and easy-to-understand data policy, and by not using personal data in inappropriate ways. An ethical framework for big data must consider identity, privacy, data ownership, and reputation. For most firms today, releasing users’ data to build predictive models without their consent would go against their established value systems. But obtaining consent may be as trivial as someone mindlessly clicking through an exorbitantly long terms-of-service agreement.

If companies are going to ask users to share their data and participate in an experiment, they should be more transparent about how the data are collected, used, and shared.

Let’s say a social network has an algorithm that analyzes a user’s activities— things they complain about, articles they share, friends’ posts they like, among other things. The AI could potentially identify a pattern suggesting the presence of a medical condition.",Social Media,WIRED,https://www.wired.com/story/social-networks-may-one-day-diagnose-disease-but-at-a-cost/,"The use of personal data on social media platforms to predict medical conditions presents a risk to users' privacy, data ownership, and reputation, and requires companies to be more transparent in how they collect, use, and share data.",Security & Privacy
153,Scribd Facebook Instant Personalization Is a Privacy Nightmare,"Online document sharing site Scribd hooked up with Facebook to create ""instant personalization"" so Scribd users can get reading recommendations based on their Facebook likes and what their friends are sharing. Sounds interesting, right?

But the document sharing and embedding service has created a privacy nightmare that involves drafting users who are already logged into Facebook without offering a clear opt out process either on the site or through e-mail.

Instead Scribd has been creating subscriptions and followers on behalf of a user by sending e-mails to contacts obtained through Facebook's friends list and notifying them -- all without requiring the user to ever click a button.

""Obviously privacy is extremely important to us,"" says Michelle Laird, spokesperson for Scribd. ""But, overall, we believe the experience is one that is welcoming you to Scribd and telling you who your friends participating in Scribd are.""

This is not the first time that it has launched a feature on an opt-out basis. Earlier this year it introduced Readcast, a feature that broadcasts the documents people download to other Scribd users. And as with instant personalization, Readcast was also opt out, rather than opt in. That move drew the ire of Santa Clara University law professor Eric Goldman, who wrote about it on his blog. This week, Scribd changed the process so users now have to clearly indicate they want to be part of Readcast.

In its latest move towards instant personalization, Scribd is counting on piggybacking on Facebook's presence on the user's browser.

Since I check Facebook a few times in the day, I am almost always logged into the service. On Wednesday — two days after Scribd launched its instant personalization service, of which I was completely unaware — I found a Scribd link in a Google search I ran.

After spending a few minutes on the Scribd link to scan through the document, I moved away to do other things. Barely three hours later, I got an e-mail from a co-worker who is also a Facebook friend saying I had subscribed to him on Scribd.

I had never clicked on any buttons on Scribd or signed up for the service. So I went to the Scribd homepage and found my Facebook photo there with a note saying I have '26 subscriptions' — basically a list of my Facebook friends who also had a Scribd profile.____

Scribd spokesperson Michelle Laird says the company gives users ""multiple opportunities to opt out"" but if users don't click the ""no thanks"" button, they get an instantly personalized, automatically-filled profile out connecting their Facebook with Scribd.

Since I never saw the ""multiple opportunities to opt-out"" I asked Scribd for a details on when users are presented with opt out options. Turns out, there's just a banner at the top of a page that tells users they can opt out by clicking on it -- so says Scribd, because I don't remember seeing it. So the default is opt in and not opt out, and the notification is something that looks like the banner ads most of us look right through -- not something trying to get your attention as a consequence of something you did or were about to let happen.",Social Media,WIRED,https://www.wired.com/2010/09/scribd-facebook-instant-personalization/,"Scribd's ""instant personalization"" feature has created a privacy nightmare by automatically signing users up and sending notifications to contacts obtained through Facebook's friends list without offering a clear opt out process.",Security & Privacy.
154,"Security, privacy experts weigh in on the ICE doxxing – TechCrunch","In what appears to be the latest salvo in a new, wired form of protest, developer Sam Lavigne posted code that scrapes LinkedIn to find Immigration and Customs Enforcement employee accounts. His code, which basically a Python-based tool that scans LinkedIn for keywords, is gone from Github and Gitlab and Medium took down his original post. The CSV of the data is still available here and here and WikiLeaks has posted a mirror.

“I find it helpful to remember that as much as internet companies use data to spy on and exploit their users, we can at times reverse the story, and leverage those very same online platforms as a means to investigate or even undermine entrenched power structures. It’s a strange side effect of our reliance on private companies and semi-public platforms to mediate nearly all aspects of our lives. We don’t necessarily need to wait for the next Snowden-style revelation to scrutinize the powerful — so much is already hiding in plain sight,” said Lavigne.

Doxxing is the process of using publicly available information to target someone online for abuse. Because we can now find out anything on anyone for a few dollars – a search for “background check” brings up dozens of paid services that can get you names and addresses in a second – scraping public data on LinkedIn seems far easier and innocuous. That doesn’t make it legal.

“Recent efforts to outlaw doxxing at the national level (like the Online Safety Modernization Act of 2017) have stalled in committee, so it’s not strictly illegal,” said James Slaby, Security Expert at Acronis. “But LinkedIn and other social networks usually consider it a violation of their terms of service to scrape their data for personal use. The question of fairness is trickier: doxxing is often justified as a rare tool that the powerless can use against the powerful to call attention to perceived injustices.”

“The problem is that doxxing is a crude tool. The torrent of online ridicule, abuse and threats that can be heaped on doxxed targets by their political or ideological opponents can also rain down on unintended and undeserving targets: family members, friends, people with similar names or appearances,” he said.

The tool itself isn’t to blame. No one would fault a job seeker or salesperson who scraped LinkedIn for targeted employees of a specific company. That said, scraping and publicly shaming employees walks a thin line.

“In my opinion, the professor who developed this scraper tool isn’t breaking the law, as it’s perfectly legal to search the web for publicly available information,” said David Kennedy, CEO of TrustedSec. “This is known in the security space as ‘open source intelligence’ collection, and scrapers are just one way to do it. That said, it is concerning to see ICE agents doxxed in this way. I understand emotions are running high on both sides of this debate, but we don’t want to increase the physical security risks to our law enforcement officers.”

“The decision by Twitter, Github and Medium to block the dissemination of this information and tracking tool makes sense – in fact, law enforcement agents’ personal information is often protected. This isn’t going to go away anytime soon, it’s only going to become more aggressive, particularly as more people grow comfortable with using the darknet and the many available hacking tools for sale in these underground forums. Law enforcement agents need to take note of this, and be much more careful about what (and how often) they post online.”

Ultimately, doxxing is problematic. Because we place our information on public forums there should be nothing to stop anyone from finding and posting it. However, the expectation that people will use our information for good and not evil is swiftly eroding. Today, wrote one security researcher, David Kavanaugh, doxxing is becoming dangerous.

“Going after the people on the ground is like shooting the messenger. Decisions are made by leadership and those are the people we should be going after. Doxxing is akin to a personal attack. Change policy, don’t ruin more lives,” he said.",Social Media,TechCrunch,https://techcrunch.com/2018/06/22/security-privacy-experts-weigh-in-on-the-ice-doxxing/,"Doxxing has become a concerning and potentially dangerous phenomenon, as it can lead to targets being subjected to online ridicule, abuse and threats from their ideological opponents, often with unintended and undeserving victims. It is important to recognize that doxxing is not a legal form of protest, and can have serious consequences.",Security & Privacy
155,The Penetrating Gaze of the Instagram Shame Silo,"This story is part of a collection of pieces on how we spend money today.

By now, you've become accustomed to the tainted synchronicity of targeted marketing. You email or talk with a friend about a given topic—Instant Pots, hiking boots, desk lamps—and boom, said topic shows up in a banner ad the next day. Isn't that crazy? you say to yourself, wondering how long it'll be before the Instant Pots stop following you around. (The answer is never. They will never stop following you. On the bright side, they really will change the way you think about cooking beans.)

How does it happen? Microphones! Location tracking! An endlessly updated holistic fingerprint generated by your unspoken innermost desires! No. Well, maybe kinda yes in some ways, but evolutionarily speaking this is prokaryotic stuff: Most likely you just Googled something. Even if the circumstances seem creepy, it's just as likely that what you're seeing an ad for isn't even something you want to know about.

However. If it's truly insidious marketing you seek—something personalized, perfectly titrated, and just a tiny bit insane—then allow me to introduce you to the Instagram Shame Silo. Specifically, to the 2019 version of the Instant Pot: Star Wars kettlebells.

It began, both literally and figuratively, at 30,000 feet. I was on a plane, scrolling through Instagram, when an ad came through my feed for an ultralightweight backpack, a black waterproof low-profile thing that could pack down into itself. I liked it, it was 20 percent off, and I have a serious backpack problem to begin with. For the first time my life, I tapped through and bought something from an Instagram ad. I already followed a bunch of bag and trail running accounts, making me a prime candidate for a company with some extra inventory to unload. Little did I know that Instagram was just getting started.

WIRED stories about money and consumption.

Around that time, I had joined a new gym. This wasn't a conventional gym, packed with treadmills and weight machines, but one of those CrossFit-adjacent places that programs excruciatingly strategized workouts. On a big whiteboard festooned with acronyms like AMRAP and EMOM, you'd find the day's assortment of kettlebell swings and Spider-Man pushups, mountain climbers and sumo deadlifts. Each night, sweaty and somehow exhilarated, I'd take a photo of the workout and throw it on my Instagram Story.

In my mind, this practice helped me stay accountable one day at a time—the precise time a Story would remain—while avoiding the narcissism of mirror selfies and other thirst trappage disguised as social-media motivation. But in doing so, I'd given my burgeoning interest a geotag and maybe even some AI-scannable text. I had, in other words, asked the vampires inside. And just like that, the ads changed.

Peter Rubin writes about media, culture, and virtual reality for WIRED.

First came the quadriceps. So many quadriceps! So much bulging! So much chalk dust! Then a whole universe of things made out of steel and iron. Clubs. Maces. Something that for some unknown reason are not called Boba Fettlebells. Then all the things to help me recover from the things I did to my quadriceps with the steel and iron! To look at my feed, you’d think I spent my days doing muscle-ups in $70 shorts and my nights strapped to the gills with various recovery devices.

Somehow, I loved it. People may be prone to rabbit holes, but most of us also know there are some interests that don't quite translate into casual conversations—no one wants to be The Guy At Work Who Talks About [Insert New All-Consuming Hobby Here]. Depending on the nerdiness or New Aginess of the hobby, there may even be mild social stigma to the hobby. Or the fact that it taps into a deeper insecurity. Hey, I'm convinced that my arms look like month-old celery stalks, I'm gonna go ahead and foreground that in our lunch conversation! The magic of the shame silo is that it short-circuits all of your reticence and wordlessly ushers you into a world filled with the buyable, fetishizable accoutrements.",Social Media,WIRED,https://www.wired.com/story/penetrating-gaze-instagram-shame-silo/,"The downside of social media is the potential for targeted marketing to exploit our interests, fears, and insecurities. Ads for products and services that we have no intention of buying, but which exploit our vulnerabilities, can appear in our feeds, creating a ""shame silo"" that may make us feel guilty or embarrassed. The result can",Social Norms & Relationships
156,Biden admin will share more info with online platforms on ‘front lines’ of domestic terror fight – TechCrunch,"The Biden administration is outlining new plans to combat domestic terrorism in light of the January 6 attack on the U.S. Capitol and social media companies have their own part to play.

The White House released on Tuesday a new national strategy on countering domestic terrorism. The plan acknowledges the key role that online platforms play in bringing violent ideas into the mainstream, going as far as calling social media sites the “front lines” of the war on domestic terrorism.

“The widespread availability of domestic terrorist recruitment material online is a national security threat whose front lines are overwhelmingly private-sector online platforms, and we are committed to informing more effectively the escalating efforts by those platforms to secure those front lines,” the White House plan states.

The Biden administration committed to more information sharing with the tech sector to fight the tide of online extremism, part of a push to intervene well before extremists can organize violence. According to a fact sheet on the new domestic terror plan, the U.S. government will prioritize “increased information sharing with the technology sector,” specifically online platforms where extremism is incubated and organized.

“Continuing to enhance the domestic terrorism-related information offered to the private sector, especially the technology sector, will facilitate more robust efforts outside the government to counter terrorists’ abuse of Internet-based communications platforms to recruit others to engage in violence,” the White House plan states.

In remarks timed with the release of the domestic terror strategy, Attorney General Merrick Garland asserted that coordinating with the tech sector is “particularly important” for interrupting extremists who organize and recruit on online platforms and emphasized plans to share enhanced information on potential domestic terror threats.

In spite of the new initiatives, the Biden administration admits that domestic terrorism recruitment material will inevitably remain available online, particularly on platforms that don’t prioritize its removal — like most social media platforms, prior to January 2021 — and on end-to-end encrypted apps, many of which saw an influx of users when social media companies cracked down on extremism in the U.S. earlier this year.

“Dealing with the supply is therefore necessary but not sufficient: we must address the demand too,” the White House plan states. “Today’s digital age requires an American population that can utilize essential aspects of Internet-based communications platforms while avoiding vulnerability to domestic terrorist recruitment and other harmful content.”

The Biden administration will also address vulnerability to online extremism through digital literacy programs, including “educational materials” and “skills-enhancing online games” designed to inoculate Americans against domestic extremism recruitment efforts, and presumably disinformation and misinformation more broadly.

The plan stops short of naming domestic terror elements like QAnon and the “Stop the Steal” movement specifically, though it acknowledges the range of ways domestic terror can manifest, from small informal groups to organized militias.

A report from the Office of the Director of National Intelligence in March observed the elevated threat to the U.S. that domestic terrorism poses in 2021, noting that domestic extremists leverage mainstream social media sites to recruit new members, organize in-person events and share materials that can lead to violence.",Social Media,TechCrunch,https://techcrunch.com/2021/06/15/social-media-domestic-terrorism-plan/,"According to the Biden administration's new national strategy on countering domestic terrorism, social media platforms are on the ""front lines"" of the war against domestic terrorism, as they are used to incubate and organize extremist ideas. The plan calls for increased information sharing with the tech sector and digital literacy programs to protect Americans from online extremism, but acknowledges that",Security & Privacy
157,"Platform power is crushing the web, warns Berners-Lee – TechCrunch","On the 29th birthday of the world wide web, its inventor, Sir Tim Berners-Lee, has sounded a fresh warning about threats to the web as a force for good, adding his voice to growing concerns about big tech’s impact on competition and society.

The web’s creator argues that the “powerful weight of a few dominant” tech platforms is having a deleterious impact by concentrating power in the hands of gatekeepers that gain “control over which ideas and opinions are seen and shared”.

His suggested fix is socially minded regulation, so he’s also lending his clout to calls for big tech to be ruled.

“These dominant platforms are able to lock in their position by creating barriers for competitors,” Berners-Lee writes in an open letter published today on the Web Foundation’s website. “They acquire startup challengers, buy up new innovations and hire the industry’s top talent. Add to this the competitive advantage that their user data gives them and we can expect the next 20 years to be far less innovative than the last.”

The concentration of power in the hands of a few mega platforms is also the source of the current fake news crisis, in Berners-Lee’s view, because he says platform power has made it possible for people to “weaponise the web at scale” — echoing comments made by the UK prime minister last year when she called out Russia for planting fakes online to try to disrupt elections.

“In recent years, we’ve seen conspiracy theories trend on social media platforms, fake Twitter and Facebook accounts stoke social tensions, external actors interfere in elections, and criminals steal troves of personal data,” he writes, pointing out that the current response of lawmakers has been to look “to the platforms themselves for answers” — which he argues is neither fair nor likely to be effective.

In the EU, for example, the threat of future regulation is being used to encourage social media companies to sign up to a voluntary code of conduct aimed at speeding up takedowns of various types of illegal content, including terrorist propaganda. Though the Commission is also seeking to drive action against a much broader set of online content issues — such as hate speech, commercial scams and even copyrighted material.

Critics argue its approach risks chilling free expression via AI-powered censorship.

Some EU member states have gone further too. Germany now has a law with big fines for social media platforms that fail to comply with hate speech takedown requirements, for example, while in the UK ministers are toying with new rules, such as placing limits on screen time for children and teens.

Both the Commission and some EU member states have been pushing for increased automation of content moderation online. In the UK last month, ministers unveiled an extremism blocking tool which the government had paid a local AI company to develop, with the Home Secretary warning she had not ruled out forcing companies to use it.

Meanwhile, in the US, Facebook has faced huge pressure in recent years as awareness has grown of how extensively its platform is used to spread false information, including during the 2016 presidential election.

The company has announced a series of measures aimed at combating the spread of fake news generally, and reducing the risk of election disinformation specifically — as well as a major recent change to its news feed algorithm ostensibly to encourage users towards having more positive interactions on its platform.

But Berners-Lee argues that letting commercial entities pull levers to try to fix such a wide-ranging problem is a bad idea — arguing that any fixes companies come up with will inexorably be restrained by their profit-maximizing context and also that they amount to another unilateral impact on users.

A better solution, in his view, is not to let tech platform giants self-regulate but to create a framework for ruling them that factors in “social objectives”.

A year ago Berners-Lee also warned about the same core threats to the web. Though he was less coherent in his thinking then that regulation could be the solution — instead flagging up a variety of initiatives aimed at trying to combat threats such as the systematic background harvesting of personal data. So he seems to be shifting towards backing the idea of an overarching framework to control the tech that’s being used to control us.

“Companies are aware of the problems and are making efforts to fix them — with each change they make affecting millions of people,” he writes now. “The responsibility — and sometimes burden — of making these decisions falls on companies that have been built to maximise profit more than to maximise social good. A legal or regulatory framework that accounts for social objectives may help ease those tensions.”

Berners-Lee’s letter also emphasizes the need for diversity of thought in shaping any web regulations to ensure rules don’t get skewed towards a certain interest or group. And he makes a strong call for investments to help close the global digital divide.

“The future of the web isn’t just about those of us who are online today, but also those yet to connect,” he warns. “Today’s powerful digital economy calls for strong standards that balance the interests of both companies and online citizens. This means thinking about how we align the incentives of the tech sector with those of users and society at large, and consulting a diverse cross-section of society in the process.”

Another specific call he makes is for fresh thinking about Internet business models, arguing that online advertising should not be accepted as the only possible route for sustaining web platforms. “We need to be a little more creative,” he argues.

“While the problems facing the web are complex and large, I think we should see them as bugs: problems with existing code and software systems that have been created by people — and can be fixed by people. Create a new set of incentives and changes in the code will follow. We can design a web that creates a constructive and supportive environment,” he adds.

“Today, I want to challenge us all to have greater ambitions for the web. I want the web to reflect our hopes and fulfil our dreams, rather than magnify our fears and deepen our divisions.”

At the time of writing Amazon, Facebook, Google and Twitter had not responded to a request for comment.",Social Media,TechCrunch,https://techcrunch.com/2018/03/12/platform-power-is-crushing-the-web-warns-berners-lee/,"The concentration of power in the hands of a few big tech platforms has enabled people to ""weaponize the web at scale"" and caused a fake news crisis, with the current response of lawmakers to look to the platforms themselves for solutions.","Information, Discourse & Governance"
158,Instagram Will Test Hiding 'Likes' in the US Starting Next Week,"If you post a picture, and no one sees how many people liked it, does it still exist? Instagram users in the US are going to find out next week. Months after the company tested hiding ""like"" counts in Australia, Brazil, Canada, Ireland, Italy, Japan, and New Zealand, CEO Adam Mosseri announced today at WIRED25 that some Instagram users in America can expect their like counts to vanish from public view.

The company will begin testing next week, at first rolling out the change to a limited number of accounts.

Instagram isn’t the only company that is attempting to remove publicly available engagement metrics from their platform. Facebook (which owns Instagram), Twitter, and YouTube have all experimented with removing engagement metrics from their platforms. As WIRED previously reported, social media researchers have argued that when users tailor their content to whatever garners the most engagement (or outrage), the result is a radicalized environment that makes healthy, happy conversations almost impossible.

Hiding like counts is just the latest step in Instagram’s quest to become the safest place on the internet, along with algorithms and filters to remove offensive or divisive comments or pictures. But the move hasn’t come without panicked pushback from users, who, among other complaints, note that hiding engagement metrics will make it harder to determine whose follower count is legitimate.

WIRED's Arielle Pardes talked to Mosseri and actor and producer Tracee Ellis Ross, best known for her starring role in the television series Black-ish. Ross recently launched Pattern Beauty, a curly hair care company. Instagram sales serve as its main source of revenue, according to Ross.

Balancing the needs of artists, brands, and the average user is difficult. But Mosseri emphasized that Instagram will always place the needs of people first. ""It means we’re going to put a 15-year-old kid’s interests before a public speaker’s interest,"" he says. ""When we look at the world of public content, we’re going to put people in that world before organizations and corporations.""",Social Media,WIRED,https://www.wired.com/story/instagram-hiding-likes-adam-mosseri-tracee-ellis-ross-wired25/,"Social media platforms are being criticized for creating a radicalized environment due to users tailoring content to garner the most engagement or outrage. To combat this, companies such as Facebook, Twitter, and YouTube are experimenting with removing engagement metrics from their platforms, with Instagram set to begin testing the change in the US next week.",Social Norms & Relationships
159,Singapore’s proposed ‘fake news’ law could stifle free speech – TechCrunch,"For many, Singapore is an idyllic and livable city in Asia. But there’s serious concern for the country and its five million population around a proposed law to curb ‘fake news’ on the internet that could have ramifications for free speech.

The ‘Protection from Online Falsehoods and Manipulation Bill’ had its first reading on Monday and one of the key takeaways is that it will allow the government to force “corrections” to be added to online content that is deemed to be “false.” Infringing articles won’t be edited, instead “the facts” will be added so that “the facts can travel together with the falsehood.”

The scope of the proposed bill goes beyond media to cover social media platforms, too. Those found to be “malicious actors” face a fine of up to SG$50,000 ($37,000) or five years in prison for their content. If posted using “an inauthentic online account or a bot,” the fine jumps to a maximum of SG$100,000 ($74,000) or a potential 10-year jail term. Platforms such as Facebook or Twitter face fines of up to SG$1 million ($740,000) in such situations.

What’s particularly alarming about the proposal is that it can be activated by any government minister if they believe that “a false statement of fact… has been or is being communicated in Singapore” or if they feel that issuing a correction is “in the public interest.”

The proposed act is focused on Singapore, but it will cover any piece of content worldwide. While, beyond merely covering content pertinent to the security of Singapore, the harmony of its people, its national politics and services, the process can be triggered “in the interest of friendly relations of Singapore with other countries.”

There’s also a clause that covers “a diminution of public confidence in the performance of any duty or function of government” and its associated organizations.

While it is fairly well established that social media and new media content can be a threat to multicultural societies and the democratic process, critics have pointed out that the proposed law has seriously scope to be misused, potentially against valid criticism. While Singapore’s Ministry of Law said it will not cover “opinions, criticisms, satire or parody,” simply defining what is an opinion or opinionated is not easy.

Indeed, an example last year involving Reuters shows the type of pushback that the government could exert if the bill becomes law as is expected — Singapore’s ruling People’s Action Party (PAP) party dominates parliament having won 83 of the 89 seats it contested in the most recent general election in 2015.

Reuters rewrote a contentious headline around a politician’s potential to become Prime Minister following condemnation from Singapore’s Ministry of Communications and Information (MCI), which rebuked Reuters running with a “fabricated headline.” The publication later changed the headline and parts of its story, as Cherian George — the author of a 2012 book on Singapore’s political system — explained in a recent blog post.

In this case, however, the law covers content produced outside Singapore, which could make its application messy, particularly if media companies covering international topics are caught in the crosshairs and they have employees located in Singapore.

Despite footnotes that try to untangle the policing of accurate content with censoring free speech — “the bill targets falsehoods, not free speech,” a press release issued by the ministry claims — free speech groups are concerned at the potentially immense power that would be wielded.

“Singapore’s ministers should not have the power to singlehandedly decree what is true and what is false,” Phil Robertson, deputy Asia director of Human Rights Watch said in a statement. “Given Singapore’s long history of prohibiting speech critical of the government, its policies or its officials, its professed concerns about ‘online falsehoods’ and alleged election manipulation are farcical.”

That was echoed by the Asia Internet Coalition, a group that represents Facebook, Google, Twitter, LinkedIn, Line and others.

“We are concerned that the proposed legislation gives the Singapore government full discretion over what is considered true or false. As the most far-reaching legislation of its kind to date, this level of overreach poses significant risks to freedom of expression and speech, and could have severe ramifications both in Singapore and around the world,” read a statement from AIC managing director Jeff Paine.

“Prescriptive legislation should not be the first solution in addressing what is a highly nuanced and complex issue,” Paine added.

Media freedom concern is not new to Singapore, where lawsuits and legal cases involving the government and citizens for content posted online are not uncommon.

Human Rights Watch’s 2017 report concluded that Singapore’s press is “not free.” Reporters Without Borders, another organization that tracks media freedom worldwide, ranked Singapore 151th out of 180 countries. The organization cited an “intolerant government” and media “self-censorship” among its top line conclusions. The proposed law could take things a step further.",Social Media,TechCrunch,https://techcrunch.com/2019/04/03/singapore-fake-news-law-free-speech/,Critics of Singapore's proposed law to curb 'fake news' on the internet are concerned that it could have serious ramifications for free speech and be misused to stifle valid criticism. The legislation could also lead to significant fines and jail terms for those found to be malicious actors.,"Information, Discourse & Governance"
160,WhatsApp raises minimum age to 16 in Europe ahead of GDPR – TechCrunch,"Tech giants are busy updating their T&Cs ahead of the EU’s incoming data protection framework, GDPR. Which is why, for instance, Facebook-owned Instagram is suddenly offering a data download tool. You can thank European lawmakers for being able to take your data off that platform.

Facebook-owned WhatsApp is also making a pretty big change as a result of GDPR — noting in its FAQs that it’s raising the minimum age for users of the messaging platform to 16 across the “European Region“. This includes in both EU and non-EU countries (such as Switzerland), as well as the in-the-process-of-brexiting UK (which is set to leave the EU next year).

In the US, the minimum age for WhatsApp usage remains 13.

Where teens are concerned GDPR introduces a new provision concerning children’s personal data — setting a 16-year-old age limit on kids being able to consent to their data being processed — although it does allow some wiggle room for individual countries to write a lower age limit into their laws, setting a hard cap at 13-years-old.

WhatsApp isn’t bothering to try to vary the age gate depending on limits individual EU countries have set, though. Presumably to reduce the complexity of complying with the new rules.

But also likely because it’s confident WhatsApp-loving teens won’t have any trouble circumventing the new minimum age limit. And therefore that there’s no real risk to its business because teenagers will easily ignore the rules.

Certainly it’s unclear whether WhatsApp and its parent Facebook will do anything at all to enforce the age limit — beyond asking users to state they are at least 16 (and taking them at their word). So in practice, while on paper the 16-years-old minimum seems like a big deal, the change may do very little to protect teens from being data-mined by the ad giant.

We’ve asked WhatsApp whether it will cross-check users’ accounts with Facebook accounts and data holdings to try to verify a teen really is 16, for example, but nothing in its FAQ on the topic suggests it plans to carry out any active enforcement at all — instead it merely notes:

Creating an account with false information is a violation of our Terms

Registering an account on behalf of someone who is underage is also a violation of our Terms

Ergo, that does sound very much like a buck being passed. And it will likely be up to parents to try to actively enforce the limit — by reporting their own underage WhatApp-using kids to the company (which would then have to close the account). Clearly few parents would relish the prospect of doing that.

Yet Facebook does already share plenty of data between WhatsApp and its other companies for all sorts of self-serving, business-enhancing purposes — and even including, as it couches it, “to ensure safety and security”. So it’s hardly short of data to carry out some age checks of its own and proactively enforce the limit.

One curious difference is that Facebook’s approach to teen usage of WhatsApp is notably distinct to the one it’s taking with teens on its main social platform — also as it reworks the Facebook T&Cs ahead of GDPR.

Under the new terms there Facebook users between the ages of 13 and 15 will need to get parental permission to be targeted with ads or share sensitive info on Facebook.

But again, as my TC colleague Josh Constine pointed out, the parental consent system Facebook has concocted is laughably easy for teens to circumvent — merely requiring they select one of their Facebook friends or just enter an email address (which could literally be an alternative email address they themselves control). That entirely unverified entity is then asked to give ‘consent’ for their ‘child’ to share sensitive info. So, basically, a total joke.

As we’ve said before, Facebook’s approach to GDPR ‘compliance’ is at best described as ‘doing the minimum possible’. And data protection experts say legal challenges are inevitable.

Update: WhatsApp has now confirmed to us it is raising the minimum age to use its service from 13 to 16 across the EU in order to comply with GDPR. It also said that because it collects limited categories of information from its users it had to make a tradeoff between collecting more personal information or keeping it simple and raising the minimum age across the board in the region. Hence it’s taking a different approach here vs Facebook.

The company also told us that users in the region will be asked to confirm whether they are at least 16 years old when they are presented with its updated terms of service. But it will not start asking for a user’s date of birth.

Also in Europe Facebook has previously been forced via regulatory intervention to give up one portion of the data sharing between its platforms — specifically for ad targeting purposes. However its WhatsApp T&Cs also suggest it is confident it will find a way to circumvent that in future, as it writes it “will only do so when we reach an understanding with the Irish Data Protection Commissioner on a future mechanism to enable such use” — i.e. when, not if.

Last month it also signed an undertaking with the DPC on this related to GDPR compliance, so again appears to have some kind of regulatory-workaround ‘mechanism’ in the works.",Social Media,TechCrunch,https://techcrunch.com/2018/04/25/whatsapp-raises-minimum-age-to-16-in-europe-ahead-of-gdpr/,"The main undesirable consequence of Social Media discussed here is the very low minimum age for usage, which allows for teenagers to be exposed to potential data-mining by giants such as Facebook. Furthermore, the enforcement of this age requirement is minimal and easily circumvented, leading to further potential risks.",Security & Privacy
161,Not on Nextdoor? You can still grab your neighbors’ stuff on Free Finds – TechCrunch,"Nextdoor, the app that helps neighbors connect, launched a new feature called Free Finds today, which will help people browse the free items available in their neighborhoods. Since the start of 2020, monthly listings to buy, sell and give away items on Nextdoor have increased by 80%, but 25% of these listings advertised free stuff. So, the company decided to create a more streamlined way to get the word out about cool, free stuff on the curbs of your neighborhood.

You don’t have to be a member of Nextdoor to scroll through the free listings. Typically, becoming a member can be a complicated process that requires you to verify your home address via snail mail. But now, whether you’re looking for a free blender or seeking your next trash-to-treasure upcycle project, you can browse what’s up for grabs in your neighborhood.

To contact the seller, you need to set up a free account and go through the standard Nextdoor sign-up process. If your cell service billing address is at the same address where you live, the sign-up process is quick — you can verify your address via text. But, if the addresses don’t match (read: if you’re still on your parents’ family plan), it can take up to 10 days to receive an invitation letter to become a verified Nextdoor member. By then, that lightly worn pair of boots might be long gone.

If you live in a densely populated area, you can probably find a neighborhood “free and for sale”-themed group pretty easily on Facebook. But, at the outset, Nextdoor adds a level of functionality by filtering items into categories, like “for young ones,” “for plant parents,” “spoil your pets” and “hidden treasures.” It could also appeal to those who don’t want to deal with browsing through multiple Facebook groups, including those that stretch beyond their neighborhood to nearby areas that would require a commute.

Nextdoor emphasizes the environmental benefits of a feature like Free Finds, which can help neighbors reduce waste when they discard perfectly usable items — instead, they can share resources with their neighbors. But more broadly, Free Finds is about leveraging people’s interest in free stuff to grow Nextdoor’s user base.

It also comes at a time when Facebook is threatening Nextdoor more directly. The tech giant launched its Neighborhoods feature in Canada last month, which is an obvious Nextdoor clone (Facebook copying other social media apps? Stop me if you’ve heard this one before). The feature should roll out soon for U.S. users.

Over the last year, Nextdoor has launched multiple initiatives that aim to support communities, like Sell for Good, which allowed users to sell items on the social network and donate proceeds to nonprofit causes. In response to the coronavirus outbreak, it also added features like Help Maps, Groups, a fundraising option for local businesses and a neighborly assistance program created with Walmart.

Still, some consumers have become understandably skeptical of neighborhood-based social media apps. The “Black Mirror”-adjacent crime-reporting app Citizen recently came under fire when its CEO Andrew Frame bribed users with $30,000 to catch an arsonist using the app’s new livestreaming service, but had targeted the wrong person. Nextdoor, meanwhile, had in the past developed such reputation for racial profiling that the company eventually had to roll out special tools to address this. Today, it still faces accusations of allowing unneighborly behavior, including political discussions and other posts that can make minority groups feel unwelcome or even unsafe.

Ultimately, investing in new products that encourage the opposite behavior — neighbors helping neighbors, as Free Finds offers — can only go so far to combat the app’s reputation.

The new Free Finds feature is live today in all the countries where Nextdoor operates at either nextdoor.com/freefinds or by visiting the Nextdoor Finds section in the Nextdoor app.",Social Media,TechCrunch,https://techcrunch.com/2021/06/03/not-on-nextdoor-you-can-still-grab-your-neighbors-stuff-on-free-finds/,"Nextdoor's new Free Finds feature seeks to connect neighbors and reduce waste by allowing people to browse free items in their neighborhoods, but the app has faced criticism in the past for allowing unneighborly behavior and making minority groups feel unwelcome or unsafe.",Social Norms & Relationships
162,Instagram's New Story Highlights Save Your Disappearing Videos Forever,"If you're ever in the mood for a minor existential crisis, go through your Instagram profile and try to decipher what it says about you. Are you an insatiable foodie, traversing the world to find the best bite? Are you always on-trend, always well-lit, always wearing the very latest? Do you have kids? Do you selfie? On a platform that encourages every post to be perfect and beautiful for maximum engagement, you can tell a lot about someone's priorities from their image grid. And as Instagram's popularity continues exploding, that impression matters more than ever.

Starting today, Instagram profiles might start to look a little different. Don't panic: The image grid is still there, and it's still three by three. But above the grid, if you choose, you can now offer a sort of mixtape of your Instagram Stories, which the company hopes will help you give fans an even better sense of who you are.

From now on, every story you capture will automatically download to your Instagram archive. (You still have to manually save anything you want to keep on your phone's camera roll.) They'll go their by default, next to the posts you've chosen to yank off your profile because that beer-pong photo doesn't really fit your #brand anymore. But you can choose to share them through what Instagram is calling ""Story Highlights,"" permanent groups of your stories that live on your profile. They'll still leave your story after 24 hours, but they can now live forever.

Let's say you're a travel photographer, on a trip to the Maldives. You're only going to post a couple of times, but you'll be taking stories the whole time. Normally those stories would be gone forever after 24 hours. Now you can create a collection called ""Maldives trip,"" save your favorite stories there, and keep it on your profile as a behind-the-scenes experience. Other people can respond to those stories as they normally would, and your view counts accrue over time.

For Instagram, the move seems to reflect the company's effort to make profiles a little less stuffy and pristine, and a little more human and fun. ""People get to know you through your profile,"" says Robby Stein, Instagram's product lead for sharing, and the company wants to make the experience as well-rounded as possible. Stories in general tend to be less edited and curated, and thus a better window into real life. With Story Highlights, you can show that side of yourself to new people, too, not just those who were already following you.

The risk, of course, is that by allowing stories to live forever, Instagram could change the nature of stories altogether. Maybe you won't be so goofy when you're thinking about where this'll go in your highlight reel. Not to mention, there's something decidedly different about a stories product that saves everything forever, even when nobody but you can see it. But as stories become a dominant form of sharing, it makes sense that Instagram would look for ways to keep them around. Because there's no fun in doing it for the 'gram if nobody can see it tomorrow.",Social Media,WIRED,https://www.wired.com/story/instagrams-new-story-highlights-save-your-disappearing-videos-forever/,"The risk of Instagram's Story Highlights feature is that by allowing stories to live forever, users may be less willing to post less edited and curated content, which can limit the ability to give followers a genuine look into their lives.",User Experience & Entertainment
163,A mathematician walks into a bar (of disinformation) – TechCrunch,"Disinformation, misinformation, infotainment, algowars — if the debates over the future of media the past few decades have meant anything, they’ve at least left a pungent imprint on the English language. There’s been a lot of invective and fear over what social media is doing to us, from our individual psychologies and neurologies to wider concerns about the strength of democratic societies. As Joseph Bernstein put it recently, the shift from “wisdom of the crowds” to “disinformation” has indeed been an abrupt one.

What is disinformation? Does it exist, and if so, where is it and how do we know we are looking at it? Should we care about what the algorithms of our favorite platforms show us as they strive to squeeze the prune of our attention? It’s just those sorts of intricate mathematical and social science questions that got Noah Giansiracusa interested in the subject.

Giansiracusa, a professor at Bentley University in Boston, is trained in mathematics (focusing his research in areas like algebraic geometry), but he’s also had a penchant of looking at social topics through a mathematical lens, such as connecting computational geometry to the Supreme Court. Most recently, he’s published a book called “How Algorithms Create and Prevent Fake News” to explore some of the challenging questions around the media landscape today and how technology is exacerbating and ameliorating those trends.

I hosted Giansiracusa on a Twitter Space recently, and since Twitter hasn’t made it easy to listen to these talks afterwards (ephemerality!), I figured I’d pull out the most interesting bits of our conversation for you and posterity.

This interview has been edited and condensed for clarity.

Danny Crichton: How did you decide to research fake news and write this book?

Noah Giansiracusa: One thing I noticed is there’s a lot of really interesting sociological, political science discussion of fake news and these types of things. And then on the technical side, you’ll have things like Mark Zuckerberg saying AI is going to fix all these problems. It just seemed like, it’s a little bit difficult to bridge that gap.

Everyone’s probably heard this recent quote of Biden saying, “they’re killing people,” in regards to misinformation on social media. So we have politicians speaking about these things where it’s hard for them to really grasp the algorithmic side. Then we have computer science people that are really deep in the details. So I’m kind of sitting in between, I’m not a real hardcore computer science person. So I think it’s a little easier for me to just step back and get the bird’s-eye view.

At the end of the day, I just felt I kind of wanted to explore some more interactions with society where things get messy, where the math is not so clean.

Crichton: Coming from a mathematical background, you’re entering this contentious area where a lot of people have written from a lot of different angles. What are people getting right in this area and what have people perhaps missed some nuance?

Giansiracusa: There’s a lot of incredible journalism; I was blown away at how a lot of journalists really were able to deal with pretty technical stuff. But I would say one thing that maybe they didn’t get wrong, but kind of struck me was, there’s a lot of times when an academic paper comes out, or even an announcement from Google or Facebook or one of these tech companies, and they’ll kind of mention something, and the journalist will maybe extract a quote, and try to describe it, but they seem a little bit afraid to really try to look and understand it. And I don’t think it’s that they weren’t able to, it really seems like more of an intimidation and a fear.

One thing I’ve experienced a ton as a math teacher is people are so afraid of saying something wrong and making a mistake. And this goes for journalists who have to write about technical things, they don’t want to say something wrong. So it’s easier to just quote a press release from Facebook or quote an expert.

One thing that’s so fun and beautiful about pure math, is you don’t really worry about being wrong, you just try ideas and see where they lead and you see all these interactions. When you’re ready to write a paper or give a talk, you check the details. But most of math is this creative process where you’re exploring, and you’re just seeing how ideas interact. My training as a mathematician you think would make me apprehensive about making mistakes and to be very precise, but it kind of had the opposite effect.

Second, a lot of these algorithmic things, they’re not as complicated as they seem. I’m not sitting there implementing them, I’m sure to program them is hard. But just the big picture, all these algorithms nowadays, so much of these things are based on deep learning. So you have some neural net, doesn’t really matter to me as an outsider what architecture they’re using, all that really matters is, what are the predictors? Basically, what are the variables that you feed this machine learning algorithm? And what is it trying to output? Those are things that anyone can understand.

Crichton: One of the big challenges I think of analyzing these algorithms is the lack of transparency. Unlike, say, the pure math world which is a community of scholars working to solve problems, many of these companies can actually be quite adversarial about supplying data and analysis to the wider community.

Giansiracusa: It does seem there’s a limit to what anyone can deduce just by kind of being from the outside.

So a good example is with YouTube — teams of academics wanted to explore whether the YouTube recommendation algorithm sends people down these conspiracy theory rabbit holes of extremism. The challenge is that because this is the recommendation algorithm, it’s using deep learning, it’s based on hundreds and hundreds of predictors based on your search history, your demographics, the other videos you’ve watched and for how long — all these things. It’s so customized to you and your experience, that all the studies I was able to find use incognito mode.

So they’re basically a user who has no search history, no information and they’ll go to a video and then click the first recommended video then the next one. And let’s see where the algorithm takes people. That’s such a different experience than an actual human user with a history. And this has been really difficult. I don’t think anyone has figured out a good way to algorithmically explore the YouTube algorithm from the outside.

Honestly, the only way I think you could do it is just kind of like an old-school study where you recruit a whole bunch of volunteers and sort of put a tracker on their computer and say, “Hey, just live life the way you normally do with your histories and everything and tell us the videos that you’re watching.” So it’s been difficult to get past this fact that a lot of these algorithms, almost all of them, I would say, are so heavily based on your individual data. We don’t know how to study that in the aggregate.

And it’s not just that me or anyone else on the outside who has trouble because we don’t have the data. It’s even people within these companies who built the algorithm and who know how the algorithm works on paper, but they don’t know how it’s going to actually behave. It’s like Frankenstein’s monster: they built this thing, but they don’t know how it’s going to operate. So the only way I think you can really study it is if people on the inside with that data go out of their way and spend time and resources to study it.

Crichton: There are a lot of metrics used around evaluating misinformation and determining engagement on a platform. Coming from your mathematical background, do you think those measures are robust?

Giansiracusa: People try to debunk misinformation. But in the process, they might comment on it, they might retweet it or share it, and that counts as engagement. So a lot of these measurements of engagement, are they really looking at positive or just all engagement? You know, it kind of all gets lumped together.

This happens in academic research, too. Citations are the universal metric of how successful research is. Well, really bogus things like Wakefield’s original autism and vaccines paper got tons of citations, a lot of them were people citing it because they thought it’s right, but a lot of it was scientists who were debunking it, they cite it in their paper to say, we demonstrate that this theory is wrong. But somehow a citation is a citation. So it all counts towards the success metric.

So I think that’s a bit of what’s happening with engagement. If I post something on my comments saying, “Hey, that’s crazy,” how does the algorithm know if I’m supporting it or not? They could use some AI language processing to try but I’m not sure if they are, and it’s a lot of effort to do so.

Crichton: Lastly, I want to talk a bit about GPT-3 and the concern around synthetic media and fake news. There’s a lot of fear that AI bots will overwhelm media with disinformation — how scared or not scared should we be?

Giansiracusa: Because my book really grew out of a class from experience, I wanted to try to stay impartial, and just kind of inform people and let them reach their own decisions. I decided to try to cut through that debate and really let both sides speak. I think the newsfeed algorithms and recognition algorithms do amplify a lot of harmful stuff, and that is devastating to society. But there’s also a lot of amazing progress of using algorithms productively and successfully to limit fake news.

There’s these techno-utopians, who say that AI is going to fix everything, we’ll have truth-telling, and fact-checking and algorithms that can detect misinformation and take it down. There’s some progress, but that stuff is not going to happen, and it never will be fully successful. It’ll always need to rely on humans. But the other thing we have is kind of irrational fear. There’s this kind of hyperbolic AI dystopia where algorithms are so powerful, kind of like singularity type of stuff that they’re going to destroy us.

When deep fakes were first hitting the news in 2018, and GPT-3 had been released a couple years ago, there was a lot of fear that, “Oh shit, this is gonna make all our problems with fake news and understanding what’s true in the world much, much harder.” And I think now that we have a couple of years of distance, we can see that they’ve made it a little harder, but not nearly as significantly as we expected. And the main issue is kind of more psychological and economic than anything.

So the original authors of GPT-3 have a research paper that introduces the algorithm, and one of the things they did was a test where they pasted some text in and expanded it to an article, and then they had some volunteers evaluate and guess which is the algorithmically-generated one and which article is the human-generated one. They reported that they got very, very close to 50% accuracy, which means barely above random guesses. So that sounds, you know, both amazing and scary.

But if you look at the details, they were extending like a one line headline to a paragraph of text. If you tried to do a full, The Atlantic-length or New Yorker-length article, you’re gonna start to see the discrepancies, the thought is going to meander. The authors of this paper didn’t mention this, they just kind of did their experiment and said, “Hey, look how successful it is.”

So it looks convincing, they can make these impressive articles. But here’s the main reason, at the end of the day, why GPT-3 hasn’t been so transformative as far as fake news and misinformation and all this stuff is concerned. It’s because fake news is mostly garbage. It’s poorly written, it’s low quality, it’s so cheap and fast to crank out, you could just pay your 16-year-old nephew to just crank out a bunch of fake news articles in minutes.

It’s not so much that math helped me see this. It’s just that somehow, the main thing we’re trying to do in mathematics is to be skeptical. So you have to question these things and be a little skeptical.",Social Media,TechCrunch,https://techcrunch.com/2021/08/20/a-mathematician-walks-into-a-bar-of-disinformation/,"Social media has had an undesirable impact on society, leading to an increase in misinformation, disinformation, and the spread of conspiracy theories, which can have damaging effects on individuals and democratic societies. Algorithmic decision-making has made it difficult to know what content is true and which is false, making it hard to combat these negative effects.","Information, Discourse & Governance"
164,"Facebook loses final appeal in defamation takedown case, must remove same and similar hate posts globally – TechCrunch","Austria’s Supreme Court has dismissed Facebook’s appeal in a long running speech takedown case — ruling it must remove references to defamatory comments made about a local politician worldwide for as long as the injunction lasts.

The Austrian Supreme Court has now delivered its final judgment in the #Glawischnig-Piesczek case. #Facebook has to remove the defamatory comment and any comments that are identical or equivalent in meaning worldwide.https://t.co/zhbYRr4DnG https://t.co/Y2W5SWRgO6 — Clara Rauchegger (@ClaraRauchegger) November 12, 2020

We’ve reached out to Facebook for comment on the ruling.

Green Party politician Eva Glawischnig successfully sued the social media giant seeking removal of defamatory comments made about her by a user of its platform after Facebook had refused to take down the abusive postings — which referred to her as a “lousy traitor”, a “corrupt tramp” and a member of a “fascist party”.

After a preliminary injunction in 2016, Glawischnig won local removal of the defamatory postings the next year but continued her legal fight — pushing for similar postings to be removed and take downs to also be global.

Questions were referred up to the EU’s Court of Justice. And in a key judgement last year the CJEU decided platforms can be instructed to hunt for and remove illegal speech worldwide without falling foul of European rules that preclude platforms from being saddled with a “general content monitoring obligation”. Today’s Austrian Supreme Court ruling flows naturally from that.

Austrian newspaper Der Standard reports that the court confirmed the injunction applies worldwide, both to identical postings or those that carry the same essential meaning as the original defamatory posting.

It said the Austrian court argues that EU Member States and civil courts can require platforms like Facebook to monitor content in “specific cases” — such as when a court has identified user content as unlawful and “specific information” about it — in order to prevent content that’s been judged to be illegal from being reproduced and shared by another user of the network at a later point in time with the overarching aim of preventing future violations.

The case has important implications for the limitations of online speech.

Regional lawmakers are also working on updating digital liability regulations. Commission lawmakers have said they want to force platforms to take more responsibility for the content they fence and monetize — fuelled by concerns about the impact of online hate speech, terrorist content and divisive disinformation.

A longstanding EU rule, prohibiting Member States from putting a general content monitoring obligation on platforms, limits how they can be forced to censor speech. But the CJEU ruling has opened the door to bounded monitoring of speech — in instances where it’s been judged to be illegal — and that in turn may influence the policy substance of the Digital Services Act which the Commission is due to publish in draft early next month.

In a reaction to last year’s CJEU ruling, Facebook argued it “opens the door to obligations being imposed on internet companies to proactively monitor content and then interpret if it is ‘equivalent’ to content that has been found to be illegal”.

“In order to get this right national courts will have to set out very clear definitions on what ‘identical’ and ‘equivalent’ means in practice. We hope the courts take a proportionate and measured approach, to avoid having a chilling effect on freedom of expression,” it added.",Social Media,TechCrunch,https://techcrunch.com/2020/11/12/facebook-loses-final-appeal-in-defamation-takedown-case-must-remove-same-and-similar-hate-posts-globally/,"The Austrian Supreme Court ruling has implications for the limitations of online speech, potentially forcing platforms to take more responsibility for the content they fence and monetize, and potentially having a chilling effect on freedom of expression.","Information, Discourse & Governance"
165,Lawmaker Calls for Limits on Exporting Net-Spying Tools,"Congressman Bill Keating plans to introduce legislation putting limits on U.S. companies selling net monitoring equipment to repressive regimes, after news that a Boeing subsidiary sold powerful net inspection technology to Egypt's state telecom.

""The Iranian and Egyptian protests have taught us that social media can be as powerful as any gun,"" said Rep. Keating (D-Massachusetts). ""Companies that are selling technology to countries that are using it to perpetuate human rights abuses must work with Congress to make this right.

""We should have the same safeguards – such as end user monitoring agreements – that we do when we sell weapons abroad.""

At issue is a company called Narus, which makes powerful deep packet inspection technology that can monitor the net's fattest pipes to see what traffic is passing through -- including reconstructing online phone calls, e-mails, instant messages, and web surfing activities.

Tim Karr, the campaign director for the Washington, D.C.-based advocacy group Free Press, noticed last week that Narus had sold its surveillance technology to the state-run Telecom Egypt, as well as to other repressive regimes including Saudi Arabia.

Karr says its time the U.S. government realized the power of such equipment to repress people and put limits on its distribution. That's especially true in light of evidence from the revolutions in Tunisia and Egypt that social networking sites such as Facebook can be powerful tools for organizing, publicizing, recruiting and sustaining pro-democracy forces, according to Karr.

Mubarak's regime, which was toppled Friday after weeks of protest, was so threatened by power of the net to allow citizens to mobilize that it took the extraordinary step of shutting down Egypt's internet and mobile phone networks for almost a week in late January.

During the protests, Egypt also imprisoned a number of online activists, including Google executive Wael Ghonim who administered one of the Facebook pages that served as an online café for organizing the protests.

It's not clear what technology, if any, Egypt's once feared intelligence services used to track them down.

But, as Evegny Morozov argues in his recent book ""The Net Delusion"", social networking tools can make it easy for a repressive regime to track down activists. That's clearly seen in Tunisia, where a government controlled ISP stole Facebook usernames and passwords in an attempt to erase anti-government pages.

That's why Karr finds Narus's sale of its technology to Egypt so egregious.

""Narus basically gave a hammer to a Mubarak regime that sees its political opponent as nails,"" Karr said. ""Congress or the state department can convince them to disclose the ways they are selling this tech and to whom and for what purposes.""

Egypt in particular galled Karr, since the Mubarak regime routinely jailed bloggers, and is counted as one of 13 ""enemies of the internet"" as compiled by Reporters Without Borders.

Narus declined to respond to multiple voice mail messages left for its CEO Greg Oslan this week.

Controversy is not new to the Sunnyvale company, which was founded in 1997 and purchased by defense contracting giant Boeing in 2010. The company first came to notoriety in 2005, when it was found to be the processing brain behind the NSA's warrantless wiretapping of the internet inside an AT&T facility in San Francisco.",Social Media,WIRED,https://www.wired.com/2011/02/narus/,"Social media can be used by repressive regimes to track down activists, and companies like Narus are providing the technology to do this, making it an urgent issue for Congress to confront and limit.",Security & Privacy
166,Parler crawls back online empty and with a Tea Party CEO – TechCrunch,"Parler, a social network adopted by the far right and recently kicked off AWS for its userbase’s habit of advocating violence, is back online. The restoration questions the notion that “big tech” can take and keep an unwanted presence offline, but Parler’s return is not quite a triumph, and its new CEO doesn’t suggest much of a change in philosophy.

Users can now log in to Parler on the web, but when they do they will find that all their old posts and content have been removed. It’s unclear whether this was a consequence of the hurried exit from AWS last month, a scorched-earth policy regarding the content that got the site in hot water in the first place or for some other reason.

Fortunately someone had the presence of mind to make a backup, though not with the intention of restoring it. @donk_enby scraped millions of posts and media files from the site for posterity, something that has already borne fruit as researchers have used the files to show, for example, where certain users were on the day of the Capitol riots. (She is currently pointing out various problems with the new Parler’s web rollout.)

The new site is described in a statement as using “sustainable, independent technology and not reliant on so-called ‘Big Tech’ for its operations.” The new host is SkySilk, seemingly a reseller of OVHcloud, and I’ve asked if the company plans to enforce its terms, which generally but not specifically prohibit things like threats of violence. (The details of the terms violations were made more public in Parler’s attempt to force Amazon to reinstate it.)

Update: SkySilk has issued a statement explaining that it is hosting Parler because of its position on free speech, which reads in part:

Skysilk does not advocate nor condone hate, rather, it advocates the right to private judgment and rejects the role of being the judge, jury, and executioner. Unfortunately, too many of our fellow technology providers seem to differ in their position on this subject. SkySilk truly believes and supports the freedom of speech and more specifically the rights afforded to us in the First Amendment. This is a non-negotiable issue for us. And while we may disagree with some of the sentiment found on the Parler platform, we cannot allow first amendment rights to be hampered or restricted by anyone or any organization. SkySilk will support Parler in their efforts to be a nonpartisan Public Square as we are convinced this is the only appropriate course of action.

Parler, for its part, aims to make itself a bit less of an easy target by upping its moderation game. The site will supposedly be using both AI and human moderators to watch for content that could rock the boat — though Facebook has been trying this for years and still hasn’t quite got the hang of it.

They may have an easier job of it, considering Parler is still barred from the Google Play Store and iOS App Store. That’s a huge damper on activity, since mobile users make up a large part of social networks. So the flood of content the site could not adequately monitor in early January may have slowed to a trickle. (I’ve asked the company for more information on this and other matters and will update this post if I hear back.)

Meanwhile, the operation is being overseen by a new interim CEO after the ouster of John Matze by the board. The one to fill the role is Mark Meckler, founder of the Tea Party Patriots, staunch opponents of Obamacare and big fans of debunked COVID-19 treatment hydroxychloroquine. The group was also behind the infamous “America’s Frontline Doctors” event and was one of the organizers of the March to Save America that turned into the Capitol Riots.

Meckler’s pedigree suggests that despite the claimed moderation improvements, this is hardly Parler turning a new leaf, and SkySilk may be disappointed that its “nonpartisan public square” will be led by a hyperpartisan conservative activist (and is funded and populated by same). With the deliberate (and apparently unavoidable) break with “Big Tech,” however it is defined, and a CEO who embodies the same qualities that ran amok before, it seems a lot more like stubborn defiance than introspection and graceful compromise.",Social Media,TechCrunch,https://techcrunch.com/2021/02/15/parler-crawls-back-online-empty-and-with-a-tea-party-ceo/,"Parler, a social network adopted by the far right, is back online with a new CEO, but all old posts and content have been removed and its new host is SkySilk, which may not enforce its terms prohibiting threats of violence.",Politics
167,What Mark Zuckerberg Gets Wrong—and Right—About Hate Speech,"When he testified before Congress last month, Facebook CEO Mark Zuckerberg discussed the problem of using artificial intelligence to identify online hate speech. He said he was optimistic that in five to 10 years, “We will have AI tools that can get into some of the linguistic nuances of different types of content to be more accurate in flagging content for our systems, but today we’re not just there on that.”

WIRED OPINION ABOUT Brittan Heller (@brittanheller) is director of the Anti-Defamation League’s Center for Technology and Society and works with social media companies to reduce cyberhate and online harassment.

As an expert on hate speech who recently developed an AI-based system to study online hate, I can confidently say that Zuckerberg is both right and wrong. He is right that AI is not a panacea, since hate speech relies on nuances that algorithms cannot fully detect. At the same time, just because AI does not solve the problem entirely doesn’t mean it's useless.

In fact, it’s just the opposite. Instead of relying on AI to eliminate the need for human review of hate speech, Facebook and other social media platforms should invest in intelligent systems that assist human discretion. The technology is already here. It can not only help tech companies deal with the scale of this challenge; it can also make platforms more transparent and accountable.

At its core, AI identifies patterns in data sets. In his testimony, Zuckerberg may have been trying to say that AI is not a good mechanism by itself to remove hate speech. That’s true. Even the best filters will not replace human reviewers.

This is because hate speech evolves. For example, Shrinky Dinks are plastic toys from the 1980s that are designed to get smaller when baked in an oven. Toys by themselves certainly aren’t hate speech. But when those same words are used to describe Jews, as they are today by some white supremacists, the name of a child’s plaything can be transformed into an offensive Holocaust metaphor. Another example came in 2016 when white supremacists started putting triple parentheses around Jewish people’s names on Twitter in an effort to harass and intimidate them.

Imagine trying to build an artificial intelligence that could capture this subtlety. The technology simply doesn’t exist yet. Because hate speech is nuanced, even the best AI can't replace human beings. Computation will not solve the hate speech dilemma.

The clearest proof that AI alone can’t solve hate speech is the false-positive problem. As Zuckerberg explained in his testimony, “Until we get it more automated, there’s a higher error rate than I’m happy with.” However, even if AI was 99 percent effective at removing controversial content like hate speech, there would still be real consequences, made worse by the immense scale and reach of online platforms.

Take the example of terrorist propaganda: Facebook already relies on AI to tackle that. In February, the British government announced it would use AI to filter out extremist content on social media and claims its automated tool can detect 94 percent of Islamic State propaganda with 99.995 percent accuracy. With a false positive rate of 0.005 percent, if the tool analyzed 1 million randomly selected videos on YouTube, only 50 of them would require additional human review. That means 50 harmless videos that, without additional human review, would be falsely flagged as potentially criminal.

But Facebook produces much more than 1 million pieces of content every day. Facebook users produce more than 350 million photos per day. At a false positive rate of 0.005 percent, that’s 15,000 falsely flagged pieces of content every day. This equation only looks at photos on one social media platform—only showing us part of the massive problem.",Social Media,WIRED,https://www.wired.com/story/what-mark-zuckerberg-gets-wrongand-rightabout-hate-speech/,"The use of AI alone to remove hate speech from social media platforms is not an effective solution, since it cannot capture the nuanced language and context of hate speech, and this can lead to massive numbers of false positives which can have serious consequences for innocent users.",Equality & Justice
168,Egypt and Thailand: When the military turns against free speech – TechCrunch,"You wouldn’t normally mention Egypt and Thailand in the same breath. But both countries underwent military coups within the last five years, and even among the many oppressive regimes in the world, they are going to extra lengths today to prosecute free speech.

Abbas and Hall are just two examples of hundreds of recent prosecutions. In 2017 alone, Egyptian security forces arrested at least 240 people based on online posts. Three years after the coup, Thai authorities had charged more than 105 people just for posting comments deemed offensive to the monarchy.",Social Media,TechCrunch,https://techcrunch.com/2018/10/21/egypt-and-thailand-when-the-military-turns-against-free-speech/,"In Egypt and Thailand, oppressive regimes have used social media to prosecute people for their posts and comments, with more than 345 people arrested in the last five years for expressing their views. This highlights the dangers of using social media to freely express opinion, as it can result in legal repercussions.",Security & Privacy
169,"Twitter launches the ‘Hide Replies’ feature, in hopes of civilizing conversations – TechCrunch","Twitter today is beginning its test of a radical and controversial change to its service with the launch of a new “Hide Replies” feature. Effectively, this option gives users the ability to wrestle back control over a conversation they’ve started by hiding any replies they feel aren’t worthy contributions — for example, replies that are irrelevant or outright offensive.

One of the problems with Twitter — and with many social networks, for that matter — is that an otherwise healthy conversation can easily be disrupted by a single individual or a small number of people who don’t contribute in a positive fashion. They come into a thread to start drama or they make inappropriate, rude or even hateful remarks.

Of course, users can choose for themselves to either Mute or Block people like this, which limits their ability to affect their own personal experience on Twitter. But this doesn’t remove their comments from others’ view. The “Hide Replies” feature, however, will.

But it’s not the equivalent of a delete button. In other words, hidden replies are not removed from Twitter entirely, they are just placed behind an icon. If people want to see the hidden replies, they can press this icon to view them.

Twitter’s goal with the feature is to encourage more civil conversation on its platform. It could work, as those who want their comments seen by a wide audience will have to find a way to express themselves in an appropriate fashion — without taking the conversation off course or resorting to insults or trolling. Otherwise, they know their replies could be hidden from the default view.

But this change is not without significant downsides.

For example, a user could choose to hide replies that simply (and even politely!) disagreed with their view. This would then create a “filter bubble” where only people who shared the original poster’s same opinion would have their comments prominently displayed. In this case, the feature would be silencing other viewpoints — and that’s in direct opposition to Twitter’s larger goal of creating a public town square on the web, where every voice has a chance to be heard.

More worryingly, a user could choose to hide replies that attempt to correct misinformation or offer a fact check. That’s a significant concern at a time when social media platforms have turned into propaganda dissemination machines, and have been infiltrated by state-supported actors from foreign governments looking to manipulate public sentiment and influence elections.

Twitter claims the feature provides transparency because hidden replies are still available for viewing to anyone who wishes to see them. But this assumes that people will notice the small “hidden replies” icon and bother to click it.

The ability to hide replies is initially available only to users in Canada, but tweets with hidden replies will be accessible by all Twitter users worldwide.

We’re testing a feature to hide replies from conversations. This experience will be available for everyone around the world, but at this time, only people in Canada can hide replies to their Tweets. We want to know what you think. Please Tweet us your feedback and questions! https://t.co/H7iMtEhCUP — Twitter Support (@TwitterSupport) July 17, 2019

In a statement posted as a series of tweets and replies to others, Twitter explained its goals around the new addition:

We’re testing a feature to hide replies from conversations. This experience will be available for everyone around the world, but at this time, only people in Canada can hide replies to their Tweets…They’ll be hidden from the main conversation for everyone behind a new icon. As long as it hasn’t been deleted and/or is not from an account with protected Tweets, everyone can still interact with a hidden reply by clicking the icon to view. We want everyone on Twitter to have healthy conversations, and we’re working on features that will help people feel more comfortable. We’re testing a way for people to hide replies they feel are irrelevant or offensive.

Social media is due for a course correction, and Twitter at least isn’t afraid to try significant changes to its platform. (It’s even trying a new prototype of its app, called twttr.) However, some would argue that permanent bans on rulebreakers and more attention to enforcing existing policies would negate the need for features like this.

Twitter had previously confirmed its plans to test a “Hide Replies” feature, and had announced its plan to launch the feature sometime this week.",Social Media,TechCrunch,https://techcrunch.com/2019/07/17/twitter-officially-launches-its-hide-replies-feature-initially-to-users-in-canada/,"Twitter's new ""Hide Replies"" feature has the potential to create ""filter bubbles"" where only voices that agree with the original poster are seen, potentially silencing dissenting opinions, as well as allowing for misinformation or propaganda to go unchecked.",Discourse & Governance
170,This Stripped-Down Blogging Tool Exemplifies Antisocial Media,"Recently, Rob Beschizza—a coder and the managing editor of Boing Boing—released a stripped-down blogging tool called txt.fyi. Write something, hit Publish, and voilà: your deathless prose, online.

But here’s the thing: txt.fyi has no social mechanics. None. No Like button, no Share button, no comments. No feed showing which posts are most popular. Each post has a tag telling search engines not to index it, so it won’t even show up on Google. The only way anyone will see it is if you send them the URL or post it somewhere. txt.fyi is a tool for putting stuff online—but without the usual features to help something become a pass-around hit.

I call it antiviral design. Most platforms work in precisely the opposite fashion. They’re casinos of quantification, designed to constantly tell us what’s blowing up and what isn’t. We peer at our feeble posts on Twitter or Instagram or LinkedIn and pray for likes, for hearts, for a big-smile emoji. Our attention is magnetically drawn to anything with a huge “share” number beneath it—what psychologists call the social proof: If lots of people are paying attention to something, we figure it’s worth our notice too.

This lust for virality deforms how we think in public. What do you get if you mentally focus-group every utterance before you post it? Stuff that’s panderingly dull (best not to offend anyone) or that leans into the kabuki hysteria of a sick burn (offend everyone!). Posts designed specifically to hack the attentional marketplace.

With txt.fyi, Beschizza was trying to playfully push back. “I wanted something where people could publish their thoughts without any false game of social manipulation, one-upmanship, and favor-trading,” he says. This is what I found so interesting about his creation. Its antivirality doesn’t necessarily prevent a post from becoming wildly popular. (A txt.fyi URL shared on, say, Facebook could perhaps go viral.) But its design favors messages to someone, not everyone.

So, does antivirality actually affect what people do and say? Beschizza isn’t sure. Out of pro-privacy principle, he doesn’t regularly look at the logs of his service. But once, during some debugging, Beschizza discovered someone using txt.fyi to write letters to a deceased relative. It was touching and weirdly human, precisely the sort of unconventional expression we used to see a lot more of online. But today we sand down those rough edges, those barbaric yawps, in the quest for social spread. Even if you don’t want to share something, Medium or Tumblr or Snapchat tries to make you. They have the will to virality baked in.

When you think about it, the very metaphor of “going viral” suggests bleak side effects. In the physical world, it means an infectious payload spreading uncontrollably. Smallpox, Ebola, avian flu: super viral content, dude! Reframe “virality” like that and you start to understand the emergence of white supremacy online or the hot-zone dog-piling of Gamergate. If social networks are to eliminate hate on their platforms, they’ll have to fight the exquisitely gameable mechanics of virality they themselves built.

Now for the caveats. As Beschizza admits, txt.fyi is just a tiny experiment. (There are a few others like it, including SaidSo.me.) It’s hard to make a service like this go big, because you can’t easily make ad money on an un-metricked platform. And hey, there are healthy, nontoxic reasons we’re interested in what’s popular online: Some great social good (like Black Lives Matter) has relied on the viral spread of online posts.

Nonetheless, I’d love to see some of Beschizza’s principles injected into our social media ecosystem. A bit of antivirality could be precisely the inoculation against extremism that our culture needs.

This article appears in the November issue. Subscribe now.",Social Media,WIRED,https://www.wired.com/story/this-stripped-down-blogging-tool-exemplifies-antisocial-media/,"Social media platforms incentivize users to post content that is panderingly dull or designed to hack the attentional marketplace, leading to a culture of extremism and manipulation, rather than meaningful dialogue.","Information, Discourse & Governance"
171,Facebook Stories looks like an ill-fitting mask – TechCrunch,"Facebook has finally capped off its strategy of cloning Snapchat’s USP by slotting a camera-first, ephemeral multimedia sharing feature into its entire social sharing estate.

Today it’s flicked the official switch on a global rollout of the feature in the main Facebook app, where these disappearing Stories are pinned to contacts above the News Feed — thereby making them almost impossible to ignore, especially given their fleeting lifespan.

Earlier this month the social sharing giant added a similar visual sharing feature to its Messenger app — triggering complaints that it was messing with the user experience.

It did the same, in February, with its messaging platform, WhatsApp, and also annoyed users by trying to replace a text status feature (which it’s since restored).

The Facebook Snapchat cloning strategy kicked off in August 2016 when the company debuted the disappearing Stories format on its photo and video sharing platform Instagram, clearly the most natural home for the clone.

And Instagram Stories has since apparently managed to dent Snapchat’s growth, which was clearly a core strategic aim for Facebook.

That and creating vastly more video inventory across Facebook’s portfolio of social apps — into which it can inject more lucrative video ads.

Training users to share the kind of content where ads can natively blend is really what Stories is all about.

All video, all video ads

None of this should be surprising. The company has previously publicly suggested its entire platform will be “all video” in the coming years.

It’s also taken user-hostile design decisions such as removing the ability to send text messages from its main Facebook app. (And the aforementioned attempted rubbing out of text statuses in WhatsApp).

So, basically, if you want to spam all your Facebook friends with a video of yourself wearing an animal selfie lens, Facebook will happily put all its tech at your disposal. But if you wish to swap a few words with people in your Facebook network, Facebook actively discourages that by requiring you switch to its Messenger app to do so. It’s very clear where the company’s priorities lie.

Yet it remains to be seen whether Facebook users in general are going to be flocking in droves to engage in the kind of throwaway visual sharing Stories encourages — with the format effectively asking them to repackage private lives into what amounts to a self-promoting public ‘ad format’, complete with stickers, silly effects and so on.

Thing is, for several years Facebook has had a problem with users posting fewer personal updates. This too is not surprising, given the network has something of an identity crisis these days. It’s certainly a far cry from the original concept of linking university friends across a campus.

Instagram is obviously the more natural home for people with a love of visual sharing generally (including those who want to build public followings for what they share). While WhatsApp/Messenger are for communicating privately with friends and/or in more bounded groups. So the question arises, who is Facebook for?

The people in the average Facebook network may well include a number of uni friends but also various family members, workmates across different jobs, folks you once met at a party, friends of friends, old school friends, professional connections and even random strangers.

Such an assortment of ‘connections’ likely does not constitute either a close-knit group of friends nor a unified group of people with shared interests. The only loosely linking factor is they all (maybe) met you at least once in your life.

Nor are Facebook friends likely to be a uniformly active network. I see huge variation in terms of content sharing in my own network, for example.

It will undoubtedly take a certain type of person to want to blanket broadcast Stories across such a varied and variously segmented network. (Stories can be shared with specific Facebook friends only, but the default push for the format is clearly to encourage sharing with all.)

The mask slips

Anecdotally, a very small subset of my own Facebook connections also appear to account for the vast majority of personal updates still being shared. (Doubtless exacerbated by the algorithmic effect of the Facebook News Feed promoting posts that get more engagement).

Could I imagine these most actively sharing Facebook users sharing Facebook Stories? Perhaps a few of them — so an even smaller subset.

But the handful of users I see who are still regularly sharing personal stuff on Facebook appear to be doing so either to spark debate on a particular issue/topic; to entertain and/or garner public attention/likes; or to ask for (and in so doing share) information/advice with a group — functions that all feel secondary as far as Stories is concerned, given the emphasis here is squarely on visual entertainment.

I may be wrong but it’s very hard to imagine serious or substantial topics being debated via Stories, what with all the selfie lenses, movie masks, and visual effects Facebook is touting…

Stories can also be posted to the Facebook News Feed. So there is at least the possibility that someone could use the format to try to garner comments in the usual way, by turning it into a standard piece of public Facebook content.

But I can’t imagine how such a promotional format could sensitively touch on some of the topics I’ve seen discussed across Facebook in recent years, including very difficult issues like child abuse, depression and marriage breakdown. A selfie lens really isn’t going to fit.",Social Media,TechCrunch,https://techcrunch.com/2017/03/28/facebook-stories-looks-like-an-ill-fitting-mask/,"Facebook has begun a cloning strategy of Snapchat that has caused user-hostile design decisions and removed the ability to send text messages from the main Facebook app. This strategy has also been seen to dent Snapchat's growth and created vastly more video inventory across its social apps, all in an effort to train users to share the kind of content where ads can native",User Experience & Entertainment
172,Facebook admits what we all know: that social media can be bad for democracy,"Facebook’s ongoing attempt to reckon with its impact on civil life continued today with the company acknowledging that its platform is not always good for democracy.

In a set of blog posts published as part of its “Hard Questions” series, Facebook execs and outside experts assess the company’s impact on elections, partisan politics, and fake news. As ever, Facebook tempers its self-criticism. For example, referring to “the damage that the internet can do to even a well-functioning democracy” (our emphasis), rather than damage caused by Facebook specifically. But, it does admit to a sliver more responsibility — taking the company one step further from CEO Mark Zuckerberg’s comments in 2016 that it was “crazy” to say Facebook influenced the US election.

The 2016 US elections were a wake-up call for Facebook

As Facebook’s global politics and government outreach director Katie Harbath tells it, this was the moment the company began to recognize its influence on democracy, for better or for worse. “From the Arab Spring to robust elections around the globe, social media seemed like a positive,” writes Harbarth. “The last US presidential campaign changed that, with foreign interference that Facebook should have been quicker to identify to the rise of ‘fake news’ and echo chambers.”

In another post, Facebook’s product manage for civic engagement, Samidh Chakrabarti, expands on these issues. He points out many positives — that the company helps keep people informed about politics, and that it’s a venue for debate — but cautions that the company will never be able to completely stamp out its problems. On the spread of fake news and misinformation on Facebook, he writes: “Even with all these countermeasures, the battle will never end.”

Since November 2016, Facebook has moved to address these issues in concrete ways. This month, the company started to reengineer the News Feed, demoting content from news outlets in favor of activity from friends. It’s also going to start polling users on which sources they trust. “We feel a responsibility to make sure our services aren’t just fun to use, but also good for people’s well-being,” said Zuckerberg.

Could Facebook’s solutions make things worse?

Arguably, though, these moves also exacerbate existing problems. If users get less news from news sources, they’re more likely to share sensationalized stories, say reports. And if people are given the task of judging which outlets they find trustworthy, what’s to stop them simply voting in line with sites that support their worldview? This perpetuates the problem of polarization and “echo chamber” politics — which Cass Sunstein, a professor at Harvard Law School, calls “a nightmare” in a blog post published today for Facebook.

It’s also important to note that although much of Facebook’s attention is focused on the US and the influence of Russia on the 2016 election, in other parts of the world the situation is more dire. A recent report from BuzzFeed in Cambodia illustrated Facebook’s problematic role in politics, with the country’s authoritarian prime minister Hun Sen (last year Sen banned the main opposition party) using the site to push pro-government messages while identifying, and often jailing, critics.

As Facebook’s Chakrabarti writes: “If there’s one fundamental truth about social media’s impact on democracy it’s that it amplifies human intent — both good and bad [...] I wish I could guarantee that the positives are destined to outweigh the negatives, but I can’t.”",Social Media,Verge,https://www.theverge.com/2018/1/22/16918456/facebook-democracy-civil-society-fake-news-hard-questions,"Facebook has taken steps to try and address the negative impact it has had on democracy, such as reengineering the News Feed to prioritize content from friends and polling users on which sources they trust. However, it is difficult to combat the spread of fake news and misinformation, and in some parts of the world, like Cambodia, the platform has been","Information, Discourse & Governance"
173,Europe’s parliament calls for full audit of Facebook in wake of breach scandal – TechCrunch,"The European Parliament has called for a full audit of Facebook following a string of data breach scandals — including the Cambridge Analytica affair.

MEPs are urging the company to allow European Union bodies to carry out a full audit to assess data protection and security of users’ personal data, following the scandal in which the data of 87 million Facebook users was improperly obtained and misused.

In the resolution, adopted today, they have also recommended Facebook make additional changes to combat election interference — asserting the company has not just breached the trust of European users “but indeed EU law”.

We’ve reached out to the company for comment on the parliament’s resolution.

Earlier this month the EU parliament’s civil liberties committee adopted a similar resolution, calling for a full and independent audit of Facebook and for the company to make further changes to its platform.

The Libe committee also called for an update to EU competition rules to reflect what it dubs “the digital reality”, and investigation of what it called the “possible monopoly” of big tech social media platforms.

Commenting in a statement today, following the parliament’s vote, civil liberties committee chair Claude Moraes said: “This is a global issue, which has already affected our referenda and our elections. This resolution sets out the measures that are needed, including an independent audit of Facebook, an update to our competition rules, and additional measures to protect our elections. Action must be taken now, not just to restore trust in online platforms, but to protect citizens’ privacy and restore trust and confidence in our democratic systems.”

The resolution follows an appearance by Facebook’s founder Mark Zuckerberg in front of the EU parliament’s Conference of Presidents in May, and a series of parliament committee hearings including with Facebook staffers.

The EU’s tough new data protection framework, GDPR, only came into force this May — so the Cambridge Analytica breach is being handled under the bloc’s prior data protection framework, comprising a patchwork of Member State laws.

And earlier today a fine handed to Facebook for this breach by the UK data watchdog was upheld. The £500k penalty is the maximum possible fine under the country’s prior data protection regime.

In the new resolution, MEPs have suggested the data obtained by Cambridge Analytica may have been used for political purposes, by both sides in the UK referendum on membership of the EU and to target voters during the 2016 US presidential election — describing it as a matter of urgency that electoral laws be adapted to take account of digital campaigning. (Clearly with an eye on the upcoming EU elections, next May.)

To combat electoral meddling via social media, MEPs are proposing:

applying conventional “off-line” electoral safeguards online: rules on spending transparency and limits, respect for silence periods and equal treatment of candidates;

making it easy to recognise online political paid advertisements and the organisation behind them;

banning profiling for electoral purposes, including use of online behaviour that may reveal political preferences;

that social media platforms should label content shared by bots, speed up the process of removing fake accounts and work with independent fact-checkers and academia to tackle disinformation;

investigations should be carried out by member states with the support of Eurojust, into alleged misuse of the online political space by foreign forces.

In the UK a parliamentary committee also recently urged the government to prioritize updating electoral law to take account of digital risks to democratic processes. Although the government has so far only taken a cautious approach, saying it’s still gathering evidence via a series of reviews into different aspects of the issue.

Meanwhile Facebook has been rolling out its own system of checks on political advertisers in certain regions — including the UK. Though MEPs evidently believe the company needs to go further.

The UK’s DPA also previously called for an ethical pause on political microtargeting via online platforms, saying it had a number of concerns about how data is being used and potentially misused.

Update: A Facebook spokesperson pointed us to its earlier statement after the Libe committee resolution on the same issue, in which it states: “We are grateful to the European Parliament for the number of opportunities to come and explain the changes we have made to our platform. We are working relentlessly to ensure the transparency, safety and security of people who use Facebook. Over the last months we have developed sophisticated systems that combine technology and people to prevent election interference on our services. This is part of a broader challenge for us at Facebook to be more proactive about protecting our community from harm and taking a broader view of our responsibility overall.”",Social Media,TechCrunch,https://techcrunch.com/2018/10/25/europes-parliament-calls-for-full-audit-of-facebook-in-wake-of-breach-scandal/,"The European Parliament has called for a full audit of Facebook, citing data breaches, election interference, and misuse of user data, and has suggested new measures to protect users' privacy, elections, and competition rules.",Security & Privacy
174,US government starts asking foreign travelers to disclose their social media accounts,"The US Customs and Border Protection has started demanding that foreign travelers hand over Facebook, Twitter, and other social media account information upon entering the country, according to a report from Politico. The new policy follows a proposal laid out back in June and applies only to those travelers who enter the US temporarily without a visa through the Electronic System for Travel Authorization, or ESTA, process. The goal, the government says, is to “identify potential threats,” a spokesperson tells Politico.

Activists fear the policy could be a threat to human rights

The new policy went into effect on Tuesday, and the request is currently “optional.” It asks foreign travelers to “enter information associated with your online presence,” and offers a drop-down menu allowing participants to enter in account names for most major social networks, including LinkedIn and even Google+.

It’s unclear if the information collected can be immediately used to deny travelers entry into the US. However, the express purpose of the collection is to identify individuals with ties to terrorist groups. As it stands today, Customs and Border Protection says it will not deny entry to those that refuse to submit any social media information.

Still, the implementation of such a controversial policy has enraged human rights activists and technology companies. Members of both communities expressed concern when the policy was first proposed this past summer. At the time, industry lobbying group the Internet Association, which represents companies like Facebook and Google, joined with the ACLU to condemn the proposal for its potential free speech and privacy violations.

you can see the form for yourself here btw: https://t.co/GmDb4p6ROz



yes, it does say google plus and github lol https://t.co/QftMn3an3h — Tony Romm (@TonyRomm) December 22, 2016

These groups fear that the request for social media info, although voluntary, will urge members of marginalized groups from the Middle East and elsewhere to fill out the information for fear of being denied entry. It could then be highly scrutinized, activists say, without clear and transparent guidelines. The policy could also lead to potential abuses and even security risks as the information is collected and stored by the government.

This new form of data collection happens to arrive just one month before the inauguration of President-elect Donald Trump, who on Wednesday seemingly renewed his call for a ban on Muslims entering the US following the bus attack in Berlin. Just yesterday, it was also revealed that data-mining firm Palantir, which was co-founded by Trump advisor Peter Thiel, has worked with the US Customs and Border Protection to track immigrants and foreign travelers using a wide variety of data gleaned from law enforcement databases. Social media account information could aid in this process.",Social Media,Verge,https://www.theverge.com/2016/12/22/14066082/us-customs-border-patrol-social-media-account-facebook-twitter,"Activists fear that the US Customs and Border Protection's new policy asking foreign travelers to provide social media account information could lead to potential abuses, free speech and privacy violations, and security risks due to the information being collected and stored by the government.",Security & Privacy
175,Tech and politics clash in Cameroon as government restores internet – TechCrunch,"The government of Cameroon ended its internet blackout of parts of the country last week, according to news reports and confirmation from the country’s Ambassador to the U.S.

The three month outage forced the hand of Africa’s largest telecom, halted operations of its leading e-commerce startup, and created digital refugees. The disruption also prompted a grassroots #BringBackOurInternet campaign that could serve as a global model for countering government internet meddling.

Cameroon’s outage started on January 17 when net access went dead nationally, and then in the North-West and South-West regions of the country, according to Dyn Research, and Internet Without Borders.

The affected areas―primarily English speaking―had been the center of protests over policies of the country’s 36 year incumbent president, Paul Biya, and linguistic disputes rooted in Cameroon’s colonial past.

The country adopted English and French as official languages upon independence in 1961. Discontent over marginalization of English speakers in the Anglophone regions erupted into protests and strikes in January.

“The internet and social media was key in mobilizing and organizing this movement,” said Cameroonian activist and 2011 presidential candidate Kah Walla. “I believe this prompted the government to shut down the net.”

According to Walla, the outage followed arrests of activists. “Everything was shut down across the entire country, then they put the internet back on in other parts of the country and left it off in the Northwest and Southwest,” she said. Cameroonians also reported a receiving a countrywide SMS from the Ministry of Posts and Communications warning citizens of imprisonment for spreading inaccurate information on social media.

Initially, Cameroon’s government did not issue any statement acknowledging the outage. But as Rebecca Enonchong, the CEO of Cameroonian firm AppsTech, pointed out, “letters surfaced from the government to the telcos requiring the shutdown.”

According to Enonchong, internet activity in the country is managed through the national telco, Camtel, which resells to all ISP’s and mobile providers―MTN and Vodaphone among them. A January 18 letter materialized from Camtel to the state telecommunications minister confirming suspension instructions had been delivered to the country’s internet operators.

Cameroon has a burgeoning IT scene and the outage impacted it immediately. The center for the country’s startup ecosystem―dubbed Silicon Mountain―lies in Buea, the capital of the South-West region impacted in the internet blackout. Cameroonian Rebecca Enonchong is also one of Africa’s more recognized techies. The CEO and Chair of Cameroon’s Active Spaces innovation hub is a global speaker and advisor on multiple facets of the continent’s technology sector.

Enonchong explained how Cameroon’s net outage forced digital dislocation and some unique measures to access email.

“It created internet refugees. ActiveSpaces was shutdown. There was no net and no ability for entrepreneurs to work out of the space,” she said. “People travelling to the connected areas started doing things like taking a whole bunch of email passwords or mobile phones from people in the no-internet zones, printing out or downloading emails, and then taking the papers and phones back to those without connectivity.”

Eventually, Enonchong and other tech leaders set up impromptu internet cafes. “At one point the travel had become so expensive that we rented small buildings right at the border between internet and no internet zones. We created internet refugee camps,” she said.

Local techies also coordinated a local and global response to the internet blackout. “Internet activists reacted very quickly. Rebecca coined the hashtag #BringBackOurInternet. Somebody came up with the visual, a public letter was drafted and signed by many different people and organizations asking that internet be restored,” said Walla.

“We set out to make as many people aware as possible of what was going on,” said Enonchong. The activists also “made sure the social media campaigns included the handles of Cameroon’s president, key political officials, and institutions,” said Walla.

#BringBackOurInternet attracted the attention and support of a number of individuals and organizations, becoming a Twitter cause célèbre. Edward Snowden chimed in several times with Twitter support. Global organizations such as the UN and Access Now intervened. Though the Vatican would not verify, one source (speaking on background) said presidential aides confirmed the Pope raised the internet shutdown during his March meeting with Biya.

The economic costs of Cameroon’s internet blackout also became apparent. In an email to TechCrunch, African e-commerce giant Jumia’s MD for Cameroon, Roland de Heere, said the internet outage led to an 18 percent decline in orders over the period. French telco Orange saw a 20 percent revenue drop in Cameroon. Access Now estimates the shutdown cost the country $4.5 million in economic activity.

There was also the international reputational risk. The Cameroonian government has taken to touting achievements of the country’s tech entrepreneurs.

“Ironically, senior officials were talking up Silicon Mountain at the same time the government had cut it off from the net,” said Enonchong. Cameroon’s Ministry of Telecoms announced several youth startup initiatives during the outage. Global press reported that the first African winner of Google’s annual coding competition, Cameroonian teen Nji Collins Gbah, lived in a blacked out town.

On April 21, Cameroon’s government restored internet connectivity to the country’s Anglophone regions. A statement by the Minister of Communication included a caveat, that the government “reserved the right to restrict internet moving forward if citizens misused it.”

While Cameroon’s Minister of Economy and Ministry of Posts and Telecommunications did not respond to requests to speak for this story, the country’s Ambassador to the U.S. stated in a letter that “the conditions that led to the temporary suspension of Internet…have greatly improved. Therefore, Internet connectivity has been reinstated.”

As for lessons from Cameroon’s state forced internet blackout, “It’s a big mistake for governments in Africa or anywhere to underestimate the tech community,” said Enonchong. She also noted Cameroon’s digital debacle “politicized tech entrepreneurs who weren’t previously involved in politics” and sparked conversations between Cameroonian and global internet activists on best practices to overcome blackouts. This includes contingency plans―such as mesh networks―to bypass government network restrictions altogether, explained Enonchong.

Kah Walla underscored the effectiveness of local initiative paired to global support. “The victory is in the fact that Cameroonians came together, used social media, and used our internal pressure to bring the external pressure,” she said.

Walla also flagged Cameroon’s 2017 net blackout as an example of the complexity of contemporary tech and politics. “Internet is a basic right. Our government cut off access to that right and then used the internet to justify why,” she said. “But because some of us still had access, we were able to use the internet to bring back our internet.”",Social Media,TechCrunch,https://techcrunch.com/2017/04/30/1483467/,"The government of Cameroon recently ended an internet blackout that lasted three months, affecting English-speaking regions of the country. This disruption caused digital refugees, forced the hand of Africa's largest telecom, halted operations of its leading e-commerce startup, and prompted a grassroots #BringBackOurInternet campaign as a global model for countering government internet meddling.",Politics
176,Watch Sacha Baron Cohen skewer Zuckerberg’s ‘twisted logic’ on hate speech and fakes – TechCrunch,"Comedian Sacha Baron Cohen has waded into the debate about social media regulation.

In an award-acceptance speech to the Anti-Defamation League yesterday, the creator of Ali G and Borat delivered a precision take-down of what he called Facebook founder Mark Zuckerberg’s “bullshit” arguments against regulating his platform.

The speech is well worth watching in full as Cohen articulates, with a comic’s truth-telling clarity, the problem with “the greatest propaganda machine in history” (aka social media platform giants) and how to fix it: Broadcast-style regulation that sets basic standards and practices of what content isn’t acceptable for them to amplify to billions.

“There is such a thing as objective truth,” said Cohen. “Facts do exist. And if these internet companies really want to make a difference, they should hire enough monitors to actually monitor, work closely with groups like the ADL and the NAACP, insist on facts and purge these lies and conspiracies from their platforms.”

Attacking social media platforms for promulgating “a sewer of bigotry and vile conspiracy theories that threaten our democracy and to some degree our planet,” he pointed out that freedom of speech is not the same as freedom of reach.

“This can’t possibly be what the creators of the internet had in mind,” he said. “I believe that’s it’s time for a fundamental rethink of social media and how it spreads hate, conspiracies and lies.”

“Voltaire was right. Those who can make you believe absurdities can make you commit atrocities — and social media lets authoritarians push absurdities to billions of people,” he added.

Cohen also rubbished Zuckerberg’s recent speech at Georgetown University in which the Facebook founder sought to appropriate the mantle of “free speech” to argue against social media regulation.

“This is not about limiting anyone’s free speech. This is about giving people — including some of the most reprehensible people in history — the biggest platform in history to reach a third of the planet.”

“We are not asking these companies to determine the boundaries of free speech across society, we just want them to be responsible on their platforms,” Cohen added.

On Facebook’s decision to stick by its morally bankrupt position of allowing politicians to pay it to spread lying, hatefully propaganda, Cohen also had this to say: “Under this twisted logic if Facebook were around in the 1930s it would have allowed Hitler to post 30-second ads on his solution to the ‘Jewish problem.’ ”

Ouch.

YouTube also came in for criticism during the speech, including for its engagement-driven algorithmic recommendation engine which Cohen pointed out had single-handedly recommended videos by conspiracist Alex Jones “billions of times.”

Just six people decide what information “so much of the world sees,” he noted, name-checking the “silicon six” — as he called Facebook’s Zuckerberg, Google’s Sundar Pichai, Alphabet’s Larry Page and Sergey Brin, YouTube’s Susan Wojcicki and Twitter’s Jack Dorsey.

“All billionaires, all Americans, who care more about boosting their share price than about protecting democracy. This is ideological imperialism,” he went on. “Six unelected individuals in Silicon Valley imposing their vision on the rest of the world, unaccountable to any government and acting like they’re above the reach of law.

“It’s like we’re living in the Roman Empire and Mark Zuckerberg is Caesar. At least that would explain his haircut.”

Cohen ended the speech with an appeal for societies to “prioritize truth over lies, tolerance over prejudice, empathy over indifference, and experts over ignoramuses” and thereby save democracy from the greed of “high tech robber barons.”",Social Media,TechCrunch,https://techcrunch.com/2019/11/22/watch-sacha-baron-cohen-skewer-zuckerbergs-twisted-logic-on-hate-speech-and-fakes/,"Social media is being used to spread lies, bigotry, and conspiracies, threatening democracy and potentially the planet. The ""silicon six"" billionaires who run these platforms prioritize their share prices over protecting democracy, and are seen as unaccountable and above the reach of law. Sacha Baron Cohen is calling for responsible regulation and prioritizing truth over","Information, Discourse & Governance"
177,"Facebook, are you kidding? – TechCrunch","Facebook is making a video camera. The company wants you to take it home, gaze into its single roving-yet-unblinking eye and speak private thoughts to your loved ones into its many-eared panel.

The thing is called Portal and it wants to live on your kitchen counter or in your living room or wherever else you’d like friends and family to remotely hang out with you. Portal adjusts to keep its subject in frame as they move around to enable casual at-home video chat. The device minimizes background noise to boost voice clarity. These tricks are neat but not revelatory.

Sounds useful, though. Everyone you know is on Facebook. Or they were anyway… things are a bit different now.

Facebook, champion of bad timing

As many users are looking for ways to compartmentalize or scale back their reliance on Facebook, the company has invited itself into the home. Portal is voice activated, listening for a cue-phrase (in this case “Hey Portal”) and leverages Amazon’s Alexa voice commands, as well. The problem is that plenty of users are already creeped out enough by Alexa’s always-listening functionality and habit of picking up snippets of conversation from the next room over. It may have the best social graph in the world, but in 2018 people are looking to use Facebook for less — not more.

Facebook reportedly planned to unveil Portal at F8 this year but held the product back due to the Cambridge Analytica scandal, among other scandals. The fact that the company released the device on the tail end of a major data breach disclosure suggests that the company couldn’t really hold back the product longer without killing it altogether and didn’t see a break in the clouds coming any time soon. Facebook’s Portal is another way for Facebook to blaze a path that its users walk daily to connect to one another. Months after its original intended ship date, the timing still couldn’t be worse.

Over the last eight years Facebook insisted time and time again that it is not and never would be a hardware company. I remember sitting in the second row at a mysterious Menlo Park press event five years ago as reporters muttered that we might at last meet the mythological Facebook phone. Instead, Mark Zuckerberg introduced Graph Search.

It’s hard to overstate just how much better the market timing would have been back in 2013. For privacy advocates, the platform was already on notice, but most users still bobbed in and out of Facebook regularly without much thought. Friends who’d quit Facebook cold turkey were still anomalous. Soul-searching over social media’s inexorable impact on social behavior wasn’t quite casual conversation except among disillusioned tech reporters.

Trusting Facebook (or not)

Onion headline-worthy news timing aside, Facebook showed a glimmer of self-awareness, promising that Portal was “built with privacy and security in mind.” It makes a few more promises:

“Facebook doesn’t listen to, view, or keep the contents of your Portal video calls. Your Portal conversations stay between you and the people you’re calling. In addition, video calls on Portal are encrypted, so your calls are always secure.” “For added security, Smart Camera and Smart Sound use AI technology that runs locally on Portal, not on Facebook servers. Portal’s camera doesn’t use facial recognition and doesn’t identify who you are.” “Like other voice-enabled devices, Portal only sends voice commands to Facebook servers after you say, ‘Hey Portal.’ You can delete your Portal’s voice history in your Facebook Activity Log at any time.”

This stuff sounds okay, but it’s standard. And, like any Facebook product testing the waters before turning the ad hose on full-blast, it’s all subject to change. For example, Portal’s camera doesn’t identify who you are, but Facebook commands a powerful facial recognition engine and is known for blurring the boundaries between its major products, a habit that’s likely to worsen with some of the gatekeepers out of the way.

Facebook does not command a standard level of trust. To recover from recent lows, Facebook needs to establish an extraordinary level of trust with users. A fantastic level of trust. Instead, it’s charting new inroads into their lives.

Hardware is hard. Facebook isn’t a hardware maker and its handling of Oculus is the company’s only real trial with the challenges of making, marketing — and securing — something that isn’t a social app. In 2012, Zuckerberg declared that hardware has “always been the wrong strategy” for Facebook. Two years later, Facebook bought Oculus, but that was a bid to own the platform of the future after missing the boat on the early mobile boom — not a signal that Facebook wanted to be a hardware company.

Reminder: Facebook’s entire raison d’être is to extract personal data from its users. For intimate products — video chat, messaging, kitchen-friendly panopticons — it’s best to rely on companies with a business model that is not diametrically opposed to user privacy. Facebook isn’t the only one of those companies (um, hey Google) but Facebook’s products aren’t singular enough to be worth fooling yourself into a surfeit of trust.

Gut check

Right now, as consumers, we only have so much leverage. A small handful of giant tech companies — Facebook, Apple, Amazon, Google and Microsoft — make products that are ostensibly useful, and we decide how useful they are and how much privacy we’re willing to trade to get them. That’s the deal and the deal sucks.

As a consumer it’s worth really sitting with that. Which companies do you trust the least? Why?

It stands to reason that if Facebook cannot reliably secure its flagship product — Facebook itself — then the company should not be trusted with experimental forays into wildly different products, i.e. physical ones. Securing a software platform that serves 2.23 billion users is an extremely challenging task, and adding hardware to that equation just complicates existing concerns.

You don’t have to know the technical ins and outs of security to make secure choices. Trust is leverage — demand that it be earned. If a product doesn’t pass the smell test, trust that feeling. Throw it out. Better yet, don’t invite it onto your kitchen counter to begin with.

If we can’t trust Facebook to safely help us log in to websites or share news stories, why should we trust Facebook to move into our homes an always-on counter-mounted speaker capable of collecting incredibly sensitive data? Tl;dr: We shouldn’t! Of course we shouldn’t. But you knew that.",Social Media,TechCrunch,https://techcrunch.com/2018/10/08/facebook-portal-are-you-serious-rn/,"The main undesirable consequence of social media discussed in this article is a lack of trust in Facebook products. With the recent Cambridge Analytica scandal and other data breaches, people are more wary than ever of Facebook and its products. Despite the company's promises of privacy and security, its poor track record and lack of experience in the hardware space make",Security & Privacy
178,Minutiae: The Curious App That Captures Your Unfiltered Life,"Open your camera roll and give it a hard swipe, so your photos scroll past in a blur. What do you see when it stops? Your dog? A selfie? A perfectly timed shot of a beautiful sunset? It could be anything, but it's probably not a dull, ordinary moment. Camera rolls tend to look like Instagram outtakes, filled with moments that are good but not good enough to make the social media cut.

Daniel Wilson and Martin Adolfsson want to change that. Earlier this year, the duo---a neuroscientist and photographer, respectively---launched Minutiae, an app designed to document life's less glamorous moments. It works like this: Once a day, at a random time, Minutiae prompts you to take a photo. You have one minute to respond before the notification disappears forever. You open the app, aim your camera, and then have five seconds to capture the moment. There’s no time to think about framing. No opportunity to look for something cooler to shoot. The result, Wilson says, is a more authentic snapshot of what your life really looks like. “You’re recording what you would not normally record,” he says.

In the age of social media, that’s an unorthodox idea. Technology has made it easier than ever to document our lives; meanwhile, apps like Instagram make us second-guess what's worth photographing in the first place. Social media, with its like-fav-heart feedback loop, has retrained people to take and share only the photos that will please others. “We call this 'self-presentational concern,'” says Alix Barasch, who studies behavioral marketing at New York University. “When we take photos with the goal of sharing, it makes us think about how others are going to evaluate those photos.”

Be honest: How many times have you pulled out your camera to take a photo, already imagining the filter you might use? How many times have you deleted a photo because it wasn't cool or interesting enough to share? People tend to forego documenting the ordinary, says Ting Zhang, a researcher at Columbia University. Instead, they capture life's big moments---weddings, graduations, exciting nights out. “And then we end up overlooking the little moments that tend to make our everyday lives special,” she says.

That’s a problem if only because research shows that documenting the ordinary can ultimately make people happier than just documenting the extraordinary. That photo from your birthday party? Of course you'll cherish it in the future. But you probably underestimate how much you'd enjoy looking back on quiet moments, too, like a shot of the books stacked on your nightstand. “We tend to assume what’s mundane to me today will continue to remain mundane to me,"" Zhang says, ""when in fact, because circumstances change, all of a sudden what was considered mundane no longer is.”

Social media platforms are beginning to catch on to this idea. Companies like Beme and Narrative focused on capturing life’s less glamorous moments (both have shut down). From the start, Snapchat offered a more authentic alternative to Instagram, where a glossy, curated lifestyle can turn into a legitimate business model. Now even Instagram has started to capitalize on the increased desire for authenticity with features like Stories and Live. Yet, in every instance, the push for a less curated feed is undermined by the fact that people know they're sharing those moments with other people.",Social Media,WIRED,https://www.wired.com/2017/05/minutiae-curious-app-captures-unfiltered-life/,"Social media has created a culture where people feel pressure to share only the most perfect and interesting moments, causing them to overlook and miss out on the little things that make everyday life special.",Social Norms & Relationships
179,WhatsApp hires ‘grievance officer’ to help combat false information in India – TechCrunch,"Following widespread criticism of the way its service is used to spread false information and news in India, WhatsApp has hired a “grievance officer” for the country.

U.S-based Komal Lahiri is the lead who handles complaints about the service from users, law enforcement and other government officials, according to Mint.

WhatsApp did not respond to a request for comment on Lahiri’s hiring and her role. Her LinkedIn account, however, states that she has been “Senior Director, Global Customer Operations & Localization” at WhatsApp since March. Prior to that, she spent over three years with Facebook and Instagram in the U.S.

WhatsApp claims over 1.5 billion active users per month, India is its largest market and it accounts for an estimated 200 million of that figure. Despite the many benefits of a more connected society, information on WhatsApp has led to a number of troubling incidents. That includes the killing of five people who visited a rural village but were beaten to death after being falsely accused of being child kidnappers due to the spread of rumors on WhatsApp.

WhatsApp has tried to advise people by taking out full-page advertisements in an assortment of daily newspapers as well as educational camps, but the Indian government has pressured WhatsApp to do more. The IT minister met with WhatsApp COO Chris Daniels after it had sent two letters to the company asking it make its service more accountable and enable law enforcement agencies to use it.

The company has instead said it is impossible to track messages since its service is end-to-end encrypted. It has added features in India — one indicates when a message has been forwarded, and another limited the number of times messages can be forwarded on — but that hasn’t solved the problem. WhatsApp is at least stepping up its efforts with this hire, but it remains to be seen how adequate a job Lahiri can do from the U.S.

More widely, WhatsApp does seem to be struggling make hires in India.

While this role is located in the U.S, the company is still trying to fill the position of ‘head of India’ and its head of policy in the country, too. Facebook itself is also hiring for a managing director for its India business.",Social Media,TechCrunch,https://techcrunch.com/2018/09/24/whatsapp-hires-grievance-officer/,"WhatsApp has faced criticism for its role in the spread of false information and news in India, leading to the deaths of five people due to the spread of rumors on the platform. In response, they have hired a 'Grievance Officer' for the country and have taken out advertisements in newspapers and held educational camps in an attempt",Equality & Justice
180,Facebook Viewpoints pays users for well-being surveys & tasks – TechCrunch,"Facebook is launching a new market research, task, and product testing program that lets users earn money. Starting today, people in the US who are over 18 can download Viewpoints and participate in a well-being survey so Facebook can learn to “limit the negative impacts of social media and enhance the benefits.” Other opportunities include completing online chores on behalf of Facebook, or trying out new apps or devices ahead of launch so Facebook can refine them.

The well-being survey will take about 15 minutes score users 1000 points, which translates into a $5 reward that’s paid over PayPal. People interested in signing up can join Viewpoints here. The company claims it will only use the data collected internally and won’t sell it. Facebook Viewpoints is available on iOS and Android, and the company plans to open the app to more countries next year.

The question is whether users will be comfortable giving up even more data Facebook. Many are already creeped out by Facebook, but the monetary incentive might override their morals.

Meanwhile, Facebook will have to work to prevent the app from beinh abused. Most importantly, it needs to figure out how to make sure underage minors aren’t slipping into the app. They might be more vulnerable to coercion by cash, and less aware of the consequences of sharing their data.

I tried using Viewpoints but wasn’t invited to the well-being study or any other opportunities, so I couldn’t earn any money or try it out further as som studies are open only to people in certain locations or demographics. For now you have to log in with a Facebook account but it showed greyed out options for Google, phone, and email login that Facebook says are coming soon. Payments can take up to 10 days to process and your points expire after 5 years. Facebook won’t post or publicly share any info you provide through the app.

The launch of Viewpoints comes after Facebook shut down its paid market surveillance program Research and its free VPN that collected users’ data Onavo in the wake of a TechCrunch investigation that found the company was paying teenagers for their data while breaking Apple’s rules about distributing employee-only apps outside of a company.

The social network relaunched its market research efforts under the name Study From Facebook in June with a commitment to not allowing kids access. But in the meantime, leaked court documents have shown that Facebook purposefully used market research collected from Onavo to find potential rivals to cut off from its data. Facebook is now under anti-trust investigations surrounding concerns that disadvantaging its competitors hurt consumer choice in social apps.",Social Media,TechCrunch,https://techcrunch.com/2019/11/25/facebook-viewpoints/,"Facebook's new program, Viewpoints, is facing criticism for its potential to exploit users and minors, as well as its past history of using market research data to disadvantage its competitors, leading to anti-trust investigations.",Security & Privacy
181,Twitter claims tech wins in quashing terror tweets – TechCrunch,"In its latest Transparency report, which covers requests it’s received from governments pertaining to content on its platform, Twitter has reported a big decline in the proportion of pro-terrorism accounts being reported over the past six months, saying this is down 80 per cent since its last report, as well as reporting a drop in the number of accounts it removed for terrorism-related content during this period.

Twitter claims pro-terrorism account reports have shrunk by a fifth in the past six months.

It also reports that the vast majority (95 per cent) of account suspensions pertaining to the promotion of terrorism resulted from use of its in-house tech tools, up from 74 per cent on the prior six-month report period — with government requests accounting for less than one per cent of pro-terror account suspensions.

Along with other social media platform giants, Twitter is facing increased political pressure to promptly eject terrorist content and hate speech from its platform — especially in Europe where new laws have been proposed in some countries that could see governments introducing a regime of financial penalties attached to failures of performance for social media content takedown as a stick to encourage faster removals of illegal content.

~300,000 accounts nixed for terrorism in six months

Between January and June 2017, the six-month period covered by this, Twitter’s 11th Transparency Report, the tech firm said it removed a total of 299,649 pro-terrorism accounts — surfaced by both reports from governments and its own in-house tech (though the lion’s share of identifications were generated by its tech tools).

It says this represents a 20 per cent drop in terrorism-promoting Twitter accounts since the last reporting period, of July 1, 2016 through December 31, 2016.

Which — coupled with the 80 per cent drop in government agencies reporting pro-terror Twitter accounts — suggests the company is at least managing to squeeze terrorist activity on its platform, given it seems unlikely there’s been such a large reduction in globally active terrorists online over the same period. (Even as there are still hundreds of thousands of pro-terrorism Twitter accounts being created every half a year.)

The company further emphasizes it killed a majority of the pro-terrorism accounts set up on its platform before they could post anything: “Notably, 75% of these accounts were suspended before posting their first Tweet,” it writes.

Which seems a big win. And a figure to watch to see if Twitter is able to further increase the proportion of non-tweeter terrorism account suspensions in its next Transparency Report.

A spokeswoman for Twitter confirmed to us that this is the first time it’s published data on “that particular metric” when we asked whether there has been a rise in Twitter being able to cut-off terrorist accounts before they’ve sent a single tweet.

“In the last six months we have seen our internal, spam-fighting tools play an increasingly valuable role in helping us get terrorist content off of Twitter,” she added. “Our anti-spam tools are getting faster, more efficient, and smarter in how we take down accounts that violate our TOS.”

The figure for total suspensions of pro-terrorism Twitter accounts is now approaching 1M over two years. (To be exact, the company reports 935,897 pro-terrorism account suspensions between August 1, 2015 through June 30, 2017.)

Asked for more details about the changes it’s made to its anti-terrorism tools — to apparently deliver better results — the spokeswoman told us: “We are reluctant to share details of how these tools work as we do not want to provide information that could be used to try to avoid detection.”

“We can say that these tools enable us to take signals from accounts found to be in violation of our TOS and to work to continuously strengthen and refine the combinations of signals that can accurately surface accounts that may be similar,” she added.

Another Twitter spokesperson also pointed to a few pieces of academic research which suggest the Islamic State terror group has shifted its social media strategy from relying on Twitter’s platform to distribute violent propaganda to utilizing the messaging platform Telegram (which lets users broadcast missives to large groups).

The spokesman also made a point of flagging how the latter has been called out for a lack of co-operation by security agencies. So the company is clearly hoping to shift the big red finger of terrorism propaganda blame onto the rival Telegram messaging platform.

Abusive behavior triggered 98% of gov’t TOS reports

In this 11th edition of its Transparency Report Twitter has also expanded the categories it breaks out in the government TOS reports section (which it added in its 10th report) to now show a break down of four categories of these types of reports — namely: Abusive Behavior, Copyright, Promotion of Terrorism, and Trademark reports.

This shows that the vast majority of reports Twitter is receiving from governments relate to abusive behavior on Twitter — which it says accounted for 98 per cent of global government TOS reports it received — with pro-terrorism content a very, very distant second (accounting for around 2 per cent of the reports).

This is interesting as it underlines the huge difference in how Twitter is approaching terrorism-related content vs abusive behavior — with the vast majority (92 per cent) of accounts reported for terrorism going on to be removed by Twitter from its platform vs just 13 per cent (as Twitter reports it) of those reported for abusive behavior actually being suspended.

In the report Twitter says the fact that the vast majority of abuse-related reports resulted in no content being removed is down to “a variety of reasons” —

… such as the reporter failing to identify content on Twitter or our investigation finding that the reported content did not violate our Terms. As we take an objective approach to processing global Terms of Service reports, the fact that the reporters in these cases happened to be government officials had no bearing on whether any action was taken under our Rules.

You could argue that terrorism is a rather easier category of content to identify than ‘abusive behavior’, with the latter representing something of a subjective spectrum when you’re talking in terms of a package of content delivered in tweet form (and of course depending on how high you dial up your ‘free speech’ setting); and likely a much more subjective spectrum vs pro-terrorism content specifically.

Though there’s no doubt Twitter is still the target of fierce criticism, including by many users, for how its platform continues to enable, for example, misogynist troll armies to pile in and harass women en masse. And such co-ordinated harassment clearly undermines the free speech rights of those being targeted. (Though Twitter has claimed to be stepping up its anti-abuse measures and tools.)

The company also continues to be criticized for racist speech on its platform. Even though its TOC expressively forbid “hateful conduct” including “on the basis of race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, disability, or disease”.

Just this August the company was called out — in this instance by a UK parliamentary committee — for failing to act on abusive tweets, including failing to taken down graphic images of suspected rape and abuse which, its critics argue, clearly violate its own community standards — which forbid inciting or engaging in “targeted abuse or harassment of others”.

in that instance the Guardian reported that the committee chair wrote to Twitter asking it to explain its methodology and timescales for removing graphic pictures and sexually explicit messages, and also asking it to provide details of the average time taken to investigate reports and take down tweets, as well as what action is being taken to speed up removals.

The MP also sought information on how many staff Twitter employs actively looking for abusive content, and for more detail on its policy on the removal of tweets and suspension of accounts.

Which are exactly the sorts of questions Twitter’s Transparency Report does not answer. Although it is at least now breaking out abusive behavior as a government TOS reports category and revealing it to be the overwhelmingly number one issue being reported by government agencies.

We can’t compare this with prior Transparency Reports as Twitter was not previously breaking government reports into specific categories. But its inclusion and prominence now does suggest politicians are feeling under pressure to take action to try to curb abuse taking place on Twitter.

Of the government-reported abusive content that Twitter did remove, the company reports the largest proportion was related to harassment and “hateful conduct” — stating that: “The majority was removed for “violating rules under these areas: harassment (37%), hateful conduct (35%), and impersonation (13%)”.

“The remainder of the violating content fell within other areas of our prohibitions against abusive behavior as set forth in the Twitter Rules,” it adds.

Asked if it could disclose the geographical locations where it receives the most government reports relating to abusive behavior on its platform, the Twitter spokeswoman told us it cannot provide “that level of granularity this time”.

Nor, she told us, is it able to disclose the geographies where it did take action on the minority of government reports on abusive behavior and remove accounts.

The company does not reveal how many reports of abusive behavior it receives generally, from all users, i.e. rather than just government-related reports — per this report. But now that it’s breaking out government agency reports of abusive behavior it should at least be possible to see how political pressure on Twitter over this issue rises (or falls) going forward.

Elsewhere in the Transparency Report, Twitter notes it has expanded its U.S. country report, adding a breakdown of California state information requests at the county level — and says it has plans to introduce this section to other states in future to help users “get a better idea of how frequently their local authorities seek user account information”.

Over the report period, it also says it received 6 per cent more global government requests for account information which affected 3% fewer accounts than in the previous period. It further notes requests originated from four new countries: Nepal, Paraguay, Panama, and Uruguay.

“In addition, we received approximately 10% more global legal requests to remove content impacting roughly 12% more accounts compared to the previous reporting period. These included requests from nine new countries: Bahrain, China, Croatia, Finland, Nepal, Paraguay, Poland, Qatar, Ukraine, and Uruguay,” it adds.",Social Media,TechCrunch,https://techcrunch.com/2017/09/19/twitter-claims-tech-wins-in-quashing-terror-tweets/,"Social Media platforms such as Twitter face increasing political pressure to promptly remove terrorist content, hate speech and other forms of abuse from their platforms. In the latest Transparency Report, Twitter reported a drop in the number of accounts it has removed for terrorism-related content and that the vast majority of government-related reports of abuse have resulted in no content being",Security & Privacy
182,The Mysteries of Getting Instagram to Delete Fake Accounts,"Branden Harvey is no stranger to people impersonating him or stealing his work on Instagram.

This happens more often than you'd think, and you don't have to be famous, or even Internet famous, to be a victim. Harvey is a photographer, and the first time he discovered someone posting his photos was about three years ago when his account was really taking off. It's happened a few times since then.

Sometimes it's just obnoxious, where people are ""spammy, or just impersonating."" But sometimes, it's much more than that. ""The aggravating time was more malicious. Like, [someone] pretending to be me in a really condescending, rude way. It was the type of thing that if misinterpreted as real, could do some real damage.""

So Harvey reported it. And eventually, the bogus account simply vanished, disappearing into the ether without any word from Instagram. And that's a problem. Instagram doesn't have a mechanism for letting users know it's investigating bogus accounts, and doesn't tell people when, or if, those accounts are deleted.

It Could Happen to You

I've had some experience with this myself, though nothing so disturbing as what Harvey endured. For reasons that can only be traced to unadulterated vanity, I'd recently been thinking that my Instagram handle, @mollygrams, isn’t as clever as it could be. @mollygram would be just a bit more on point. Anyone who has a moderate interest in Instagram or Twitter has thought about their handle—you think our own David Pierce originally owned @pierce? Think again. So I searched for ""mollygram"" to see if it was available. It wasn't. It was being used---with my photos.

My first thought was someone is impersonating me. But the more I thought about it, (I think a lot about stuff like this), the more that didn't make sense. First, @mollygram started five years ago, and the person stopped posting almost immediately. It featured nine photos---my family, a few unflattering selfies, that type of thing. It was too old, and had too few photos, to be effective spam. I wondered if it might be a test account I'd created and abandoned (I am always trying new apps, so this is not unusual). I tried logging in. No dice. I tried changing my password, but never received a reset email. I tried, twice, without success to contact Instagram support via email. Reporting the account via the web didn't help, either.

Granted, this was, at best, mildly annoying. Many people actually are being impersonated, and running into the same problems trying to sort it all out. “I just pounded on the report button a few times, maybe filled out a form, and it disappeared months later,"" Harvey says about one of his impersonation reports. He never received a response from Instagram, and only discovered that Instagram had 86'd the account when he searched for it.

Often, Instagram offers no indication that anything is being done about the problem. Sometimes the offending account remains, other times it simply vanishes.

That seems to be how it happens. Often, Instagram offers no indication that anything is being done about the problem. Sometimes the offending account remains, other times it simply vanishes.

That's eventually what happened to @mollygram. I talked to someone at Instagram about my problem. All signs pointed to this being a test account, I was told, created simply to experiment with Instagram. I created it before @mollygrams, and used the same device to create both accounts. So, yes, my initial reaction---""Hey! Someone stole my photos!"" was wrong. (In my defense, I have had photos stolen and posted in obscure Internet forums, so that reaction was not unfounded.) But the problem remained---I could not delete an account I did not want.",Social Media,WIRED,https://www.wired.com/2016/01/what-happens-reporting-instagram-accounts/,The lack of a clear system to report and take action against malicious accounts on social media can result in users becoming victims of impersonation or having their work stolen without any way of knowing if or when it has been resolved. This can be especially frustrating when users have difficulty even deleting accounts they have created themselves.,Security & Privacy
183,Why the world must pay attention to the fight against disinformation and fake news in Taiwan – TechCrunch,"On Saturday, Taiwan will hold its presidential election. This year, the outcome is even more important than usual because it will signal in which direction the country’s people want its relationship with China, which claims Taiwan as its territory, to move. Also crucial are efforts against fake news. Taiwan has one of the worst disinformation problems in the world, and how it is handled is an important case study for other countries.

Yesterday, Twitter said in a blog post that it has held trainings for the two main political parties in Taiwan, the Democratic Progressive Party (DPP) and the Kuomintang (KMT), as well as Taiwan’s Central Election Commission, in addition to setting up a portal for feedback during the election. Late last month, the state-owned Central News Agency reported that Facebook will set up a “war room” to counteract disinformation before the election, echoing its efforts in other countries (the company previously established a regional elections center at its Asia-Pacific headquarters in Singapore).

But the fight against disinformation in Taiwan started years before the current presidential election. It now encompasses the government, tech companies and nonprofit groups like the Taiwan FactCheck Center, and will continue after the election. As in other countries, the fake news problem in Taiwan takes advantage of complex, deep-rooted ideological, cultural and political rifts among Taiwan’s population of 24 million, and it demonstrates that fake news isn’t just a tech or media literacy problem, but also one that needs to be examined from a social psychological perspective.

How fake news spreads in Taiwan

This year’s election is taking place as the Chinese government, under President Xi Jinping, makes increasingly aggressive efforts to assert control over Taiwan, and as the ongoing demonstrations in Hong Kong underscore the fissures in China’s “one country, two systems” model. The two leading candidates are incumbent Tsai Ing-wen, a member of the DPP, and opponent Han Kuo-yu of the KMT, who favors a more conciliatory relationship with China.

The Chinese government has been linked to disinformation campaigns in Taiwan. Last year, the Varieties of Democracy Institute (V-Dem) at the University of Gothenburg in Sweden researched foreign influence in domestic politics and placed Taiwan in its “worst” category, along with Latvia and Bahrain, as the countries where foreign governments most frequently use social media to spread false information for “all key political issues.” By comparison, the United States ranked 13th worst on the list, despite being targeted by Russian disinformation operations.

But disinformation also comes from many other sources, including Taiwanese politicians from different parties and their supporters, “cyber armies” whose aim is to influence voters and content farms that sensationalize and repost content from media outlets. It is spread through platforms, including Facebook, Twitter, messaging app Line, online bulletin board PTT and YouTube.

The problem has escalated over the past five years, according to an April 2019 report by CommonWealth Magazine reporters Rebecca Lin and Felice Wu. During the previous presidential election, cyber armies consisting of supporters for some politicians, or workers for political parties and public relations companies, began engaging in online information wars, creating an opportunity for foreign influence. Wu Hsun-hsiao, former legal counsel to PTT, told the magazine that in 2015, more dummy accounts entering from China began to appear on the bulletin board and Facebook. “The rise of emerging online media has generated a considerable amount of noise, and China has discovered the influence it has,” he told CommonWealth.

Over the last two years, YouTube has also become an increasingly potent way to spread disinformation, often through short videos that take clips and photos from news outlets and re-edit them to present misleading narratives about major news events. “They take advantage of the myth that ‘to see is to believe’ by massively disseminating false information in the run-up to the election,” Wang Tai-li, a professor in National Taiwan University’s Graduate Institute of Journalism told the Liberty Times. Last year, Google published posts on its Taiwan blog that said it had partnered with volunteers and organizations like MyGoPen, Taiwan FactCheck Center and the Poynter Institute’s International Fact-Checking Network to increase awareness of disinformation and flag fake news on its platforms, including YouTube.

Earlier this month, Taiwan’s legislature passed the DPP-backed Anti-Infiltration Bill, meant to stop Chinese influence in Taiwanese politics. The legislation was opposed by KMT politicians, including former president Ma Ying-jeou, who made controversial statements comparing the bill to restoring Taiwan’s four decades of martial law, which ended in 1987.

But part of the challenge of fighting fake news in Taiwan is lack of awareness. After local elections in November 2018, Wang conducted a survey that found 52% of respondents did not believe there was foreign interference, or said they did not know enough to judge.

Fighting back

Much of the work, however, is being done by volunteers and private citizens. Last month, Los Angeles Times reporter Alice Su wrote about organizations like the Taiwan FactCheck Center, a nonprofit that does not receive funding from the government, political parties or politicians. In July 2018, the group began collaborating with Facebook, where posts flagged as containing false information bring up a screen with a link that takes users to a Taiwan FactCheck Center report before they are allowed to view the content.

Su also covered other groups, like DoubleThink Labs, which monitors Chinese disinformation networks in Taiwan, and CoFacts, a crowdsourced database that operates a factchecking Line chatbot. But these groups and social media platforms are up against thousands of posts containing disinformation each day, including in private chat groups that can’t be monitored.

Last month, Facebook said it had removed for rules violations 118 fan pages, 99 groups and 51 accounts, including an unofficial fan page for Han called “Kaohsiung Fan Group” that had more than 150,000 members.

In a statement to TechCrunch, a Facebook spokesperson said, “Over the last three years, we have dedicated unprecedented resources to fighting malicious activity on our platform and, in particular, to protecting the integrity of elections on Facebook–including this week’s election in Taiwan. Our approach includes removing fake accounts, reducing the spread of misinformation, bringing transparency to political advertising, disrupting information operations and working with Taiwan’s Central Election Commission to promote civic engagement. We have teams of experts dedicated to protecting Taiwan’s election, and we look forward to ensuring that Facebook can play a positive role in the democratic process.”

The pervasiveness of fake news

Efforts to combat fake news often results in more disinformation spread by people who believe their views have been unfairly targeted. According to the Stanford Internet Observatory (SIO), by the time the Kaohsiung Fan Group was removed, it had 109 admins and moderators, a number the SIO said was “unusually high compared to the average admin and moderator counts for Taiwanese political groups of either affiliation (pro-Han Kuo-yu groups averaged 27, and pro-Tsai Ing-wen Groups, 10).” Furthermore, several moderators had “suspicious” profiles, including zero or one friend, profile photos that were not of people and “minimal human engagement” on their posts.

But despite that evidence, the SIO also noted that the mass removal “prompted conspiratorial theories about why they were taken down in the first place,” with posts in other pro-KMT groups speculating that it had been done in coordination with the DPP.

An illustration of how sticky fake news can be once it takes root is disinformation about the validity of Tsai’s PhD from the London School of Economics, which continue to circulate through Taiwan even though the university issued a statement confirming the degree.

As in other countries, disinformation in Taiwan also highlight and widen existing political social and cultural rifts. In the United States, for example, fake news campaigns by Russian agents capitalize on already highly polarizing issues, including race, immigration and gun control.

In Taiwan, the specific issues may be different, but the objective is the same. As Su wrote in the Los Angeles Times, many posts “try to stir emotions on hot-button issues–for example, false claims that Tsai’s government has misused pension funds to lure Korean and Japanese tourists to make up for a drop in visitors from the mainland, and that organizers of Taiwan’s annual gay-rights parade received stipends to invite overseas partners to march with them.”

Taiwan became the first country in Asia to legalize same-sex marriage last year, despite aggressive efforts by conservative groups to stop it. Before it was passed, CoFacts documented viral posts that linked same-sex marriage with the spread of of HIV. Creators of fake news continue to take advantage of the issue by spreading homophobic disinformation, including claims that the DPP spent NT$30 million (about $980,000) to organize Taipei’s Pride Parade, even though the event is funded by its organizers and does not receive sponsorship from political parties.

Disinformation is difficult to combat, and the use of online platforms to spread it is rapidly emerging as one of this century’s most serious problems. But online tools, observers and fact-checkers, more vigilance by social media platforms and understanding the social and cultural issues exploited by disinformation can also be used to fight it. Taiwan’s rampant fake news and disinformation problem has reached the point of crisis, but it may also reveal which solutions are most effective at combating it around the world.",Social Media,TechCrunch,https://techcrunch.com/2020/01/07/why-the-world-must-pay-attention-to-the-fight-against-disinformation-and-fake-news-in-taiwan/,"Social media has become a major platform for the proliferation of disinformation and fake news, which has escalated over the past five years in Taiwan, exploiting social and cultural rifts in the country to influence the upcoming election.","Information, Discourse & Governance"
184,How Russia ‘Pushed Our Buttons’ With Fake Online Ads,"Many Americans this week got their first looks at fake Facebook ads placed by Russian propagandists during the 2016 election campaign to sow discord in the US. The ads, made public during congressional hearings with social-media executives, targeted Americans on both sides of divisive issues such as Islam, gun rights, and the Black Lives Matter movement.

One ad, nominally from an account called Heart of Texas, showed silhouettes of cowboys behind a map of America with a rainbow flag and a poster about Islam taking over the world. “Get Ready to Secede!” the ad screams at the bottom. Another ad shows the somber image of policemen in uniform carrying a casket at a funeral with the words, “Another Gruesome Attack on Police By a BLM Movement Activist.”

Psychologists and students of advertising say the ads were cleverly designed to look like other internet memes, and to appeal to readers’ emotions. Jay Van Bavel, an associate professor of psychology at NYU, says he was surprised at the sophistication of the campaign. “It wasn’t transparent lies. It was just pushing our buttons,” says Van Bavel. “To me, this is more pernicious. It’s not a matter of fiction that we can root out with fact-checking. It’s more about turning Americans against each other.”

The ads took issues that voters care about and then “fed them to us as aggressively as possible,” he says.

Facebook estimates that 10 million people saw the paid ads and up to 150 million people saw other content from the fake accounts, which Facebook has traced to the Internet Research Agency, a Kremlin-backed troll farm. The ads were placed by fake accounts with names like United Muslims of America, Blacktivist, and LGBT United that could have passed for real Facebook groups.",Social Media,WIRED,https://www.wired.com/story/how-russia-pushed-our-buttons-with-fake-online-ads/,"The use of sophisticated fake ads on Social Media by Russian propagandists during the 2016 election campaign has been revealed, and experts have noted that such tactics are aimed at stoking division and discord among Americans by targeting them on both sides of divisive issues.",Equality & Justice
185,Facebook’s Brexit probe unearths three Russian-bought “immigration” ads – TechCrunch,"Facebook has provided more details about the extent of Russian digital interference related to the UK’s Brexit vote last year.

Last month the social media giant confirmed that Russian agents had used its platform to try to interfere in the UK’s referendum on EU membership — but said it had not found “significant coordination of ad buys or political misinformation targeting the Brexit vote”.

Today’s findings apparently bear out that conclusion, with Facebook claiming it’s unearthed just three ads and less than $1 spent.

The Brexit related Russian-backed ads ran for four days in May, ahead of the UK’s June referendum vote, and apparently garnered around 200 views on Facebook.

It says the ads targeted both UK and US audiences — and “concerned immigration”, rather than being explicitly about the UK’s EU referendum vote.

Which appears to be in line with the strategy Kremlin agents have deployed in the US, where Russian-bought ads have targeted all sorts of socially divisive issues in an apparent attempt to drive different groups and communities further apart.

The Brexit-related ads were paid for by the same Russian-backed 470 accounts that it previously revealed spent ~$100,000, between June 2016 and May 2017, to run more than 3,000 ads targeting US users.

And Facebook linked these accounts to Russia as a consequence of its investigation into Kremlin interference in the wake of the 2016 US presidential election.

For the Brexit audit, it’s worth noting that Facebook appears to have only looked at identified Internet Research Agency (IRA) pages or account profiles — IRA being the previously unmasked Russian troll-farm — so there could be scope for other Russian-backed accounts to have bought ads intending to meddle with Brexit without Facebook realizing it. (Although given the levels of ad buys by IRA accounts targeting US Facebook users it’s perhaps unlikely there’s a second layer to the Russian political dis-ops campaign. Albeit still possible.)

It also does not look like Facebook has attempted to measure and quantify non-paid Brexit-related disinformation posts by Russian-backed accounts — since it’s only talking in terms of “funded advertisements”. We’ve asked and will update this post with any response.

Update: TechCrunch understands that since the scope of the Electoral Commission enquiry relates to activity funded by Russia, Facebook has — thus far — limited its Brexit scrutiny to ad buys. (Thereby making its scrutiny pretty limited.)

We’ve also asked Facebook to share the three Russian-bought “immigration” ads, and to confirm whether they were anti-immigration in sentiment.

So far the company has provided us with the following extract from a letter to the Electoral Commission as commentary on its findings:

We strongly support the Commission’s efforts to regulate and enforce political campaign finance rules in the United Kingdom, and we take the Commission’s request very seriously. Further to your request, we have examined whether any of the identified Internet Research Agency (IRA) pages or account profiles funded advertisements to audiences in the United Kingdom during the regulated period for the EU Referendum. We have determined that these accounts associated with the IRA spent a small amount of money ($0.97) on advertisements that delivered to UK audiences during that time. This amount resulted in three advertisements (each of which were also targeted to US audiences and concerned immigration, not the EU referendum) delivering approximately 200 impressions to UK viewers over four days in May 2016.

An Electoral Commission spokesperson we contacted for a response emphasized that its discussions with social media companies are at a very early stage.

The spokesperson also confirmed that Google and Twitter have both also provided information in response to its request they do so, to feed its ongoing enquiry into whether the use of digital ads and bots on social media might break existing political campaigning rules.

In a statement, the spokesperson added: “Facebook, Google and Twitter have responded to us. We welcome their cooperation. There is further work to be done with these companies in response to our request for details of campaign activity on their platforms funded from outside the UK. Following those discussions we will say more about our conclusions.”

At the time of writing Twitter and Google had not responded to a request for details of the information they have passed to the Electoral Commission — which late last month Twitter said it would be providing “in the coming weeks”.

Update: Twitter has now released its own Russian-bought Brexit ads report — saying it found six ads, and a total spend of just over $1k. (Though it also has not yet released any information about non-paid Russian-backed propaganda — i.e. bots and their free-to-post tweets — which were likely a lot more plentiful than actual ads.)

A recent academic study of tweet data — looking at how political information diffused on Twitter’s platform specifically around the Brexit vote and the US election — identified more than 156,000 Russian accounts which mentioned #Brexit.

The study also found Russian accounts posted almost 45,000 messages pertaining to the EU referendum in the 48 hours around the vote.

Update: A Google spokesperson has now provided the following response — claiming not to have found any evidence of Russian disinformation ops. “We took a thorough look at our systems and found no evidence of this activity on our platform,” they told us.

Social media’s still unaudited role in political campaigning looks set to remain in the domestic spotlight for the foreseeable future — as the Commission continues to investigate.

Though it remains to be seen whether the body will recommend amending UK law to better regulate political activity on digital platforms.

The UK’s Prime Minister waded into the disinformation debate herself last month by publicly accusing the Russian government of seeking to “weaponize information” by planting fake stories and photoshopped images to try to sow discord in the West.

And the so-far disclosed extent of Russian divisive content targeting the US electorate — which in October Facebook admitted could have reached as many as 126 million people — should give politicians in any democracy plenty of pause for thought about major tech platforms.",Social Media,TechCrunch,https://techcrunch.com/2017/12/13/facebooks-brexit-probe-unearths-three-russian-bought-immigration-ads/,"The extent of Russian interference in the UK's Brexit vote has been revealed to be minimal, with only three ads and less than $1 spent. However, the lack of regulation and oversight of political activity on Social Media platforms could lead to greater manipulation of electorates in the future.",Politics
186,YouTube sets a goal of having half of trending videos coming from its own site – TechCrunch,"YouTube wants to have half of the featured videos in its trending tab come from streams originating on the company’s own site going forward, according to the latest quarterly letter from chief executive Susan Wojcicki.

The letter, directed to YouTube’s users, is meant to help ease concerns the site’s biggest stars have over copyright challenges, advertising policies and video monetization — along with their shrinking presence on the site’s trending feature.

YouTube’s biggest contributors are worried that their footprint on the trending tab is shrinking as the company favors “safer” content coming from other, more traditional, media like repurposed television clips, movie trailers, and music videos.

It’s been a rough quarter for YouTube. The company had to deal with yet another child predator scandal, which prompted the company to completely shut down comment sections on most videos featuring minors.

The Alphabet-owned video company was also forced to wrestle with its role in the spread of a global anti-vaccination campaign that has helped foster a resurgence in Measles cases around the world — creating a new epidemic in the U.S. of a disease that had been largely eradicated in the country.

Beyond monetizing anti-vaccination videos, YouTube’s role in the dissemination of videos taken by the white supremacist mass-murderer who killed scores of people in attacks on mosques in Christchurch, New Zealand has created a backlash against the company in capitals around the world.

Wojcicki addressed both incidents in the letter, writing:

In February, we announced the suspension of comments on most YouTube videos that feature minors. We did this to protect children from predatory comments (with the exception of a small number of channels that have the manpower needed to actively moderate their comments and take additional steps to protect children). We know how vital comments are to creators. I hear from creators every day how meaningful comments are for engaging with fans, getting feedback, and helping guide future videos. I also know this change impacted so many creators who we know are innocent—from professional creators to young people or their parents who are posting videos. But in the end, that was a trade-off we made because we feel protecting children on our platform should be the most important guiding principle. The following month, we took unprecedented action in the wake of the Christchurch tragedy. Our teams immediately sprung into action to remove the violative content. To counter the enormous volume of uploaded videos showing violent imagery, we chose to temporarily break some of our processes and features. That meant a number of videos that didn’t actually violate community guidelines, including a small set of news and commentary, were swept up and kept off the platform (until appealed by its owners and reinstated). But given the stakes, it was another trade-off that we felt was necessary. And with the devastating Sri Lankan attacks, our teams worked around the clock to make sure we removed violative content. In both cases, our systems triggered authoritative news and limited the spread of any hate and misinformation.

Given those examples, the commitment that Wojcicki is making to ensure that half of the videos in the company’s trending tab come from YouTube itself seems… risky.

The company needs to do something, though. The talent on which it depends to bring in advertisers and an audience is very worried about a number of recent steps YouTube has taken.

From the perspective of YouTube’s top talent, the company is abandoning them even as regulators restrict the ways in which they’re able to make the videos that have defined the site throughout its history.

In Europe, meme culture is under attack by lawmakers who have passed legislation muddying the waters around what constitutes fair use — and YouTube’s users are worried that the company may start restricting the distribution of their videos on flimsy copyright claims.

“[We] are also still very concerned about Article 13 (now renamed Article 17) — a part of the Copyright directive that recently passed in the E.U.,” Wojcicki wrote. “While we support the rights of copyright holders—YouTube has deals with almost all the music companies and TV broadcasters today—we are concerned about the vague, untested requirements of the new directive. It could create serious limitations for what YouTube creators can upload. This risks lowering the revenue to traditional media and music companies from YouTube and potentially devastating the many European creators who have built their businesses on YouTube.”

In many ways the letter is just a continuation of themes that Wojcicki laid out in her first address to the company’s core user base.

It’s a pivotal moment for YouTube as public pressures mount for the company to take more responsibility for the videos it distributes and the users that make up the bulk of its creative community start chafing under their increasing constraints.

The company appears to be responding with a commitment to be more transparent going forward, but it’s going to be increasingly difficult for the company to navigate between the pressures of advertisers for “safe” videos and producers for greater creative freedoms — all with traditional media putting the company increasingly in its crosshairs and new players like TikTok commanding greater attention.",Social Media,TechCrunch,https://techcrunch.com/2019/04/30/youtube-sets-a-goal-of-having-half-of-trending-videos-coming-from-its-own-site/,"YouTube is facing criticism for its role in the spread of anti-vaccination campaigns and videos from white supremacist mass-murderers, leading to a backlash from governments and a decrease in trust from its core user base. The company is attempting to respond by committing to have half of the featured videos in its trending tab come from streams originating on",User Experience & Entertainment
187,'Kara Versus Jack' Proves That Twitter Needs an Edit Button,"On Tuesday afternoon, New York Times technology columnist and Recode editor-at-large Kara Swisher interviewed Twitter CEO Jack Dorsey—on Twitter, in what Swisher called a “live chat.” She peppered Dorsey with hard questions about why the platform is still so full of abuse and pushed for specific examples of how Dorsey has improved the “health” of conversations on Twitter. Because of the format of the interview—a series of tweets and replies—the details of the conversation were almost impossible to follow. But one thing was crystal clear: Twitter still needs an Edit button.

Things got off to a rocky start right away, when Swisher’s first tweet included a typo. “You know my jam these days is tech responsibility. What grade do you gave Silicon Valley? Yourself? #KaraJack,” she tweeted.

The copyedit came fast, from @mojaam, who noted that Swisher meant to write “give,” not “gave,” adding, “You know, that edit feature would have come in handy right about now.”

Twitter users have been asking for an Edit button for years. Back in 2013, former WIRED writer Mat Honan cited this as the feature Twitter most needed. Since then, article after article has been written about why Twitter needs editing. Possibly the single most common tweet sent to @jack himself is some variation of “give us our Edit button.” Over the years, he has teased doing so, while cautioning that this could change the historical-record nature of Twitter. Last week, on Joe Rogan’s podcast, Dorsey said the team is considering a function that would preserve the original tweet for posterity but display edited text (which sounds like how editing works on Facebook, for the most part).

Dorsey had typos of his own in the interview with Swisher. At one point, in reply to a question about why Twitter doesn’t kick more bad actors off the site, he wrote, “We action all we can against our policies.” The sentence prompted New Republic contributing editor Jeet Heer to ask, “Does anyone here speak gibberish?”

Was Dorsey using “action” as a verb (inspired, perhaps, by Jeff Bezos’ “complexifying” of the English language in last week’s Medium post)? Or did he forget a word? Maybe he meant to type “We take action all we can against our policies.” Before he could clarify, the conversation had moved on.

Complicating the conversation even further, Swisher and Dorsey tried to organize their dialog with a hashtag, but not all of the tweets included it. Even when they remembered, the thread of the conversation was hard to follow. Depending on which tweet you saw or clicked on first, you might get different aspects of the conversation. At one point, Swisher started a new thread while Dorsey continued to reply to the old one.

People trying to follow along called out Dorsey, making fun of him for not knowing how to use his own site. In fairness, Twitter wasn’t set up to be used like this; it’s probably not the best platform for a live interview. Not, at least, without some substantial design changes.",Social Media,WIRED,https://www.wired.com/story/kara-jack-twitter-interview/,"The main issue discussed in this article is the lack of an ""Edit"" button on Twitter, which makes it difficult to follow conversations and leads to confusion, typos, and humorous mocking of those using the platform. This lack of an editing feature is seen as a major disadvantage of social media, and is a source of frustration for many users",User Experience & Entertainment
188,Google is banning Irish abortion referendum ads ahead of vote – TechCrunch,"Google is suspending adverts related to a referendum in Ireland on whether or not to overturn a constitutional clause banning abortion. The vote is due to take place in a little over two weeks time.

“Following our update around election integrity efforts globally, we have decided to pause all ads related to the Irish referendum on the eighth amendment,” a Google spokesperson told us.

The spokesperson said enforcement of the policy — which will cover referendum adverts that appear alongside Google search results and on its video sharing platform YouTube — will begin in the next 24 hours, with the pause remaining in effect through the referendum, with the vote due to take place on May 25.

The move follows an announcement by Facebook yesterday saying it had stopped accepting referendum related ads paid for by foreign entities. However Google is going further and pausing all ads targeting the vote.

Given the sensitivity of the issue a blanket ban is likely the least controversial option for the company, as well as also the simplest to implement — whereas Facebook has said it has been liaising with local groups for some time, and has created a dedicated channel where ads that might be breaking its ban on foreign buyers can be reported by the groups, generating reports that Facebook will need to review and act on quickly.

Given how close the vote now is both tech giants have been accused of acting too late to prevent foreign interests from using their platforms to exploit a loophole in Irish law to get around a ban on foreign donations to political campaigns by pouring money into unregulated digital advertising instead.

Speaking to the Guardian, a technology spokesperson for Ireland’s opposition party Fianna Fáil, described Google’s decision to ban the adverts as “too late in the day”.

“Fake news has already had a corrosive impact on the referendum debate on social media,” James Lawless TD told it, adding that the referendum campaign had made it clear Ireland needs legislation to restrict the activities of Internet companies’ ad products “in the same way that steps were taken in the past to regulate political advertising on traditional forms of print and broadcast media”.

We’ve asked Google why it’s only taken the decision to suspend referendum ad buys now, and why it did not act months earlier — given the Irish government announced its intention to hold a 2018 referendum on repealing the Eighth Amendment in mid 2017 — and will update this post with any response.

In a public policy blog post earlier this month, the company’s policy SVP Kent Walker talked up the steps the company is taking to (as he put it) “support… election integrity through greater advertising transparency”, saying it’s rolling out new policies for U.S. election ads across its platforms, including requiring additional verification for election ad buyers, such as confirmation that an advertiser is a U.S. citizen or lawful permanent resident.

However this U.S.-first focus leaves other regions vulnerable to election fiddlers — hence Google deciding to suspend ad buys around the Irish vote, albeit tardily.

The company has also previously said it will implement a system of disclosures for ad buyers to make it clear to users who paid for the ad, and that it will be publishing a Transparency Report this summer breaking out election ad purchases. It also says it’s building a searchable library for election ads.

Although it’s not clear when any of these features will be rolled out across all regions where Google ads are served.

Facebook has also announced a raft of similar transparency steps related to political ads in recent years — responding to political pressure and scrutiny following revelations about the extent of Kremlin-backed online disinformation campaigns that had targeted the 2016 US presidential election.",Social Media,TechCrunch,https://techcrunch.com/2018/05/09/google-is-banning-irish-abortion-referendum-ads-ahead-of-vote/,Google and Facebook have been criticized for not acting earlier to prevent foreign interests from exploiting their platforms to influence the upcoming Irish referendum on abortion. Their late decision to suspend ad buys related to the referendum has raised questions about the effectiveness of their election integrity efforts.,Equality & Justice
189,"Facebook takes down 16,000 groups trading fake reviews after another poke by UK’s CMA – TechCrunch","Facebook has removed 16,000 groups that were trading fake reviews on its platform after another intervention by the UK’s Competition and Markets Authority (CMA), the regulator said today.

The CMA has been leaning on tech giants to prevent their platforms being used as thriving marketplaces for selling fake reviews since it began investigating the issue in 2018 — pressuring both eBay and Facebook to act against fake review sellers back in 2019.

The two companies pledged to do more to tackle the insidious trade last year, after coming under further pressure from the regulator — which found that Facebook-owned Instagram was also a thriving hub of fake review trades.

The latest intervention by the CMA looks considerably more substantial than last year’s action — when Facebook removed a mere 188 groups and disabled 24 user accounts. Although it’s not clear how many accounts the tech giant has banned and/or suspended this time it has removed orders of magnitude more groups. (We’ve asked.)

Update: We understand that the regulator has focused on the removal of groups trading misleading/fake reviews, rather than individual accounts — as banned or suspended users are able to create new profiles, whereas removing the group in which fake reviews are being traded is seen as a more effective way to impact and deter the activity.

Facebook was also contacted with questions but it did not answer what we asked directly, sending us this statement instead:

“We have engaged extensively with the CMA to address this issue. Fraudulent and deceptive activity is not allowed on our platforms, including offering or trading fake reviews. Our safety and security teams are continually working to help prevent these practices.”

Since the CMA has been raising the issue of fake review trading, Facebook has been repeatedly criticised for not doing enough to clean up its platforms, plural.

Today the regulator said the social media giant has made further changes to the systems it uses for “identifying, removing and preventing the trading of fake and/or misleading reviews on its platforms to ensure it is fulfilling its previous commitments”.

It’s not clear why it’s taken Facebook well over a year — and a number of high profile interventions — to dial up action against the trade in fake reviews. But the company suggested that the resources it has available to tackle the problem had been strained as a result of the COVID-19 pandemic and associated impacts, such as home working. (Facebook’s full year revenue increased in 2020 but so too did its expenses.)

According to the CMA changes Facebook has made to its system for combating traders of fake reviews include:

suspending or banning users who are repeatedly creating Facebook groups and Instagram profiles that promote, encourage or facilitate fake and misleading reviews

introducing new automated processes that will improve the detection and removal of this content

making it harder for people to use Facebook’s search tools to find fake and misleading review groups and profiles on Facebook and Instagram

putting in place dedicated processes to make sure that these changes continue to work effectively and stop the problems from reappearing

Again it’s not clear why Facebook would not have already been suspending or banning repeat offenders — at least, not if it was actually taking good faith action to genuinely quash the problem, rather than seeing if it could get away with doing the bare minimum.

Commenting in a statement, Andrea Coscelli, chief executive of the CMA, essentially makes that point, saying: “Facebook has a duty to do all it can to stop the trading of such content on its platforms. After we intervened again, the company made significant changes — but it is disappointing it has taken them over a year to fix these issues.”

“We will continue to keep a close eye on Facebook, including its Instagram business. Should we find it is failing to honour its commitments, we will not hesitate to take further action,” Coscelli added.

A quick search on Facebook’s platform for UK groups trading in fake reviews appears to return fewer obviously dubious results than when we’ve checked in on this problem in 2019 and 2020. Although the results that were returned included a number of private groups so it was not immediately possible to verify what content is being solicited from members.

We did also find a number of Facebook groups offering Amazon reviews intended for other European markets, such as France and Spain (and in one public group aimed at Amazon Spain we found someone offering a “fee” via PayPal for a review; see below screengrab) — suggesting Facebook isn’t applying the same level of attention to tackling fake reviews that are being traded by users in markets where it’s faced fewer regulatory pokes than it has in the UK.",Social Media,TechCrunch,https://techcrunch.com/2021/04/09/facebook-takes-down-16000-groups-trading-fake-reviews-after-another-poke-by-uks-cma/,"Facebook has been criticized for not doing enough to stop the trade of fake reviews on its platforms, leading to a strong intervention from the Competition and Markets Authority (CMA). The CMA has now prompted Facebook to remove 16,000 groups, suspend or ban users, and introduce automated processes to better detect and remove content related to fake reviews.","Information, Discourse & Governance"
190,Block Zuck? Not So Fast ... (Updated),"One of the most fun ways to lash out at Mark Zuckerberg with zero chance of it having any impact whatsoever seems to have been thwarted. Yes, the Empire has struck back, making it impossible -- impossible! -- to block the Facebook CEO on his own social network.

Blocking someone on Facebook is a sort of last-straw maneuver when you don't even want to ignore someone you had friended anymore, or want to prevent another member from contacting you in (nearly) every way possible. It's a great way to be rid of that creepy guy who asks you ""S'up?!"" 2.5 seconds after you log on. To permanently excuse yourself from the ""friend"" who is actually a spam machine. To stop hearing from the high school friend you didn't even like in high school.

Blocking Zuckerberg isn't about barring the door against a nuisance, but rather a harmless way to register a protest against any number of grievances about Facebook and its CEO -- a preemptive strike against some who almost certainly would have had absolutely nothing to do with you anyway.

BlockZuck.com made something of a campaign out of this by publishing the (deadly simple) instructions for blocking Zuckerberg, the same instructions for blocking anyone.

As of today, however, those halcyon days are over -- a victim, perhaps of BlockZuck.com's success.

Now when you try to block Zuck, you get a message which says: ""General Block failed error: Block failed."" Not for everyone, mind you. I checked. (Sorry, Adam Rowe - I've unblocked you again). Blocking only fails for Zuck.

Facebook, in an e-mail to Wired.com, called this ""an error"" which is ""generated when a person has been blocked a certain large number of times.""

""In very rare instances, a viral campaign will develop instructing lots of people to all wrongly block the same person,"" the spokesperson said. ""The purpose of this system is to protect the experience for people targeted by these campaigns. We're constantly working to improve our systems and are taking a closer look at this one.""

What now, for the die-hard activist, after this petty reaction to pettiness? Since you can't block Zuck anymore, maybe the new best civil disobedience is to friend him. And then -- just as only a friend can tell you that you have bad breath -- let him know what's on your mind about privacy, Facebook Places, tagging, censored e-mail -- you know, from one buddy to another.

Until Zuck blocks you, that is.

Follow us for disruptive tech news: John C. Abell and Epicenter on Twitter.

See Also:",Social Media,WIRED,https://www.wired.com/2010/08/block-zuck-not-so-fast-there/,"The blocking of Mark Zuckerberg on Facebook, previously a harmless way to protest grievances about the social network and its CEO, has been thwarted, leaving those who wish to express their displeasure with no recourse other than to attempt to friend Zuckerberg and voice their opinions to him directly.",Security & Privacy
191,UK eyeing ‘extremism’ tax on social media giants – TechCrunch,"The UK government has kicked off the new year with another warning shot across the bows of social media giants.

In an interview with the Sunday Times newspaper, security minister Ben Wallace hit out at tech platforms like Facebook and Google, dubbing such companies “ruthless profiteers” and saying they are doing too little to help the government combat online extremism and terrorism despite hateful messages spreading via their platforms.

“We should stop pretending that because they sit on beanbags in T-shirts they are not ruthless profiteers. They will ruthlessly sell our details to loans and soft-porn companies but not give it to our democratically elected government,” he said.

Wallace suggested the government is considering a tax on tech firms to cover the rising costs of policing related to online radicalization.

“If they continue to be less than co-operative, we should look at things like tax as a way of incentivizing them or compen­sating for their inaction,” he told the newspaper.

Although the minister did not name any specific firms, a reference to encryption suggests Facebook-owned WhatsApp is one of the platforms being called out (the UK’s Home Secretary has also previously directly attacked WhatsApp’s use of end-to-end encryption as an aid to criminals, as well as repeatedly attacking e2e encryption itself).

“Because of encryption and because of radicalization, the cost… is heaped on law enforcement agencies,” Wallace said. “I have to have more human surveil­lance. It’s costing hundreds of millions of pounds. If they continue to be less than co-operative, we should look at things like tax as a way of incentiviz­ing them or compen­sating for their inaction.

“Because content is not taken down as quickly as they could do, we’re having to de-radicalize people who have been radicalized. That’s costing millions. They can’t get away with that and we should look at all options, including tax,” he added.

Last year in Europe the German government agreed a new law targeting social media firms over hate speech takedowns. The so-called NetzDG law came into effect in October — with a three-month transition period for compliance (which ended yesterday). It introduces a regime of fines of up to €50M for social media platforms that fail to remove illegal hate speech after a complaint (within 24 hours in straightforward cases; or within seven days where evaluation of content is more difficult).

UK parliamentarians investigating extremism and hate speech on social platforms via a committee enquiry also urged the government to impose fines for takedown failures last May, accusing tech giants of taking a laissez-faire approach to moderating hate speech.

Tackling online extremism has also been a major policy theme for UK prime minister Theresa May’s government, and one which has attracted wider backing from G7 nations — converging around a push to get social media firms to remove content much faster.

Responding now to Wallace’s comments in the Sunday Times, Facebook sent us the following statement, attributed to its EMEA public policy director, Simon Milner:

Mr Wallace is wrong to say that we put profit before safety, especially in the fight against terrorism. We’ve invested millions of pounds in people and technology to identify and remove terrorist content. The Home Secretary and her counterparts across Europe have welcomed our coordinated efforts which are having a significant impact. But this is an ongoing battle and we must continue to fight it together, indeed our CEO recently told our investors that in 2018 we will continue to put the safety of our community before profits.

In the face of rising political pressure to do more to combat online extremism, tech firms including Facebook, Google and Twitter set up a partnership last summer focused on reducing the accessibility of Internet services to terrorists.

This followed an announcement, in December 2016, of a shared industry hash database for collectively identifying terror accounts — with the newer Global Internet Forum to Counter Terrorism intended to create a more formal bureaucracy for improving the database.

But despite some public steps to co-ordinate counter-terrorism action, the UK’s Home Affairs committee expressed continued exasperation with Facebook, Google and Twitter for failing to effectively enforce their own hate speech rules in a more recent evidence session last month.

Though, in the course of the session, Facebook’s Milner, claimed it’s made progress on combating terrorist content, and said it will be doubling the number of people working on “safety and security” by the end of 2018 — to circa 20,000.

In response to a request for comment on Wallace’s remarks, a YouTube spokesperson emailed us the following statement:

Violent extremism is a complex problem and addressing it is a critical challenge for us all. We are committed to being part of the solution and we are doing more every day to tackle these issues. Over the course of 2017 we have made significant progress through investing in machine learning technology, recruiting more reviewers, building partnerships with experts and collaboration with other companies through the Global Internet Forum.

In a major shift last November YouTube broadened its policy for taking down extremist content — to remove not just videos that directly preach hate or seek to incite violence but also take down other videos of named terrorists (with exceptions for journalistic or educational content).

The move followed an advertiser backlash after marketing messages were shown being displayed on YouTube alongside extremist and offensive content.

Answering UK parliamentarians’ questions about how YouTube’s recommendation algorithms are actively pushing users to consume increasingly extreme content — in a sort of algorithmic radicalization — Nicklas Berild Lundblad, EMEA VP for public policy, admitted there can be a problem but said the platform is working on applying machine learning technology to automatically limit certain videos so they would not be algorithmically surfaceable (and thus limit their ability to spread).

Twitter also moved to broaden its hate speech policies last year — responding to user criticism over the continued presence of hate speech purveyors on its platform despite having community guidelines that apparently forbid such conduct.

A Twitter spokesman declined to comment on Wallace’s remarks.

Speaking to the UK’s Home Affairs committee last month, the company’s EMEA VP for public policy and communications, Sinead McSweeney, conceded that it has not been “good enough” at enforcing its own rules around hate speech, adding: “We are now taking actions against 10 times more accounts than we did in the past.”

But regarding terrorist content specifically, Twitter reported a big decline in the proportion of pro-terrorism accounts being reported on its platform as of September, along with apparent improvements in its anti-terrorism tools — claiming 95 per cent of terrorist account suspensions had been picked up by its systems (vs manual user reports).

It also said 75 per cent of these accounts were suspended before they’d sent their first tweet.",Social Media,TechCrunch,https://techcrunch.com/2018/01/02/uk-eyeing-extremism-tax-on-social-media-giants/,"The UK government has warned social media giants that they must do more to help combat online extremism and terrorism, or else face the possibility of being taxed for the rising costs of policing these issues. The government has also criticised the platforms for failing to take down illegal hate speech quickly enough, with parliamentarians accusing them of taking a laissez-",Security & Privacy
192,Former head of Facebook app Fidji Simo defends company following whistleblower testimony – TechCrunch,"The former head of the Facebook app, who reported directly to CEO Mark Zuckerberg, Fidji Simo, defended the social network at the start of an interview at the WSJ Tech Live event this afternoon. The exec was there to discuss her new role as Instacart CEO and her vision for the future of food delivery, but was asked to comment on the recent Facebook whistleblower’s testimony and the attention it has since raised.

Simo said she understood the scrutiny given Facebook’s impact on people’s lives. But she’s also worried that Facebook will never be able to do enough to appease its critics at this point, despite the complexity of the issues Facebook is grappling with as one of the world’s largest social networks.

“They are spending billions of dollars in keeping people safe. They are doing the most in-depth research of any company I know to understand their impact,” she argued, still very much on Facebook’s side, despite her recent departure. “And I think my worry is that people want ‘yes’ or ‘no’ answers to this question, but really these questions require a lot of nuance,” she added.

While the whistleblower, Frances Haugen, suggested that Facebook’s decision to prioritize user engagement through its algorithms was ultimately putting profits over people, Simo cautioned the choices weren’t quite as binary as have been described to date. She explained that making changes based on the research Facebook had invested in wasn’t just a matter of turning a dial and “all of a sudden, magically problems disappear — because Facebook is fundamentally a reflection of humanity,” she said.

Instead, Simo said that the real issues at Facebook were around how every change Facebook makes can have significant societal applications at this point. It has to work to determine how it can improve upon the potentially problematic areas of its business without incidentally affecting other things along the way.

“When we discuss trade-offs, it’s usually trade-offs between two types of societal impacts,” she noted.

As an example, Simo used what would seem like a fairly straightforward adjustment to make: determine which posts make Facebook users angry then show people less of those.

As Haugen had testified, Facebook’s algorithms have been designed to reward engagement. That means posts with “likes” and other interactions spread more widely and are distributed higher up in people’s News Feeds. But she also said engagement doesn’t just come from likes and positive reactions. Engagement-based algorithms will ultimately prioritize clickbait and posts that make people angry. This, in turn, can help to boost the spread of posts eliciting stronger reactions, like misinformation or even toxic and violent content.

Simo, however, said it’s not as simple as it sounds to just dial down the anger across Facebook, as doing so would lead to another type of societal impact.

“You start digging in and you realize that the biggest societal movements were created out of anger,” she said. That led the company to question how it could make a change that could impact people’s activism.

(This isn’t quite how that situation unfolded, according to a report by the WSJ. Instead, when the algorithm was tweaked to prioritize personal posts over professionally produced content, publishers and political parties adjusted their posts toward outrage and sensationalism. And Zuckerberg resisted some of the proposed fixes to this problem, the report said.)

“That’s just a random example,” Simo said of the “anger” problem. “But literally, on every issue, there is always a trade-off that is another type of societal impact. And I can tell you for having been in these rooms for many, many years, it’s really never about like, ‘oh, are we doing the right thing for society, versus the right thing for Facebook and for profits’…the debate was really between some kinds of societal impact and another kind — which is a very hard debate to have as a private company.”

This, she added, was why Facebook wanted regulations.

“It’s not surprising that Facebook has been calling for regulation in this space for a very long time because they never want to be in a position of being the ones deciding which implications, which ramifications, which trade-offs they need to make between one type of societal impact and another type of societal impact. The governments are better positioned to do that,” she said.

Given the increasing amount of evidence coming out that Facebook itself understood, through its own internal research, that there were areas of its business that negatively impact society, Simo didn’t chalk up her departure from the social network to anything that was going on with Facebook itself.

Instead, she said she just wasn’t learning as much after 10 years with the company, and Instacart presented her with a great opportunity where she could learn “a different set of things,” she said.",Social Media,TechCrunch,https://techcrunch.com/2021/10/19/former-head-of-facebook-app-fidji-simo-defends-company-following-whistleblower-testimony/,"The main consequence of Social Media discussed here is that it can lead to the spread of misinformation and toxic or violent content, since algorithms prioritize posts that elicit more engagement, regardless of the implications. This has led Facebook to call for regulation, as the company does not want to be held responsible for making such difficult choices.","Information, Discourse & Governance."
193,"At social media hearing, lawmakers circle algorithm-focused Section 230 reform – TechCrunch","Rather than a CEO-slamming sound bite free-for-all, Tuesday’s big tech hearing on algorithms aimed for more of a listening session vibe — and in that sense it mostly succeeded.

The hearing centered on testimony from the policy leads at Facebook, YouTube and Twitter rather than the chief executives of those companies for a change. The resulting few hours didn’t offer any massive revelations but was still probably more productive than squeezing some of the world’s most powerful men for their commitments to “get back to you on that.”

In the hearing, lawmakers bemoaned social media echo chambers and the ways that the algorithms pumping content through platforms are capable of completely reshaping human behavior.

“… This advanced technology is harnessed into algorithms designed to attract our time and attention on social media, and the results can be harmful to our kids’ attention spans, to the quality of our public discourse, to our public health, and even to our democracy itself,” said Sen. Chris Coons (D-DE), chair of the Senate Judiciary’s subcommittee on privacy and tech, which held the hearing.

Coons struck a cooperative note, observing that algorithms drive innovation but that their dark side comes with considerable costs

None of this is new, of course. But Congress is crawling closer to solutions, one repetitive tech hearing at a time. The Tuesday hearing highlighted some zones of bipartisan agreement that could determine the chances of a tech reform bill passing the Senate, which is narrowly controlled by Democrats. Coons expressed optimism that a “broadly bipartisan solution” could be reached.

What would that look like? Probably changes to Section 230 of the Communications Decency Act, which we’ve written about extensively over the years. That law protects social media companies from liability for user-created content and it’s been a major nexus of tech regulation talk, both in the newly Democratic Senate under Biden and the previous Republican-led Senate that took its cues from Trump.

A broken business model

In the hearing, lawmakers pointed to flaws inherent to how major social media companies make money as the heart of the problem. Rather than criticizing companies for specific failings, they mostly focused on the core business model from which social media’s many ills spring forth.

“I think it’s very important for us to push back on the idea that really complicated, qualitative problems have easy quantitative solutions,” Sen. Ben Sasse (R-NE) said. He argued that because social media companies make money by keeping users hooked to their products, any real solution would have to upend that business model altogether.

“The business model of these companies is addiction,” Sen. Josh Hawley (R-MO) echoed, calling social media an “attention treadmill” by design.

Ex-Googler and frequent tech critic Tristan Harris didn’t mince words about how tech companies talk around that central design tenet in his own testimony. “It’s almost like listening to a hostage in a hostage video,” Harris said, likening the engagement-seeking business model to a gun just offstage.

Spotlight on Section 230

One big way lawmakers propose to disrupt those deeply entrenched incentives? Adding algorithm-focused exceptions to the Section 230 protections that social media companies enjoy. A few bills floating around take that approach.

One bill introduced in 2020 from Sen. John Kennedy (R-LA) and Reps. Paul Gosar (R-AZ) and Tulsi Gabbard (D-HI) would require platforms with 10 million or more users to obtain consent before serving users content based on their behavior or demographic data if they want to keep Section 230 protections. The idea is to revoke 230 immunity from platforms that boost engagement by “funneling information to users that polarizes their views” unless a user specifically opts in.

In another bill, the Protecting Americans from Dangerous Algorithms Act, Reps. Anna Eshoo (D-CA) and Tom Malinowski (D-NJ) propose suspending Section 230 protections and making companies liable “if their algorithms amplify misinformation that leads to offline violence.” That bill would amend Section 230 to reference existing civil rights laws.

Section 230’s defenders argue that any insufficiently targeted changes to the law could disrupt the modern internet as we know it, resulting in cascading negative impacts well beyond the intended scope of reform efforts. An outright repeal of the law is almost certainly off the table, but even small tweaks could completely realign internet businesses, for better or worse.

During the hearing, Hawley made a broader suggestion for companies that use algorithms to chase profits. “Why shouldn’t we just remove Section 230 protection from any platform that engages in behavioral advertising or algorithmic amplification?” he asked, adding that he wasn’t opposed to an outright repeal of the law.

Sen. Amy Klobuchar (D-MN), who leads the Senate’s antitrust subcommittee, connected the algorithmic concerns to anti-competitive behavior in the tech industry. “If you have a company that buys out everyone from under them … we’re never going to know if they could have developed the bells and whistles to help us with misinformation because there is no competition,” Klobuchar said.

Subcommittee members Klobuchar and Sen. Mazie Hirono (D-HI) have their own major Section 230 reform bill, the Safe Tech Act, but that legislation is less concerned with algorithms than ads and paid content.

At least one more major bill looking at Section 230 through the lens of algorithms is still on the way. Prominent Big Tech critic House Rep. David Cicilline (D-RI) is due to propose a Section 230 bill soon that could suspend liability protections for companies that rely on algorithms to boost engagement and line their pockets.

“That’s a very complicated algorithm that is designed to maximize engagement to drive up advertising prices to produce greater profits for the company,” Cicilline told Axios last month. ” … That’s a set of business decisions for which, it might be quite easy to argue, that a company should be liable for.”",Social Media,TechCrunch,https://techcrunch.com/2021/04/27/section-230-bills-algorithms-congress-hearing/,"The hearing focused on how the algorithms driving Social Media can be harmful - to public discourse, kids' attention spans, public health, and even democracy - and proposed solutions to limit companies' liability protections under Section 230 to reduce incentives for user engagement and maximize profits.","Information, Discourse & Governance"
194,Facebook's Quest to Quash Boredom by Moving Beyond Friends,"It's a question that the news industry seems to be coming to terms with---or at least with which it seems willing to experiment. Last week Facebook announced that dozens of additional media companies, from People to the NBA to Business Insider, will join Instant Articles---and The Washington Post is going all in.

Unlike the engineering culture that drives much of Facebook’s work, this part of Facebook depends on something much more traditional: good, old-fashioned relationships.

“We’re sending 100 percent of our articles,” says Cory Haik, Post executive director of emerging news products. “The idea is that this is the best way to figure out if this works. Get some scale to it.”

A crucial aspect of Instant Articles, Haik says, is that publishers will be able to the get analytic data for articles hosted there, which is important for publishers to understand where their audience is (and for selling ads). With help from the partnership team, Facebook has tried to incorporate features seen as essential, like sharing traffic data, into Instant Articles to meet publishers' needs.

""We have a good relationship with those guys, we can talk to them,"" Haik says of Facebook's news team when I ask about the feelings of uncertainty in the industry. ""We thought it would be a risk not to try it, but we'll be paying attention.""

Distributed, Codependent, Competitive

A few days after chatting with Facebook’s partnership team in Silicon Valley, I call Sibyl Goldman, head of the company’s entertainment partnerships, in Los Angeles. Her team works with celebrities as well as networks and labels to launch trailers, release videos, promote causes, or celebrate events on Facebook.

Goldman's been in the entertainment business for 21 years. She eagerly talks about how Facebook works with AwesomenessTV, Maker, and Fullscreen---the digital-first agencies known for their stables of YouTube stars---as well as big name studios too.

The difference is that for major companies like, say, HBO or Disney, Facebook is an additional way to reach an audience---not the main way. Star Wars, for example, doesn’t need Facebook as much as Facebook needs Star Wars. Facebook, like traditional media, is one part of Hollywood’s enormous PR machine.

On the other hand, Facebook surpassed Google this year as the number-one driver of traffic to major news publishers, according to analytics service Parse.ly. The *Times *gets 16 percent of its traffic from Facebook referrals. BuzzFeed sees 75 percent of its traffic come from social platforms; 27 percent of it from Facebook native video alone.

Facebook and news publishers want to see the relationship as symbiotic---that's where those relationships come in. But the tech giant's power is ultimately disproportionate to any one media brand. Yes, it needs publishers. It needs entertainment companies. It needs sports. But it doesn't need all of them.

“This wonderful universe that we imagined 10 or 15 years ago of a great decentralized web where people can compete on an equal playing field is quickly giving way to an environment controlled by these enormous tech companies that have their own agendas,” says Dan Kennedy, Northeastern University journalism professor.

“It's a difficult space for small independent news organizations to play in. The rise of these partnerships with Facebook and Apple is only going to encourage a news diet in which people are looking at large national platforms from The New York Times to BuzzFeed and not paying an awful lot of attention to small or even decent-sized local newspapers.”

'It's a difficult space for small independent organizations to play in.' Dan Kennedy, Northeastern University journalism professor

Facebook, for its part, seems intent on partnering with a mix of publishers for its Instant Articles rollout, including Gannett and Billy Penn, both of which publish local news. And for some older media companies, it's still early in an experiment that started a decade or more ago. Kinsey Wilson, executive vice president of product and technology at the Times, says Instant Articles isn't terribly different than the days when the Times shared stories with AOL.

And Facebook also isn't the only place to play. Apple recently launched Apple News, its native news app on iOS. Snapchat offers news and entertainment in the Discover portion of its app. YouTube, Google, Twitter, Yahoo, and other platforms will continue to play a role in the distributed web, especially as Google develops its own version of Instant Articles, its open source AMP project, to help mobile web pages load faster. As other tech giants join the game, Facebook won’t have the sole power in determining who you see.

The problem, however, remains that Facebook won't say that it will pick winners or losers. It says it wants to create a more open and connected world. But to do that it's building a closed ecosystem that it controls. If Facebook's algorithm favors Instant Articles or native video---even if only because people click on those stories more---that could still hurt smaller publishers or those who aren't given the chance to post natively on its platform. It could adversely impact the news you see today and the videos---or even virtual reality---you're served up in the future. It’s a complicated situation. And it’s always in flux. But one thing’s for sure: It’s not boring.",Social Media,WIRED,https://www.wired.com/2015/10/facebooks-quest-to-quash-boredom-by-moving-beyond-friends/,"The rise of partnerships with Facebook and Apple could lead to a news diet that favors large national platforms and disadvantages small or local newspapers, creating an environment controlled by tech giants with their own agendas.","Information, Discourse & Governance"
195,Ireland’s draft GDPR decision against Facebook branded a joke – TechCrunch,"Facebook’s lead data protection regulator in the European Union is inching toward making its first decision on a complaint against Facebook itself. And it looks like it’s a doozy.

Privacy campaign not-for-profit noyb today published a draft decision by the Irish Data Protection Commission (DPC) on a complaint made under the EU’s General Data Protection Regulation (GDPR).

The DPC’s draft decision proposes to fine Facebook $36 million — a financial penalty that would take the adtech giant just over two and a half hours to earn in revenue, based on its second quarter earnings (of $29 billion).

Yeah, we lol’d too.

But even more worrying for privacy advocates is the apparent willingness of the DPC to allow Facebook to simply bypass the regulation by claiming users are giving it their data because they’re in a contract with it to get, er, targeted ads.

In a summary of its findings, the DPC writes: “There is no obligation on Facebook to seek to rely solely on consent for the purposes of legitimising personal data processing where it is offering a contract to a user which some users might assess as one that primarily concerns the processing of personal data. Nor has Facebook purported to rely on consent under the GDPR.”

“I find the Complainant’s case is not made out that the GDPR does not permit the reliance by Facebook on 6(1)(b) GDPR in the context of its offering of Terms of Service,” the DPC also writes, suggesting it’s totally bona fide for Facebook to claim a legal right to process people’s information for ad targeting because it’s now suggesting users actually signed up for a contract with it to deliver them ads.

Yet — simultaneously — the DPC’s draft decision does find that Facebook infringed GDPR transparency requirements — specifically: Articles 5(1)(a), 12(1) and 13(1)(c) — meaning that users were unlikely to have understood they were signing up for a Facebook ad contract when they clicked “I agree” on Facebook’s T&Cs.

So the tl;dr here is that Facebook’s public-facing marketing — which claims its service “helps you connect and share with the people in your life” — appears to be missing a few critical details about the advertising contract it’s actually asking you to enter into, or something.

Insert your own facepalm emoji right here.

Mind the enforcement gap

The GDPR came into application across the EU back in May 2018 — ostensibly to cement and strengthen long-standing privacy rules in the region which had historically suffered from a lack of enforcement, by adding new provisions such as supersized fines (of up to 4% of global turnover).

However EU privacy rules have also suffered from a lack of universally vigorous enforcement since the GDPR update. And those penalties that have been issued — including a handful against Big Tech — have been far lower than that theoretical maximum. Nor has enforcement led to an obvious retooling of privacy hostile business models — yet.

So the reboot hasn’t exactly gone as privacy advocates hoped.

Adtech giants especially have managed to avoid a serious reckoning in Europe over their surveillance-based business models despite the existence of the GDPR — through the use of forum shopping and cynical delay tactics.

So while there is no shortage of GDPR complaints being filed against adtech, complaints over the lack of regulatory enforcement in this area are equally stacking up.

And complainants are now also resorting to legal action.

The issue is, under GDPR’s one-stop-shop mechanism, cross-border complaints and investigations, such as those targeted at major tech platforms, are led by a single agency — typically where the company in question has its legal base in the EU.

And in Facebook’s case (and many other tech giants’) that’s Ireland.

The Irish authority has long been accused of being a bottleneck to effective enforcement of the GDPR, with critics pointing to a glacial pace of enforcement, scores of complaints simply dropped without any discernible activity and — in instances where the complaints aren’t totally ignored — underwhelming decisions eventually popping out the other end.

One such series of adtech-related GDPR complaints were filed by noyb immediately the regulation came into application three years ago — targeting a number of adtech giants (including Facebook) over what noyb called “forced consent.” And these complaints of course ended up on the DPC’s desk.

noyb’s complaint against Facebook argues that the tech giant does not collect consent legally because it does not offer users a free choice to consent to their data being processed for advertising.

This is because under EU law consent must be freely given, specific (i.e., not bundled) and informed in order to be valid. So the substance of the complaint is not exactly as complicated as rocket science.

Yet a decision on noyb’s complaint has taken years to emerge from the DPC’s desk — and even now, in dilute draft form, it looks entirely underwhelming.

Per noyb, the Irish DPC has decided to accept what the campaign group dubs Facebook’s “trick” to bypass the GDPR — in which the company claims it switched away from relying on consent from users as a legal basis for processing people’s data for ad targeting to claiming users are actually in a contract with it to get ads injected into their eyeballs the very moment the GDPR came into force.

“It is painfully obvious that Facebook simply tries to bypass the clear rules of the GDPR by relabeling the agreement on data use as a ‘contract,'” said noyb founder and chair, Max Schrems, in a statement which goes on to warn that were such a basic wheeze allowed to stand it would undermine the whole regulation. Talk about a cunning plan!

“If this would be accepted, any company could just write the processing of data into a contract and thereby legitimize any use of customer data without consent. This is absolutely against the intentions of the GDPR, that explicitly prohibits to hide consent agreements in terms and conditions.”

“It is neither innovative nor smart to claim that an agreement is something that it is not to bypass the law,” he adds. “Since Roman times, the Courts have not accepted such ‘relabeling’ of agreements. You can’t bypass drug laws by simply writing ‘white powder’ on a bill, when you clearly sell cocaine. Only the Irish DPC seems to fall for this trick.”

Ireland has only issued two GDPR decisions in complaints against Big Tech thus far: Last year in a case against a Twitter security breach ($550,000 fine); and earlier this year in an investigation into the transparency of (Facebook-owned) WhatsApp T&Cs ($267 million fine).

Under the GDPR, a decision on these type of cross-border GDPR complaints must go through a collective review process — where other DPAs get a chance to object. It’s a check and balance on one agency getting too cosy with business and failing to enforce the law.

And in both the aforementioned cases objections were raised on the DPC drafts that ended up increasing the penalties.

So it is highly likely that Ireland’s Facebook decision will face plenty of objections that end in a tougher penalty for Facebook.

noyb also points to guidelines put out by the European Data Protection Board (EDPB) — which it says make it clear that bypassing the GDPR isn’t legal and must be treated as consent. But it quotes the Irish DPC saying it is “simply not persuaded” by the view of its European Colleagues and suggests the EDPB will therefore have to step in yet again.

“Our hope lies with the other European authorities. If they do not take action, companies can simply move consent into terms and thereby bypass the GDPR for good,” says Schrems.

noyb has plenty more barbs for the DPC — accusing the Irish authority of holding “secret meetings” with Facebook on its “consent bypass” (not for the first time); and of withholding documents it requested — going on to denounce the regulator as acting like a “‘Big Tech’ adviser” (not, y’know, a law enforcer).

“We have cases before many authorities, but the DPC is not even remotely running a fair procedure,” adds Schrems. “Documents are withheld, hearings are denied and submitted arguments and facts are simply not reflected in the decision. The [Facebook] decision itself is lengthy, but most sections just end with a ‘view’ of the DPC, not an objective assessment of the law.”

We reached out to the DPC for comment on noyb’s assertions — but a spokesperson declined, citing an “ongoing process.”

One thing is beyond doubt at this point, over three years into Europe’s flagship data protection reboot: There will be even more delay in any GDPR enforcement against Facebook.

The GDPR’s one-stop-shop mechanism — of review plus the chance for other DPAs to file objections — already added multiple months to the two earlier DPC Big Tech decisions. So the DPC issuing another weak draft decision on a late-running investigation looks like it’s becoming a standard procedural lever to decelerate the pace of GDPR enforcement across the EU.

This will only increase pressure for EU lawmakers to agree alternative enforcement structures for the bloc’s growing suite of digital regulations.

In the meanwhile, as DPAs fight it out to try to hit Facebook with a penalty Mark Zuckerberg can’t just laugh off, Facebook gets to continue its lucrative data-mining business as usual — while EU citizens are left asking where are my rights?",Social Media,TechCrunch,https://techcrunch.com/2021/10/13/irelands-draft-gdpr-decision-against-facebook-branded-a-joke/,"In spite of the GDPR coming into application three years ago, Social Media giants have been able to avoid a serious reckoning in Europe over their surveillance-based business models by using forum shopping and cynical delay tactics. This has led to a lack of vigorous enforcement and penalties that are far lower than the theoretical maximum, leaving EU citizens asking where are",Security & Privacy
196,Why Are There So Many Porn Ads on Britney Spears' Facebook Page?,"Apologies for the link-bait, but it's true: Britney Spears' Facebook page is overrun with erotic pictures, many of them linking to pornographic websites. Either Spears' ""social media expert"" is asleep at the switch or this is part of some sort of misguided marketing campaign to sex up the pop star's apparently-still-too-staid image.

Regardless, when her fans click the Photos link on her Facebook page, they're confronted with the images like the ones above, many leading directly to hard-core ads for remarkably forward young women advertising their services for free.

We're hardly ones to proselytize, but this does seem to be a bit much for the singing star, who this week is Tweeting up a storm about her appearance on the mostly squeaky clean Fox show, Glee.

The occasional photo of an actual Britney Spears fan or Spears herself does appear in her list of over 10,000 ""Photos from Others,"" but the majority or at least the most recent are advertisements for people like ""Hilary Portman,"" whose message reveals that she is ""seeking an above-average guy who is willing to keep up with a 21 years old, fit, drug, disease and drama free chick. It's gonna be a whole night of 'pleasing' and discovering all our erogenous zones."" Her link, like the others, leads to an adult personals site where payment for sex in the style of Craigslist is implied.

It might seem like a lot to ask for Spears' people to remove the offending ""fan photos from Britney Spears"" from her Facebook page. (We've asked her camp for a response and have yet to hear back). On the other hand, large media presences like her regularly hire social media experts and interns to handle monotonous tasks like accepting friend requests, and it's easy enough for the regular user to un-tag themselves from unwanted photos.

Why can't one of Spears' people get on this? By our rough estimation, based on the fact that the 20th-most-recent photo in the section was added yesterday, it would only take about five minutes per day to keep Britney Spears' page free of ads for escort services.

We don't want to waste too much time on this, but it seems worth mentioning that the regularly-updated, Britney Spears-controlled official Facebook page, which presumably attracts lots of her young fan base, is only a couple of clicks away from hardcore advertisements for erotic services (NSFW).

Follow us for disruptive tech news: Eliot Van Buskirk and Epicenter on Twitter.

See Also:",Social Media,WIRED,https://www.wired.com/2010/08/why-are-there-so-many-porn-ads-on-britney-spears-facebook-page/,"Britney Spears' Facebook page is overrun with erotic pictures, many of them linking to pornographic websites, which could potentially expose her young fan base to ads for escort services.",User Experience & Entertainment
197,"When it comes to social media moderation, reach matters – TechCrunch","Social media in its current form is broken.

In 20 years, we’ll look back at the social media of 2020 like we look at smoking on airplanes or road trips with the kids rolling around in the back of a station wagon without seatbelts. Social media platforms have grown into borderless, society-scale misinformation machines. Any claim that they do not have editorial influence on the flow of information is nonsense. Just as a news editor selects headlines of the day, social media platforms channel content with engagement-maximizing algorithms and viral dynamics. They are by no means passive observers of our political discourse.

At the same time, I sympathize with the position that these companies are in — caught between the interests of shareholders and the public. I’ve started technology companies and helped build large-scale internet platforms. So I understand that social media CEOs have a duty to maximize the value of the business for their shareholders. I also know that social media companies can do better. They are not helpless to improve themselves. Contrary to Mark Zuckerberg’s recent heel dragging in dealing with President Trump’s reckless posts, the executives and boards at these companies have full dominion over their products and policies, and they have an obligation to their shareholders and society to make material changes to them.

The way to fix social media starts with realizing it is two different things: personal media and mass media.

Personal media is most of social media. Selfies from a hike or a shot of that Oreo sundae, stuff you share with friends and family. Mass media is content that reaches large audiences — such as a tweet that reaches a Super Bowl-sized audience in real-time. To be clear, it’s not just about focusing on people with a lot of followers. High-reach content can also be posts that go viral and get viewed by a large audience.

Twitter’s decision to annotate a couple of Trump’s tweets is a baby step in this direction. By applying greater scrutiny to a mega-visibility user, the company is treating those posts differently than low-reach tweets. But this extra attention should not be tied to any particular individual, but rather applied to all tweets that reach a large audience.

Reach is an objective measure of the impact of a social media post. It makes sense. Tweets that go to more people carry more weight and therefore should be the focus of any effort at cleaning up disinformation. The audience size of a message is as important, if not more, than its content. So, reach is a useful first-cut filter removed from the hornet’s nest of interpreting the underlying content or beliefs of the sender.

From a technology perspective, it is very doable. When a social media post exceeds a reach threshold, the platform should automatically subject the content to additional processes to reduce disinformation and promote community standards. One idea, an extension of what Twitter recently did, would be to prominently connect a set of links to relevant articles from a pool of trusted sources — to add context, not censor. The pool of trusted content would need to be vetted and diverse in its point of view, but that’s possible, and users could even be involved in crowd-sourcing those decisions. For the highest-reach content, there could be additional human curation and even journalistic-style fact checking. If these platforms can serve relevant ads in milliseconds, they can serve relevant content from trusted sources.

From a regulatory perspective, reach is also the right framework for reforming Section 230 of the Communications Decency Act. That’s the pre-social media law that gives internet platforms a broad immunity from liability for the content they traffic. Conceptually, Section 230 continues to make sense for low-reach content. Facebook should not be held liable for every comment your uncle Bob makes. It’s when posts reach a vast number of people that Twitter and Facebook start to look more like The Wall Street Journal or The New York Times than an internet service provider. In these cases, it’s reasonable that they should be subject to similar legal liability as mass media outlets for broadly distributing damaging falsehoods.

Improving social media intelligently starts with breaking the problem down based on the reach of the content. Social media is two very different things thrown together in an internet blender: personal media and mass media. Let’s start treating it that way.",Social Media,TechCrunch,https://techcrunch.com/2020/06/11/when-it-comes-to-social-media-moderation-reach-matters/,"Social media platforms have become powerful tools for spreading misinformation, and their current form is broken. The reach of posts should be taken into account to reduce disinformation and reform Section 230 of the Communications Decency Act to hold platforms liable for the content they traffic.","Information, Discourse & Governance"
198,"Facebook should cancel Instagram Kids, not put it on ‘pause’ – TechCrunch","In a blog post announcing plans to temporarily stop developing a new app targeted at children, Instagram head Adam Mosseri wrote, “the reality is that kids are already online, and we believe that developing age-appropriate experiences designed specifically for them is far better for parents than where we are today.”

It’s rare that I find common ground with the head of Instagram, but he is right about this point. Kids and tweens need access to technology that meets their unique needs.

He’s just dead wrong about the solution.

Instagram Kids is a terrible idea, and, thankfully, plans for the pint-size version of the app have been put on hold. This development comes on the heels of yet another controversy — this one about the effects Instagram has on young people’s mental health. Instagram’s internal research suggests that the platform can be a toxic place for teens.

It’s possible for companies to build platforms that better support parents, kids and tweens — but it can’t be done by moving fast and breaking things.

As a father and the founder of a tech company for kids, I always felt that platforms built on social validation, comparison and FOMO weren’t appropriate for younger users. And even though Facebook claimed in public that their research suggested a net-positive effect on mental health, behind closed doors, they had evidence that might not be the case.

Despite all this, I know that healthy technology for kids does exist. It’s possible for companies to build platforms that better support parents, kids and tweens — but it can’t be done by moving fast and breaking things.

Kids are growing up connected

Technology is an ever-present part of our lives and the lives of our children. According to Common Sense Media’s 2020 census on media use, kids from birth to age 8 have almost two and a half hours of screen time daily.

And that was before COVID-19 shuttered schools and made playdates impossible. Kids have turned to screens for remote learning, entertainment, and socializing with friends and family, and many parents will tell you that this increased screen time is a serious source of anxiety.

I believe that lots of parents inherently see the value in technology, especially when it’s been a lifeline keeping us connected during a pandemic. But there’s ambivalence baked in here since there’s a serious dearth of high-quality, safe environments for kids online. As a result, families turn to platforms that were never designed to meet the needs of children, often at the cost of their peace of mind.

Retrofitting adult platforms isn’t the answer

When you think of popular kids’ apps, what comes to mind? Facebook Messenger Kids? YouTube Kids? These platforms all have something in common: They’re repackaged versions of adult apps, and they’ve had safety- and privacy-related scandals. That’s because adult apps simply don’t retrofit well for children. There are a few different reasons for this.

First, a lot of adult platforms are designed to be sticky. This is reflected in features like endless feeds, auto-playing content and arbitrary “streaks.”

These apps also have a way of using our own psychology against us to keep us scrolling. They exploit our need to belong by quantifying social validation. Because of follower counts, like buttons, comments and shares, we can see exactly how popular we are — and we can compare our metrics to others. Many adults can find these features anxiety inducing, and I don’t believe they belong in platforms designed for young users with brains that are still developing. They show up again and again in retrofitted apps for children because they’re at the core of these products.

Second, many tech platforms are wide-open networks. This is not the one-way media that we grew up with. Lots of social and gaming platforms encourage users to amass friends and followers and exchange messages and comments, which can pose serious safety risks for young, inexperienced internet users.

A recent report from Thorn found that scores of children are using adult platforms before they turn 13, and a startling majority of them encounter “abuse, harassment or sexual solicitation from adults.”

When companies try to retrofit platforms for kids that eliminate the stranger danger, they’ve had varied success. Facebook Messenger Kids infamously included a design flaw that allowed children to connect and chat with strangers. That’s because it’s really difficult to take an open network and work backward to lock it up. When it comes to safety, you really need to start from the ground up.

Finally, retrofitted platforms rarely appeal to kids the same way that their adult counterparts do. Ask any parent: Kids love YouTube. Not so much with YouTube Kids. Children are always in a hurry to grow up. They want to feel empowered, and mini versions of adult apps do the opposite. It’s the technical version of the kids’ table, so getting buy-in from young users can be a struggle.

Parental controls can only take you so far

With all the dangers that exist out there for children online, you might think that parental controls are the obvious answer. That’s clearly what Facebook is thinking with their plans for Instagram Kids. But if you ask me, no parental control in the world will stop kids from comparing themselves to others. Again, it’s inherent to the platform.

Parental controls also won’t stop kids from finding creative workarounds. Children are resourceful when they want to circumvent screen time limits. They’re often just tiny, motivated hackers. With all that in mind, the best thing parents can do is get involved — like, really involved — with their kids’ digital lives.

Build a foundation with parental involvement and co-play

If you’ve ever googled “screen time recommendations,” you already know that experts in digital parenting rarely agree on anything. But one thing we hear consistently is that parents need to be involved in their kids’ digital lives.

We need to explore and play together. This gives us the chance to model appropriate behaviors for children and educate them about the big, bad online world. It’s also a lovely way to spend quality time with your kids. Ask them about the games they like. Get them to teach you how they work. Dedicate one night a week to enjoy screen time together.

When tech companies sell parental controls as the ultimate answer for kids’ apps, they’re doing us a disservice. Rules and restrictions are obviously important, but what we truly need are opportunities to use technology together with our kids. We need apps that the whole family can enjoy together — not just where parents can toggle a switch, set a limit and leave.

We need to think beyond parental controls if we want to help children develop healthy relationships with technology. We need to talk to them about social validation. We need to make sure they understand their digital footprints. And we need to help them understand what motivates Big Tech companies.

A junior version of Instagram won’t make the internet a better place for kids or parents. It’ll just hook ’em young — which is something Facebook is clearly motivated to do.

Thankfully, plans for the new platform have been paused, but I sincerely hope they abandon it altogether. Facebook has a questionable track record, and it’s probably not the right company to develop products for children. And Instagram is definitely the wrong template.",Social Media,TechCrunch,https://techcrunch.com/2021/10/06/facebook-should-cancel-instagram-kids-not-put-it-on-pause/,"Social Media can have a detrimental effect on children's mental health, leading to anxiety and a need to compare themselves to others. It is also a source of danger, as young users may be more vulnerable to abuse, harassment, and sexual solicitation from adults.",Social Norms & Relationships
199,Facebook Groups Are Destroying America,"The Covid-19 “infodemic” has laid bare how vulnerable the United States is to disinformation. The country is less than five months away from the 2020 presidential election, and Americans by the thousands are buying into conspiracy theories about vaccines containing microchips and wondering about the healing powers of hair dryers. Where does all this come from? Let’s not be too distracted by a fear of rumormonger bots on the rampage or divisive ads purchased with Russian rubles. As two of the leading researchers in this field, we’re much more worried about Facebook groups pumping out vast amounts of false information to like-minded members.

For the past several years, Facebook users have been seeing more content from “friends and family” and less from brands and media outlets. As part of the platform’s “pivot to privacy” after the 2016 election, groups have been promoted as trusted spaces that create communities around shared interests. “Many people prefer the intimacy of communicating one-on-one or with just a few friends,” explained Mark Zuckerberg in a 2019 blog post. “People are more cautious of having a permanent record of what they've shared.”

But as our research shows, those same features—privacy and community—are often exploited by bad actors, foreign and domestic, to spread false information and conspiracies. Dynamics in groups often mirror those of peer-to-peer messaging apps: People share, spread, and receive information directly to and from their closest contacts, whom they typically see as reliable sources. To make things easier for those looking to stoke political division, groups provide a menu of potential targets organized by issue and even location; bad actors can create fake profiles or personas tailored to the interests of the audiences they intend to infiltrate. This allows them to seed their own content in a group and also to repurpose its content for use on other platforms.

This was already evident in 2018, when associates of Shiva Ayyadurai, an independent candidate for US Senate, used groups as part of their astroturfing campaign to boost his online support. Today, Ayyadurai is one of the most dangerous vectors of health disinformation, racking up millions of engagements on posts that rail against vaccinations, claim Anthony Fauci is a member of the “deep state,” and instruct followers to point blow dryers down their throats to kill the coronavirus.

Groups continue to be used for political disinformation. The “Obamagate” conspiracy theory has yet to be defined in clear terms, even by its own adherents, and yet our analysis of Facebook groups shows that the false narrative that the Obama administration illegally spied against people associated with the Trump campaign is being fueled and nurtured there. Related memes and links to fringe right-wing websites have been shared millions of times on Facebook in the past few months. Users coordinating their activities across networks of groups and pages managed by a small handful of people boost these narratives. At least nine coordinated pages and two groups—with more than 3 million likes and 71,000 members, respectively—are set up to drive traffic to five “news” websites that promote right-wing clickbait and conspiracy theories. In May, those five websites published more than 50 posts promoting Obamagate, which were then shared in the linked pro-Trump groups and pages. The revolving door of disinformation continues to spin.

A recent Wall Street Journal investigation revealed that Facebook was aware of groups’ polarizing tendencies from 2016. And despite the company’s recent efforts to crack down on misinformation related to Covid-19, the Groups feature continues to serve as a vector for lies. As we wrote this story, if you were to join the Alternative Health Science News group, for example, Facebook would then recommend, based on your interests, that you join a group called Sheep No More, which uses Pepe the Frog, a white supremacist symbol, in its header, as well as Q-Anon Patriots, a forum for believers in the crackpot QAnon conspiracy theory. As protests in response to the death of George Floyd spread across the country, members of these groups claimed that Floyd and the police involved were “crisis actors” following a script. In recent days, Facebook stopped providing suggestions on the landing pages of certain groups, but they still populate the Discover tab, where Facebook recommends content to users based on their recent engagement and activity.",Social Media,WIRED,https://www.wired.com/story/facebook-groups-are-destroying-america/,"Due to Social Media's ""pivot to privacy"" and emphasis on content from ""friends and family"", platforms like Facebook have become vulnerable to the proliferation of false information and conspiracy theories, which have been used to spread disinformation by bad actors, foreign and domestic.","Information, Discourse & Governance"
200,Facebook finds and kills another 512 Kremlin-linked fake accounts – TechCrunch,"Two years on from the U.S. presidential election, Facebook continues to have a major problem with Russian disinformation being megaphoned via its social tools.

In a blog post today the company reveals another tranche of Kremlin-linked fake activity — saying it’s removed a total of 471 Facebook pages and accounts, as well as 41 Instagram accounts, which were being used to spread propaganda in regions where Putin’s regime has sharp geopolitical interests.

In its latest reveal of “coordinated inauthentic behavior” — aka the euphemism Facebook uses for disinformation campaigns that rely on its tools to generate a veneer of authenticity and plausibility in order to pump out masses of sharable political propaganda — the company says it identified two operations, both originating in Russia, and both using similar tactics without any apparent direct links between the two networks.

One operation was targeting Ukraine specifically, while the other was active in a number of countries in the Baltics, Central Asia, the Caucasus, and Central and Eastern Europe.

“We’re taking down these Pages and accounts based on their behavior, not the content they post,” writes Facebook’s Nathaniel Gleicher, head of cybersecurity policy. “In these cases, the people behind this activity coordinated with one another and used fake accounts to misrepresent themselves, and that was the basis for our action.”

Sputnik link

Discussing the Russian disinformation op targeting multiple countries, Gleicher says Facebook found what looked like innocuous or general interest pages to be linked to employees of Kremlin propaganda outlet Sputnik, with some of the pages encouraging protest movements and pushing other Putin lines.

“The Page administrators and account owners primarily represented themselves as independent news Pages or general interest Pages on topics like weather, travel, sports, economics, or politicians in Romania, Latvia, Estonia, Lithuania, Armenia, Azerbaijan, Georgia, Tajikistan, Uzbekistan, Kazakhstan, Moldova, Russia, and Kyrgyzstan,” he writes. “Despite their misrepresentations of their identities, we found that these Pages and accounts were linked to employees of Sputnik, a news agency based in Moscow, and that some of the Pages frequently posted about topics like anti-NATO sentiment, protest movements, and anti-corruption.”

Facebook has included some sample posts from the removed accounts in the blog which show a mixture of imagery being deployed — from a photo of a rock concert, to shots of historic buildings and a snowy scene, to obviously militaristic and political protest imagery.

In all Facebook says it removed 289 Pages and 75 Facebook accounts associated with this Russian disop; adding that around 790,000 accounts followed one or more of the removed Pages.

It also reveals that it received around $135,000 for ads run by the Russian operators (specifying this was paid for in euros, rubles, and U.S. dollars).

“The first ad ran in October 2013, and the most recent ad ran in January 2019,” it notes, adding: “We have not completed a review of the organic content coming from these accounts.”

These Kremlin-linked Pages also hosted around 190 events — with the first scheduled for August 2015, according to Facebook, and the most recent scheduled for January 2019. “Up to 1,200 people expressed interest in at least one of these events. We cannot confirm whether any of these events actually occurred,” it further notes.

Facebook adds that open source reporting and work by partners which investigate disinformation helped identify the network. (For more on the open source investigation check out this blog post from DFRLab.)

It also says it has shared information about the investigation with U.S. law enforcement, the U.S. Congress, other technology companies, and policymakers in impacted countries.

Ukraine tip-off

In the case of the Ukraine-targeted Russian disop, Facebook says it removed a total of 107 Facebook Pages, Groups, and accounts, and 41 Instagram accounts, specifying that it was acting on an initial tip off from U.S. law enforcement.

In all it says around 180,000 Facebook accounts were following one or more of the removed pages. While the fake Instagram accounts were being followed by more than 55,000 accounts.

Again Facebook received money from the disinformation purveyors, saying it took in around $25,000 in ad spending on Facebook and Instagram in this case — all paid for in rubles this time — with the first ad running in January 2018, and the most recent in December 2018. (Again it says it has not completed a review of content the accounts were generating.)

“The individuals behind these accounts primarily represented themselves as Ukrainian, and they operated a variety of fake accounts while sharing local Ukrainian news stories on a variety of topics, such as weather, protests, NATO, and health conditions at schools,” writes Gleicher. “We identified some technical overlap with Russia-based activity we saw prior to the US midterm elections, including behavior that shared characteristics with previous Internet Research Agency (IRA) activity.”

In the Ukraine case it says it found no Events being hosted by the pages.

“Our security efforts are ongoing to help us stay a step ahead and uncover this kind of abuse, particularly in light of important political moments and elections in Europe this year,” adds Gleicher. “We are committed to making improvements and building stronger partnerships around the world to more effectively detect and stop this activity.”

A month ago Facebook also revealed it had removed another batch of politically motivated fake accounts. In that case the network behind the pages had been working to spread misinformation in Bangladesh 10 days before the country’s general elections.

This week it also emerged the company is extending some of its nascent election security measures by bringing in requirements for political advertisers to more international markets ahead of major elections in the coming months, such as checks that a political advertiser is located in the country.

However in other countries which also have big votes looming this year Facebook has yet to announced any measures to combat politically charged fakes.",Social Media,TechCrunch,https://techcrunch.com/2019/01/17/facebook-finds-and-kills-another-512-kremlin-linked-fake-accounts/,"Facebook continues to have a major problem with Russian disinformation being spread through its social tools, with the company recently having removed 471 Facebook pages, 41 Instagram accounts and received $135,000 in ads from Russian operators. This has caused a rise in political propaganda and misinformation, with Facebook having implemented measures to combat it in certain countries.",Politics
201,Reminder: Other people’s lives are not fodder for your feeds – TechCrunch,"Reminder: Other people’s lives are not fodder for your feeds

#PlaneBae

You should cringe when you read that hashtag. Because it’s a reminder that people are being socially engineered by technology platforms to objectify and spy on each other for voyeuristic pleasure and profit.

The short version of the story attached to the cringeworthy hashtag is this: Earlier this month an individual, called Rosey Blair, spent all the hours of a plane flight using her smartphone and social media feeds to invade the privacy of her seat neighbors — publicly gossiping about the lives of two strangers.

Her speculation was set against a backdrop of rearview creepshots, with a few barely there scribbles added to blot out actual facial features. Even as an entire privacy invading narrative was being spun unknowingly around them.

#PlanePrivacyInvasion would be a more fitting hashtag. Or #MoralVacuumAt35000ft

And yet our youthful surveillance society started with a far loftier idea associated with it: Citizen journalism.

Once we’re all armed with powerful smartphones and ubiquitously fast Internet there will be no limits to the genuinely important reportage that will flow, we were told.

There will be no way for the powerful to withhold the truth from the people.

At least that was the nirvana we were sold.

What did we get? Something that looks much closer to mass manipulation. A tsunami of ad stalking, intentionally fake news and social media-enabled demagogues expertly appropriating these very same tools by gamifying mind-less, ethically nil algorithms.

Meanwhile, masses of ordinary people + ubiquitous smartphones + omnipresent social media feeds seems, for the most part, to be resulting in a kind of mainstream attention deficit disorder.

Yes, there is citizen journalism — such as people recording and broadcasting everyday experiences of aggression, racism and sexism, for example. Experiences that might otherwise go unreported, and which are definitely underreported.

That is certainly important.

But there are also these telling moments of #hashtaggable ethical blackout. As a result of what? Let’s call it the lure of ‘citizen clickbait’ — as people use their devices and feeds to mimic the worst kind of tabloid celebrity gossip ‘journalism’ by turning their attention and high tech tools on strangers, with (apparently) no major motivation beyond the simple fact that they can. Because technology is enabling them.

Social norms and common courtesy should kick in and prevent this. But social media is pushing in an unequal and opposite direction, encouraging users to turn anything — even strangers’ lives — into raw material to be repackaged as ‘content’ and flung out for voyeuristic entertainment.

It’s life reflecting commerce. But a particularly insidious form of commerce that does not accept editorial let alone ethical responsibility, has few (if any) moral standards, and relies, for continued function, upon stripping away society’s collective sense of privacy in order that these self-styled ‘sharing’ (‘taking’ is closer to the mark) platforms can swell in size and profit.

But it’s even worse than that. Social media as a data-mining, ad-targeting enterprise relies upon eroding our belief in privacy. So these platforms worry away at that by trying to disrupt our understanding of what privacy means. Because if you were to consider what another person thinks or feels — even for a millisecond — you might not post whatever piece of ‘content’ you had in mind.

For the platforms it’s far better if you just forget to think.

Facebook’s business is all about applying engineering ingenuity to eradicate the thoughtful friction of personal and societal conscience.

That’s why, for instance, it uses facial recognition technology to automate content identification — meaning there’s almost no opportunity for individual conscience to kick in and pipe up to quietly suggest that publicly tagging others in a piece of content isn’t actually the right thing to do.

Because it’s polite to ask permission first.

But Facebook’s antisocial automation pushes people away from thinking to ask for permission. There’s no button provided for that. The platform encourages us to forget all about the existence of common courtesies.

So we should not be at all surprised that such fundamental abuses of corporate power are themselves trickling down to infect the people who use and are exposed to these platforms’ skewed norms.

Viral episodes like #PlaneBae demonstrate that the same sense of entitlement to private information is being actively passed onto the users these platforms prey on and feed off — and is then getting beamed out, like radiation, to harm the people around them.

The damage is collective when societal norms are undermined.

#PlaneBae

Social media’s ubiquity means almost everyone works in marketing these days. Most people are marketing their own lives — posting photos of their pets, their kids, the latte they had this morning, the hipster gym where they work out — having been nudged to perform this unpaid labor by the platforms that profit from it.

The irony is that most of this work is being done for free. Only the platforms are being paid. Though there are some people making a very modern living; the new breed of ‘life sharers’ who willingly polish, package and post their professional existence as a brand of aspiration lifestyle marketing.

Social media’s gift to the world is that anyone can be a self-styled model now, and every passing moment a fashion shoot for hire — thanks to the largess of highly accessible social media platforms providing almost anyone who wants it with their own self-promoting shopwindow in the world. Plus all the promotional tools they could ever need.

Just step up to the glass and shoot.

And then your vacation beauty spot becomes just another backdrop for the next aspirational selfie. Although those aquamarine waters can’t be allowed to dampen or disrupt photo-coifed tresses, nor sand get in the camera kit. In any case, the makeup took hours to apply and there’s the next selfie to take…

What does the unchronicled life of these professional platform performers look like? A mess of preparation for projecting perfection, presumably, with life’s quotidian business stuffed higgledy piggledy into the margins — where they actually sweat and work to deliver the lie of a lifestyle dream.

Because these are also fakes — beautiful fakes, but fakes nonetheless.

We live in an age of entitled pretence. And while it may be totally fine for an individual to construct a fictional narrative that dresses up the substance of their existence, it’s certainly not okay to pull anyone else into your pantomime. Not without asking permission first.

But the problem is that social media is now so powerfully omnipresent its center of gravity is actively trying to pull everyone in — and its antisocial impacts frequently spill out and over the rest of us. And they rarely if ever ask for consent.

What about the people who don’t want their lives to be appropriated as digital windowdressing? Who weren’t asking for their identity to be held up for public consumption? Who don’t want to participate in this game at all — neither to personally profit from it, nor to have their privacy trampled by it?

The problem is the push and pull of platforms against privacy has become so aggressive, so virulent, that societal norms that protect and benefit us all — like empathy, like respect — are getting squeezed and sucked in.

The ugliness is especially visible in these ‘viral’ moments when other people’s lives are snatched and consumed voraciously on the hoof — as yet more content for rapacious feeds.

#PlaneBae

Think too of the fitness celebrity who posted a creepshot + commentary about a less slim person working out at their gym.

Or the YouTuber parents who monetize videos of their kids’ distress.

Or the men who post creepshots of women eating in public — and try to claim it’s an online art project rather than what it actually is: A privacy violation and misogynistic attack.

Or, on a public street in London one day, I saw a couple of giggling teenage girls watching a man at a bus stop who was clearly mentally unwell. Pulling out a smartphone, one girl hissed to the other: “We’ve got to put this on YouTube.”

For platforms built by technologists without thought for anything other than growth, everything is a potential spectacle. Everything is a potential post.

So they press on their users to think less. And they profit at society’s expense.

It’s only now, after social media has embedded itself everywhere, that platforms are being called out for their moral vacuum; for building systems that encourage abject mindlessness in users — and serve up content so bleak it represents a form of visual cancer.

#PlaneBae

Human have always told stories. Weaving our own narratives is both how we communicate and how we make sense of personal experience — creating order out of events that are often disorderly, random, even chaotic.

The human condition demands a degree of pattern-spotting for survival’s sake; so we can pick our individual path out of the gloom.

But platforms are exploiting that innate aspect of our character. And we, as individuals, need to get much, much better at spotting what they’re doing to us.

We need to recognize how they are manipulating us; what they are encouraging us to do — with each new feature nudge and dark pattern design choice.

We need to understand their underlying pull. The fact they profit by setting us as spies against each other. We need to wake up, personally and collectively, to social media’s antisocial impacts.

Perspective should not have to come at the expense of other people getting hurt.

Additionally, I’ve not earned anything off of this. And do not wish to. The greatest gift I’ve been given – when I shouldn’t have received anything to begin with – is perspective. — Rosey Blair (@roseybeeme) July 10, 2018

This week the women whose privacy was thoughtlessly repackaged as public entertainment when she was branded and broadcast as #PlaneBae — and who has suffered harassment and yet more unwelcome attention as a direct result — gave a statement to Business Insider.

“#PlaneBae is not a romance — it is a digital-age cautionary tale about privacy, identity, ethics and consent,” she writes. “Please continue to respect my privacy, and my desire to remain anonymous.”

And as a strategy to push against the antisocial incursions of social media, remembering to respect people’s privacy is a great place to start.",Social Media,TechCrunch,https://techcrunch.com/2018/07/14/1672711-planefeo/,"Social Media platforms have created a culture of entitlement to privacy invasion and voyeurism, and this is resulting in people turning the lens on strangers and even the vulnerable for their own entertainment, with no regard for the consequences.",Social Norms & Relationships
202,UK watchdog wants disclosure rules for political ads on social media – TechCrunch,"The UK’s data protection agency will push for increased transparency into how personal data flows between digital platforms to ensure people being targeted for political advertising are able to understand why and how it is happening.

Information commissioner Elizabeth Deham said visibility into ad targeting systems is needed so that people can exercise their rights — such as withdrawing consent to their personal data being processed should they wish.

“Data protection is not a back-room, back-office issue anymore,” she said yesterday. “It is right at the centre of these debates about our democracy, the impact of social media on our lives and the need for these companies to step up and take their responsibilities seriously.”

“What I am going to suggest is that there needs to be transparency for the people who are receiving that message, so they can understand how their data was matched up and used to be the audience for the receipt of that message. That is where people are asking for more transparency,” she added.

The commissioner was giving her thoughts on how social media platforms should be regulated in an age of dis(and mis)information during an evidence session in front of a UK parliamentary committee that’s investigating fake news and the changing role of digital advertising.

Her office (the ICO) is preparing its own report this spring — which she said is likely to be published in May — which will lay out its recommendations for government.

“We want more people to participate in our democratic life and democratic institutions, and social media is an important part of that, but we also do not want social media to be a chill in what needs to be the commons, what needs to be available for public debate,” she said.

“We need information that is transparent, otherwise we will push people into little filter bubbles, where they have no idea about what other people are saying and what the other side of the campaign is saying. We want to make sure that social media is used well.

“It has changed dramatically since 2008. The Obama campaign was the first time that there was a lot of use of data analytics and social media in campaigning. It is a good thing, but it needs to be made more transparent, and we need to control and regulate how political campaigning is happening on social media, and the platforms need to do more.”

Last fall UK prime minister Theresa May publicly accused Russia of weaponizing online information in an attempt to skew democratic processes in the West.

And in January the government announced it would set up a dedicated national security unit to combat state-led disinformation campaigns.

Last month May also ordered a review of the law around social media platforms, as well as announcing a code of conduct aimed at cracking down on extremist and abusive content — another Internet policy she’s prioritized.

So regulating online content has already been accelerated to the top of government in the UK — as it is increasingly on the agenda in Europe.

Although it’s not yet clear how the UK government will seek to regulate social media platforms to control political advertising.

Denham’s suggestion to the committee was for a code of conduct.

“I think the use of social media in political campaigns, referendums, elections and so on may have got ahead of where the law is,” she argued. “I think it might be time for a code of conduct so that everybody is on a level playing field and knows what the rules are.

“I think there are some politicians, some MPs, who are concerned about the use of these new tools, particularly when there are analytics and algorithms that are determining how to micro-target someone, when they might not have transparency and the law behind them.”

She added that the ICO’s incoming policy report will conclude that “transparency is important”.

“People do not understand the chain of companies involved. If they are using an app that is running off the Facebook site and there are other third parties involved, they do not know how to control their data,” she argued.

“Right now, I think we all agree that it is much too difficult and much too opaque. That is what we need to tackle. This Committee needs to tackle it, we need to tackle it at the ICO, and the companies have to get behind us, or they are going to lose the trust of users and the digital economy.”

She also spoke up generally for more education on how digital systems work — so that users of services can “take up their rights”.

“They have to take up their rights. They have to push companies. Regulators have to be on their game. I think politicians have to support new changes to the law if that is what we need,” she added.

And she described the incoming General Data Protection Regulation (GDPR) as a “game-changer” — arguing it could underpin a push for increased transparency around the data flows that are feeding and shaping public opinions. Although she conceded that regulating such data flows to achieve the sought for accountability will require a fully joined up effort.

“I would like to be an optimist. The point behind the General Data Protection Regulation as a step-up in the law is to try to give back control to individuals so that they have a say in how their data are processed, so that they do not just throw up their hands or put it on the ‘too difficult’ pile. I think that is really important. There is a whole suite of things and a whole village that has to work together to be able to make that happen.”

The committee recently took evidence from Cambridge Analytica — the UK based company credited with helping Donald Trump win the US presidency by creating psychological profiles of US voters for ad targeting purposes.

Denham was asked for her response to seeing CEO Alexander Nix’s evidence. But said she could not comment to avoid prejudicing the ICO’s own ongoing investigation into data analytics for political purposes.

She did confirm that a data request by US voter and professor David Carroll, who has been trying to use UK data protection law to access the data held on him for political ad targeting purposes by Cambridge Analytica, is forming one of the areas of the ICO enquiry — saying it’s looking at “how an individual becomes the recipient of a certain message” and “what information is used to categorise him or her, whether psychographic technologies are used, how the categories are fixed and what kind of data has fed into that decision”.

Although she also said the ICO’s enquiry into political data analytics is ranging more widely.

“People need to know the provenance and the source of the data and information that is used to make decisions about the receipt of messages. We are really looking at — it is a data audit. That is really what we are carrying out,” she added.",Social Media,TechCrunch,https://techcrunch.com/2018/03/07/uk-watchdog-wants-disclosure-rules-for-political-ads-on-social-media/,"The UK's data protection agency has warned of the dangers of social media platforms' lack of transparency in political advertising, which can lead to people being targeted without their knowledge and pushed into ""filter bubbles"" of misinformation.",Politics
203,It’s not just Logan Paul and YouTube — the moral compass of social media is broken,"We’re only a few days into the new year, but it didn’t take long for the latest viral embarrassment to hit YouTube, as yet another popular, telegenic young man posted something reckless and offensive on the video-sharing platform. This time around, YouTube star Logan Paul shared a video where he discovered and awkwardly laughed at the corpse of a suicide victim in Japan’s Aokigahara forest.

“Bro, did we just find a dead person in the suicide forest?” he says in the now-deleted video.

Although Paul subsequently issued apologies, the callous stunt was just the latest in a string of incidents where popular YouTubers have posted jaw-droppingly offensive, prejudiced, or unethical content that would never pass muster at a traditional outlet.

Their behavior is enabled by YouTube’s design as an effectively accountability-free platform, particularly for its most popular, envelope-pushing stars. There are rules and community guidelines about “disgusting” content and hate speech, of course, but they’re enforced haphazardly, often with little context or transparency, and can be easy to circumvent.

It’s a problem that extends beyond YouTube as a platform to streaming and social media at large, where large platforms tiptoe around the sensibilities of loud, angry users at the expense of anyone they can sacrifice on their pyre of rage. It creates a situation where women, people of color, queer, and disabled people all lack equal access to the service, laboring under the added burden of an angry mob scrutinizing their every move, even when they’re not “famous” by any metric.

The idealistic dream these services sell to users — that anyone can be famous with a mic, a keyboard, a webcam, and a bit of elbow grease — sounds like the culmination of early cyber utopianism. But in practice, it often means elevating people to fame when they are wildly unprepared for the ethical responsibilities or consequences of broadcasting their content to millions of fans (including children) around the world. As a principle, it means companies tie their own hands when dealing with edgelords who think Nazism is cool; there are only empty platitudes about free speech to be found in their wake.

The internet’s lawlessness came about as a feature, not a bug

The internet’s lawlessness came about as a feature, not a bug, premised on a libertarian ideal of self-direction unfettered by systems. Everything would be permitted, the thinking went, and cyber-society would simply balance itself out automatically without the need for oppressive governments or organizational rules. It hasn’t worked out that way, to put it lightly. The scope has also changed dramatically since the early days of the internet: the voices the internet amplifies are no longer niche or cordoned off from the “real” world. Social media celebrities can reach tens or hundreds of millions and sometimes have more impact on their viewers than television or film stars. They’re not just influential: for many people, particularly younger users, they are the media — and they can do pretty much whatever they want.

There are two sides to this, of course. One great boon of the internet, particularly for marginalized voices, is how it allows people to share content and ideas that might never make it past old-school gatekeepers and censors. At times, it can be refreshing and enlightening to see media and perspectives that don’t labor under stultifying FCC obscenity codes, to hear voices we might not have otherwise heard. But as we’ve seen over and over again for many years, this is an increasingly sharp double-edged sword.

The vile content that is amplified through digital megaphones is a reminder of why ethics and standards can be valuable, especially for the platforms that project the loudest voices in our culture. The solution, however, is not merely to clutch our pearls and demand that social media “think of the children,” but rather to implement clear, contextual codes of conduct with transparent enforcement that is tailored to the distinctions of every case, including human oversight at every stage. The latter is important. Without it, we run the risk of employing automated systems that reproduce biases at light speed.

For a choice example, we need only look at a November incident involving the professional Twitch streamer aptly called Trainwrecks, who is a member of the platform’s Partner program. In a fit of entitled pique, he decided to stream a misogynistic rant about so-called “boobie streamers,” or female content creators on the platform who wear revealing clothing or inject elements of sexiness into their game streams. It described, in profane detail, his rage at the growing presence of these women in what his perceived as “his” community, and what he believed they were taking away from him:

“This used to be a goddamn community of gamers, nerds, kids that got bullied, kids that got fucked with, kids that resorted to the gaming world because the real world was too fucking hard, too shitty, too lonely, too sad and depressing…[Twitch now belongs to] the same sluts that rejected us, the same sluts that chose the god damn cool kids over us. The same sluts that are coming into our community, taking the money, taking the subs, the same way they did back in the day.”

It’s no secret that male-dominated online communities are often benighted by a invidious and sexist mythology, one that says their specific corner of internet culture — and particularly nerd culture — is their exclusive domain, a refuge from the real world with no room for the evil girls who rejected and bullied them in school. The women who do make their way into these spaces are trespassers and thieves who are “taking” everything away again, using their sexy wiles to steal men’s rightly earned status and money (via ad revenue and subscriptions). This is the mentality that rules social media in the absence of meaningful enforcement, the entitled anarchy that rushes into the Wild Western void.

This is the mentality that rules social media in the absence of meaningful enforcement

And like any gaping black hole, it’s never satisfied. “No matter what I fucking wear, there’s always a comment. There’s always someone calling me a titty streamer, fake gamer or a whore etc,” said female Twitch streamer ZombiUnicorn on her Twitter. “If all the titty streamers were gone tomorrow, does anyone really think shitty people would stop degrading and insulting women?” tweeted streamer Renée Reynosa. “Truth is, they’d just find another hoop for us to jump through.” That’s the key here: women and other minorities face backlash no matter what they do, how they act, how they dress, or what they say. The solution, then, shouldn’t involve regulating the behavior of the people who suffer the most abuse online, or enabling the people who inflict it.

It’s a dilemma every social media platform confronts: cave to the angry fanbases of popular users who want unfettered license to do as they please? Or try more expensive, involving, context-based moderation techniques that uphold the principle that no one is too big to ban?

For the moment, Twitch’s approach to this problem has mostly been one of monastic silence, in its own way validating the entitled complaints of men like Trainwrecks — who was banned for just five days and remains an active streamer on the platform. YouTube also remains loathe to take serious action, even as LGBT streamers have grappled with a year where their videos were effectively hidden by the company in a misguided attempt to automate moderation.

The automoderation craze, once trumpeted as an elegant solution, is now part of the problem. These companies want the PR boost from appearing to “do something” while implementing faster, cheaper systems that can’t distinguish between a trans YouTuber talking about gender identity and a Nazi inciting violence. These systems are also notoriously exploited by corporations and harassers alike to get critical videos taken down or demonetized.

There is some promise in AI moderation — jerks on the internet aren’t the most original folks, after all; there are patterns — but it requires considerably more diverse human hands at the wheel. Twitch’s AutoMod system debuted with great fanfare, but has made few strides in cleaning up hate in live-chat. Without human insight, it cannot grapple with the ever-evolving nature of online hate that dwells in double meanings, memes, and in-jokes. Community moderation is not obsolete. It’s a human skill that is needed now more than ever.

It’s tempting to take the easy way out — technologically, financially, and morally. Automoderation is simple and cost-effective. Catering to “both sides” gives the appearance of fairness. This only compounds problems of access and platform equality, however, and caving into the moral panics of a few angry users only serves as a distraction from the larger problems facing social media and streaming sites. It won’t in any way put an end to the embarrassing PR debacles that have consumed Twitter (which is now often criticized as a Nazi-friendly site), or YouTube (which has been embarrassed by one failing after another, from PewDiePie to Paul to disturbing videos aimed at children).

All of these platforms have rules, moderators, enforcement teams, and even researchers dedicated to improving safety — I’ve met some of them — but it seems like every social media company, from Reddit to Twitch to Twitter, is still overwhelmed by the explosive scale achieved by their platforms and breathlessly trying to catch up by automating as much of the process as possible. That would be a mistake. Going back to basics and strengthening their core values around this issue with human assistance is the necessary first step.

It boils down to a basic question: what is the harm being perpetrated by a user’s actions?

In addition to clarifying their moderation policies, these platforms should also engage in a bit of moral education: make it clear, in fearless terms, why someone was suspended or banned, and what behaviors contributed to it. Just as critically, they need to recognize the importance of judging the impact of a streamer’s alleged misdeeds.

For platforms like YouTube and Twitch, and indeed social media in general, codes of conduct should boil down to a basic question: what is the harm being perpetrated by a user’s actions? For instance, while one group of streamers — the women of Twitch — stood accused of a fundamentally victimless crime, Trainwrecks and his misogynistic brethren espoused views and took actions with material repercussions for the women they targeted. The same goes for PewDiePie’s “joking” Nazism, including the “gag” where he paid Indian freelancers to hold up a sign reading “Death to All Jews.” (These men later said they didn’t understand what the sign meant, and lost their jobs over it.) The ethics around issues of online speech are contextual, and it’s time to act like it.

The playful universe of online streaming has much to recommend to it. Subjecting it to the same sort of strict broadcasting codes devised when the radio was the must-have gadget of the season seems unwise and counterproductive. But platform holders have to stop treating their users like someone else’s wayward children and enforce some standards — especially where their most popular streamers are concerned. If YouTube wants to be the next broadcast network, it’ll have to act like it. Meanwhile, it should also resist the temptation to stifle the creativity and diversity of others just because a few loud, hateful people have called for their sanction.

This would be a solution in search of a problem. Worse, it would hand a victory to the very people whose poisonous attitudes are the real threat to these platforms — assuming sites like Twitch, Twitter and YouTube want to be something more than toys for kids (large and small) who can’t be told “no.” If social media platforms want to make good on the promise of a digital democracy where traditional power structures don’t stifle us all, they will have to confront the ways in which their haphazard approach has built as many walls to speaking freely as it has taken down.",Social Media,Verge,https://www.theverge.com/2018/1/4/16850798/logan-paul-youtube-social-media-twitch-moderation,"Social media platforms' lack of meaningful enforcement has enabled a toxic environment where popular users are given free license to behave however they like, creating a double-edged sword where marginalized voices can be heard but also suffer under the backlash of entitled users who can do little more than complain.",Equality & Justice
204,Zuckerberg rejects facetime call for answers from five parliaments – TechCrunch,"Facebook has declined once again to send its CEO to the UK parliament — this time turning down an invitation to face questions from a grand committee comprised of representatives from five international parliaments.

MPs from Argentina, Australia, Canada, Ireland and the UK have joined forces to try to pile pressure on the company’s founder, Mark Zuckerberg, to answer questions related to his “platform’s malign use in world affairs and democratic process”.

The UK’s Digital, Culture, Media and Sport committee, which has been running an enquiry into online disinformation for the best part of this year, revealed the latest Facebook snub yesterday. It put out the grand committee call for facetime with Zuckerberg last week.

In the latest rejection letter to DCMS, Facebook writes: “Thank you for the invitation to appear before your Grand Committee. As we explained in our letter of November 2nd, Mr Zuckerberg is not able to be in London on November 27th for your hearing and sends his apologies.”

“We remain happy to cooperate with your inquiry as you look at issues related to false news and elections,” the company’s UK head of public policy, Rebecca Stimson, adds, before going on to summarize “some of the things we have been doing at Facebook over the last year”.

This boils down to a list of Facebook activities and related research that intersects with the topics of election interference, political ads, disinformation and security, but without offering any new information of substance or data points that could be used to measure and quantify the company’s actions.

The letter does not explain why Zuckerberg is unavailable to speak to the committee remotely, e.g. via video call.

Responding to the latest snub, DCMS chair Damian Collins expressed disappointment and vowed to keep up the pressure.

“Facebook’s letter is, once again, hugely disappointing,” he writes. “We believe Mark Zuckerberg has important questions to answer about what he knew about breaches of data protection law involving their customers’ personal data and why the company didn’t do more to identify and act against known sources of disinformation; and in particular those coming from agencies in Russia.

“The fact that he has continually declined to give evidence, not just to my committee, but now to an unprecedented international grand committee, makes him look like he’s got something to hide.”

“We will not let the matter rest there, and are not reassured in any way by the corporate puff piece that passes off as Facebook’s letter back to us,” Collins adds. “The fact that the University of Michigan believes that Facebook’s ‘Iffy Quotient’ scores have recently improved means nothing to the victims of Facebook data breaches.

“We will continue with our planning for the international grand committee on 27th November, and expect to announce shortly the names of additional representatives who will be joining us and our plans for the hearing.”",Social Media,TechCrunch,https://techcrunch.com/2018/11/14/zuckerberg-rejects-facetime-call-for-answers-from-five-parliaments/,"Social Media's undesirable consequences are becoming increasingly evident, with Facebook's CEO Mark Zuckerberg being asked to answer questions related to the platform's malign use in world affairs and democratic processes, yet continually declining to do so. This has led to disappointment from the international grand committee comprised of representatives from five countries and makes Zuckerberg look like he has something to","Information, Discourse & Governance"
205,Snap is under NDA with UK Home Office discussing how to centralize age checks online – TechCrunch,"Snap is under NDA with UK Home Office discussing how to centralize age checks online

Snap is under NDA with the UK’s Home Office as part of a working group tasked with coming up with more robust age verification technology that’s able to robustly identify children online.

The detail emerged during a parliamentary committee hearing as MPs in the Department for Digital, Culture, Media and Sport (DCMS) questioned Stephen Collins, Snap’s senior director for public policy international, and Will Scougal, director of creative strategy EMEA.

A spokesman in the Home Office press office hadn’t immediately heard of any discussions with the messaging company on the topic of age verification. But we’ll update this story with any additional context on the department’s plans if more info is forthcoming.

Under questioning by the committee Snap conceded its current age verification systems are not able to prevent under 13 year olds from signing up to use its messaging platform.

The DCMS committee’s interest here is it’s running an enquiry into immersive and addictive technologies.

Snap admitted that the most popular means of signing up to its app (i.e. on mobile) is where its age verification system is weakest, with Collins saying it had no ability to drop a cookie to keep track of mobile users to try to prevent repeat attempts to get around its age gate.

But he emphasized Snap does not want underage users on its platform.

“That brings us no advantage, that brings us no commercial benefit at all,” he said. “We want to make it an enjoyable place for everybody using the platform.”

He also said Snap analyzes patterns of user behavior to try to identify underage users — investigating accounts and banning those which are “clearly” determined not to be old enough to use the service.

But he conceded there’s currently “no foolproof way” to prevent under 13s from signing up.

Discussing alternative approaches to verifying kids’ age online the Snap policy staffer agreed parental consent approaches are trivially easy for children to circumvent — such as by setting up spoof email accounts or taking a photo of a parent’s passport or credit card to use for verification.

Social media company Facebook is one such company that relies a ‘parental consent’ system to ‘verify’ the age of teen users — though, as we’ve previously reported, it’s trivially easy for kids to workaround.

“I think the most sustainable solution will be some kind of central verification system,” Collins suggested, adding that such a system is “already being discussed” by government ministers.

“The home secretary has tasked the Home Office and related agencies to look into this — we’re part of that working group,” he continued.

“We actually met just yesterday. I can’t give you the details here because I’m under an NDA,” Collins added, suggesting Snap could send the committee details in writing.

“I think it’s a serious attempt to really come to a proper conclusion — a fitting conclusion to this kind of conundrum that’s been there, actually, for a long time.”

“There needs to be a robust age verification system that we can all get behind,” he added.

The UK government is expected to publish a White Paper setting out its policy ideas for regulating social media and safety before the end of the winter.

The detail of its policy plans remain under wraps so it’s unclear whether the Home Office intends to include setting up a centralized system of online age verification for robustly identifying kids on social media platforms as part of its safety-focused regulation. But much of the debate driving the planned legislation has fixed on content risks for kids online.

Such a step would also not be the first time UK ministers have pushed the envelop around online age verification.

A controversial system of age checks for viewing adult content is due to come into force shortly in the UK under the Digital Economy Act — albeit, after a lengthy delay. (And ignoring all the hand-wringing about privacy and security risks; not to mention the fact age checks will likely be trivially easy to dodge by those who know how to use a VPN etc, or via accessing adult content on social media.)

But a centralized database of children for age verification purposes — if that is indeed the lines along which the Home Office is thinking — sounds rather closer to Chinese government Internet controls.

Given that, in recent years, the Chinese state has been pushing games companies to age verify users to enforce limits on play time for kids (also apparently in response to health concerns around video gaming addiction).

The UK has also pushed to create centralized databases of web browsers’ activity for law enforcement purposes, under the 2016 Investigatory Powers Act. (Parts of which it’s had to rethink following legal challenges, with other legal challenges ongoing.)

In recent years it has also emerged that UK spy agencies maintain bulk databases of citizens — known as ‘bulk personal datasets‘ — regardless of whether a particular individual is suspected of a crime.

So building yet another database to contain children’s ages isn’t perhaps as off piste as you might imagine for the country.

Returning to the DCMS committee’s enquiry, other questions for Snap from MPs included several critical ones related to its ‘streaks’ feature — whereby users who have been messaging each other regularly are encouraged not to stop the back and forth.

The parliamentarians raised constituent and industry concerns about the risk of peer pressure being piled on kids to keep the virtual streaks going.

Snap’s reps told the committee the feature is intended to be a “celebration” of close friendship, rather than being intentionally designed to make the platform sticky and so encourage stress.

Though they conceded users have no way to opt out of streak emoji appearing.

They also noted they have previously reduced the size of the streak emoji to make it less prominent.

But they added they would take concerns back to product teams and re-examine the feature in light of the criticism.

You can watch the full committee hearing with Snap here.",Social Media,TechCrunch,https://techcrunch.com/2019/03/19/snap-is-under-nda-with-uk-home-office-discussing-how-to-centralize-age-checks-online/,"The UK Home Office is under NDA with Snap discussing a centralised system of age verification to prevent underage users from accessing the platform. MPs are raising concerns about the addictive nature of Snap's 'streaks' feature, which encourages kids to stay online and pile on peer pressure.",Security & Privacy
206,PSA: Don’t post your coronavirus vaccination card selfie on social media,"Look I know this pandemic has been a long, depressing slog, and even if you’ve masked up, done the social distancing, and have managed to stay virus-free, we’re all good and frazzled at this point. So it’s understandable that now that we have vaccines available, everyone’s impatient to get one.

But when you finally get the jab, resist the urge to post a humblebrag on Instagram or any other social media platform, because identity thieves may be watching. And, you don’t want to be the newly-vaccinated person whose selfie provides scammers with a template to make fake vaccination record cards (because if you think isn’t already happening, you would be mistaken).

“Some of you are celebrating your second COVID-19 vaccination with the giddy enthusiasm that’s usually reserved for weddings, new babies, and other life events,” the Federal Trade Commission wrote in a blog post on Friday. “You’re posting a photo of your vaccination card on social media. Please — don’t do that! You could be inviting identity theft.”

Not only does the card have the vaccinated person’s name and birth date on it, it also includes when and where you got the shot. Unless all your social media accounts are set to private, you’re handing out a lot of free data about yourself you may not want randos on the internet to know.

The New York Times talked to some privacy experts who said a savvy scam artist could pretend to be a healthcare official to trick people who have received the first dose of the vaccine into thinking they need to pay for the second dose, and get the victims’ credit card information. And, someone could use the photo of your vaccination card to recreate the cards and possibly sell counterfeit versions— something that’s apparently already happening in the UK.

As part of its Vaccinate with Confidence campaign, the Centers for Disease Control and Prevention has a plan for states to hand out stickers to the newly-vaccinated, an excellent visual to share on social media instead of your vaccination card.

So if you have been vaccinated against the coronavirus, please accept my congratulations! We’re all happy for you. But we don’t need to see your vital information all over our social feeds.",Social Media,Verge,https://www.theverge.com/2021/2/6/22270400/coronavirus-vaccine-card-selfie-social-media,"If you post a photo of your vaccination card on social media, you could be inviting identity theft, as the card contains vital information like your name, birth date, and where you got the shot.",Security & Privacy
207,Facebook's Mandatory Anti-Malware Scan Is Invasive and Lacks Transparency,"When an Oregon science fiction writer named Charity tried to log onto Facebook on February 11, she found herself completely locked out of her account. A message appeared saying she needed to download Facebook’s malware scanner if she wanted to get back in. Charity couldn’t use Facebook until she completed the scan, but the file the company provided was for a Windows device—Charity uses a Mac.

“I could not actually run the software they were demanding I download and use,” she says. When she tried instead to log in from her computer at work, Facebook greeted her with the same roadblock. “Obviously there is no way for Facebook to know if my device is infected with anything, since this same message appeared on any computer I tried to access my account from,” says Charity.

A Facebook spokesperson said Charity may have been asked to download the wrong software because some malware can spoof what kind of computer a person is running. Still, Charity was left without any way to access her account. And her experience is far from unique.

Scantron

The internet is full of Facebook users frustrated with how the company handles malware threats. For nearly four years, people have complained about Facebook's anti-malware scan on forums, Twitter, Reddit, and on personal blogs. The problems appear to have gotten worse recently. While the service used to be optional, Facebook now requires it if it flags your device for malware. And according to screenshots reviewed by WIRED from people recently prompted to run the scan, Facebook also no longer allows every user to select what type of device they're on, which ostensibly would have prevented what happened to Charity.

'I could not actually run the software they were demanding I download and use.' Charity, Facebook User

The malware scans likely only impact a relatively small population of Facebook's billions of users, some of whose computers may genuinely be infected. But even a fraction of Facebook's users still potentially means millions of impacted people. The mandatory scan has caused widespread confusion and frustration; WIRED spoke to people who had been locked out of their accounts by the scan, or simply baffled by it, on four different continents.

The mandatory malware scan has downsides beyond losing account access. Facebook users also frequently report that the feature is poorly designed, and inconsistently implemented. In some cases, if a different user logs onto Facebook from the same device, they sometimes won’t be greeted with the malware message. Similarly, if the “infected” user simply switches browsers, the message also appears to occasionally go away.

“It is actually tied to one specific Facebook user on one specific browser—if I change either to a different account, or use Safari instead of Chrome with the locked-out account, I do not get the scanner dialog,” says Anatol Ulrich, a Facebook user from Germany who was locked out of his account after sharing several Google docs in comment threads on Facebook. He, too, was prompted to download a Windows file on a Mac device.

“Our visibility into each account on a given device isn’t complete enough for us to checkpoint based only on the device, without factoring in whether the particular account is acting in a suspicious manner,” Facebook spokesperson Jay Nancarrow said in a statement. In some ways that might be comforting; Facebook doesn't collect enough information about your computer to say whether malware has infected it.",Social Media,WIRED,https://www.wired.com/story/facebook-mandatory-malware-scan/,"Facebook users have reported widespread confusion and frustration with the platform's mandatory malware scan, which can lock them out of their accounts, cause inconsistent implementation, and requires downloading a Windows file on Mac devices.",Security & Privacy
208,Facebook tightens policies around self-harm and suicide – TechCrunch,"Timed with World Suicide Prevention Day, Facebook is tightening its policies around some difficult topics, including self-harm, suicide and eating disorder content after consulting with a series of experts on these topics. It’s also hiring a new Safety Policy Manager to advise on these areas going forward. This person will be specifically tasked with analyzing the impact of Facebook’s policies and its apps on people’s health and well-being, and will explore new ways to improve support for the Facebook community.

The social network, like others in the space, has to walk a fine line when it comes to self-harm content. On the one hand, allowing people to openly discuss their mental health struggles with family, friends and other online support groups can be beneficial. But on the other, science indicates that suicide can be contagious, and that clusters and outbreaks are real phenomena. Meanwhile, graphic imagery of self-harm can unintentionally promote the behavior.

With its updated policies, Facebook aims to prevent the spread of more harmful imagery and content.

It changed its policy around self-harm images to no longer allow graphic cutting images, which can unintentionally promote or trigger self-harm. These images will not be allowed even if someone is seeking support or expressing themselves to aid their recovery, Facebook says.

The same content will also now be more difficult to find on Instagram through search and Explore.

And Facebook has tightened its policy regarding eating disorder content to prevent an expanded range of content that could contribute to eating disorders. This includes content that focuses on the depiction of ribs, collar bones, thigh gaps, concave stomach or protruding spine or scapula when shared with terms related to eating disorders. It also will ban content that includes instructions for drastic and unhealthy weight loss, when shared with those same sorts of terms.

It will also display a sensitivity screen over healed self-harm cuts going forward to help unintentionally promote self-harm.

Even when it takes down content, Facebook says it will now continue to send resources to people who posted self-harm or eating disorder content.

Facebook will additionally include Orygen’s #chatsafe guidelines to its Safety Center and in resources on Instagram. These guidelines are meant to help those who are responding to suicide-related content posted by others or are looking to express their own thoughts and feelings on the topic.

The changes came about over the course of the year, following Facebook’s consultations with a variety of experts in the field across a number of countries, including the U.S., Canada, U.K. Australia, Brazil, Bulgaria, India, Mexico, Philippines and Thailand. Several of the policies were updated prior to today, but Facebook is now publicly announcing the combined lot.

The company says it’s also looking into sharing the public data from its platform on how people talk about suicide with academic researchers by way of the CrowdTangle monitoring tool. Before, this was made available primarily to newsrooms and media publishers

Suicide helplines provide help to those in need. Contact a helpline if you need support yourself or need help supporting a friend. Click here for Facebook’s list of helplines around the world.",Social Media,TechCrunch,https://techcrunch.com/2019/09/10/facebook-tightens-policies-around-self-harm-and-suicide/,"Facebook is tightening its policies around self-harm, suicide and eating disorder content due to the real phenomenon of suicide clusters and the potential for graphic imagery to unintentionally promote these behaviors.",Social Norms & Relationships
209,Facebook Dating launch blocked in Europe after it fails to show privacy workings – TechCrunch,"Facebook has been left red-faced after being forced to call off the launch date of its dating service in Europe because it failed to give its lead EU data regulator enough advanced warning — including failing to demonstrate it had performed a legally required assessment of privacy risks.

Yesterday, Ireland’s Independent.ie newspaper reported that the Irish Data Protection Commission (DPC) — using inspection and document seizure powers set out in Section 130 of the country’s Data Protection Act — had sent agents to Facebook’s Dublin office seeking documentation that Facebook had failed to provide.

In a statement on its website, the DPC said Facebook first contacted it about the rollout of the dating feature in the EU on February 3.

“We were very concerned that this was the first that we’d heard from Facebook Ireland about this new feature, considering that it was their intention to roll it out tomorrow, 13 February,” the regulator writes. “Our concerns were further compounded by the fact that no information/documentation was provided to us on 3 February in relation to the Data Protection Impact Assessment [DPIA] or the decision-making processes that were undertaken by Facebook Ireland.”

Facebook announced its plan to get into the dating game all the way back in May 2018, trailing its Tinder-encroaching idea to bake a dating feature for non-friends into its social network at its F8 developer conference.

It went on to test launch the product in Colombia a few months later. Since then, it’s been gradually adding more countries in South American and Asia. It also launched in the U.S. last fall after it was fined $5BN by the FTC for historical privacy lapses.

At the time of its U.S. launch, Facebook said dating would arrive in Europe by early 2020. It just didn’t think to keep its lead EU privacy regulator in the loop, despite the DPC having multiple (ongoing) investigations into other Facebook-owned products at this stage.

It’s either an extremely careless oversight or, well, an intentional fuck you to privacy oversight of its data-mining activities. (Among multiple probes being carried out under Europe’s General Data Protection Regulation, the DPC is looking into Facebook’s claimed legal basis for processing people’s data under the Facebook T&Cs, for example.)

The DPC’s statement confirms that its agents visited Facebook’s Dublin office on February 10 to carry out an inspection — in order to “expedite the procurement of the relevant documentation”. Which is a nice way of the DPC saying Facebook spent a whole week still not sending it the required information.

“Facebook Ireland informed us last night that they have postponed the roll-out of this feature,” the DPC’s statement goes on. Which is a nice way of saying Facebook fucked up and is being made to put a product rollout it’s been planning for at least half a year on ice.

The DPC’s head of communications, Graham Doyle, confirmed the enforcement action, telling us: “We’re currently reviewing all the documentation that we gathered as part of the inspection on Monday and we have posed further questions to Facebook and are awaiting the reply.”

“Contained in the documentation we gathered on Monday was a DPIA,” he added.

This begs the question why Facebook didn’t send the DPIA to the DPC on February 3. We’ve reached out to Facebook for comment and to ask when it carried out the DPIA.

Update: A Facebook spokesperson has now sent this statement:

It’s really important that we get the launch of Facebook Dating right so we are taking a bit more time to make sure the product is ready for the European market. We worked carefully to create strong privacy safeguards, and complete the data processing impact assessment ahead of the proposed launch in Europe, which we shared with the IDPC when it was requested.

We’ve asked the company why, if it’s “really important” to get the launch “right,” it did not provide the DPC with the required documentation in advance instead of the regulator having to send agents to Facebook’s offices to get it themselves. We’ll update this report with any response.

Update: A Facebook spokesman has now provided us with a second statement — in which it writes:

We’re under no legal obligation to notify the IDPC of product launches. However, as a courtesy to the Office of the Data Protection Commission, who is our lead regulator for data protection in Europe, we proactively informed them of this proposed launch two weeks in advance. We had completed the data processing impact assessment well in advance of the European launch, which we shared with the IDPC when they asked for it.

Under Europe’s GDPR, there’s a requirement for data controllers to bake privacy by design and default into products which are handling people’s information. (And a dating product clearly would be.)

While conducting a DPIA — which is a process whereby planned processing of personal data is assessed to consider the impact on the rights and freedoms of individuals — is a requirement under the GDPR when, for example, individual profiling is taking place or there’s processing of sensitive data on a large scale.

And again, the launch of a dating product on a platform such as Facebook which has hundreds of millions of regional users would be a clear-cut case for such an assessment to be carried out ahead of any launch.

In later comments to TechCrunch today, the DPC reiterated that it’s still waiting for Facebook to respond to follow-up questions it put to the company after its officers had obtained documentation related to Facebook Dating during the office inspection.

The regulator could ask Facebook to make changes to how the product functions in Europe if it’s not satisfied it complies with EU laws. So a delay to the launch may mean many things.

“We’re still examining the documentation that we have,” Doyle told us. “We’re still awaiting answers to the queries that we posed to Facebook on Tuesday [February 11]. We haven’t had any response back from them and it would be our expectation that the feature won’t be rolled out in advance of us completing our analysis.”

Asked how long the process might take, he said: “We don’t control this time process but a lot of it is dependent on how quickly we get responses to the queries that we’ve posed and how much those responses deal with the queries that we’ve raised — whether we have to go back to them again etc. So it’s just not possible to say at this stage.”

This report was updated with additional comment from Facebook and the DPC",Social Media,TechCrunch,https://techcrunch.com/2020/02/13/facebook-dating-launch-blocked-in-europe-after-it-fails-to-show-privacy-workings/,Facebook has been forced to postpone the launch of its dating service in Europe after failing to provide the Irish Data Protection Commission with the necessary documents and information proving that it had conducted a legally required assessment of privacy risks. This oversight highlights the need for Social Media companies to be more mindful of the privacy considerations of their products.,Security & Privacy
210,YouTube releases its first report about how it handles flagged videos and policy violations – TechCrunch,"YouTube has released its first quarterly Community Guidelines Enforcement Report and launched a Reporting Dashboard that lets users see the status of videos they’ve flagged for review. The inaugural report, which covers the last quarter of 2017, follows up on a promise YouTube made in December to give users more transparency into how it handles abuse and decides what videos will be removed.

“This regular update will help show the progress we’re making in removing violative content from our platform,” the company said in a post on its official blog. “By the end of the year, we plan to refine our reporting systems and add additional data, including data on comments, speed or removal and policy removal reasons.”

But the report is unlikely to quell complaints from people who believe YouTube’s rules are haphazardly applied in an effort to appease advertisers upset their commercials had played before videos with violent extremist content. The issue came to the forefront last year after a report by The Times, but many content creators say YouTube’s updated policies have made it very difficult to monetize on the platform, even though their videos don’t violate its rules.

YouTube, however, claims that its anti-abuse machine learning algorithm, which it relies on to monitor and handle potential violations at scale, is “paying off across high-risk, low-volume areas (like violent extremism) and in high-volume areas (like spam).”

Its report says that YouTube removed 8.2 million videos during the last quarter of 2017, most of which were spam or contained adult content. Of that number, 6.7 million were automatically flagged by its anti-abuse algorithms first.

Of the videos reported by a person, 1.1 million were flagged by a member of YouTube’s Trusted Flagger program, which includes individuals, government agencies and NGOs that have received training from the platform’s Trust & Safety and Public Policy teams.

YouTube’s report positions views a video received before being removed as a benchmark for the success of its anti-abuse measures. At the beginning of 2017, 8% of videos removed for violent extremist content were taken down before clocking 10 views. After YouTube started using its machine-learning algorithms in June 2017, however, it says that percentage increased to more than 50% (in a footnote, YouTube clarified that this data does not include videos that were automatically and flagged before they could be published and therefore received no views). From October to December, 75.9% of all automatically flagged videos on the platform were removed before they received any views.

During that same period, 9.3 million videos were flagged by people, with nearly 95% coming from YouTube users and the rest from its Trusted Flagger program and government agencies or NGOs. People can select a reason when they flag a video. Most were flagged for sexual content (30.1%) or spam (26.4%).

Last year, YouTube said it wanted to increase the number of people “working to address violative content” to 10,000 across Google by the end of 2018. Now it says it has almost reached that goal and also hired more full-time anti-abuse experts and expanded their regional teams. It also claims that the addition of machine-learning algorithms enables more people to review videos.

In its report, YouTube gave more information about how those algorithms work.

“With respect to the automated systems that detect extremist content, our teams have manually reviewed over two million videos to provide large volumes of training examples, which improve the machine learning flagging technology,” it said, adding that it has started applying that technology to other content violations as well.

FINALLY. @YouTube‘s new transparency report breaks out content flags by category. @ACLU_NorCal has long called for this necessary information. Your move, @Facebook. https://t.co/O0lsjHXwj7 — Jake Snow (@snowjake) April 23, 2018

YouTube’s report may not ameliorate the concerns of content creators who saw their revenue drop during what they refer to as the “Adpocalpyse” or help them figure out how to monetize successfully again. On the other hand, it is a victory for people, including free speech activists, who have called for social media platforms to be more transparent about how they handle flagged content and policy violations, and may put more pressure on Facebook and Twitter.",Social Media,TechCrunch,https://techcrunch.com/2018/04/23/youtube-releases-its-first-report-about-how-it-handles-flagged-videos-and-policy-violations/,"Content creators have seen their revenue drop due to YouTube's haphazard enforcement of its rules, leading to calls for greater transparency from social media platforms in how they handle flagged content and policy violations.",Equality & Justice
211,Facebook's New Focus on 'Community' Might Actually Depress You,"There’s a problem with Facebook’s focus on “community.”

Amid criticism of its data security and its role in the 2016 election, Facebook in June announced a change to its mission. No longer would the company strive to make the world more open and connected. Rather, the company declared, it would bring its 2.2 billion users, and thus the world, “closer together” by building community. One method for achieving this? Less passive consumption of content, more emphasis on “meaningful” social interactions with friends and family.

But seeing posts from friends and family may make young people feel worse after spending time on social networks, according to a new study from research firm Ypulse, commissioned by image-sharing site Imgur. The survey, conducted in late February, asked 2,100 13-to-35-year-olds in the US how they felt after using popular digital-media services including Netflix, YouTube, Reddit, Instagram, Twitter, Facebook, Spotify, Pinterest and Snapchat.

The study found that the more “social” elements a service has, the more likely it is to make users unhappy. On the flip side, the more a site focuses on entertainment and discovery, the happier its users say they are. “Entertainment platforms are giving [users] something to be happier, instantly lifting the mood and changing the conversation, whereas ones that are focused on self-identity and personal branding are not winning in that arena,” says Jillian Kramer, vice president of research at Ypulse.

The majority of users surveyed reported feeling better after spending time on purely entertainment-focused sites including Spotify and Netflix. About half of users felt better after spending time on sites that rely heavily on professional content, like Pinterest and YouTube. Fewer than half of users reported feeling better after visiting social media sites including Snapchat and Instagram (44 percent), Reddit (37 percent), Twitter (36 percent), and Tumblr (33 percent). Facebook fared the worst: Just 29 percent of Facebook users felt better after using the platform, and 16 percent reported feeling less happy.

The sentiment is also reflected in whether users think their time spent on a platform is “well spent” or “wasted.” Only 1 percent of Spotify users and 9 percent of Netflix users think they waste time on those platforms; the majority say their time is well spent. “Even though TV watching feels like a waste of time, I also see the benefits in unwinding and de-stressing watching stuff like Queer Eye,” said a 20-year-old woman who participated in the survey.

On the question of time well spent, Facebook has a problem. Facebook was the only platform in the survey where more users said their time was wasted than well spent. According to the survey, just 23 percent of Facebook users said the platform provided “time well spent” while 31 percent felt their time on Facebook was wasted.

Ironically, Facebook is the only platform that has explicitly adopted the “time well spent” mantra from its critics, namely Tristan Harris, an activist for ethical technology design who created a nonprofit called Time Well Spent. In January, Facebook CEO Mark Zuckerberg announced several features and changes to the Facebook platform that he hoped would “make time on Facebook time well spent.” Facebook did not respond to a request for comment.",Social Media,WIRED,https://www.wired.com/story/facebooks-new-focus-on-community-might-actually-depress-you/,"Evidence from a survey of 2,100 13-to-35-year-olds shows that social media platforms, such as Facebook, Snapchat, and Instagram, tend to make users feel worse after spending time on them. This despite Facebook's stated mission of bringing the world ""closer together"" through meaningful social interaction.",Social Norms & Relationships
212,#deletefacebook – TechCrunch,"Facebook is using us. It is actively giving away our information. It is creating an echo chamber in the name of connection. It surfaces the divisive and destroys the real reason we began using social media in the first place – human connection.

It is a cancer.

I’ve begun the slow process of weaning myself off of the platform by methodically running a script that will delete my old content. And there’s a lot. There are likes and shares. There are long posts I wrote to impress my friends. There are thousands of WordPress notifications that tell the world what I’m doing. In fact, I would wager I use Facebook more to broadcast my ego than interact with real humans. And I suspect that most of us are in a similar situation.



There is a method to my madness. I like Facebook Messenger and I like that Facebook is now a glorified version of OAuth. It’s a useful tool when it is stripped of its power. However, when it is larded with my personal details it is a weapon and a liability.

Think about it: any posts older than about a week are fodder for bots and bad actors. Posts from 2016? 2017? Why keep them? No one will read them, no one cares about them. Those “You and Joe have known each other for five years” auto-posts are fun but does anyone care? Ultimately you’ve created the largest dossier on yourself and you’ve done it freely, even gleefully. This dossier reflects your likes, your dislikes, your feelings, and political leanings. It includes clear pictures of your face from all angles, images of your pets and family, and details your travels. You are giving the world unfettered access to your life. It’s wonderful to imagine that this data will be used by a potential suitor who will fall in love with your street style. It’s wonderful to imagine you will scroll through Facebook at 80 and marvel at how you looked at the turn of the century. It’s wonderful to imagine that Facebook is a place to share ideas, dreams, and hopes, a human-to-human connection engine that gives more than it takes.

None of that will happen.

Facebook is a data collection service for those who want to sell you products. It is the definitive channel to target you based on age, sex, geographic location, political leanings, interests, and marital status. It’s an advertiser’s dream and it is wildly expensive in terms of privacy lost and cash spent to steal that privacy. It is the perfect tool for marketers, a user-generated paradise that is now run by devils.

Will you delete Facebook? Probably not. Will I? I’m working on it. I’ve already been deleting old tweets after realizing that border police and potential employers may use what I write publicly against me. I’m clearing out old social media accounts and, as I mentioned before, deleting old Facebook posts, thus ensuring that I will no longer be a target for companies like Cambridge Analytica. But we love our social media, don’t we? The power it affords. The feeling of connection. In the absence of human interaction we cling to whatever dark simulacrum is available. In the absence of the Town Square we talk to ourselves. In the absence of love and understanding we join the slow riot of online indifference.

When Travis Kalanick led his ride-sharing company down the dark path to paranoia, bro culture, and classist rantings we reacted by deleting the app. We didn’t want to do business with that particular brand of company. Yet we sit idly by while Facebook sells us out and its management pummels and destroys all competition.

I wish it didn’t have to be this way. There is plenty of good in these platforms but the dangers far outweigh the benefits. Try to recall the last time you were thankful for social media. I can. It happened twice. First, it happened when I posted on my “wall” a eulogy for my father who died in January. The outpouring of support was heartening in a dark time. It was wonderful to see friends and acquaintances tell me their own stories, thereby taking the sting out of my own. But months later that good feeling is gone, replaced by ads for fancy shoes and political rants. Out of the Facebook swamp sometimes surfaces a pearl. But it sinks just as quickly.

One more sad example: I found out, accidentally, that my friend’s wife died. It appeared on my feed as if placed there by some divine hand and I was thankful it surfaced. It beat out videos of Mister Rogers saying inspiring things and goofy pictures of Trump. It beat out ads and rants and questions about the best sushi restaurant in Scranton. The stark announcement left me crying and breathless. There it was in black and blue, splashed across her page: she was gone. There was the smiling photo of her two little children and there was the outpouring of grief under these once innocuous photos. Gone, it said. She was gone. I found out from her wall where her memorial service would be held and I finally reached back out to my old friend to try to comfort him in his grief. Facebook, in those two instances, worked.

But Facebook isn’t the only thing that can give us that feeling of connectedness. We’ve had it for centuries.

Facebook simply replaced the tools we once used to tell the world of our joys and sorrows and it replaced them with cheap knock-offs that make us less connected, not more. Decades ago, on one coal-fogged winter morning in Krakow, Poland where I was living, I passed Kościół św. Wojciecha with its collection of nekrologi – necrologies – posted on a board in front of the church. There you saw the names of the dead – and sometimes the names of the newly born – and it was there you discovered what was happening in your little corner of the world. The church wasn’t far from the central square – the Rynek – and I walked there thinking about the endless parade of humanity that had walked across those cobbles, stopping for a moment in their hustle at the church yard to see who had died. I stood in the crisp air, flanked by centuries old brickwork, and imagined who once populated this place. This was the place you met your friends and your future partners. It was there you celebrated your successes and mourned your failures. It was there, among other humans, you told the world the story of your life, but told it slant. You witnessed kindnesses and cruelties, you built a world entire based on the happenings in a few square miles.

No more. Or, at least, those places are no longer available to most of us.

We’ve moved past the superstitions and mythologies of the past. Tools like Facebook were designed to connect us to the world, giving us an almost angelic view of daily happenstance. We replaced the churchyard with the “timeline.” But our efforts failed. We are still as closed, still full of superstition, as we were a hundred years ago. We traded a market square for the Internet but all of the closed-mindedness and cynicism came with it. We still disparage the outsider, we still rant against invisible enemies, and we still keep our friends close and fear what lies beyond our door. Only now we have the whole world on which to reflect our terror.

It doesn’t have to be this way. Maybe some day we’ll get the tools we need to interact with the world. Maybe they’re already here and we just don’t want to use them.

Until we find them, however, it’s probably better for us to delete the ones we use today.",Social Media,TechCrunch,https://techcrunch.com/2018/03/19/deletefacebook/,"Social Media has become a tool for companies to target us with ads and collect our personal data, which is costly in terms of both privacy and money. It has also become a platform for us to broadcast our own egos, creating an echo chamber and destroying the real human connections we sought when we began using social media.",Security & Privacy
213,App bans won’t make US security risks disappear,"Will the US government ban TikTok and WeChat, or won’t it—and why? With the Trump administration issuing vaguely phrased executive orders and policies about the apps, even as legal challenges against potential bans move through the courts and the president gives his “blessing” to a deal to keep TikTok in US app stores, it’s hard to make out a coherent story.

The Trump administration’s actions against the two Chinese-owned social-media platforms are driven more by politics and an effort to seem tough on China than by actual privacy, safety, or national security concerns. However, that doesn’t mean there aren’t tough challenges ahead in regulating digital platforms based in China, the United States, or anywhere else.

As the TikTok and WeChat stories unfold—and no one should expect a permanent resolution anytime soon—policymakers, technologists, and citizens should look beyond this chaotic start to the deeper, unresolved questions. Now is the time to develop comprehensive policy tools that protect privacy and national security from threats foreign and domestic.

Similarly, if the Trump administration were truly serious about stopping malign actors from abusing personal data from US-based users, or serious about stopping foreign intelligence agencies from gathering massive datasets describing US society, they would go to the root of the problem: an app economy that collects and monetizes as much data as companies can manage.

TikTok and WeChat critics cite the way the apps collect location data, device identifiers, social connections, browsing histories, and more to argue that the Chinese government could use this data in some kind of machine-learning-driven analysis down the road. Cutting off the apps’ access to US-based users, they say, would shield the country from Chinese intelligence—all while protecting US citizens’ privacy.

Now is the time to develop comprehensive policy tools that protect privacy and national security from threats foreign and domestic.

Not so fast. In a 2018 study, Oxford scholars analyzed data flows coming out of almost 1 million apps on the US and UK Google Play stores. They found that the median app sent user data to five tracking companies, and 17% of apps sent data to more than 10 trackers. More than 90% of apps analyzed sent data to a US-based company, while 5% sent data to a China-based company. Granted, these numbers only capture the data’s first stop after our smartphones. Some of the data siphoned to advertising networks and trackers is for sale, and both sellers and buyers can be hard to track down.

It’s not as if the US government is unaware that companies based outside China—including those in the United States—could potentially misuse this kind of data store. The Cambridge Analytica scandal, which largely revolved around data obtained from the US tech giant Facebook, showed that the 2016 Trump campaign was well aware of how digital data could be used for political influence.

Nor are authorities blind to the other ways Chinese intelligence is thought to obtain mass data about Americans. Chinese hackers are suspected in the hack, revealed in 2015, of a poorly secured US Office of Personnel Management database, as well as breaches at Anthem health insurance, Marriott hotels, and the credit agency and data broker Equifax.

The true scandal is not that the Chinese government might exploit personal data—a well-documented and unsurprising move from a major intelligence apparatus. It’s that doing so is so easy for them and many others, and will remain so even if TikTok and WeChat are banned.

That said, the Trump administration’s attempts to ban TikTok and WeChat were a mess. They suffered from the administration’s typical erraticism as Trump, a beleaguered incumbent, tried to be seen as tough on China after weak results from a costly trade war. Moreover, they do almost nothing to address the very real privacy and security risks of corporate data exploitation run amok.

There is an upside, however, to all the attention people are paying to the administration’s claims. These would-be bans might finally drive US citizens and institutions to demand comprehensive privacy and data governance. People rightfully concerned about potential foreign threats online should unite to take on the broader challenge.",Social Media,MIT,https://www.technologyreview.com/2020/09/21/1008620/wechat-tiktok-ban-china-us-security-policy-opinion/,"The main undesirable consequence of social media discussed here is the potential for abuse of private data, both from foreign actors and companies based in the US, which could lead to a host of privacy and security risks.",Security & Privacy
214,Google's Nerds Bring Angry Birds -- and More -- to Google+,"Google upped the ante in its challenge to Facebook Thursday by introducing games into its nascent social network Google+.

Just as in Facebook, Google+ users will now be able to play games such as Angry Birds, Zynga Poker, and Crime City – and bug their friends with notices of their latest achievements and high scores.

But Google promised Thursday there's a difference between its network and Facebook's – those notifications won't show up in the general ""stream"" of updates users see. So you'll only run across your friends' gaming-centered notifications when you go to play a game yourself.

Google says it's starting now with just a few handpicked titles, but that it's looking to work with more developers – announcing a developer's blog – where many hope Google will soon announce an API for Google Plus.

The announcement isn't unexpected. Google's going head-to-head with Facebook to try to win back the identity/sharing war that Facebook has won. And games have been a huge part of Facebook's success, bringing users back daily to raise virtual chickens or run virtual street gangs.

Zynga, built almost entirely on top of Facebook's platform, has filed for an IPO and reported that it made $16.8 million dollars in net income the first three months of the year, while booking $243 million in revenue – nearly all of that money coming from power Facebook users.

Google's games announcement shows that it's doubling down on its bet that users are tired of Facebook's sharing model that makes it difficult to share information with only a select group of users.

[S]haring is about more than just conversations. The experiences we have together are just as important to our relationships. We want to make playing games online just as fun, and just as meaningful, as playing in real life. That means giving you control over when you see games, how you play them and with whom you share your experiences. Games in Google+ are there when you want them and gone when you don’t.

Facebook games often reward users who announce their ""achievements"" to their friends, which the games hope will land them new users. But the side effect has been a growing weariness among many Facebook users upon seeing those notifications about how many chickens their friends now have on their virtual farms.

Google says its new Games page will be rolling out to users in waves.

See Also:- How Online Companies Get You to Share More and Spend More",Social Media,WIRED,https://www.wired.com/2011/08/google-plus-games/,"The consequence of Social Media, such as Facebook games, is that many users grow weary of the notifications they receive about their friends' achievements, such as their virtual farm and street gang. Google's Games page aims to alleviate this by allowing users to control when they see games, how they play them and with whom they share their experiences.",User Experience & Entertainment
215,Politics Have Turned Facebook Into a Steaming Cauldron of Hate,"“Whelp,” my mother began our weekly phone conversation. “Marge and I aren’t speaking.” Marge is one of my mom’s best friends. When she moved houses, Marge showed up with curtains and a lamp and had her grown son heave boxes in from the car. When my mom found herself alone on Christmas one year, Marge set an extra spot at her table.

But here’s the thing: Marge is a Republican. My mom, a Democrat.

Jessi Hempel is Backchannel's editorial director. Sign up to get Backchannel's weekly newsletter, and follow us on Facebook, Twitter, and Instagram.

This hasn’t mattered for the better part of the last decade. Their friendship is built on warm conversations over lunches, shared confidences, and favors exchanged. But then, several weeks ago, Mom became so outraged by current events that she began posting a series of anti-Trump memes and quotes.

Marge’s daughter commented on a post, saying the sentiments were all lies. Mom responded. The daughter responded. The posts continued and the comments continued. Marge jumped in. And by the end of the string of messages, no one was friends anymore. I’m not talking about Facebook friends — I mean IRL.

My mother isn’t the only person whose real-life relationships with friends and neighbors are fraying as they get into tiffs over political differences on Facebook. Last week, a reader asked me to weigh in on politics and social media friendships. “Please write about people giving up on or blocking others on social media based on political discontent,” she wrote. “We’re getting further and further apart…I’m scared.”

Curious how significant this issue was, I turned to Facebook, asking people to message me with their stories about blocking and unfriending over political differences. To date, I’ve received 94 comments and 20 private messages, from both sides of the divide.

The going emotion was frustration: Nearly everybody reported that they’d blocked, muted, or unfollowed people with extreme — and extremely different — political views. “I never realized how right, racist, and xenophobic people from my Quaker high school could be,” wrote one middle-aged New Yorker. Despite similar sentiments, plenty of people were uncomfortable with blocking or muting commenters. One guy, himself a co-founder of an early social site, wrote: “My issue is that the echo chamber gives me such a tunnel view that I fear I’m not understanding the topics.”

Therein lies the rub. We know our country has become more partisan. We know there are communities of people in the US — on the left and the right — who hold views we don’t get. We crave the empathy that comes with understanding those views. We know the “filter bubble” about which Eli Pariser first wrote back in 2011 is part of the problem—it limits the viewpoints we see to those that reflect the opinions we already have. And yet we double down on that bubble, muting and blocking and unfriending people who think differently from us, if they make it into our social streams at all. We hate ourselves a tiny bit for this. And yet, if we do the opposite — engage on social media with people who hold different viewpoints — it almost always goes sideways. Next thing you know, you, like my mom, aren’t talking to Marge.

It’s too easy here to hate on Facebook — to blame the technology for the problem. Yet Facebook is not without responsibility. Many of our interactions on the site are guided by the subtle (or not-so-subtle) direction of its design, which is, in turn, determined by its business model. The company has honed the software to get us to spend as much time as possible on it—and to provide information about ourselves in the process—so that it can sell more ads. In Menlo Park, the company is scrambling to better navigate the tensions that arise between building a good business and building a socially responsible service.",Social Media,WIRED,https://www.wired.com/2017/02/politics-have-turned-facebook-into-a-steaming-cauldron-of-hate/,"People are increasingly blocking and unfriending those with different political views on social media, leading to an echo chamber of similar opinions and a breakdown of real-life relationships. Meanwhile, Facebook’s business model encourages users to spend more time on the platform, which can worsen the problem.",Politics
216,Former Facebook exec says social media is ripping apart society,"Another former Facebook executive has spoken out about the harm the social network is doing to civil society around the world. Chamath Palihapitiya, who joined Facebook in 2007 and became its vice president for user growth, said he feels “tremendous guilt” about the company he helped make. “I think we have created tools that are ripping apart the social fabric of how society works,” he told an audience at Stanford Graduate School of Business, before recommending people take a “hard break” from social media.

Palihapitiya’s criticisms were aimed not only at Facebook, but the wider online ecosystem. “The short-term, dopamine-driven feedback loops we’ve created are destroying how society works,” he said, referring to online interactions driven by “hearts, likes, thumbs-up.” “No civil discourse, no cooperation; misinformation, mistruth. And it’s not an American problem — this is not about Russians ads. This is a global problem.”

He went on to describe an incident in India where hoax messages about kidnappings shared on WhatsApp led to the lynching of seven innocent people. “That’s what we’re dealing with,” said Palihapitiya. “And imagine taking that to the extreme, where bad actors can now manipulate large swathes of people to do anything you want. It’s just a really, really bad state of affairs.” He says he tries to use Facebook as little as possible, and that his children “aren’t allowed to use that shit.” He later adds, though, that he believes the company “overwhelmingly does good in the world.”

Palihapitiya’s remarks follow similar statements of contrition from others who helped build Facebook into the powerful corporation it is today. In November, early investor Sean Parker said he has become a “conscientious objector” to social media, and that Facebook and others had succeeded by “exploiting a vulnerability in human psychology.” A former product manager at the company, Antonio Garcia-Martinez, has said Facebook lies about its ability to influence individuals based on the data it collects on them, and wrote a book, Chaos Monkeys, about his work at the firm.

These former employees have all spoken out at a time when worry about Facebook’s power is reaching fever pitch. In the past year, concerns about the company’s role in the US election and its capacity to amplify fake news have grown, while other reports have focused on how the social media site has been implicated in atrocities like the “ethnic cleansing” of Myanmar’s Rohingya ethnic group.

In his talk, Palihapitiya criticized not only Facebook, but Silicon Valley’s entire system of venture capital funding. He said that investors pump money into “shitty, useless, idiotic companies,” rather than addressing real problems like climate change and disease. Palihapitiya currently runs his own VC firm, Social Capital, which focuses on funding companies in sectors like healthcare and education.

Palihapitiya also notes that although tech investors seem almighty, they’ve achieved their power more through luck than skill. “Everybody’s bullshitting,” he said. “If you’re in a seat, and you have good deal flow, and you have precious capital, and there’s a massive tailwind of technological change ... Over time you get one of the 20 [companies that become successful] and you look like a genius. And nobody wants to admit that but that’s the fucking truth.”",Social Media,Verge,https://www.theverge.com/2017/12/11/16761016/former-facebook-exec-ripping-apart-society,"Former Facebook executive Chamath Palihapitiya has spoken out about the harm Social Media is doing to civil society around the world, citing examples of misinformation and hoaxes leading to the lynching of innocent people. He believes the short-term, dopamine-driven feedback loops are destroying society, and has recommended people take a ""hard break""",Social Norms & Relationships
217,"Facebook adds a Snooze button for muting people, groups and Pages for 30 days – TechCrunch","Facebook today is launching a new feature designed to give users more control over what content they see in their News Feed: a “Snooze” button. The option, which will become available via the top-right dropdown menu on a post, will mute content from a person, Page or group for 30 days.

The new feature can serve as a way to dial down the content you don’t want to see, without having to fully unfollow or unfriend someone.

For example, if you’ve had enough of someone’s political rants or baby photos, you can temporarily opt to see less of them in your News Feed. You could also turn off a particularly chatty Facebook friend whose continuous updates clutter your feed.

The option could be useful for people going through a breakup, too – that is, one where they’re staying connected socially, but don’t necessary want constant reminders of what an ex is up to. That’s an area Facebook has explored in the past, with the 2015 debut of tools to help you see less from former flames. However, not many people seem to know these features exist. Snooze, on the other hand, will be far more visible.

For Pages and Groups, having a Snooze button means they may be able to better retain their less active users, who may have otherwise unliked them or left the group to avoid their content.

TechCrunch first spotted Snooze in testing this fall, when different lengths of time were being offered. Today’s launch has settled on a month as the right amount of time spent on mute.

Snooze Posted by Facebook on Wednesday, December 13, 2017

Snooze joins a series of other content controls for News Feed, like Unfollow, Hide, Report and See First, which give people more ways to customize their experience, notes Facebook.

The update, while seemingly minor, comes at a time when many people – including some of Facebook’s early founders – are questioning whether social media is having a negative impact on people and society as a whole. A network that’s too tuned to what people want to see, and provides that to them by way of algorithms, can lead to addiction and an inability to relate to different people and opinions.

The flip side of Facebook’s toolset for deep personalization, including now Snooze, are these ongoing concerns that Facebook’s social network can become overly comfortable for people. It allows people to ensconce themselves in a world where everyone thinks like them, enjoys the same things, and posts similar news and other things. But this is not the real world, where people’s opinions can wildly differ. The result of this bubble effect is a reduction in being exposed to new ideas, and an increased intolerance for those who don’t share your same beliefs.

Snooze, in that context, could be seen not as an empowering tool, but one that could potentially lead people to further distancing themselves from friends with different perspectives – whether political, religious, cultural or otherwise – simply because it’s something you don’t want to see.

But at least Snooze’s forced cooldown period could stop people from unfriending people with these opposing viewpoints.

Facebook notes that when the Snooze period is about to end, it will notify you of this – presumably, in case you need to snooze them again. You can also reverse a snooze at any time, the company notes.

The Snooze button is rolling out today, across Facebook.",Social Media,TechCrunch,https://techcrunch.com/2017/12/15/facebook-adds-a-snooze-button-for-muting-people-groups-and-pages-for-30-days/,"Facebook's new Snooze feature could potentially lead to further distancing from friends with different perspectives, resulting in users being exposed to fewer new ideas and being less tolerant of those who don't share their beliefs.",Social Norms & Relationships
218,Twitter suspends more accounts for “engaging in coordinated manipulation” – TechCrunch,"Following last week’s suspension of 284 accounts for “engaging in coordinated manipulation,” Twitter announced today that it’s kicked an additional 486 accounts off the platform for the same reason, bringing the total to 770 accounts.

While many of the accounts removed last week appeared to originate from Iran, Twitter said this time that about 100 of the latest batch to be suspended claimed to be in the United States. Many of these were less than a year old and shared “divisive commentary.” These 100 accounts tweeted a total of 867 times and had 1,268 followers between them.

Since our initial suspensions last Tuesday, we have continued our investigation, further building our understanding of these networks. In addition, we suspended an additional 486 accounts for violating the policies outlined last week. This brings the total suspended to 770. — Twitter Safety (@TwitterSafety) August 27, 2018

As examples of the “divisive commentary” tweeted, Twitter shared screenshots from several suspended accounts that showed anti-Trump rhetoric, counter to the conservative narrative that the platform unfairly targets Republican accounts.

Fewer than 100 of the 770 suspended accounts claimed to be located in the U.S. and many of these were sharing divisive social commentary. On average, these 100 Tweeted 867 times, were followed by 1, 268 accounts, and were less than a year old. Examples below. pic.twitter.com/LQhbvFjxSo — Twitter Safety (@TwitterSafety) August 27, 2018

Twitter also said that the suspended accounts included one advertiser that spent $30 on Twitter ads last year, but added those ads did not target the U.S. and that the billing address was outside of Iran.

“As with prior investigations, we are committed to engaging with other companies and relevant law enforcement entities. Our goal is to assist investigations into these activities and where possible, we will provide the public with transparency and context on our efforts,” Twitter said on its Safety account.

After years of accusations that it doesn’t enforce its own policies about bullying, bots and other abuses, Twitter has taken a much harder line on problematic accounts in the past few months. Despite stalling user growth, especially in the United States, Twitter has been aggressively suspending accounts, including ones that were created by users to evade prior suspensions.

Twitter announced a drop of one million monthly users in the second quarter, causing investors to panic even though it posted a $100 million profit. In its earnings call, Twitter said that its efforts don’t impact user numbers because many of the “tens of millions” of removed accounts were too new or had been inactive for more than a month and were therefore not counted in active user numbers. The company did admit, however, that it’s anti-spam measures had caused it to lose three million monthly active users.

Whatever its impact on user numbers, Twitter’s anti-abuse measures may help it save face during a Senate Intelligence Committee hearing on September 5. Executives from Twitter, Facebook and Google are expected to be grilled by Sen. Mark Warner and other politicians about the use of their platforms by other countries to influence U.S. politics.",Social Media,TechCrunch,https://techcrunch.com/2018/08/27/twitter-suspends-more-accounts-for-engaging-in-coordinated-manipulation/,"Twitter has been aggressively suspending accounts for bullying, bots and other abuses, which has caused a drop in user numbers and led to a Senate Intelligence Committee hearing on September 5 to address how Social Media platforms are being used by other countries to influence U.S. politics.",Politics
219,Facebook Doesn't Need (Or Even Want) a Dislike Button,"In an informal Q&A at his company’s headquarters, Mark Zuckerberg acknowledged that Facebook would ship a feature for which many of its users have long clamored: a “dislike button.” The need to expand options beyond a thumbs up is obvious. What shape that option will take, though, is anything but.

The problem with ""Like"" is also what makes it so effective: It’s reductive. It converts human interaction into a simple binary, or even something less than that. ""The Like button has turned into a very important function,"" says Dr. Larry Rosen, who specializes in the psychology of technology. ""What I think it reflects is an easy way to do two things. One is an easy way to signal to people that you read their material. And two is to be visible to everyone else."" It's also, Rosen says, the single-most common activity on Facebook.

It’s a system that works just fine, until we run into something we're interested in, but don't like at all.

You can measure how often this happens in not days or hours but seconds. Scrolling down my Facebook News Feed just now, in seconds I ran into my first minor existential dilemma. My friend Brandon has shared a New York Times article with the headline ""How Segregation Destroys Black Wealth"". I am glad he shared it. It’s something I want to read, a topic I feel is important. I wouldn’t give it a thumbs up in a million moons.

It’s not that I’m worried anyone will interpret that ""Like"" to mean that I think the crippling effects of systemic racism (or, in your own News Feed, the death of a beloved relative, or a story about Syrian refugees, or something as mundane as a poorly timed flat tire) is good. It’s that damn icon. A thumbs up is inherently goofy. It’s for Little League coaches and guys named Chad. The last time an upward pointing thumb carried any emotional weight was in the late days of the Roman Colosseum.

""I think it's exceedingly bizarre that the only option is to put thumbs up"" says Dr. Rosen. ""Thumbs up is so ambiguous. There should be some neutral way to say 'I grok you,' but there isn't.""

This is not a new problem. It’s one that’s followed the Like button around since it was first conceived, in 2007, as the “awesome” button. The majority of what you see on Facebook is not awesome. And the mental contortions required to read a thumbs up as “I like that you shared this horrible thing” aren’t anywhere near worth the sinking feeling that accompanies clicking it beneath word of the latest Ebola outbreak.

And now an alternative option is coming. An option that will endeavor to be appropriately clickable on your friend's post about getting evicted or dumped or fired. But the real question, now that we know one is coming, becomes: How will the button (or buttons) actually work?

Thumb und Drang

It’s not like Facebook doesn’t know this is an issue. “People have asked about the dislike button for many years,” in Zuckerberg’s own words. One reason it may have taken this long, though, is that it’s not a simple fix.

The knee-jerk (to keep things anatomical) solution would be to create a thumbs down, a Dislike to balance out the Like, a perfect yin and yang of passive interest. “Now that Facebook isn’t in desperate growth mode, it can start to filter the unpleasant stuff out,” says 451 Research analyst Alan Pelz-Sharpe.",Social Media,WIRED,https://www.wired.com/2015/09/facebook-dislike-button/,"One of the main undesirable consequences of social media is the way it reduces complex interpersonal interactions into a binary, thumbs up/thumbs down system. This limits the ability of users to express nuanced emotions, leaving them feeling frustrated and unfulfilled. In response to this, Facebook has announced plans to develop a ""dislike"" button to provide",Social Norms & Relationships
220,Facebook touts beefed up hate speech detection ahead of Myanmar election – TechCrunch,"Facebook has offered a little detail on extra steps it’s taking to improve its ability to detect and remove hate speech and election disinformation ahead of Myanmar’s election. A general election is scheduled to take place in the country on November 8, 2020.

The announcement comes close to two years after the company admitted a catastrophic failure to prevent its platform from being weaponized to foment division and incite violence against the country’s Rohingya minority.

Facebook says now that it has expanded its misinformation policy with the aim of combating voter suppression and will now remove information “that could lead to voter suppression or damage the integrity of the electoral process” — giving the example of a post that falsely claims a candidate is a Bengali, not a Myanmar citizen, and thus ineligible to stand.

“Working with local partners, between now and November 22, we will remove verifiable misinformation and unverifiable rumors that are assessed as having the potential to suppress the vote or damage the integrity of the electoral process,” it writes.

Facebook says it’s working with three fact-checking organizations in the country — namely: BOOM, AFP Fact Check and Fact Crescendo — after introducing a fact-checking program there in March.

In March 2018 the United Nations warned that Facebook’s platform was being abused to spread hate speech and whip up ethnic violence in Myanmar. By November of that year the tech giant was forced to admit it had not stopped its platform from being repurposed as a tool to drive genocide, after a damning independent investigation slammed its impact on human rights.

On hate speech, which Facebook admits could suppress the vote in addition to leading to what it describes as “imminent, offline harm” (aka violence), the tech giant claims to have invested “significantly” in “proactive detection technologies” that it says help it “catch violating content more quickly”, albeit without quantifying the size of its investment nor providing further details. It only notes that it “also” uses AI to “proactively identify hate speech in 45 languages, including Burmese”.

Facebook’s blog post offers a metric to imply progress — with the company stating that in Q2 2020 it took action against 280,000 pieces of content in Myanmar for violations of its Community Standards prohibiting hate speech, of which 97.8% were detected proactively by its systems before the content was reported to it.

“This is up significantly from Q1 2020, when we took action against 51,000 pieces of content for hate speech violations, detecting 83% proactively,” it adds.

However without greater visibility into the content Facebook’s platform is amplifying, including country-specific factors such as whether hate speech posting is increasing in Myanmar as the election gets closer, it’s not possible to understand what volume of hate speech is passing under the radar of Facebook’s detection systems and reaching local eyeballs.

In a more clearly detailed development, Facebook notes that since August, electoral, issue and political ads in Myanmar have had to display a ‘paid for by’ disclosure label. Such ads are also stored in a searchable Ad Library for seven years — in an expansion of the self-styled ‘political ads transparency measures’ Facebook launched more than two years ago in the US and other western markets.

Facebook also says it’s working with two local partners to verify the official national Facebook Pages of political parties in Myanmar. “So far, more than 40 political parties have been given a verified badge,” it writes. “This provides a blue tick on the Facebook Page of a party and makes it easier for users to differentiate a real, official political party page from unofficial pages, which is important during an election campaign period.”

Another recent change it flags is an ‘image context reshare’ product, which launched in June — which Facebook says alerts a user when they attempt to share a image that’s more than a year old and could be “potentially harmful or misleading” (such as an image that “may come close to violating Facebook’s guidelines on violent content”).

“Out-of-context images are often used to deceive, confuse and cause harm. With this product, users will be shown a message when they attempt to share specific types of images, including photos that are over a year old and that may come close to violating Facebook’s guidelines on violent content. We warn people that the image they are about to share could be harmful or misleading will be triggered using a combination of artificial intelligence (AI) and human review,” it writes without offering any specific examples.

Another change it notes is the application of a limit on message forwarding to five recipients which Facebook introduced in Sri Lanka back in June 2019.

“These limits are a proven method of slowing the spread of viral misinformation that has the potential to cause real world harm. This safety feature is available in Myanmar and, over the course of the next few weeks, we will be making it available to Messenger users worldwide,” it writes.

On coordinated election interference, the tech giant has nothing of substance to share — beyond its customary claim that it’s “constantly working to find and stop coordinated campaigns that seek to manipulate public debate across our apps”, including groups seeking to do so ahead of a major election.

“Since 2018, we’ve identified and disrupted six networks engaging in Coordinated Inauthentic Behavior in Myanmar. These networks of accounts, Pages and Groups were masking their identities to mislead people about who they were and what they were doing by manipulating public discourse and misleading people about the origins of content,” it adds.

In summing up the changes, Facebook says it’s “built a team that is dedicated to Myanmar”, which it notes includes people “who spend significant time on the ground working with civil society partners who are advocating on a range of human and digital rights issues across Myanmar’s diverse, multi-ethnic society” — though clearly this team is not operating out of Myanmar.

It further claims engagement with key regional stakeholders will ensure Facebook’s business is “responsive to local needs” — something the company demonstrably failed on back in 2018.

“We remain committed to advancing the social and economic benefits of Facebook in Myanmar. Although we know that this work will continue beyond November, we acknowledge that Myanmar’s 2020 general election will be an important marker along the journey,” Facebook adds.

There’s no mention in its blog post of accusations that Facebook is actively obstructing an investigation into genocide in Myanmar.

Earlier this month, Time reported that Facebook is using US law to try to block a request for information related to Myanmar military officials’ use of its platforms by the West African nation, The Gambia.

“Facebook said the request is ‘extraordinarily broad’, as well as ‘unduly intrusive or burdensome’. Calling on the U.S. District Court for the District of Columbia to reject the application, the social media giant says The Gambia fails to ‘identify accounts with sufficient specificity’,” Time reported.

“The Gambia was actually quite specific, going so far as to name 17 officials, two military units and dozens of pages and accounts,” it added.

“Facebook also takes issue with the fact that The Gambia is seeking information dating back to 2012, evidently failing to recognize two similar waves of atrocities against Rohingya that year, and that genocidal intent isn’t spontaneous, but builds over time.”

In another recent development, Facebook has been accused of bending its hate speech policies to ignore inflammatory posts made against Rohingya Muslim immigrants by Hindu nationalist individuals and groups.

The Wall Street Journal reported last month that Facebook’s top public-policy executive in India, Ankhi Das, opposed applying its hate speech rules to T. Raja Singh, a member of Indian Prime Minister Narendra Modi’s Hindu nationalist party, along with at least three other Hindu nationalist individuals and groups flagged internally for promoting or participating in violence — citing sourcing from current and former Facebook employees.

Update: Facebook has sent the following statement regarding data disclosure to the Gambia investigation:",Social Media,TechCrunch,https://techcrunch.com/2020/09/01/facebook-touts-beefed-up-hate-speech-detection-ahead-of-myanmar-election/,"Facebook has been accused of failing to prevent its platform from being used to spread hate speech and incite violence in Myanmar, obstructing an investigation into genocide in Myanmar, and bending its hate speech policies to ignore inflammatory posts made against Rohingya Muslim immigrants in India. These issues have raised serious concerns about how Facebook is handling the potential misuse of its platform.",Equality & Justice
221,EU to review TikTok’s ToS after child safety complaints – TechCrunch,"TikTok has a month to respond to concerns raised by European consumer protection agencies earlier this year, EU lawmakers said today.

The Commission has launched what it described as “a formal dialogue” with the video sharing platform over its commercial practices and policy.

Areas of specific concern include hidden marketing, aggressive advertising techniques targeted at children and certain contractual terms in TikTok’s policies that could be considered misleading and confusing for consumers, per the Commission.

Commenting in a statement, justice commissioner Didier Reynders added: “The current pandemic has further accelerated digitalisation. This has brought new opportunities but it has also created new risks, in particular for vulnerable consumers. In the European Union, it is prohibited to target children and minors with disguised advertising such as banners in videos. The dialogue we are launching today should support TikTok in complying with EU rules to protect consumers.”

The background to this is that back in February the European Consumer Organisation (BEUC) sent the Commission a report calling out a number of TikTok’s policies and practices — including what it said were unfair terms and copyright practices. It also flagged the risk of children being exposed to inappropriate content on the platform, and accused TikTok of misleading data processing and privacy practices.

Complaints were filed around the same time by consumer organisations in 15 EU countries — urging those national authorities to investigate the social media giant’s conduct.

The multi-pronged EU action means TikTok has not just the Commission looking at the detail of its small print but is facing questions from a network of national consumer protection authorities — which is being co-led by the Swedish Consumer Agency and the Irish Competition and Consumer Protection Commission (which handles privacy issues related to the platform).

Nonetheless, the BEUC queried why the Commission hasn’t yet launched a formal enforcement procedure.

“We hope that the authorities will stick to their guns in this ‘dialogue’ which we understand is not yet a formal launch of an enforcement procedure. It must lead to good results for consumers, tackling all the points that BEUC raised. BEUC also hopes to be consulted before an agreement is reached,” a spokesperson for the organization told us.

Also reached for comment, TikTok sent us this statement on the Commission’s action, attributed to its director of public policy, Caroline Greer:



As part of our ongoing engagement with regulators and other external stakeholders over issues such as consumer protection and transparency, we are engaging in a dialogue with the Irish Consumer Protection Commission and the Swedish Consumer Agency and look forward to discussing the measures we’ve already introduced. In addition, we have taken a number of steps to protect our younger users, including making all under-16 accounts private-by-default, and disabling their access to direct messaging. Further, users under 18 cannot buy, send or receive virtual gifts, and we have strict policies prohibiting advertising directly appealing to those under the age of digital consent.

The company told us it uses age verification for personalized ads — saying users must have verified that they are 13+ to receive these ads; as well as being over the age of digital consent in their respective EU country; and also having consented to receive targeted ads.

However, TikTok’s age verification technology has been criticized as weak before now — and recent emergency child-safety-focused enforcement action by the Italian national data protection agency has led to TikTok having to pledge to strengthen its age verification processes in the country.

The Italian enforcement action also resulted in TikTok removing more than 500,000 accounts suspected of belonging to users aged younger than 13 earlier this month — raising further questions about whether it can really claim that under-13s aren’t routinely exposed to targeted ads on its platform.

In further background remarks it sent us, TikTok claimed it has clear labelling of sponsored content. But it also noted it’s made some recent changes — such as switching the label it applies on video advertising from “sponsored” to “ad” to make it clearer.

It also said it’s working on a toggle that aims to make it clearer to users when they may be exposed to advertising by other users by enabling the latter users to prominently disclose that their content contains advertising.

TikTok said the tool is currently in beta testing in Europe but it said it expects to move to general availability this summer and will also amend its ToS to require users to use this toggle whenever their content contains advertising. (But without adequate enforcement that may just end up as another overlooked and easily abused setting.)

The company recently announced a transparency center in Europe in a move that looks intended to counter some of the concerns being raised about its business in the region, as well as to prepare it for the increased oversight that’s coming down the pipe for all digital platforms operating in the EU — as the bloc works to update its digital rulebook.",Social Media,TechCrunch,https://techcrunch.com/2021/05/28/eu-to-review-tiktoks-tos-after-child-safety-complaints/,"The European Commission has launched a formal dialogue with TikTok to address concerns raised by consumer protection agencies about hidden marketing, aggressive advertising, and misguiding contractual terms for consumers, particularly younger users, who may be exposed to inappropriate content or targeted ads.",Security & Privacy
222,Facebook and YouTube’s moderation failure is an opportunity to deplatform the platforms – TechCrunch,"Facebook, YouTube, and Twitter have failed their task of monitoring and moderating the content that appears on their sites; what’s more, they failed to do so well before they knew it was a problem. But their incidental cultivation of fringe views is an opportunity to recast their role as the services they should be rather than the platforms they have tried so hard to become.

The struggles of these juggernauts should be a spur to innovation elsewhere: While the major platforms reap the bitter harvest of years of ignoring the issue, startups can pick up where they left off. There’s no better time to pass someone up as when they’re standing still.

Asymmetrical warfare: Is there a way forward?

At the heart of the content moderation issue is a simple cost imbalance that rewards aggression by bad actors while punishing the platforms themselves.

To begin with, there is the problem of defining bad actors in the first place. This is a cost that must be borne from the outset by the platform: With the exception of certain situations where they can punt (definitions of hate speech or groups for instance), they are responsible for setting the rules on their own turf.

That’s a reasonable enough expectation. But carrying it out is far from trivial; you can’t just say “here’s the line; don’t cross it or you’re out.” It is becoming increasingly clear that these platforms have put themselves in an uncomfortable lose-lose situation.

If they have simple rules, they spend all their time adjudicating borderline cases, exceptions, and misplaced outrage. If they have more granular ones, there is no upper limit on the complexity and they spend all their time defining it to fractal levels of detail.

Both solutions require constant attention and an enormous, highly-organized and informed moderation corps, working in every language and region. No company has shown any real intention to take this on — Facebook famously contracts the responsibility out to shabby operations that cut corners and produce mediocre results (at huge human and monetary cost); YouTube simply waits for disasters to happen and then quibbles unconvincingly.",Social Media,TechCrunch,https://techcrunch.com/2019/07/29/facebook-and-youtubes-moderation-failure-is-an-opportunity-to-deplatform-the-platforms/,"Social media platforms have failed to adequately monitor and moderate their content, leading to a situation where bad actors can take advantage of the cost imbalance, creating an environment of aggression that is difficult to combat. This has resulted in a lose-lose situation for the platforms, as they struggle to define and enforce the right rules.","Information, Discourse & Governance."
223,@Fault: Besieged U.S. Embassy #Fails Its Twitter Defense,"[Update, 9 a.m., Sept. 12: U.S. Ambassador to Libya Christopher Stevens has been killed by a mob in Benghazi. There are reports that other Americans are dead. The Pentagon is not able to confirm as yet reports that Marines are dead or that additional U.S. military assets are en route. As soon as I have something solid, I will report it in a separate post that will link back to this one.

A statement released by the White House from President Obama reads in part: ""I have directed my Administration to provide all necessary resources to support the security of our personnel in Libya, and to increase security at our diplomatic posts around the globe. While the United States rejects efforts to denigrate the religious beliefs of others, we must all unequivocally oppose the kind of senseless violence that took the lives of these public servants.""]

When Egyptians stormed the gates of the U.S. Embassy in Cairo, the diplomats took to their computers and mobile devices -- and blamed an American video for stirring up the locals. But the Embassy's tweets against the movie and ""religious incitement"" instantly came in for more criticism, from Americans who thought the diplomats had conceded far, far too much -- like basic principles of free speech.

An estimated 2,000 protesters breached the walls of the U.S. embassy and burned an American flag on Tuesday, replacing it with an Islamic flag. The protesters were said to be enraged by a film made inside the United States that ""depicts Muhammad as a fraud, and shows him having sex and calling for massacres,"" as the Associated Press put it. The United States government had nothing to do with making the film -- except for guaranteeing the freedom of its citizens to issue offensive speech. The embassy in Cairo had nothing to do with it.

Except, apparently before the protests broke out in force, it issued a statement that denounced the film as ""religious incitement."" Tying the condemnation to the 9/11 anniversary, the Embassy statement read, in part: ""Respect for religious beliefs is a cornerstone of American democracy. We firmly reject the actions by those who abuse the universal right of free speech to hurt the religious beliefs of others."" It tweeted the statement, posted it to its Arabic-language Facebook page -- and stirred a hornet's nest.

If the statement was meant to calm tensions in Cairo, it didn't stop the protests -- which spread to U.S. diplomatic outposts in neighboring Libya. It also conspicuously didn't call the protests ""religious incitement,"" nor did it condemn the incursion onto sovereign American soil. But conspicuously -- and alarmingly to several Americans on Twitter -- it didn't make an effort to defend the principle that even offensive speech deserves toleration, a basic democratic value that the Obama administration is supposed to be promoting while a revolutionary fervor sweeps through the Middle East.

Writing at the conservative National Review, Nina Shea, a human-rights advocate formerly with Freedom House, a nonprofit that promotes open societies, blogged that the Embassy's statement ""redefines and limits freedom of speech to that speech which others, and, explicitly Muslims, do not find offensive."" Shea called for the U.S. ambassador in Egypt, Anne Patterson -- a veteran of the beleaguered U.S. embassy in Pakistan as well -- to resign.",Social Media,WIRED,https://www.wired.com/2012/09/embassy-cairo-twitter/,"The U.S Embassy in Cairo was criticized for issuing a statement that failed to defend the principle of free speech, which their statement seemed to redefine and limit to that speech which others, and specifically Muslims, do not find offensive. This has caused alarm among Americans, and calls for the resignation of the U.S. ambassador in Egypt.",Equality & Justice
224,Jack Dorsey and Twitter ignored opportunity to meet with civic group on Myanmar issues – TechCrunch,"Responding to criticism from his recent trip to Myanmar, Twitter CEO Jack Dorsey said he’s keen to learn about the country’s racial tension and human rights atrocities, but it has emerged that both he and Twitter’s public policy team ignored an opportunity to connect with a key civic group in the country.

A loose group of six companies in Myanmar has engaged with Facebook in a bid to help improve the situation around usage of its services in the country — often with frustrating results — and key members of that alliance, including Omidyar-backed accelerator firm Phandeeyar, contacted Dorsey via Twitter DM and emailed the company’s public policy contacts when they learned that the CEO was visiting Myanmar.

The plan was to arrange a forum to discuss the social media concerns in Myanmar to help Dorsey gain an understanding of life on the ground in one of the world’s fastest-growing internet markets.

“The Myanmar tech community was all excited, and wondering where he was going,” Jes Kaliebe Petersen, the Phandeeyar CEO, told TechCrunch in an interview. “We wondered: ‘Can we get him in a room, maybe at a public event, and talk about technology in Myanmar or social media, whatever he is happy with?'”

The DMs went unread. In a response to the email, a Twitter staff member told the group that Dorsey was visiting the country strictly on personal time with no plans for business. The Myanmar-based group responded with an offer to set up a remote, phone-based briefing for Twitter’s public policy team with the ultimate goal of getting information to Dorsey and key executives, but that email went unanswered.

When we contacted Twitter, a spokesperson initially pointed us to a tweet from Dorsey in which he said: “I had no conversations with the government or NGOs during my trip.”

We know we can’t do this alone, and continue to welcome conversation with and help from civil society and NGOs within the region. I had no conversations with the government or NGOs during my trip. We’re always open to feedback on how to best improve. — jack (@jack) December 11, 2018

However, within two hours of our inquiry, a member of Twitter’s team responded to the group’s email in an effort to restart the conversation and set up a phone meeting in January.

“We’ve been in discussions with the group prior to your outreach,” a Twitter spokesperson told TechCrunch in a subsequent email exchange.

That statement is incorrect.

Still, on the bright side, it appears that the group may get an opportunity to brief Twitter on its concerns on social media usage in the country after all.

The micro-blogging service isn’t as well-used in Myanmar as Facebook, which has some 20 million monthly users and is practically the de facto internet, but there have been concerns in Myanmar. For one thing, there has been the development of a somewhat sinister bot army in Myanmar and other parts of Southeast Asia, while it remains a key platform for influencers and thought-leaders.

“[Dorsey is] the head of a social media company and, given the massive issues here in Myanmar, I think it’s irresponsible of him to not address that,” Petersen told TechCrunch.

“Twitter isn’t as widely used as Facebook but that doesn’t mean it doesn’t have concerns happening with it,” he added. “As we’d tell Facebook or any large tech company with a prominent presence in Myanmar, it’s important to spend time on the ground like they’d do in any other market where they have a substantial presence.”

The UN has concluded that Facebook plays a “determining” role in accelerating ethnic violence in Myanmar. While Facebook has tried to address the issues, it hasn’t committed to opening an office in the country and it released a key report on the situation on the eve of the U.S. mid-term elections, a strategy that appeared designed to deflect attention from the findings. All of which suggests that it isn’t really serious about Myanmar.",Social Media,TechCrunch,https://techcrunch.com/2018/12/14/jack-dorsey-twitter-myanmar-civic-group/,"The use of social media in Myanmar has resulted in a rise in ethnic violence and the development of a sinister bot army. Twitter CEO Jack Dorsey's recent trip to Myanmar was criticized as he ignored an opportunity to engage with a key civic group in the country, while Facebook has failed to address the issues and has not committed to opening an office",Equality & Justice
225,Facebook to penalize rule-breaking Groups’ members by demoting them in News Feed – TechCrunch,"Facebook has been steadily rolling out updates to Facebook Groups aimed at giving admins better ways to manage and moderate their online communities. Recently, this has included a combination of product releases — like access to automated moderation aids and alerts about heated debates — as well as new policies aimed at keeping Groups in check. Today, Facebook says it’s introducing two other changes. It will now enforce stricter measures against Group members who break its rules and it will make some of its removals more transparent, with a new “Flagged by Facebook” feature.

Specifically, Facebook says it will begin to demote all Groups content from members who have broken Facebook’s Community Standards anywhere across its platform. In other words, bad actors on Facebook may see the content they share in Groups downranked, even if they haven’t violated any of that Groups’ rules and policies.

By “demoting,” Facebook means it will show the content shared by these members lower in the News Feed. This is also called downranking and is an enforcement measure Facebook has used in the past to penalize content it wanted less of in News Feed — like clickbait, spam or even posts from news organizations.

In addition, Facebook says these demotions will get more severe as the members accrue more violations across Facebook. Because Facebook algorithms rank content in News Feed in a way that’s personalized to users, it could be difficult to track how well such demotions may or may not be working going forward.

Facebook also tells us the demotions currently only apply to the main News Feed, not the dedicated feed in the Groups tab where you can browse posts from your various Groups in one spot.

The company hopes this change will reduce members’ ability to reach others and notes it joins existing Groups penalties for rule violations that include restricting users’ ability to post, comment, add new members to a group or create new groups.

Another change is the launch of a new feature called “Flagged by Facebook.”

This feature will show Group admins which content has been flagged for removal before it’s shown to their broader community. Admins can then choose to remove the content themselves or review the content to see if they agree with Facebook’s decision. If not, they can ask for a review by Facebook and provide additional feedback as to why they believe the content should remain. This could be helpful in the case of automated moderation errors. By allowing admins to step in and request reviews, they could potentially protect members ahead of an unnecessary strike and removal.

The feature joins an existing option to appeal a takedown to the Group admins, when a post is found to violate Community Standards. Instead, it’s about giving admins a way to be more proactively involved in the process.

Unfortunately for Facebook, systems like this only work when Groups are actively moderated. That’s not always the case. Though Groups may have admins assigned, if they decide to stop using Facebook or managing the Group but don’t add another admin or moderator to take over, their Group can plunge into chaos — particularly if it’s larger. One group member of a sizable group with over 40,000 members told us their admin had not been active in the group since 2017. The members know this and some will take advantage of the lack of moderation to post anything they want at times.

This is just one example of how Facebook’s Groups infrastructure is still largely a work-in-progress. If a company was building a platform for private groups from scratch, policies and procedures — like how content removals work, or the penalty for rule-breaking, for instance — wouldn’t likely be years-later additions. They would be foundational elements. And yet, Facebook is just now rolling out what ought to have been established protocols for a product that arrived in 2010.",Social Media,TechCrunch,https://techcrunch.com/2021/10/20/facebook-to-penalize-rule-breaking-groups-members-by-demoting-them-in-news-feed/,"The lack of adequate moderation and enforcement of rules in Facebook Groups can lead to chaos, as members take advantage of the lack of oversight to post inappropriate content. Without proper policies and procedures in place, the platform is subject to misuse and abuse.",Security & Privacy
226,UK watchdog orders Cambridge Analytica to give up data in US voter test case – TechCrunch,"Another big development in the personal data misuse saga attached to the controversial Trump campaign-linked UK-based political consultancy, Cambridge Analytica — which could lead to fresh light being shed on how the company and its multiple affiliates acquired and processed US citizens’ personal data to build profiles on millions of voters for political targeting purposes.

The UK’s data watchdog, the ICO, has today announced that it’s served an enforcement notice on Cambridge Analytica affiliate SCL Elections, under the UK’s 1998 Data Protection Act.

The company has been ordered to give up all the data it holds on one US academic within 30 days — with the ICO warning that: “Failure to do so is a criminal offence, punishable in the courts by an unlimited fine.”

The notice follows a subject access request (SAR) filed in January last year by US-based academic, David Carroll after he became suspicious about how the company was able to build psychographic profiles of US voters. And while Carroll is not a UK citizen, he discovered his personal data had been processed in the UK — so decided to bring a test case by requesting his personal data under UK law.

Carroll’s complaint, and the ICO’s decision to issue an enforcement notice in support of it, looks to have paved the way for millions of US voters to also ask Cambridge Analytica for their data (the company claimed to have up to 7,000 data points on the entire US electorate, circa 240M people — so just imagine the class action that could be filed here… ).

The Guardian reports that Cambridge Analytica had tried to dismiss Carroll’s argument by claiming he had no more rights “than a member of the Taliban sitting in a cave in the remotest corner of Afghanistan”. The ICO clearly disagrees.

Important development. @ICOnews agrees with our complaint and orders full disclosure to @profcarroll following findings of non-cooperation by Cambridge Analytica / SCL. We look forward to full disclosure within 30 days. Decision here: https://t.co/X5g1FY95j0 https://t.co/ZsonQhPsKQ — Ravi Naik (@RaviNa1k) May 5, 2018

Cambridge Analytica/SCL Group responded to Carroll’s original SAR in March 2017 but he was unimpressed by the partial data they sent him — which ranked his interests on a selection of topics (including gun rights, immigration, healthcare, education and the environment) yet did not explain how the scores had been calculated.

It also listed his likely partisanship and propensity to vote in the 2016 US election — again without explaining how those predictions had been generated.

So Carroll complained to the UK’s data watchdog in September 2017 — which began sending its own letters to CA/SCL, leading to further unsatisfactory responses.

“The company’s reply refused to address the ICO’s questions and incorrectly stated Prof Caroll had no legal entitlement to it because he wasn’t a UK citizen or based in this country. The ICO reiterated this was not legally correct in a letter to SCL the following month,” the ICO writes today. “In November 2017, the company replied, denying that the ICO had any jurisdiction or that Prof Carroll was legally entitled to his data, adding that SCL did “.. not expect to be further harassed with this sort of correspondence”.”

In a strongly worded statement, information commissioner Elizabeth Denham further adds:

The company has consistently refused to co-operate with our investigation into this case and has refused to answer our specific enquiries in relation to the complainant’s personal data — what they had, where they got it from and on what legal basis they held it. The right to request personal data that an organisation holds about you is a cornerstone right in data protection law and it is important that Professor Carroll, and other members of the public, understand what personal data Cambridge Analytica held and how they analysed it. We are aware of recent media reports concerning Cambridge Analytica’s future but whether or not the people behind the company decide to fold their operation, a continued refusal to engage with the ICO will potentially breach an Enforcement Notice and that then becomes a criminal matter.

Since mid-March this year, Cambridge Analytica’s name (along with the names of various affiliates) has been all over headlines relating to a major Facebook data misuse scandal, after press reports revealed in granular detail how an app developer had used the social media’s platform’s 2014 API structure to extract and process large amounts of users’ personal data, passing psychometrically modeled scores on US voters to Cambridge Analytica for political targeting.

But Carroll’s curiosity about what data Cambridge Analytica might hold about him predates the scandal blowing up last month. Although journalists had actually raised questions about the company as far back as December 2015 — when the Guardian reported that the company was working for the Ted Cruz campaign, using detailed psychological profiles of voters derived from tens of millions of Facebook users’ data.

Though it was not until last month that Facebook confirmed as many as 87 million users could have had personal data misappropriated.

Carroll, who has studied the Internet ad tech industry as part of his academic work, reckons Facebook is not the sole source of the data in this case, telling the Guardian he expects to find a whole host of other companies are also implicated in this murky data economy where people’s personal information is quietly traded and passed around for highly charged political purposes — bankrolled by billionaires.

“I think we’re going to find that this goes way beyond Facebook and that all sorts of things are being inferred about us and then used for political purposes,” he told the newspaper.

Under mounting political, legal and public pressure, Cambridge Analytica claimed to be shutting down this week — but the move appears more like a rebranding exercise, as parent entity, SCL Group, maintains a sprawling network of companies and linked entities. (Such as one called Emerdata, which was founded in mid-2017 and is listed at the same address as SCL Elections, and has many of the same investors and management as Cambridge Analytica… But presumably hasn’t yet been barred from social media giants’ ad platforms, as its predecessor has.)

Closing one of the entities embroiled in the scandal could also be a tactic to impede ongoing investigations, such as the one by the ICO — as Denham’s statement alludes, by warning that any breach of the enforcement notice could lead to criminal proceedings being brought against the owners and operators of Cambridge Analytica’s parent entity.

In March ICO officials obtained a warrant to enter and search Cambridge Analytica’s London offices, removing documents and computers for examination as part of a wider, year-long investigation into the use of personal data and analytics by political campaigns, parties, social media companies and other commercial actors. And last month the watchdog said 30 organizations — including Facebook — were now part of that investigation.

The Guardian also reports that the ICO has suggested to Cambridge Analytica that if it has difficulties complying with the enforcement notice it should hand over passwords for the servers seized during the March raid on its London office – raising questions about how much data the watchdog has been able to retrieve from the seized servers.

SCL Group’s website contains no obvious contact details beyond a company LinkedIn profile — a link which appears to be defunct. But we reached out to SCL Group’s CEO Nigel Oakes, who has maintained a public LinkedIn presence, to ask if he has any response to the ICO enforcement notice.

Meanwhile Cambridge Analytica continues to use its public Twitter account to distribute a stream of rebuttals and alternative ‘facts’.",Social Media,TechCrunch,https://techcrunch.com/2018/05/05/uk-watchdog-orders-cambridge-analytica-to-give-up-data-in-us-voter-test-case/,"The misuse of personal data by companies such as Cambridge Analytica and its affiliates, enabled by the 2014 API structure of Social Media platforms, has led to a situation where the personal data of millions of people is being used to build profiles for political targeting, without the understanding or approval of the data owners. This has resulted in a major scandal",Security & Privacy
227,Facebook was warned about app permissions in 2011 – TechCrunch,"Who’s to blame for the leaking of 50 million Facebook users’ data? Facebook founder and CEO Mark Zuckerberg broke several days of silence in the face of a raging privacy storm to go on CNN this week to say he was sorry. He also admitted the company had made mistakes; said it had breached the trust of users; and said he regretted not telling Facebookers at the time their information had been misappropriated.

Meanwhile, shares in the company have been taking a battering. And Facebook is now facing multiple shareholder and user lawsuits.

Pressed on why he didn’t inform users, in 2015, when Facebook says it found out about this policy breach, Zuckerberg avoided a direct answer — instead fixing on what the company did (asked Cambridge Analytica and the developer whose app was used to suck out data to delete the data) — rather than explaining the thinking behind the thing it did not do (tell affected Facebook users their personal information had been misappropriated).

Essentially Facebook’s line is that it believed the data had been deleted — and presumably, therefore, it calculated (wrongly) that it didn’t need to inform users because it had made the leak problem go away via its own backchannels.

Except of course it hadn’t. Because people who want to do nefarious things with data rarely play exactly by your rules just because you ask them to.

There’s an interesting parallel here with Uber’s response to a 2016 data breach of its systems. In that case, instead of informing the ~57M affected users and drivers that their personal data had been compromised, Uber’s senior management also decided to try and make the problem go away — by asking (and in their case paying) hackers to delete the data.

Aka the trigger response for both tech companies to massive data protection fuck-ups was: Cover up; don’t disclose.

Facebook denies the Cambridge Analytica instance is a data breach — because, well, its systems were so laxly designed as to actively encourage vast amounts of data to be sucked out, via API, without the check and balance of those third parties having to gain individual level consent.

So in that sense Facebook is entirely right; technically what Cambridge Analytica did wasn’t a breach at all. It was a feature, not a bug.

Clearly that’s also the opposite of reassuring.

Yet Facebook and Uber are companies whose businesses rely entirely on users trusting them to safeguard personal data. The disconnect here is gapingly obvious.

What’s also crystal clear is that rules and systems designed to protect and control personal data, combined with active enforcement of those rules and robust security to safeguard systems, are absolutely essential to prevent people’s information being misused at scale in today’s hyperconnected era.

But before you say hindsight is 20/20 vision, the history of this epic Facebook privacy fail is even longer than the under-disclosed events of 2015 suggest — i.e. when Facebook claims it found out about the breach as a result of investigations by journalists.

What the company very clearly turned a blind eye to is the risk posed by its own system of loose app permissions that in turn enabled developers to suck out vast amounts of data without having to worry about pesky user consent. And, ultimately, for Cambridge Analytica to get its hands on the profiles of ~50M US Facebookers for dark ad political targeting purposes.

European privacy campaigner and lawyer Max Schrems — a long time critic of Facebook — was actually raising concerns about the Facebook’s lax attitude to data protection and app permissions as long ago as 2011.

Indeed, in August 2011 Schrems filed a complaint with the Irish Data Protection Commission exactly flagging the app permissions data sinkhole (Ireland being the focal point for the complaint because that’s where Facebook’s European HQ is based).

“[T]his means that not the data subject but “friends” of the data subject are consenting to the use of personal data,” wrote Schrems in the 2011 complaint, fleshing out consent concerns with Facebook’s friends’ data API. “Since an average facebook user has 130 friends, it is very likely that only one of the user’s friends is installing some kind of spam or phishing application and is consenting to the use of all data of the data subject. There are many applications that do not need to access the users’ friends personal data (e.g. games, quizzes, apps that only post things on the user’s page) but Facebook Ireland does not offer a more limited level of access than “all the basic information of all friends”.

“The data subject is not given an unambiguous consent to the processing of personal data by applications (no opt-in). Even if a data subject is aware of this entire process, the data subject cannot foresee which application of which developer will be using which personal data in the future. Any form of consent can therefore never be specific,” he added.

As a result of Schrems’ complaint, the Irish DPC audited and re-audited Facebook’s systems in 2011 and 2012. The result of those data audits included a recommendation that Facebook tighten app permissions on its platform, according to a spokesman for the Irish DPC, who we spoke to this week.

The spokesman said the DPC’s recommendation formed the basis of the major platform change Facebook announced in 2014 — aka shutting down the Friends data API — albeit too late to prevent Cambridge Analytica from being able to harvest millions of profiles’ worth of personal data via a survey app because Facebook only made the change gradually, finally closing the door in May 2015.

“Following the re-audit… one of the recommendations we made was in the area of the ability to use friends data through social media,” the DPC spokesman told us. “And that recommendation that we made in 2012, that was implemented by Facebook in 2014 as part of a wider platform change that they made. It’s that change that they made that means that the Cambridge Analytica thing cannot happen today.

“They made the platform change in 2014, their change was for anybody new coming onto the platform from 1st May 2014 they couldn’t do this. They gave a 12 month period for existing users to migrate across to their new platform… and it was in that period that… Cambridge Analytica’s use of the information for their data emerged.

“But from 2015 — for absolutely everybody — this issue with CA cannot happen now. And that was following our recommendation that we made in 2012.”

Given his 2011 complaint about Facebook’s expansive and abusive historical app permissions, Schrems has this week raised an eyebrow and expressed surprise at Zuckerberg’s claim to be “outraged” by the Cambridge Analytica revelations — now snowballing into a massive privacy scandal.

In a statement reflecting on developments he writes: “Facebook has millions of times illegally distributed data of its users to various dodgy apps — without the consent of those affected. In 2011 we sent a legal complaint to the Irish Data Protection Commissioner on this. Facebook argued that this data transfer is perfectly legal and no changes were made. Now after the outrage surrounding Cambridge Analytica the Internet giant suddenly feels betrayed seven years later. Our records show: Facebook knew about this betrayal for years and previously argues that these practices are perfectly legal.”

So why did it take Facebook from September 2012 — when the DPC made its recommendations — until May 2014 and May 2015 to implement the changes and tighten app permissions?

The regulator’s spokesman told us it was “engaging” with Facebook over that period of time “to ensure that the change was made”. But he also said Facebook spent some time pushing back — questioning why changes to app permissions were necessary and dragging its feet on shuttering the friends’ data API.

“I think the reality is Facebook had questions as to whether they felt there was a need for them to make the changes that we were recommending,” said the spokesman. “And that was, I suppose, the level of engagement that we had with them. Because we were relatively strong that we felt yes we made the recommendation because we felt the change needed to be made. And that was the nature of the discussion. And as I say ultimately, ultimately the reality is that the change has been made. And it’s been made to an extent that such an issue couldn’t occur today.”

“That is a matter for Facebook themselves to answer as to why they took that period of time,” he added.

Of course we asked Facebook why it pushed back against the DPC’s recommendation in September 2012 — and whether it regrets not acting more swiftly to implement the changes to its APIs, given the crisis its business is now faced having breached user trust by failing to safeguard people’s data.

We also asked why Facebook users should trust Zuckerberg’s claim, also made in the CNN interview, that it’s now ‘open to being regulated’ — when its historical playbook is packed with examples of the polar opposite behavior, including ongoing attempts to circumvent existing EU privacy rules.

A Facebook spokeswoman acknowledged receipt of our questions this week — but the company has not responded to any of them.

The Irish DPC chief, Helen Dixon, also went on CNN this week to give her response to the Facebook-Cambridge Analytica data misuse crisis — calling for assurances from Facebook that it will properly police its own data protection policies in future.

“Even where Facebook have terms and policies in place for app developers, it doesn’t necessarily give us the assurance that those app developers are abiding by the policies Facebook have set, and that Facebook is active in terms of overseeing that there’s no leakage of personal data. And that conditions, such as the prohibition on selling on data to further third parties is being adhered to by app developers,” said Dixon.

“So I suppose what we want to see change and what we want to oversee with Facebook now and what we’re demanding answers from Facebook in relation to, is first of all what pre-clearance and what pre-authorization do they do before permitting app developers onto their platform. And secondly, once those app developers are operative and have apps collecting personal data what kind of follow up and active oversight steps does Facebook take to give us all reassurance that the type of issue that appears to have occurred in relation to Cambridge Analytica won’t happen again.”

Firefighting the raging privacy crisis, Zuckerberg has committed to conducting a historical audit of every app that had access to “a large amount” of user data around the time that Cambridge Analytica was able to harvest so much data.

So it remains to be seen what other data misuses Facebook will unearth — and have to confess to now, long after the fact.

But any other embarrassing data leaks will sit within the same unfortunate context — which is to say that Facebook could have prevented these problems if it had listened to the very valid concerns data protection experts were raising more than six years ago.

Instead, it chose to drag its feet. And the list of awkward questions for the Facebook CEO keeps getting longer.",Social Media,TechCrunch,https://techcrunch.com/2018/03/24/facebook-was-warned-about-app-permissions-in-2011/,"The Facebook-Cambridge Analytica data misuse crisis is a stark reminder of the risks posed by a lax attitude to data protection and app permissions, and exposes the dangers of a company choosing to drag its feet on recommendations to tighten security and protect user data.",Security & Privacy
228,"Facebook users becoming more cautious and critical, says Pew – TechCrunch","Ahead of Facebook COO Sheryl Sandberg testifying before Congress later today, where she will be questioned alongside Twitter CEO Jack Dorsey as US lawmakers wrestle with how to regulate social media platforms (and even just to get bums on seats, given Google’s Larry Page declined to attend), the Pew Research Center has published new research suggesting Americans have become more cautious and critical in their use of Facebook over the past year.

It’s certainly been a year of scandals for the social media behemoth, which started 2018 already on the back foot already in the wake of Kremlin-backed election interference revelations — and with Mark Zuckerberg saying his annual personal mission for the new year would be the embarrassingly unfun challenge of “fixing Facebook”.

Since then things have only got worse, with a major global scandal kicking off in March after fresh revelations about the Cambridge Analytica data misuse sandal snowballed and went on to drag all sorts of other data malfeasance skeletons out of Facebook’s closet.

‘Locking down the platform’ is the new ‘closing the stable door after the horse has bolted’.

All of which led Zuckerberg to be perched on a booster cushion in the Senate and Congress, where he was berated by US lawmakers for dodging their questions. (Lawmakers in Europe berated him for avoiding their questions too.)

So if US Facebook users are changing how they use the platform as a result of all this scandalabra it’s hardly surprising (or only if you believe the lie Zuckerberg has fenced for years that people don’t care about privacy).

Pew sees evidence of a change in US users’ relationship with the service, noting that many users have adjusted privacy settings (52%); taken a break from Facebook of at least a few weeks (42%); while around a quarter (26%) said they had deleted the Facebook app from their cellphone.

All told, it said that almost three-quarters (74%) of Facebook users told it they have taken at least one of these three actions in the past year.

Most worryingly for Facebook, it found differences in the reaction of users depending on their age — with younger users (18 to 29) by far the most likely to say they had deleted the Facebook app in the past year.

So privacy alive and kicking among the young then — not, er, dead.

Similarly, Pew found that older users are much less likely to say they have adjusted their Facebook privacy settings in the past 12 months: Only a third of users 65 and older have done this vs a full 64% of younger users.

It also found that only around one-in-ten Facebook users (9%) have downloaded the personal data about them available on Facebook — a possibility which was given extra publicity by Zuckerberg’s testimony to US lawmakers.

Despite this group representing a relatively small slice as a share of the Facebook population, Pew couches these users as “highly privacy-conscious” — saying roughly half (47%) of the users who have downloaded their personal data from Facebook have deleted the app from their cellphone, while 79% have elected to adjust their privacy settings.

Ergo it looks like awareness of data risks strengthens pro-privacy behavior among users. Who’d have thought it?!

Pew’s study was carried out between May 29 and June 11, 2018 — so well after the Cambridge Analytica scandal had snowballed into a major PR crisis for Facebook — with the research firm polling more than 4,500 respondents

A separate Pew study, also conducted at the same time and published today, suggests many US users don’t understand how Facebook’s News Feed works.

The survey found that “notable shares” of Facebook users ages 18 and older lack a clear understanding of how the site’s News Feed operates; feel they have little control over what appears there; and have not actively tried to influence the content the feed delivers to them.

When asked whether they understand why certain posts but not others are included in their news feed, around half of U.S. adults who use Facebook (53%) say they do not — with a fifth (20%) saying they do not understand the feed at all well.

Older users are especially likely to say they do not understand the workings of the News Feed: Just 38% of Facebook users ages 50 and older say they have a good understanding of why certain posts are included in it vs 59% of users ages 18 to 29.

While Facebook offers some tools for users to control what they see in the Feed, such as by giving priority to certain people or hiding posts from others, Pew found that just 14% of Facebook users believe ordinary users have a lot of control over the content that appears there – and twice that share (28%) said they felt they have no control.

Older Facebook users in particular feel they have little agency in this regard: Some 37% of Facebook users 50 and older think users have no control over their News Feed, roughly double the share among users ages 18 to 49 (20%), Pew also found.

The existence of user controls on Facebook’s platform was a factor that Zuckerberg deferred to multiple times during his testimony to lawmakers earlier this year, claiming for example that Facebook users have “complete control” over how their information is used as a result of the settings the platform offers them.

Whatever the truth of the substance of that claim, many Facebook users clearly feel they have little control over what they are exposed to on the service — which in turn undermines the company’s claims that it puts its users in the driving seat when it comes to data (be that how their own data is used, or what other data the service exposes them to).

The lack of transparency around algorithmically controlled content platforms is likely to be a key theme in the Senate Select Intelligence Committee’s questioning of the social media execs later today. (More for how to watch the hearings here.)

We’ve reached out to Facebook for comment on Pew’s research. Update: A Facebook spokesperson told us: “People choose their friends and the Pages they follow; and that’s what they see on their News Feed. Posts from friends and Pages they follow are then rank-ordered based on what is most meaningful to them. Helping people better understand this process is really important to us. That’s why we’ve launched an advertising campaign on Facebook, expanded our Help Center resources, and created Inside Feed, a video blog which is devoted to explaining how News Feed works. We know there is more work to do and we’re committed to doing more of it.”",Social Media,TechCrunch,https://techcrunch.com/2018/09/05/facebook-users-becoming-more-cautious-and-critical-says-pew/,"Pew Research Center's recent findings suggest that US Facebook users have become more cautious and critical in their use of the social media platform over the past year. Many users have adjusted their privacy settings, taken a break from the platform, or deleted the app from their cell phones, while a lack of transparency around algorithmically controlled content is a key",Security & Privacy
229,How to Use Social Media Responsibly During a Hurricane,"This story was originally published in September 2018, before Hurricane Florence made landfall in the Carolinas. Its advice, however, is timeless, and we've updated the post accordingly.

Social media can be a lifeline during natural disasters. It has become an essential tool for people to find accurate information---about highway closures, weather forecasts, evacuations---and to contact loved ones and emergency workers. When 911 dispatchers couldn't be reached during Hurricane Harvey in 2017, people trapped by the rising waters used Twitter, Facebook, and even NextDoor to reach authorities.

But even during the best of times, social media is full of misinformation, bad jokes, and other noise. “Everyone who uses Twitter knows it can either be a great source of information or a nightmarish cesspool of bullshit,” says Matt Gertz, a senior fellow at Media Matters for America, a progressive nonprofit that monitors misinformation. During moments of crisis, that misinformation quagmire can make things worse.

Whenever a hurricane hits the US, for example, people circulate falsified news of sharks in the sky. (Let’s be clear: Hurricanes don't contain sharks.) Besides doctored images of sharks swimming next to cars—a hoax that’s been showing up on social media during storms since at least 2015—there are often fake reports advising people to store valuables in the dishwasher (don’t do that, please), as well the now traditional viral Facebook event that since Hurricane Irma has invited people to shoot guns into the storm (again, please don’t do that).

As you monitor social media for news during a hurricane or any natural disaster, remember that social media makes everyone a publisher: With each tweet and retweet you have power to affect events. That has democratized access to information and storytelling but also contributes to information (and misinformation) overload. Like traditional publishers, social media denizens have a responsibility during crises to try not to make things worse. Here are some things to keep in mind while reading and sharing during a natural disaster.

Think Before You Retweet

As with many breaking news events, if your main goal is to understand what is going on during a natural disaster, social media may not be your best bet. You will inevitably see incorrect and incomplete information cross your feed as news unfolds and people try to piece together what is happening.

We get it, though—it can be hard to tear yourself away from Twitter and Facebook when something newsworthy is taking place. If that’s the case, then you need to be responsible. Read and watch developing news with a critical eye (WNYC's ""Breaking News Consumer's Handbook"" has some solid advice). And think before you retweet. The worst thing you can do is spread misinformation.

""You need to hold yourself accountable, too. If you see a Twitter feed that you don’t know tweeting information or images, be careful before you push that along."" Matt Gertz, Media Matters for America

""Make sure, if you are going to repost something, that the source is credible, number one, because a lot of hysteria happens,"" Steven Stalinsky, of the Middle East Media Research Institute, told WIRED in 2017 about how to behave online during breaking news. Responsible journalists vet information before repeating it and rely on primary sources rather than hearsay. You can do something similar on social media.

Beware of Videos and Images

Before you share what appears to be footage from the storm, double check who is sharing it and see if they cite a reliable source. Even if it might not be doctored---although if it’s a shark flying, it’s doctored!---you might be looking at a picture or video from a different storm or natural disaster.",Social Media,WIRED,https://www.wired.com/story/how-to-use-social-media-responsibly-during-storm/,"Social media can provide lifelines during natural disasters, but it can also spread misinformation and hysteria. People should be careful before they share information and make sure that the source is credible.","Information, Discourse & Governance"
230,"US legislator, David Cicilline, joins international push to interrogate platform power – TechCrunch","US legislator David Cicilline will be joining the next meeting of the International Grand Committee on Disinformation and ‘Fake News’, it has been announced. The meeting will be held in Dublin on November 7.

Chair of the committee, the Irish Fine Gael politician Hildegarde Naughton, announced Cicilline’s inclusion today.

The congressman — who is chairman of the US House Judiciary Committee’s Antitrust, Commercial, and Administrative Law Subcommittee — will attend as an “ex officio member” which will allow him to question witnesses, she added.

Exactly who the witnesses in front of the grand committee will be is tbc. But the inclusion of a US legislator in the ranks of a non-US committee that’s been seeking answers about reining in online disinformation will certainly make any invitations that get extended to senior executives at US-based tech giants much harder to ignore.

Naughton points out that the addition of American legislators also means the International Grand Committee represents ~730 million citizens — and “their right to online privacy and security”.

“The Dublin meeting will be really significant in that it will be the first time that US legislators will participate,” she said in a statement. “As all the major social media/tech giants were founded and are headquartered in the United States it is very welcome that Congressman Cicilline has agreed to participate. His own Committee is presently conducting investigations into Facebook, Google, Amazon and Apple and so his attendance will greatly enhance our deliberations.”

“Greater regulation of social media and tech giants is fast becoming a priority for many countries throughout the world,” she added. “The International Grand Committee is a gathering of international parliamentarians who have a particular responsibility in this area. We will coordinate actions to tackle online election interference, ‘fake news’, and harmful online communications, amongst other issues while at the same time respecting freedom of speech.”

The international committee met for its first session in London last November — when it was forced to empty-chair Facebook founder Mark Zuckerberg who had declined to attend in person, sending UK policy VP Richard Allan in his stead.

Lawmakers from nine countries spent several hours taking Allan to task over Facebook’s lack of accountability for problems generated by the content it distributes and amplifies, raising myriad examples of ongoing failure to tackle the democracy-denting, society-damaging disinformation — from election interference to hate speech whipping up genocide.

A second meeting of the grand committee was held earlier this year in Canada — taking place over three days in May.

Again Zuckerberg failed to show. Facebook COO Sheryl Sandberg also gave international legislators zero facetime, with the company opting to send local head of policy, Kevin Chan, and global head of policy, Neil Potts, as stand ins.

Lawmakers were not amused. Canadian MPs voted to serve Zuckerberg and Sandberg with an open summons — meaning they’ll be required to appear before it the next time they step foot in the country.

Parliamentarians in the UK also issued a summons for Zuckerberg last year after repeat snubs to testify to the Digital, Culture, Media and Sport committee’s enquiry into fake news — a decision that essentially gave birth to the international grand committee, as legislators in multiple jurisdictions united around a common cause of trying to find ways to hold social media giants to accounts.

Delighted to hear that @davidcicilline will be joining us for the next International Grand Committee on disinformation in Dublin in November https://t.co/TI07XEVwLm @CommonsCMS — Damian Collins (@DamianCollins) August 15, 2019

While it’s not clear who the grand committee will invite to the next session, Facebook’s founder seems highly unlikely to have dropped off their list. And this time Zuckerberg and Sandberg may find it harder to turn down an invite to Dublin, given the committee’s ranks will include a homegrown lawmaker.

In a statement on joining the next meeting, Cicilline said: “We are living in a critical moment for privacy rights and competition online, both in the United States and around the world. As people become increasingly connected by what seem to be free technology platforms, many remain unaware of the costs they are actually paying.

“The Internet has also become concentrated, less open, and growingly hostile to innovation. This is a problem that transcends borders, and it requires multinational cooperation to craft solutions that foster competition and safeguard privacy online. I look forward to joining the International Grand Committee as part of its historic effort to identify problems in digital markets and chart a path forward that leads to a better online experience for everyone.”

Multiple tech giants (including Facebook) have their international headquarters in Ireland — making the committee’s choice of location for their next meeting a strategic one. Should any tech CEOs thus choose to snub an invite to testify to the committee they might find themselves being served with an open summons to testify by Irish parliamentarians — and not being able to set foot in a country where their international HQ is located would be more than a reputational irritant.

Ireland’s privacy regulator is also sitting on a stack of open investigations against tech giants — again with Facebook and Facebook owned companies producing the fattest file (some 11 investigations). But there are plenty of privacy and security concerns to go around, with the DPC’s current case file also touching tech giants including Apple, Google, LinkedIn and Twitter.",Social Media,TechCrunch,https://techcrunch.com/2019/08/15/us-legislator-david-cicilline-joins-international-push-to-interrogate-platform-power/,"Social media has caused a variety of issues, from election interference and hate speech to lack of privacy and security for users, and the International Grand Committee is convening to tackle these issues. US legislator David Cicilline will be attending the next meeting, held in Dublin, to ensure a coordinated effort to hold tech giants accountable.",Security & Privacy
231,Facebook's Continued Removal of Breastfeeding Pictures,"When I was pregnant with my daughter, my husband and I decided that I was going to try to breastfeed. Our decision was purely economic as we knew that formula was very expensive and breastfeeding was free. When my daughter was born, she latched on in the recovery room and I was able to breastfeed without any issues.

Those first months of her life, she would nurse a lot while I surfed the internet. I kept reading more and more things about how nursing moms were harassed in public for nursing, even though it is perfectly legal. With the advent of social media, nursing moms would use Facebook to help organize nurse-ins, which are protests where a group of moms gather and nurse their babies in public.

But then Facebook started taking down pictures of nursing babies. My daughter was very small when this started happening because I remember reading about the outrage of fellow nursing moms. There was even a movement that asked breastfeeding moms to change their profile picture to one of their baby nursing. I did this, but my picture was never taken down and has sat in its own folder, Nursing, on Facebook for all these years.

Since my daughter weaned a little less than a year ago, I haven't been following the breastfeeding news as much. But recently a c|net article talked about how Facebook is still removing pictures of nursing babies. Facebook has said that the reason they are taking down nursing pictures is because of the amount of teenage Facebook users. I personally think this is pretty weak excuse since most nursing pictures show less boob than a low-cut shirt or dress.

You can read the article at c|net's website. What do you all think about Facebook removing nursing baby pictures?",Social Media,WIRED,https://www.wired.com/2012/02/facebooks-continued-removal-of-breastfeeding-pictures/,"Facebook's policy of removing pictures of nursing babies is creating outrage amongst breastfeeding moms, due to the lack of respect for their rights to feed their babies in public. This policy is considered to be unacceptable by many, as it does not take into account the fact that most nursing pictures show less boob than a low-cut shirt or dress.",Equality & Justice
232,A young startup with a timely offer: fighting propaganda campaigns online – TechCrunch,"The prevalence of so-called fake news is far worse than we imagined even a few months ago. Just last week, Twitter admitted there were more than 50,000 Russian bots trying to confuse American voters ahead of the 2016 presidential election.

It isn’t just elections that should concern us, though. So argues Jonathon Morgan, the co-founder and CEO of New Knowledge, a two-and-a-half-year-old, Austin-based cybersecurity company that’s gathering up clients looking to fight online disinformation. (Worth noting: The 15-person outfit has also quietly gathered up $1.9 million in seed funding led by Moonshots Capital, with participation from Haystack, GGV Capital, Geekdom Fund, Capital Factory and Spitfire Ventures.)

We talked earlier this week with Morgan, a former digital content producer and State Department counterterrorism advisor, to learn more about his product, which is smartly using concerns about fake social media accounts and propaganda campaigns to work with brands eager to preserve their reputation. Our chat has been edited lightly for length and clarity.

TC: Tell us a little about your background.

JM: I’ve spent my career in digital media, including as a [product manager] at AOL when magazines were moving onto the internet. Over time, my career moved into machine-learning and data science. During the early days of the application-focused web, there wasn’t a lot of engineering talent available, as it wasn’t seen as sophisticated enough. People like me who didn’t have an engineering background but who were willing to spend a weekend learning JavaScript and could produce code fast enough didn’t really need much of a pedigree or experience.

TC: How did that experience lead to you focusing on tech that tries to understand how social media platforms are manipulated?

TC: When ISIS was employing techniques to jam conversations into social media, conversations that were elevated in the American press, we started trying to figure out how they were pushing their message. I did a little work for the Brookings Institution, which led to some work as a data science advisor to the State Department — developing counterterrorism strategies and understanding what public discourse looks like online and the difference between mainstream communication and what that looks like when it’s been hijacked.

TC: Now you’re pitching this service you’ve developed with your team to brands. Why?

JM: The same mechanics and tactics used by ISIS are now being used by much more sophisticated actors, from hostile governments to kids who are coordinating activity on the internet to undermine things they don’t like for cultural reasons. They’ll take Black Lives activists and immigration-focused conservatives and amplify their discord, for example. We’ve also seen alt-right supporters on 4chan undermine movie releases. These kinds of digital insurgencies are being used by a growing number of actors to manipulate the way that the public has conversations online.

We realized we could use the same ideas and tech to defend companies that are vulnerable to these attacks. Energy companies, financial institutions, other companies managing critical infrastructure — they’re all equally vulnerable. Election manipulation is just the canary in the coal mine when it comes to the degradation of our discourse.

TC: Yours is a SaaS product, I take it. How does it work?

JM: Yes, it’s enterprise software. Our tech analyzes conversations across multiple platforms — social media and otherwise — and looks for signs that it’s being tampered with, identifies who is doing the tampering and what messaging they are using to manipulate the conversation. With that information, our [customer] can decide how to respond. Sometimes it’s to work with the press. Sometimes it’s to work with social media companies to say, “These are disingenuous and even fraudulent.” We then work with the companies to remediate the threat.

TC: Which social media companies are the most responsive to these attempted interventions?

JM: There’s a strong appetite for fixing the problem at all the media companies we talk with. Facebook and Google have addressed this publicly, but there’s action taking place between friends behind closed doors. A lot of individuals at these companies think there are problems that need to be solved, and they are amendable to [working with us].

The challenge for them is that I’m not sure they have a sense for who is responsible for [disinformation much of they time]. That’s why they’ve been slow to address the problem. We think we add value as a partner because we’re focused on this at a much smaller scale. Whereas Facebook is thinking about billions of users, we’re focused on tens of thousands of accounts and conversations, which is still a meaningful number and can impact public perception of a brand.

TC: Who are some of your customers?

JM: We [aren’t authorized to name them but] we sell to companies in the entertainment and energy and finance industries. We’ve also worked with public interest organizations, including the Alliance for Securing Democracy.

TC: What’s the sales process like? Are you looking for shifts in conversations, then reaching out to the companies impacted, or are companies finding you?

JM: Both. Either we discover something or we’ll be approached and do an initial threat assessment to understand the landscape and who might be targeting an organization and from there, [we’ll decide with the potential client] whether there’s value in them in engaging with us in an ongoing way.

TC: A lot of people have been talking this week about a New York Times piece that seemed to offer a glimmer of hope that blockchain platforms will move us beyond the internet as we know it today and away from the few large tech companies that also happen to be breeding grounds for disinformation. Is that the future or is “fake news” here to stay?

JM: Unfortunately, online disinformation is becoming increasingly sophisticated. Advances in AI mean that it will soon be possible to manufacture images, audio and even video at unprecedented scale. Automated accounts that seem almost human will be able to engage directly with millions of users, just like your real friends on Facebook, Twitter or the next social media platform.

New technologies like blockchain that give us robust ways to establish trust will be a part of the solution, if they’re not a magic bullet.",Social Media,TechCrunch,https://techcrunch.com/2018/01/26/a-young-startup-with-a-timely-offer-fighting-propaganda-campaigns-on-social-media/,"Social media has become a breeding ground for disinformation, with foreign governments, alt-right supporters, and other malicious actors using digital insurgencies to manipulate conversations online. This has created a need for companies to protect their reputation by using tools to detect and respond to malicious actors.","Information, Discourse & Governance"
233,"Secret Facebook Groups Are the Trump Era's Worst, Best Echo Chamber","When Emily Farris went online for parenting advice, all she got was anti-vaxxer hogwash or mom shaming. So she created a new parenting group on Facebook, and no one could join without an invitation. ""I wanted a place where smart moms, working moms, moms who 'get it' could ask important questions, drop an F-bomb, and not have to wade through all of the fluff in other mom groups,"" says Farris, a freelance writer based in Missouri.

She built what's called a secret Facebook group, a choice many people are making amid the partisan rancor of the 2016 election and its aftermath. This flight behind figurative closed doors is at odds with the internet's promise of open dialogue, but it is also a reaction to what people like Farris increasingly perceive as the toxicity of that openness.

It is not a choice without risk. The first rule of secret Facebook groups is you do not talk about secret Facebook groups. The second rule of secret Facebook groups is that someone, inevitably, always talks about secret Facebook groups. And much like fight clubs, these groups are based on radical trust---where people of like mind can express their true selves---and have a tendency to slip into solipsism. Still, despite the risks to privacy and the devolution into echo chambers, private Facebook groups may provide an important refuge: places where new ideas can germinate while shielded from the toxicity of the open internet.

The Anti-Twitter

Got an interest? You too could start your own closed group. But odds are, one already exists that suits your needs: fetish, hobby, professional and medical groups, and groups for people who love a certain TV show. Political groups run from liberal to conservative, conspiracy theorist to pragmatist. Some focus on taking action, others on kvetching, still others on exchanging advice. Facebook won't divulge exact numbers but says one billion people actively use Groups each day---a vast majority of its daily active users. A company spokesman points to the now famous pro-Hillary Pantsuit Nation as an example of the popularity of closed and (not-so) secret groups (nearly 3.9 million members). Some, though closed, are searchable. Still others are secret. The only way to find them is to be invited by someone already in the group.

What they have in common is a desire among their members to be with like-minded folks in a place no outsider can penetrate. ""It's lonely always fighting with people. You end up censoring a lot of your ideas,"" says Judith Donath, author of The Social Machine: Designs for Living Online and an expert in how people form communities on the internet. These closed environments where everyone must use their real names are the opposite of Twitter, which in 2016 began to feel mainly like a haven for often-anonymous trolls.

During his farewell address last week, soon-to-be ex-President Obama suggested that if people are so sick of fighting with strangers online maybe they should try talking to them in real life. But as Americans' sense of social cohesion has moved from the church pews and bowling clubs to Facebook and Twitter, the likelihood of people going offline on a grand scale is small. Instead, increasingly, when people are sick of fighting with people online, they find a place online where they can agree with people instead. The easiest place to go is Facebook, because 1.7 billion people are already there.

A Question of Trust

In one secret Facebook group I was added to right before the election, high-profile people sound off on politics in ways that could possibly get them fired—or at least alienate their friends—if their unguarded sentiments went public. These people come from the worlds of academia, media, politics, and prominent law firms. When they post, they are depending on a social contract that any member of the group could violate at any moment---like, say, a journalist. I would have no ethical obligation as a journalist to respect the ""secret"" nature of this group if someone said something newsworthy there—these groups aren't off the record. So far no one has; the chatter is passionate and silly and mostly irrelevant. In my estimation of this particular group, there is no public interest in airing their particular concerns, while the risk to them is clear. But their blind trust in me is misguided. If they were to reveal something the public should know, I would publish it here.

At the same time, this radical trust is part of the appeal of these groups. ""There is something very pleasant about becoming a member of a group and developing relationships that have increasing amounts of trust,"" says Donath. But this means membership itself is a risk. Your privacy could be breached at any moment.

That's what many members feel happened with Pantsuit Nation, whose creator Libby Chamberlain is currently facing backlash after announcing she had signed a book deal about the now-not-so-secret group, a book that would include posts from members who agreed. The press had already outed her group, but many viewed her monetary gain as a betrayal. (She did not respond to questions.)",Social Media,WIRED,https://www.wired.com/2017/01/secret-facebook-groups-trump-eras-worst-best-echo-chamber/,"The main undesirable consequence of Social Media discussed here is the risk of privacy breach. People who join secret Facebook groups are trusting that their conversations will remain private, but they are at risk of having their conversations revealed to the public at any moment, either through a malicious member of the group or through a journalist. This risk of privacy breach can lead",Security & Privacy
234,How Facebook has reacted since the data misuse scandal broke – TechCrunch,"Facebook founder Mark Zuckerberg will be questioned by US lawmakers today about the “use and abuse of data” — following weeks of breaking news about a data misuse scandal dating back to 2014.

The Guardian published its first story linking Cambridge Analytica and Facebook user data in December 2015. The newspaper reported that the Ted Cruz campaign had paid UK academics to gather psychological profiles about the US electorate using “a massive pool of mainly unwitting US Facebook users built with an online survey”.

Post-publication, Facebook released just a few words to the newspaper — claiming it was “carefully investigating this situation”.

Yet more than a year passed with Facebook seemingly doing nothing to limit third party access to user data nor to offer more transparent signposting on how its platform could be — and was being — used for political campaigns.

Through 2015 Facebook had actually been ramping up its internal focus on elections as a revenue generating opportunity — growing the headcount of staff working directly with politicians to encourage them to use its platform and tools for campaigning. So it can hardly claim it wasn’t aware of the value of user data for political targeting.

Yet in November 2016 Zuckerberg publicly rubbished the idea that fake news spread via Facebook could influence political views — calling it a “pretty crazy idea”. This at the same time as Facebook the company was embedding its own staff with political campaigns to help them spread election messages.

Another company was also involved in the political ad targeting business. In 2016 Cambridge Analytica signed a contract with the Trump campaign. According to former employee Chris Wylie — who last month supplied documentary evidence to the UK parliament — it licensed Facebook users data for this purpose.

The data was acquired and processed by Cambridge University professor Aleksandr Kogan whose personality quiz app, running on Facebook’s platform in 2014, was able to harvest personal data on tens of millions of users (a subset of which Kogan turned into psychological profiles for CA to use for targeting political messaging at US voters).

Cambridge Analytica has claimed it only licensed data on no more than 30M Facebook users — and has also claimed it didn’t actually use any of the data for the Trump campaign.

But this month Facebook confirmed that data on as many as 87M users was pulled via Kogan’s app.

What’s curious is that since March 17, 2018 — when the Guardian and New York Times published fresh revelations about the Cambridge Analytica scandal, estimating that around 50M Facebook users could have been affected — Facebook has released a steady stream of statements and updates, including committing to a raft of changes to tighten app permissions and privacy controls on its platform.

The timing of this deluge is not accidental. Facebook itself admits that many of the changes it’s announced since mid March were already in train — long planned compliance measures to respond to an incoming update to the European Union’s data protection framework, the GDPR.

If GDPR has a silver lining for Facebook — and a privacy regime which finally has teeth that can bite is not something you’d imagine the company would welcome — it’s that it can spin steps it’s having to make to comply with EU regulations as an alacritous and fine-grained response to a US political data scandal and try to generate the impression it’s hyper sensitive to (now highly politicized) data privacy concerns.

Reader, the truth is far less glamorous. GDPR has been in the works for years and — like the Guardian’s original Cambridge Analytica scoop — its final text also arrived in December 2015.

On the GDPR prep front, in 2016 — during Facebook’s Cambridge Analytica ‘quiet period’ — the company itself told us it had assembled “the largest cross functional team” in the history of its family of companies to support compliance.

Facebook and Zuckerberg really has EU regulators to thank for forcing it to do so much of the groundwork now underpinning its response to this its largest ever data scandal.

Below is a quick timeline of how Facebook has reacted since mid March — when the story morphed into a major public scandal…

March 16, 2018: Just before the Guardian and New York Times publish fresh revelations about the Cambridge Analytica scandal, Facebook quietly drops the news that it has finally suspended CA/SCL. Why it didn’t do this years earlier remains a key question

March 17: In an update on the CA suspension Facebook makes a big show of rejecting the notion that any user data was ‘breached’. “People knowingly provided their information, no systems were infiltrated, and no passwords or sensitive pieces of information were stolen or hacked,” it writes

March 19: Facebook says it has hired digital forensics firm Stroz Friedberg to perform an audit on the political consulting and marketing firm Cambridge Analytica. It subsequently confirms its investigators have left the company’s UK offices at the request of the national data watchdog which is running its own investigation into use of data analytics for political purposes. The UK’s information commissioner publicly warns the company its staff could compromise her investigation

March 21: Zuckerberg announces further measures relating to the scandal — including a historical audit, saying apps and developers that do not agree to a “thorough audit” will be banned, and committing to tell all users whose data was misused. “We will investigate all apps that had access to large amounts of information before we changed our platform to dramatically reduce data access in 2014, and we will conduct a full audit of any app with suspicious activity. We will ban any developer from our platform that does not agree to a thorough audit. And if we find developers that misused personally identifiable information, we will ban them and tell everyone affected by those apps. That includes people whose data Kogan misused here as well,” he writes on Facebook.

He also says developers’ access to user data will be removed if people haven’t used the app in three months. And says Facebook will also reduce the data users give to an app when they sign in — to just “your name, profile photo, and email address”.

Facebook will also require developers to not only get approval but also “sign a contract in order to ask anyone for access to their posts or other private data”, he says.

Another change he announces in the post: Facebook will start showing users a tool at the top of the News Feed “to make sure you understand which apps you’ve allowed to access your data” and with “an easy way to revoke those apps’ permissions to your data”.

He concedes that while Facebook already had a tool to do this in its privacy settings people may not have seen or known that it existed.

These sorts of changes are very likely related to GDPR compliance.

Another change the company announces on this day is that it will expand its bug bounty program to enable people to report misuse of data.

It confirms that some of the changes it’s announced were already in the works as a result of the EU’s GDPR privacy framework — but adds: “This week’s events have accelerated our efforts”

March 25: Facebook apologizes for the data scandal with a full page ad in newspapers in the US and UK

March 28: Facebook announces changes to privacy settings to make them easier to find and use. It also says terms of services changes aimed at improving transparency are on the way — also all likely to be related to GDPR compliance

March 29: Facebook says it will close down a 2013 feature called Partner Categories — ending the background linking of its user data holdings with third party data held by major data brokers. Also very likely related to GDPR compliance

At the same time, in an update on parallel measures it’s taking to fight election interference, Facebook says it will launch a public archive in the summer showing “all ads that ran with a political label”. It specifies this will show the ad creative itself; how much money was spent on each ad; the number of impressions it received; and the demographic information about the audience reached. Ads will be displayed in the archive for four years after they ran

April 1: Facebook confirms to us that it is working on a certification tool that requires marketers using its Custom Audience ad targeting platform to guarantee email addresses were rightfully attained and users consented to their data being used them for marketing purposes — apparently attempting to tighten up its ad targeting system (again, GDPR is the likely driver for that)

April 3: Facebook releases the bulk app deletion tool Zuckerberg trailed as coming in the wake of the scandal — though this still doesn’t give users a select all option, but it makes the process a lot less tedious than it was.

It also announces culling a swathe of IRA Russian troll farm pages and accounts on Facebook and Instagram. It adds that it will be updating its help center tool “in the next few weeks” to enable people to check whether they liked or followed one of these pages. It’s not clear whether it will also proactively push notifications to affected users

April 4: Facebook outs a rewrite of its T&Cs — again, likely a compliance measure to try to meet GDPR’s transparency requirements — making it clearer to users what information it collects and why. It doesn’t say why it took almost 15 years to come up with a plain English explainer of the user data it collects

April 4: Buried in an update on a range of measures to reduce data access on its platform — such as deleting Messenger users’ call and SMS metadata after a year, rather than retaining it — Facebook reveals it has disabled a search and account recovery tool after “malicious actors” abused the feature — warning that “most” Facebook users will have had their public info scraped by unknown entities.

The company also reveals a breakdown of the top ten countries affected by the Cambridge Analytica data leakage, and subsequently reveals 2.7M of the affected users are EU citizens

April 6: Facebook says it will require admins of popular pages and advertisers buying political or “issue” ads on “debated topics of national legislative importance” like education or abortion to verify their identity and location — in an effort to fight disinformation on its platform. Those that refuse, are found to be fraudulent or are trying to influence foreign elections will have their Pages prevented from posting to the News Feed or their ads blocked

April 9: Facebook says it will begin informing users if their data was passed to Cambridge Analytica from today by dropping a notification into the News Feed.

It also offers a tool where people can do a manual check

April 9: Facebook also announces an initiative aimed at helping social science researchers gauge the product’s impact on elections and political events.

The initiative is funded by the Laura and John Arnold Foundation, Democracy Fund, the William and Flora Hewlett Foundation, the John S. and James L. Knight Foundation, the Charles Koch Foundation, the Omidyar Network, and the Alfred P. Sloan Foundation.

Facebook says the researchers will be given access to “privacy-protected datasets” — though it does not detail how people’s data will be robustly anonymized — and says it will not have any right or review or approval on research findings prior to publication.

Zuckerberg claims the election research commission will be “independent” of Facebook and will define the research agenda, soliciting research on the effects of social media on elections and democracy

April 10: Per its earlier announcement, Facebook begins blocking apps from accessing user data 90 days after non-use. It also rolls out the earlier trailed updates to its bug bounty program",Social Media,TechCrunch,https://techcrunch.com/2018/04/10/how-facebook-has-reacted-since-the-data-misuse-scandal-broke/,"The main undesirable consequence of social media discussed here is the misuse and abuse of user data, which has been revealed in a data scandal dating back to 2014. Facebook has since taken measures to reduce data access and tighten app permissions and privacy controls on its platform, though much of the groundwork for this was already in place due to the introduction of the",Security & Privacy
235,Don’t Want to Fall for Fake News? Don’t Be Lazy,"On Wednesday night, White House press secretary Sarah Huckabee Sanders shared an altered video of a press briefing with Donald Trump, in which CNN reporter Jim Acosta's hand makes brief contact with the arm of a White House Intern. The clip is of low quality and edited to dramatize the original footage; it's presented out of context, without sound, at slow speed with a close-crop zoom, and contains additional frames that appear to emphasize Acosta's contact with the intern.

And yet, in spite of the clip's dubious provenance, the White House decided to not only share the video but cite it as grounds for revoking Acosta's press pass. ""[We will] never tolerate a reporter placing his hands on a young woman just trying to do her job as a White House intern,"" Sanders said. But the consensus, among anyone inclined to look closely, has been clear: The events described in Sanders' tweet simply did not happen.

This is just the latest example of misinformation roiling our media ecosystem. The fact that it continues to not only crop up but spread—at times faster and more widely than legitimate, factual news—is enough to make anyone wonder: How on Earth do people fall for this schlock?

To put it bluntly, they might not be thinking hard enough. The technical term for this is ""reduced engagement of open-minded and analytical thinking."" David Rand—a behavioral scientist at MIT who studies fake news on social media, who falls for it, and why—has another name for it: ""It's just mental laziness,"" he says.

Misinformation researchers have proposed two competing hypotheses for why people fall for fake news on social media. The popular assumption—supported by research on apathy over climate change and the denial of its existence—is that people are blinded by partisanship, and will leverage their critical-thinking skills to ram the square pegs of misinformation into the round holes of their particular ideologies. According to this theory, fake news doesn't so much evade critical thinking as weaponize it, preying on partiality to produce a feedback loop in which people become worse and worse at detecting misinformation.

The other hypothesis is that reasoning and critical thinking are, in fact, what enable people to distinguish truth from falsehood, no matter where they fall on the political spectrum. (If this sounds less like a hypothesis and more like the definitions of reasoning and critical thinking, that's because they are.)

Several of Rand's recent experiments support theory number two. In a pair of studies published this year in the journal Cognition, he and his research partner, University of Regina psychologist Gordon Pennycook, tested people on the Cognitive Reflection Test, a measure of analytical reasoning that poses seemingly straightforward questions with non-intuitive answers, like: A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost? They found that high scorers were less likely to perceive blatantly false headlines as accurate, and more likely to distinguish them from truthful ones, than those who performed poorly.",Social Media,WIRED,https://www.wired.com/story/dont-want-to-fall-for-fake-news-dont-be-lazy/,"Social media has enabled the spread of misinformation, which people can fall prey to due to mental laziness, influenced by their own political biases. Research shows that people who are more analytically minded are better able to distinguish truth from falsehood, so it is critical to stay vigilant and engage in critical thinking when consuming news media.","Information, Discourse & Governance"
236,Yesterday’s Earthquake Caused More Twitter Traffic than Bin Laden’s Death,"What happens when you mix a relatively mild seismic event with an extremely dense population of the tech-savvy and self-centered east coast? Twitter-splosion! Yesterday's quake let loose 5,500 tweets per second, beating Dead Osama and tying Fukushima's 9.0. Priorities!

[partner id=""gizmodo""]Between the marble and brick federal egocentrism of DC and the media self-fellation of New York, it's no wonder there were 40,000 tweets regarding the 5.8 tremor within one minute of it happening—and that they reached us in New York before the ground started to shake. People love talking about themselves, especially when they are shaking because of an earthquake.

But for many soft, sensitive east coasters like me, it was also our first earthquake. And that's kind of a big deal! And a particularly rare one for this side of the continent. But still—the fact that yesterday trumped the surprise killing of the most wanted, most heinous terrorist in the history of the world and a unfathomably powerful earthquake that triggered an ongoing nuclear disaster says something about Twitter. It's at its most popular when it gives us a mirror to look into. And next to that mirror is a megaphone to scream at everyone about it. And nothing makes for a good ego screamin' like a non-dangerous earthquake. [Twitter]

By +Sam Biddle, Gizmodo

See Also:",Social Media,WIRED,https://www.wired.com/2011/08/twitter-traffic-quake-osama/,"The undesirable consequence of Social Media discussed here is that it gives people a platform to prioritize talking about themselves, even in the face of major world events such as the killing of Osama bin Laden and the Fukushima earthquake. This self-centered attitude can lead to a lack of empathy and understanding for others.",Social Norms & Relationships
237,President Obama warns against getting ‘cocooned’ in bias via social media – TechCrunch,"Former President Barack Obama sat down with the UK’s Prince Harry for an extended and far-ranging interview with the BBC this week, and their conversation touched on social media, the use thereof, and Obama’s take on what the current state of social media means for human discourse.

The full interview covers a lot of ground, but the breakouts regarding social media include an admonition against those “in leadership” using it in ways that prevent establishing “a common space on the internet,” which seems an oblique reference to Donald Trump and his use of Twitter, which is often divisive, and seemingly intentionally so.

“One of the dangers of the internet is that people can have entirely different realities,” Obama told the Prince, according to the BBC. “They can be cocooned in information that reinforces their current biases.”

Obama never overtly named Trump in his comments, but he did make reference to a need for us to “harness this technology in a way that allows a multiplicity of voices” without leading to “a Balkinisation of society,” per the news agency’s transcript.

The former U.S. President didn’t go so far as to completely condemn social media — in fact, he referenced it as a “really powerful tool for people of common interest to convene and get to know each other and connect.” But, he also said that people should then take that further and meet and become familiar in public spaces, too, in order to deepen their mutual understanding.",Social Media,TechCrunch,https://techcrunch.com/2017/12/27/president-obama-warns-against-getting-cocooned-in-bias-via-social-media/,"Obama warned against the misuse of Social Media by those in leadership, which often serves to divide people into their own separate realities, unable to understand one another. He suggested that while Social Media is a powerful tool, it should be used to facilitate connection and deeper understanding in public spaces.","Information, Discourse & Governance"
238,Welcome to TikTok’s endless cycle of censorship and mistakes,"It’s not necessarily a surprise that these videos make news. People make their videos because they work. Getting views has been one of the more effective strategies to push a big platform to fix something for years. Tiktok, Twitter, and Facebook have made it easier for users to report abuse and rule violations by other users. But when these companies appear to be breaking their own policies, people often find that the best route forward is simply to try to post about it on the platform itself, in the hope of going viral and getting attention that leads to some kind of resolution. Tyler’s two videos on the Marketplace bios, for example, each have more than 1 million views.

""Content’s getting flagged because they are someone from a marginalized group who is talking about their experiences with racism. Hate speech and talking about hate speech can look very similar to an algorithm.” Casey Fiesler, University of Colorado, Boulder

“I probably get tagged in something about once a week,” says Casey Fiesler, an assistant professor at the University of Colorado, Boulder, who studies technology ethics and online communities. She’s active on TikTok, with more than 50,000 followers, but while not everything she sees feels like a legitimate concern she says the app’s regular parade of issues is real. TikTok has had several such errors over the past few months, all of which have disproportionately impacted marginalized groups on the platform.

MIT Technology Review has asked TikTok about each of these recent examples, and the responses are similar: after investigating, TikTok finds that the issue was created in error, emphasizes that the blocked content in question is not in violation of their policies, and links to support the company gives such groups.

The question is whether that cycle—some technical or policy error, a viral response and apology—can be changed.

Solving issues before they arise

“There are two kinds of harms of this probably algorithmic content moderation that people are observing,” Fiesler says. “One is false negatives. People are like, ‘why is there so much hate speech on this platform and why isn’t it being taken down?’”

The other is a false positive. “Their content’s getting flagged because they are someone from a marginalized group who is talking about their experiences with racism,” she says. “Hate speech and talking about hate speech can look very similar to an algorithm.”

Both of these categories, she noted, harm the same people: those who are disproportionately targeted for abuse end up being algorithmically censored for speaking out about it.

TikTok’s mysterious recommendation algorithms are part of its success—but its unclear and constantly changing boundaries are already having a chilling effect on some users. Fiesler notes that many TikTok creators self-censor words on the platform in order to avoid triggering a review. And although she’s not sure exactly how much this tactic is accomplishing, Fielser has also started doing it, herself, just in case. Account bans, algorithmic mysteries, and bizarre moderation decisions are a constant part of the conversation on the app.",Social Media,MIT,https://www.technologyreview.com/2021/07/13/1028401/tiktok-censorship-mistakes-glitches-apologies-endless-cycle/,"The use of algorithms to moderate content on Social Media has resulted in a chilling effect on some users, who self-censor to avoid triggering a review. Such moderation decisions can disproportionately target marginalized groups, leading to false positives and false negatives in the process.",Equality & Justice
239,An Army of Volunteers Is Taking On Vaccine Disinformation Online,"As researchers, pharma companies, and governments around the world are racing to make a vaccine against the pandemic coronavirus in record time, there’s a growing concern that many Americans won’t want it when it arrives. In a series of recent polls, only about half of US adults say they would get a Covid-19 vaccine, even though more than 1,000 people are still dying from the disease every day in the US. Some of those surveyed are rightly concerned about the perils of rushed science. But according to one poll conducted by Yahoo News/YouGov, more than a quarter of Americans would decline a shot in part because they believe Microsoft cofounder Bill Gates is trying to slip them a microchip.

Everything You Need to Know About the Coronavirus Here's all the WIRED coverage in one place, from how to keep your children entertained to how this outbreak is affecting the economy.

Conspiracy theories thrive in times of great uncertainty, and the coronavirus pandemic has proven to be a petri dish for particularly harmful ones. This one can be traced back to May 4, when a little-known filmmaker named Mikki Willis posted a 26-minute video called Plandemic to Facebook, YouTube, Vimeo, and a designated website. It featured a discredited scientist describing a bizarre, unsubstantiated plot by global elites like Gates to use a vaccine against the virus to seize power. These ideas, hailing from many sources, had already been swirling on many parts of the internet and were congealing into a narrative involving Gates and microchips, but the Plandemic video became their biggest signal boost. According to an analysis by The New York Times, the video spent about three days incubating on Facebook pages dedicated to conspiracy theories and the anti-vaccine movement. Then, like any efficient pathogen, it went viral. Just a week after its release, the now widely debunked video had been viewed more than 8 million times.

But it didn’t have to be that way. So says Joe Smyser, CEO of the Public Good Projects, or PGP, a public health nonprofit that specializes in using social network analysis to implement large-scale behavioral change programs. His group has built online surveillance tools for tracking outbreaks of misinformation, disinformation, and downright conspiracies. He says they saw most of the sharing activity that fueled this particular theory’s eventual virality within the first 24 hours. “It was right there in the data,” he says. “We didn’t have to wait days to respond to it, because the outcome was predictable. What was lacking was coordination.”

Smyser wants to bring coordination to combating a growing anti-vaccine movement that contributed to a record outbreak of measles last year—the worst in four decades. This week, his organization is launching a vaccine advocacy campaign unlike any other before.

Called Stronger, it aims to take the fight to anti-vaccine organizers where they’ve long had the upper hand: on social media. To do so, PGP plans to conscript the vast but largely silent majority of Americans who support vaccines into any army of keyboard warriors trained to block, hide, and report vaccine misinformation. (According to a recent Gallup poll, 84 percent of Americans say vaccinating children is important.) The effort is backed by a number of pro-immunization coalitions and the National Foundation for Infectious Diseases, and has funding from BIO, the world’s largest biotechnology lobbying group. “We have this tradition in the US that vaccines are solely the domain of public health workers who are trained to not get into fights,” says Smyser. “I think that’s a very antiquated perspective, and it’s left those on the public health side completely outgunned in this new era of social media.”",Social Media,WIRED,https://www.wired.com/story/can-a-keyboard-crusade-stem-the-vaccine-infodemic/,"The lack of coordination in responding to misinformation on social media has led to the rapid spread of conspiracy theories, such as the one linking Bill Gates and a coronavirus vaccine to microchips, which has impeded efforts to get people to receive a vaccine. To combat this, a new campaign, Stronger, is aiming to recruit people","Information, Discourse & Governance"
240,Twitter's Winning the War on Harassment—So Says Twitter,"When people get harassed on Twitter—which happens all the time—it's terrible for them, and also for the company, which has earned a reputation as a place that doesn't take abuse seriously. For years it hasn't focused on fixing the problem of harassment in any meaningful way. Then six months ago, Ed Ho, Twitter’s general manager of consumer product and engineering, vowed to address the problem “in days and hours, not weeks and months.” What followed were a slew of updates—seemingly every month or so. Today, in a blog post, Ho announced what affect those updates have had on curbing Twitter harassment.

Based on its internal data, Twitter appears to have made pretty good progress. According to Ho, Twitter is taking action on “ten times the number of abusive accounts every day compared to the same time last year.” It suspends and limits the reach of thousands more abusive accounts each day, and over the past four months, Ho says, new tech deployed by Twitter has been able to weed out and remove twice the number of repeat offenders—those abusive Twitter users who find themselves suspended then promptly create a new account to continue harassing someone under a new name. But Twitter did not disclose the year-over-year drop in total abuse reports, or release any kind of baseline—making it hard to know how much harassment was actually eliminated from the platform as a result of the company's efforts.

Ho also highlights in the post a team of humans at Twitter who work reviewing reports. Twitter says it is now doing a better job of communicating to people who are accused of harassment on the site. For one thing, when it restricts someone's account over abusive content, Twitter informs the account owner that they have been put into a time-out phase, and why. During this Twitter ""timeout,"" only the followers of the account can see what these users have said—and Twitter can enforce this penalty even when no one has reported these accounts as abusive. Twitter says accounts in this kind of timeout, meanwhile, have subsequently received 25-percent fewer abuse reports. More than half of the accounts accused of abusive behavior receive this warning state just once, Twitter says, which could indicate that people learn their lesson. But by this calculation, about one in three abusive accounts repeats abusive behavior, a proportion that highlights how much work there is left to do.

James Grimmelmann, a law professor who studies social networks at Cornell University, cautions that none of these statistics are meaningful on their own. ""Is ten times as many account flags good or bad?"" he asks. ""It depends on how many Twitter was flagging before, and how many they were missing."" Of course, ten harassing tweets aimed at a user is obviously better than 100. But ten harassing tweets is still harassment.

""You want to look at crime rates, and not arrest rates,"" says Grimmelmann.

To really know if Twitter's efforts had a positive effect on people's experience, the company should ask people directly, Grimmelmann says. ""You need some kind of user-facing instrument,"" he says. ""You need a survey of people about their experiences: 'How do you feel Twitter is doing on harassment?'""

Twitter could also do a better job of explaining to people why their reports are not followed up by Twitter's human reviews. A recent BuzzFeed report from earlier this week, recounted that a call for anecdotes about harassment on Twitter yielded 89 messages from people who said they received improper dismissals of harassment reports, including one still active account with 899 abusive tweets all targeted at a female sportswriter. Why didn't these reports yield action? No one knows. Twitter is doing a better job of being transparent with those it blocks about why it has blocked them, but still isn't communicating to people who have reported harassment that doesn't result in action exactly why no action was taken.

Today's report shows Twitter is starting to focus on a problem that's plagued people on its platform since its earliest days. And it's aiming to be transparent—on its own terms. Along with progress, Twitter today reveals it can and must do more.",Social Media,WIRED,https://www.wired.com/story/twitter-harassment-update/,"People on Social Media, especially Twitter, face frequent harassment, which has created a reputation for Twitter as a platform that does not take abuse seriously. Twitter has made progress in curbing harassment, but more work is needed to eliminate it completely.",Equality & Justice
241,Facebook Reboots Trending Topics—Again—as Fake News Festers,"Facebook's Trending Topics—an innocuous-seeming little sidebar module that has become the seething epicenter of controversy over bias and fake news—is getting another reboot.

In the past, Facebook might have identified a topic as ""trending"" based on a single, popular article or post. Now Facebook says designating a topic as ""trending"" will depend on how many publishers are posting stories on the same topic, as well as how many people are liking, sharing, and commenting on those articles.

Facebook is trying to address the hits it has taken to its reputation as a venue for spreading disinformation.

A headline from a single article will now accompany each Trending Topic link, selected based on a mix of factors: the number of shares, ""engagement"" on Facebook with the publisher, and whether other articles are link to the piece. Facebook points out that headlines that appear when you hover over or click on a Trending Topic; this tweak makes the headline more visible. The company also says it won't strictly limit which publishers' headlines could potentially accompany a topic—a departure from the list the company reportedly once kept of sites and publications whose links it would allow in Trending Topics.

Lastly, Facebook says it will no longer personalize Trending Topics based on your interests. Instead, the module will give more prominence to the news in your region. ""This is designed to help make sure people don’t miss important topics being discussed on Facebook that might not show up in their News Feed,"" Will Cathcart, a vice-president of product management at Facebook, writes in a blog post.

Facebook 'Bias'

Facebook last overhauled Trending Topics in August while still embroiled in a months-long scandal over bias stemming from a Gizmodo report that found human curators were routinely suppressing conservative news. Senator John Thune (Republican-SD) demanded an explanation from Facebook, and CEO Mark Zuckerberg met with influential conservatives in a gesture of peace-making. (Facebook said an internal investigation found no bias.) Facebook ultimately dropped the human curators in favor of what it called a ""more automated"" approach.

Which didn't go so well, either. Fake news stories proliferated, such as the false claim that ex-Fox News anchor Megyn Kelly had been fired from the television network after pledging support for Hillary Clinton. Now once again the company seems to want to signal that it hasn’t forgotten all the concerns Trending Topics has brought up. The question is how effective these changes will actually be at killing the spread of fake news on the social network.

Yes, Facebook has already taken a few stabs at the problem, including hiring former CNN anchor Campbell Brown to manage its relationships with news organizations. Last month, under withering criticism, Facebook revealed a plan to crack down on fake news by relying heavily on third-party fact checkers and choking off ad dollars to fake news peddlers. Looking to shore up its journalistic bona fides, the company this month unveiled its “Facebook Journalism Project” this month, an effort by the social network to work with news organizations on emerging business models, train journalists on Facebook tools, and visit newsrooms to discuss best practices. All of which go to show that Facebook is trying to address the hits it has taken to its reputation, especially since the election, as a venue for spreading disinformation.

Whether any of it will work is another question. The latest Trending Topics update, for instance, prioritizes aggregated news Google-style, an approach that seems at least possible to game. But the company needs to be seen as doing something, at least if it wants to be viewed as a credible platform for news at all.",Social Media,WIRED,https://www.wired.com/2017/01/facebook-reboots-trending-topics-fake-news-festers/,"Facebook's reputation as a credible platform for news has taken hits due to its role in spreading disinformation, leading to controversy over bias and fake news. Its latest changes to Trending Topics are meant to address this, but it remains to be seen if they will be effective.","Information, Discourse & Governance"
242,"Lacking eyeballs, Facebook’s ad review system fails to spot coronavirus harm – TechCrunch","Facebook’s ad review system is failing to prevent coronavirus misinformation from being targeted at its users, according to an investigation by Consumer Reports.

The not-for-profit consumer advocacy organization set out to test Facebook’s system by setting up a page for a made-up organization, called the Self Preservation Society, and creating ads that contained false or deliberately misleading information about the coronavirus — including messaging that claimed (incorrectly) that people under 30 are “safe”, or that coronavirus is a “HOAX”.

Another of the bogus ads urged people to “stay healthy with SMALL daily doses” of bleach, per the report.

The upshot of the experiment? Facebook’s system waived all the ads through, apparently failing to spot any problems or potential harms. “Facebook approved them all,” writes Consumer Reports. “The advertisements remained scheduled for publication for more than a week without being flagged by Facebook.”

Of course the organization pulled the ads before they were published, saying it made certain no Facebook users were exposed to the false or misleading claims. But the test appears to expose how few barriers there are within Facebook’s current ad review system for picking up and preventing harmful ads targeting the coronavirus pandemic.

The only ad in the experiment Facebook rejected was flagged because of its image, per Consumer Reports — which says it had used a stock shot of a respirator-style face mask. After swapping the image for a “similar alternative” it says Facebook approved that too.

Last month, as part of its own business response to the threat posed by COVID-19, Facebook announced it was sending home all global content reviewers “until further notice” — saying it would be relying on more automated review as a consequence of this decision.

“As we rely more on our automated systems, we may make mistakes,” it wrote then.

Consumer Reports’ investigation highlights how serious those mistakes can be, as a result of Facebook’s decision to lean so heavily on AI moderation — given the company is waiving through clearly harmful messages that urge users to ignore public health advice to stay home and socially distance themselves, or even drink a harmful substance to stay “safe”.

In response to the Consumer Reports investigation Facebook defended itself — saying it has removed “millions” of listings for policy violations related to the coronavirus. Though it also conceded its enforcement around COVID-19 misinformation is far from perfect.

“While we’ve removed millions of ads and commerce listings for violating our policies related to COVID-19, we’re always working to improve our enforcement systems to prevent harmful misinformation related to this emergency from spreading on our services,” a Facebook spokesperson, Devon Kearns, told Consumer Reports.

A Facebook spokeswoman declined to specify how many humans it has working on ad review during the coronavirus crisis when we asked. Though the company told Consumer Reports it has a “few thousand” reviewers now able to work from home.

Back in 2018 Facebook reported having some 15,000 people employed doing content review.

It’s never been clear what proportion of those are focused on (user) content review vs ad review. But a “few thousand” vs 15k suggests there has likely been a very considerable drop in the number of eyeballs checking ads. (Pre-COVID, Facebook also liked to refer to having a safety and security team of over 35,000 people globally — with the 15k reviewers sitting within that.)

Facebook’s content review team has clearly shrunk considerably as a result of coronavirus-related disruption to its business. Though the company is refusing to come clean on exactly how many (few) people it has doing content review right now.

It’s also clear that the risk of harm from tools like Facebook’s ad platform — that can be used to easily and cheaply amplify damaging online disinformation — could hardly be higher than during a pandemic, when there is a pressing need for governments and health authorities to be able to communicate facts, official guidance and best practice to their populations to keep them safe.

Facebook’s platform becoming a conduit for false and/or maliciously misleading messaging risks undermining public health at a critical time.

Last month the company was also revealed to have blocked links to legitimate news and other websites that were sharing coronavirus-related content — following its switch to AI-led moderation.

While, in recent weeks, the company has faced criticism for failing to live up to a pledge to take down ads for coronavirus masks.

At the same time, Facebook’s platform remains a hotbed of user generated coronavirus-related misinformation — with individuals widely reported sharing posts that claim bogus home remedies such as gargling with salt water to kill the virus (it doesn’t) or playing down the seriousness of the COVID-19 pandemic by claiming it’s ‘just the flu’ (it’s not).",Social Media,TechCrunch,https://techcrunch.com/2020/04/08/lacking-eyeballs-facebooks-ad-review-system-fails-to-spot-coronavirus-harm/,"Facebook's ad review system is failing to stop the spread of false or misleading coronavirus-related information, potentially causing serious harm to the public. This risk is heightened by the company's switch to automated review, which is leading to the blocking of legitimate news websites, and the widespread sharing of misinformation by individual users.",Security & Privacy
243,Doctors are now social-media influencers. They aren’t all ready for it.,"You should not, as VanWingen initially suggested, wash your produce with soap—it’s better to just rinse fruits and vegetables in cold water, because soap residue can cause digestive issues. And his suggestion to leave groceries outside or in the garage for a few days before bringing them into your home needed a clarification that this would not be a safe procedure for perishable goods.

VanWingen lobbied YouTube to let him edit the video and remove the portion with potentially harmful advice, but there wasn’t much he could do aside from take the whole thing down. He decided against that, instead littering the video’s description with updates linking to new and more accurate information. But, he says, he still stands by the majority of the advice in the video.

“If you associate Dr. VanWingen with misinformation, that weighs on me extremely heavily,” he says. Compare with others, he says, his mistake was innocent and would be unlikely to have dire consequences. “There are doctors that I’ve seen that are promoting like, for instance, hydroxychloroquine and maybe even promoting fear,” he says, referring to the unproven and, according to the FDA, potentially dangerous covid-19 treatment that was promoted by Trump. “That is certainly not where I would see myself coming from.”

“There are doctors I’ve seen promoting hydroxychloroquine and maybe even promoting fear.”

And the people who can get views for a medical message on social media aren’t necessarily the ones most qualified to craft it. Eric Feigl-Ding, an epidemiologist who now has a large following on Twitter thanks to his evocative tweets about covid-19, has found his expertise and analysis questioned by other epidemiologists.

Varshavski—that is, Doctor Mike—became YouTube’s go-to medical expert after a 2015 Buzzfeed article about his Instagram account dubbed him the “hot doctor.” And although he often stresses to his audience that “expert opinion,” including his, is “the lowest form of evidence,” his viewers are more likely to trust what he says in his videos than they are to track down and read a randomized controlled study on the same topic. That’s not necessarily a bad thing, if the information is sound and clearly presented—and he described his role during the pandemic as essentially turning himself into a mouthpiece and platform for the CDC, the WHO, and leading experts in the field.

But it’s easy to lose that balance.

“If you are a doctor and you’re popular and people look to you for guidance, and you believe your expert opinion without any kind of research to substantiate it outweighs that of the guidance from the CDC and WHO, you’ve crossed the line,” he says.

And that’s the central challenge: people will turn to the internet for information during a health crisis, whether it’s a personal crisis or one facing the entire world. But the best, most accurate information isn’t always packaged and optimized in a way that is appealing to a curious public searching for certainty. For every CDC video about the latest studies on the coronavirus, there’s someone out there claiming to be the one person willing to tell you what “doctors don’t want you to know.” Alongside that is a president amplifying potentially dangerous ideas so that they become significant news stories.

Doctors becoming brands

There’s another challenge facing these doctor-influencers, too: branding and money. Personalities like Doctor Mike can make accurate information interesting by becoming influencers, but they also have to figure out a way to do that without falling into ethical quicksand.

People become famous online by becoming human brands. But “turning ourselves into brands can also drive people in a different direction,” says Chiang. “Some people out there are aligning us with big pharma already. The last thing they want to see is that we are selling a product or idea.”

Varshavski, like many content creators, accepts sponsors for his Instagram and YouTube accounts, but he says he has to make sure that those sponsorships don’t look like medical endorsements. Chiang, who also serves as the chief medical social-media officer for his hospital, has to carefully screen which TikTok challenges he participates in, and the songs he uses with them, to avoid associating his image and that of his profession with something offensive or tasteless. Chiang is informative on TikTok, but he manages to engage effectively with how people already use the app. And that’s not always something doctors are capable of—or interested in trying to learn how to do.

“Historically, there’s never been any sort of teaching in medical training in how to communicate on a public level with our communities and our patients,” he says.

Online fame takes skill and maintenance to a degree that most people underestimate. And especially for doctors and other people who work in fields that are targets of disinformation, there are some more serious risks. Chiang points out that some companies will simply steal content from medical professionals on social media and use it to sell their products. And battling medical misinformation online can anger those who believe in it, potentially endangering the personal safety of doctors who try to take it on.

But Chiang and Varshavski say that the risks are worth it, especially if having more doctors online helps people find better information about their health.

As doctors who are on the internet but treat real patients too, they can see firsthand how misinformation affects people. In one recent weekend Varshavski treated five covid-19 patients with mild symptoms, and each asked for hydroxychloroquine, a risky possible treatment that can cause serious heart issues in some patients. Some told Varshavski that they heard about it on TV.",Social Media,MIT,https://www.technologyreview.com/2020/04/26/1000602/covid-coronavirus-doctors-tiktok-youtube-misinformation-pandemic/,"The risks of doctors using social media to provide medical advice are real; from potentially harmful advice given without adequate research, to the danger of being associated with misinformation, to the risks of companies stealing content and using it to sell products, to the risks of facing public backlash for battling medical misinformation.","Information, Discourse & Governance"
244,"Facebook, Twitter Under Fire From Activist Investors","A big pension fund and an activist investment firm Thursday said they had filed shareholder proposals pushing Facebook and Twitter to take more responsibility for managing content on their platforms, including mistreatment of women, fake news, election interference, violence, and hate speech---in other words, the same issues that have kept social-media giants in the crosshairs for the past year.

The resolutions were filed by New York State Common Retirement Fund, the nation’s third-largest public-pension fund, with assets of more than $200 billion, and Arjuna Capital, an activist-investment firm known for pressuring companies in tech, banking, and retail to pay men and women equally. The proposals accuse both companies of responding to concerns with inadequate disclosures and content policies that “seem reactive, not proactive.”

The resolutions are the latest sign that concerns about potential ill effects spawned by a handful of giant technology companies have moved from Washington to Wall Street. On Sunday, a pair of activist investors — the California State Teachers’ Retirement System (CalSTRS) and the hedge fund Jana Partners — targeted Apple with a public letter urging the company to address a growing body of research that smartphones can harm children’s mental and physical health. In a response, Apple promised new tools for parents, and said, “We take this responsibility very seriously and we are committed to meeting and exceeding our customers' expectations, especially when it comes to protecting kids.”

Jana Partners’ role in the highly publicized letter took some by surprise because the hedge fund usually wages those kind of public campaigns to restructure companies, not crusade for social issues---a sign that financially focused investors believe concerns about smartphone addiction in children could affect Apple’s bottom line.

Unlike the letter, the shareholder resolutions filed Thursday come with a legal obligation for the companies to respond. They aim to force votes by the companies’ shareholders at their upcoming annual meetings. But the companies may ask the Securities and Exchange Commission to allow them to exclude the proposals, requests that the commission was more inclined to grant last year. Facebook and Twitter did not immediately respond to requests for comment.

Thursday’s resolutions are revised versions of shareholder proposals that Arjuna Capital filed in October, wiith Illinois State Treasurer. But the addition of the New York pension fund adds significant heft to their demands.

Michael Connor, executive director of Open Mic, a nonprofit group that uses shareholder engagement to increase accountability at media and tech companies, says past attempts to engage with Facebook have not been fruitful. Illinois’ treasurer, for example, could only get a 15-minute phone call. “They don’t seem to sense any real accountability to outside stakeholders. They will humor you, there will tell you that they respect your opinion, but they don’t take you seriously,” he said in an earlier interview with WIRED.

Connor dismissed Facebook CEO Mark Zuckerberg’s recent statements that the company is adding employees to review content, which may impact profits. “If they say we’re going to add another 3,000 employees, you have to ask yourself why 3,000 and not 4,000? We don’t know. We have no idea what the most influential social-media player in the world thinks is necessary to come up with a responsible product,” Connor says.

In Thursday’s proposals, shareholders are asking Facebook and Twitter for reports assessing whether the company’s own terms of service agreements have been effectively enforced and information about the risk to the company’s finances, operations, and reputation posed by their policies and “content management controversies.”

On a conference call with reporters, Patrick Doherty, co-director of corporate governance for New York State Patrick DiNapoli, who is a trustee of the state pension fund, mentioned traveling to Russia a few years ago and talking to people about what life had been like under the communist regime. “After awhile, they just tuned out Pravda,” because a lot of it was misinformation or disinformation. “That’s our major concern that people are going to lose faith in these platforms in the long term.”",Social Media,WIRED,https://www.wired.com/story/investors-join-calls-for-facebook-twitter-to-take-more-responsibility/,"The concern among investors is that Social Media companies have been slow to address the risks posed by fake news, hate speech, violence, and other troubling content, and that people may eventually lose faith in the platforms, leading to serious financial and reputational consequences.","Information, Discourse & Governance"
245,"Zuckerberg must face public scrutiny over latest data breach, say UK MPs – TechCrunch","UK members of parliament have once again called for Facebook’s founder, Mark Zuckerberg, to travel to the country to face questions about how his business operates.

They’re renewing calls for facetime with the Facebook CEO in light of the massive data breach it disclosed on Friday — which the company said could affect as many as 90 million users, with 50M confirmed to have been compromised. It’s not clear exactly how many UK (or European) accounts are involved at this stage.

Facebook said on Friday that it had fixed the flaws, which were introduced after an update in July, and had been exploited by hackers to swipe access tokens. Attackers had been able to use its APIs to scrape some user data, it also said. It reset all potentially affected tokens once it discovered the hack late last month.

Damian Collins, who chairs a UK parliamentary select committee which, earlier this year, spent several months this year interrogating data protection issues, and recently called for a levy on social media platforms to help defend democratic institutions from online disinformation, told the Telegraph: “Facebook’s latest data breach demonstrates more clearly than ever why Mark Zuckerberg should face public scrutiny about the practices and policies his company employs to keep British users’ data safe.”

Julian Knight, another member of the committee, also said: “It would be helpful to hear from Mr Zuckerberg, but I won’t be holding my breath.”

Earlier this year MPs on the Department for Digital, Culture, Media and Sport (DCMS) select committee appealed for Zuckerberg to personally give evidence as they scrutinized the impact of online disinformation on democractic processes. However Facebook repeatedly declined to send its founder — instead sending some alternative staffers, including — finally — its CTO.

The committee was not satisfied, complaining that the reps it sent were unable to answer their questions. Collins also slammed the company for what he described as an evasive “pattern of behaviour” — and “a desire to hold onto information and not disclose it”.

It also kept up its pressure for Zuckerberg to testify — offering the chance for him to answer questions remotely, via video link. Still Facebook declined.

In May, in a pretty extraordinary development, the DCMS committee then told Facebook that if its founder stepped foot on UK soil they would issue him with a formal summons.

Safe to say, Zuckerberg made no trips to the UK, although he did attend a meeting of the EU parliament’s conference of presidents towards the end of May (where he was heckled for also avoiding MEPs’ questions).

Given his record of rejecting invitations from the UK parliament, it seems unlikely the company will suddenly offer its CEO up now — to discuss an awkward security breach to boot.

Though Facebook’s lack of engagement with UK politicians might make the government keener to seize on the committee’s recommendation of a social media levy to offset damage caused by tech platforms’ accelerating online disinformation.

We’ve reached out to Facebook with questions and will up date this story with any response.

The data breach is the first that falls clearly under new EU-wide privacy rules which carry beefed up penalties for violations.

On Friday, in a statement commenting on the Facebook hack, the UK’s data protection agency said: “It’s always the company’s responsibility to identify when UK citizens have been affected as part of a data breach and take steps to reduce any harm to consumers. We will be making enquiries with Facebook and our overseas counterparts to establish the scale of the breach and if any UK citizens have been affected.”

The company does appear to have abided by the requirements of GDPR to report major breaches within 72 hours of discovery.",Social Media,TechCrunch,https://techcrunch.com/2018/10/01/zuckerberg-must-face-public-scrutiny-over-latest-data-breach-say-uk-mps/,"This data breach has demonstrated yet again why it is important for Facebook's founder, Mark Zuckerberg, to face public scrutiny about the practices and policies his company employs to keep user data safe. MPs on the Department for Digital, Culture, Media and Sport (DCMS) select committee have repeatedly called for Zuckerberg to personally give evidence but have been repeatedly",Security & Privacy
246,Will Facebook Kill All Future Facebooks?,"In 2010, Foursquare cofounder Naveen Selvadurai believed that his company, and several other social-media upstarts—Twitter, Tumblr, Path—could carve out successful niches against Facebook.

But Facebook had other plans. That year the company introduced a feature that allowed users to “check in” at any location, a copy of the main feature of Foursquare’s app. In response, Selvadurai conceived an “anti-Facebook alliance” of up-and-coming social-media Davids taking on their industry’s Goliath. At a minimum, they could share survival tactics. Selvadurai had informal discussions with friends at Path, Instagram and Twitter, all of which had faced threats of Facebook copying key features. “It was common knowledge, even back then, that Facebook would just approach a company and say something to the effect of, ‘Join us or we will copy you,’ ” he says. More broadly, they believed that Facebook’s closed-off “walled garden,” was hurting the thing they loved most about the open Internet—the fact that anyone could build something that could reach millions of people.

The alliance didn’t get very far. In 2012, Facebook snapped up Instagram for $1 billion. The next year Yahoo acquired Tumblr for $1.1 billion. Path declined in popularity and eventually sold. Foursquare remains independent, but had to split its app into two products and adopt a new strategy.

Selvadurai, who left Foursquare in 2012, cites many reasons for the company’s struggles, from arriving before users were comfortable sharing their locations online to Facebook’s mimickry. “It’s really fun to work” on consumer Internet products, he says. “But why is it getting harder and harder over the years? Because these big players are getting bigger and bigger.”

Now, Facebook is facing challenges on many fronts. Congress is investigating how Russia used Facebook to influence the 2016 election. Privacy activists criticize Facebook’s cooperation with censorious governments. Some regulators believe the company has become too large and powerful. The media industry is wary of its control over distributing content. And, in its own backyard, many in Silicon Valley believe Facebook’s aggressive competitive strategy is stifling innovation.

Since 2012, Facebook has repeatedly copied or acquired social-media apps that gain traction. There’s the Instagram deal, and more astonishingly, its $22 billion acquisition of WhatsApp. Facebook attempted to acquire Snap for $3 billion, was turned down, and made at least 10 attempts to copy its most distinctive features. Last week the company acquired tbh, an anonymous app for teens that has bubbled up in recent months.

Facebook likely found out about tbh through one of its other acquisitions. In 2013, Facebook bought Onavo, an Israeli startup that makes an app that lets people monitor how much mobile data they’re using. After Facebook bought Onavo, it used the aggregated data from its millions of users to track which apps are growing in popularity, the Wall Street Journal reported in August. Onavo data reportedly convinced Facebook it should pursue WhatsApp and copy live video streaming services Periscope and Meerkat.

Facebook isn’t the only Silicon Valley company that competes aggressively against upstarts. Amazon started a price war with Diapers.com, then bought the weakened rival. When Google Maps competitor Waze became popular, the company bought it. But the speed at which Facebook identifies its targets, the amount of money it is willing to pay, and the shamelessness of its copycat products goes beyond its peers.",Social Media,WIRED,https://www.wired.com/story/facebooks-aggressive-moves-on-startups-threaten-innovation/,"Facebook's aggressive competitive strategy has stifled innovation and allowed them to copy or acquire many of their competitors, often resulting in a lack of diversity and choice in the social media market.",Equality & Justice
247,Facebook releases community standards enforcement report – TechCrunch,"Facebook has just released its latest community standards enforcement report and the verdict is in: people are awful, and happy to share how awful they are with the world.

The latest effort at transparency from Facebook on how it enforces its community standards contains several interesting nuggets. While the company’s algorithms and internal moderators have become exceedingly good at tracking myriad violations before they’re reported to the company, hate speech, online bullying, harassment and the nuances of interpersonal awfulness still have the company flummoxed.

In most instances, Facebook is able to enforce its own standards and catches between 90% and over 99% of community standards violations itself. But those numbers are far lower for bullying, where Facebook only caught 14% of the 2.6 million instances of harassment reported; and hate speech, where the company internally flagged 65.4% of the 4.0 million moments of hate speech users reported.

By far the most common violation of community standards — and the one that’s potentially most worrying heading into the 2020 election — is the creation of fake accounts. In the first quarter of the year, Facebook found and removed 2.19 billion fake accounts. That’s a spike of 1 billion fake accounts created in the first quarter of the year.

Spammers also keep trying to leverage Facebook’s social network — and the company took down nearly 1.76 billion instances of spammy content in the first quarter.

For a real window into the true awfulness that people can achieve, there are the company’s self-reported statistics around removing child pornography and graphic violence. The company said it had to remove 5.4 million pieces of content depicting child nudity or sexual exploitation and that there were 33.6 million takedowns of violent or graphic content.

Interestingly, the areas where Facebook is the weakest on internal moderation are also the places where the company is least likely to reverse a decision on content removal. Although posts containing hate speech are among the most appealed types of content, they’re the least likely to be restored. Facebook reversed itself 152,000 times out of the 1.1 million appeals it heard related to hate speech. Other areas where the company seemed immune to argument was with posts related to the sale of regulated goods like guns and drugs.

In a further attempt to bolster its credibility and transparency, the company also released a summary of findings from an independent panel designed to give feedback on Facebook’s reporting and community guidelines themselves.

Facebook summarized the findings from the 44-page report by saying the commission validated Facebook’s approach to content moderation was appropriate and its audits well-designed “if executed as described.”

The group also recommended that Facebook develop more transparent processes and greater input for users into community guidelines policy development.

Recommendations also called for Facebook to incorporate more of the reporting metrics used by law enforcement when tracking crime.

“Law enforcement looks at how many people were the victims of crime — but they also look at how many criminal events law enforcement became aware of, how many crimes may have been committed without law enforcement knowing and how many people committed crimes,” according to a blog post from Facebook’s Radha Iyengar Plumb, head of Product Policy Research. “The group recommends that we provide additional metrics like these, while still noting that our current measurements and methodology are sound.”

Finally the report recommended a number of steps for Facebook to improve, which the company summarized below:

Additional metrics we could provide that show our efforts to enforce our polices such as the accuracy of our enforcement and how often people disagree with our decisions

Further break-downs of the metrics we already provide, such as the prevalence of certain types of violations in particular areas of the world, or how much content we removed versus apply a warning screen to when we include it in our content actioned metric

Ways to make it easier for people who use Facebook to stay updated on changes we make to our policies and to have a greater voice in what content violates our policies and what doesn’t

Meanwhile, examples of what regulation might look like to ensure that Facebook is taking the right steps in a way that is accountable to the countries in which it operates are beginning to proliferate.

It’s hard to moderate a social network that’s larger than the world’s most populous countries, but accountability and transparency are critical to preventing the problems that exist on those networks from putting down permanent, physical roots in the countries where Facebook operates.",Social Media,TechCrunch,https://techcrunch.com/2019/05/23/facebook-releases-community-standards-enforcement-report/,"The prevalence of hate speech, online bullying, harassment, and other forms of interpersonal awfulness on Social Media have become increasingly difficult to moderate and regulate, leading to a decrease in accountability and transparency in the countries where these networks are based.",Equality & Justice
248,Twitter nixed 635k+ terrorism accounts between mid-2015 and end of 2016 – TechCrunch,"Twitter has revealed it suspended a total of 376,890 accounts between July 1 through December 31 last year for violations related to promotion of terrorism. It says the majority of the suspensions (74 percentage) were surfaced via its own “internal, proprietary spam-fighting tools.”

The figures are revealed in a new section of its biannual Transparency Report, which also details government requests to remove content deemed to be promoting terrorism and thus in violation of Twitter’s Terms of Service.

Government TOS requests pertaining to terrorism represented less than 2 percent of all account suspensions in the reported time period, with Twitter saying it received 716 reports, covering 5,929 accounts, and deemed 85 percent to be in violation.

Twitter also notes that it suspended a total of 636,248 accounts over the period of August 1, 2015 through December 31, 2016, and adds that it will be sharing “future updates on our efforts to combat violent extremism by including them in this new section of our transparency report.”

Twitter has previously said 125,000 accounts were suspended for promoting terrorism between mid-2015 and early 2016 — and a further 235,000 suspensions were made for this reason in the six months after that, to August 2016.

In December last year Twitter, Microsoft, Facebook and YouTube also announced a collaboration on a shared industry database aimed at identifying terrorist content spreading across their respective platforms to speed up takedowns.

But it’s fair to say that the issue of terrorist takedowns is just the tip of the political iceberg that has crashed into social media giants in recent times.

Twitter and Facebook, for example, have come under increasing pressure to do more to combat trolling, “fake news” and hate speech circulating on their platforms, especially in the wake of the U.S. election last year — when commentators criticized social media companies of skewing political discourse by incentivizing the sharing of misinformation and enabling the propagation of far right extremist views.

There is a disturbance in the political bot-troll universe. A bunch of them being revived. Hmmm. Which election/event, I do wonder? — zeynep tufekci (@zeynep) December 21, 2016

Twitter’s response to criticism of how it handles accounts that are doling out abuse in tweet-form has so far included updating its abuse policy and adding more muting/filtering tools for users. Although last month it ended up rolling back some additional anti-abuse measures after they were criticized by users.

It also continues to be called out for failing to combat botnets — such as those working to amplify far-right political views via the use of automated spam tactics.

On the issue of political bot-troll armies, it’s fair to say Twitter appears far more agnostic/less interested in taking a stand. A study by two U.S. universities last month suggested 15 percent of Twitter accounts are bots — which gives the veteran “pro-free speech” company a pretty sizable reason to tread carefully here.

And with user growth an ongoing problem for Twitter, it’s hardly going to be keen to nix millions of robot accounts. (Plus, of course, not all bots are seeking to subvert democracy — even if some demonstrably are.)

When one 350,000-strong botnet was discovered in January, the company told the BBC it has a clear policy on automation that is “strictly enforced.” Although it also said it relies on user reports to combat spam.

This month it has also said it’s working on building tools to identify accounts engaging in abusive behavior even where the accounts haven’t been reported by users — such as identifying accounts that are repeatedly tweeting without solicitation at non-followers, or engaging in patterns of abusive behavior.

Automating fighting bots is likely the only approach that will scale to meet the noise of spammers, political or otherwise — yet Twitter is clearly conflicted, with a spokesperson arguing earlier this month that “many bot accounts are extremely beneficial.”

Safe to say the company has shown no appetite for wanting to wade into making fine-grained judgments about what constitutes “beneficial” versus malicious spam. Yet user reporting clearly will not scale to meet the spam challenge. Which means Twitter is going to continue to have a political bot-troll problem.

Additionally, in this — its tenth — Transparency Report, Twitter has added another new section to the legal removals section of the report that covers requests to remove content from verified journalists and other media/news outlets.

“Given the concerning global trend of various governments cracking down on press freedom, we want to shine a brighter light on these requests,” it writes. “During this reporting period, Twitter received 88 legal requests from around the world directing us to remove content posted by verified journalists or news outlet accounts.

“We did not take any action on the majority of these requests, with limited exceptions in Germany and Turkey, the latter of which accounted for 88% of all requests of this nature. For example, we were compelled to withhold Tweets sharing graphic imagery following terror attacks in Turkey in response to a court order.”",Social Media,TechCrunch,https://techcrunch.com/2017/03/21/twitter-nixed-635k-terrorism-accounts-between-mid-2015-and-end-of-2016/,"Social media platforms, such as Twitter and Facebook, are facing increased pressure to tackle issues such as trolling, 'fake news', and hate speech circulating on their platforms. Twitter has suspended hundreds of thousands of accounts for promoting terrorism and other violations of their Terms of Service, and has come under pressure to combat political bot-troll armies and automated",Security & Privacy
249,"'Dating' Site Imports 250,000 Facebook Profiles, Without Permission","How does a unknown dating site, with the absurd intention of destroying Facebook, launch with 250,000 member profiles on the first day?

Simple.

You scrape data from Facebook.

At least, that's the approach taken by two provocateurs who launched Lovely-Faces.com this week, with profiles -- names, locations and photos -- scraped from publicly accessible Facebook pages. The site categorizes these unwitting volunteers into personality types, using a facial recognition algorithm, so you can search for someone in your general area who is ""easy going,"" ""smug"" or ""sly.""

Or you can just search on people's real names.

The duo behind the site say it's art, not commerce.

In what seems to be liberal-arts-grad-schoolese, Paolo Cirio, a media artist, and Alessandro Ludovico, media critic and editor in chief of Neural magazine, explain why they made the site.

""Facebook, an endlessly cool place for so many people, becomes at the same time a goldmine for identity theft and dating -- unfortunately, without the user's control,"" the two explain. ""But that's the very nature of Facebook and social media in general. If we start to play with the concepts of identity theft and dating, we should be able to unveil how fragile a virtual identity given to a proprietary platform can be.""

And, the duo speculate, if people pull hard enough on that bothersome thread, Facebook's $50 billion valuation will unravel.

Facebook, as you might expect, is not amused.

""Scraping people's information violates our terms,"" said Barry Schnitt, Facebook's director of policy communications. ""We have taken, and will continue to take, aggressive legal action against organizations that violate these terms. We're investigating this site and will take appropriate action.""

Facebook's terms of service require those who want to collect data from its pages to apply for permission, which Cirio and Ludovic did not do when they pulled down publicly available profile information on a million Facebook users. (They aren't the first to scrape a million Facebook profiles.)

Cirio and Ludovic say they will take down a user's profile, if a person asks and the site doesn't have any indication they are actually trying to make any money. Instead, it's part of a series of prank sites, the first two of which aimed at Google and Amazon, intended to make people think more about data in the age of internet behemoths.

Moreover, it's a bit funny hearing Facebook complain about scraping of personal data that is quasi-public.

Mark Zuckerberg, the company's founder, made his name at Harvard in 2003 by scraping the names and photos of fellow classmates off school servers to feed a system called FaceMash. With the photos, Zuckerberg created a controversial system that pitted one co-ed against another, by allowing others to vote on which one was better looking.

So even if Facebook's anticipated legal nasty gram makes its way to the duo, who seem to be based somewhere in Europe, they'll have an excellent defense.

""I learned it by watching you, Zuck.""

See Also:- Your Facebook Profile Makes Marketers' Dreams Come True",Social Media,WIRED,https://www.wired.com/2011/02/facebook-dating/,"The launch of an unknown dating site, Lovely-Faces.com, with 250,000 member profiles scraped from publicly accessible Facebook pages demonstrates how vulnerable our virtual identities can be when placed on a proprietary platform like Facebook. Such scraping violates Facebook's terms of service, and the company is taking legal action against organizations that violate those terms.",Security & Privacy
250,Senators press Facebook for answers about why it cut off misinformation researchers – TechCrunch,"Facebook’s decision to close accounts connected to a misinformation research project last week prompted a broad outcry from the company’s critics — and now Congress is getting involved.

A handful of lawmakers criticized the decision at the time, slamming Facebook for being hostile toward efforts to make the platform’s opaque algorithms and ad targeting methods more transparent. Researchers believe that studying those hidden systems is crucial work for gaining insight on the flow of political misinformation.

The company specifically punished two researchers with NYU’s Cybersecurity for Democracy project who work on Ad Observer, an opt-in browser tool that allows researchers to study how Facebook targets ads to different people based on their interests and demographics.

After years of abusing users' privacy, it's rich for Facebook to use it as an excuse to crack down on researchers exposing its problems. I've asked the FTC to confirm that this excuse is as bogus as it sounds. https://t.co/eHuPiVYFe9 — Ron Wyden (@RonWyden) August 4, 2021

In a new letter, embedded below, a trio of Democratic senators are pressing Facebook for more answers. Senators Amy Klobuchar (D-MN), Chris Coons (D-DE) and Mark Warner (D-VA) wrote to Facebook CEO Mark Zuckerberg asking for a full explanation on why the company terminated the researcher accounts and how they violated the platform’s terms of service and compromised user privacy. The lawmakers sent the letter on Friday.

“While we agree that Facebook must safeguard user privacy, it is similarly imperative that Facebook allow credible academic researchers and journalists like those involved in the Ad Observatory project to conduct independent research that will help illuminate how the company can better tackle misinformation, disinformation, and other harmful activity that is proliferating on its platforms,” the senators wrote.

Lawmakers have long urged the company to be more transparent about political advertising and misinformation, particularly after Facebook was found to have distributed election disinformation in 2016. Those concerns were only heightened by the platform’s substantial role in spreading election misinformation leading up to the insurrection at the U.S. Capitol, where Trump supporters attempted to overturn the vote.

In a blog post defending its decision, Facebook cited compliance with FTC as one of the reasons the company severed the accounts. But the FTC called Facebook’s bluff last week in a letter to Zuckerberg, noting that nothing about the agency’s guidance for the company would preclude it from encouraging research in the public interest.

“Indeed, the FTC supports efforts to shed light on opaque business practices, especially around surveillance-based advertising,” Samuel Levine, the FTC’s acting director for the Bureau of Consumer Protection, wrote.",Social Media,TechCrunch,https://techcrunch.com/2021/08/09/facebook-klobuchar-warner-letter-nyu-ad-observatory/,"Facebook's decision to close accounts connected to a research project examining the platform's opaque algorithms and ad targeting methods has sparked outcry from critics and has now drawn the attention of Congress, who are demanding an explanation from the company on why the accounts were terminated and how they violated user privacy.",Security & Privacy
251,"Pew: Most prolific Twitter users tend to be Democrats, but majority of users still rarely tweet – TechCrunch","Pew: Most prolific Twitter users tend to be Democrats, but majority of users still rarely tweet

A new study from Pew Research Center, released today, digs into the different ways that U.S. Democrats and Republicans use Twitter. Based on data collected between November 11, 2019 and September 14, 2020, the study finds that members of both parties tweet fairly infrequently, but a majority of Twitter’s most prolific users tend to swing left.

The report updates Pew’s 2019 study with similar findings. At that time, Pew found that 10% of U.S. adults on Twitter were responsible for 80% of all tweets from U.S. adults.

Today, those figures have changed. During the study period, the most active 10% of users produced 92% of all tweets by U.S. adults.

And of these highly active users, 69% identify as Democrats or Democratic-leaning independents.

In addition, the 10% most active Democrats typically produce roughly twice the number of tweets per month (157) compared with the most active Republicans (79).

These highly active users don’t represent how most Twitter users tweet, however.

Regardless of party affiliation, the majority of Twitter users post very infrequently, Pew found.

The median U.S. adult Twitter user posted just once per month during the time of the study. The median Democrat posts just once per month, while the median Republican posts even less often than that.

The typical adult also has very few followers, with the median Democrat having 32 followers while the median Republican has 21. Democrats, however, tend to follow more accounts than Republicans, at 126 versus 71, respectively.

The new study additionally examined other differences in how members of the two parties use the platforms, beyond frequency of tweeting.

For starters, it found 60% of the Democrats on Twitter would describe themselves as very or somewhat liberal, compared with 43% of Democrats who don’t use Twitter. Self-identified conservatives on Twitter versus conservatives not on the platform had closer shares, at 60% and 62%, respectively.

Pew also found that the two Twitter accounts followed by the largest share of U.S. adults were those belonging to former President Barack Obama (@BarackObama) and President Donald Trump

(@RealDonaldTrump).

Not surprisingly, more Democrats followed Obama — 42% of Democrats did, versus just 12% of Republicans. Trump, meanwhile, was followed by 35% of Republicans and just 13% of Democrats.

Other top political accounts saw similar trends. For instance, Rep. Alexandria Ocasio-Cortez (@AOC) is followed by 16% of Democrats and 3% of Republicans. Fox News personalities Tucker Carlson (@TuckerCarlson) and Sean Hannity (@seanhannity), meanwhile, are both followed by 12% of Republicans but just 1% of Democrats.

This is perhaps a more important point than Pew’s study indicates, as it demonstrates that even though Twitter’s original goal was to build a “public town square” of sorts, where conversations could take place in the open, Twitter users have built the same isolated bubbles around themselves as they have elsewhere on social media.

Because Twitter’s main timeline only shows tweets and retweets from people you follow, users are only hearing their side of the conversation amplified back to them.

This problem is not unique to Twitter, of course. Facebook, for years, has been heavily criticized for delivering two different versions of reality to its users. An article from The WSJ in 2016 demonstrated how stark this contrast could be, when it showed a “blue” feed and “red” feed, side-by-side.

The problem is being exacerbated even more in recent months, as users from both parties are now exiting mainstream platforms, like Twitter, and isolating themselves even more. On the conservative side, users fled to free speech-favoring and fact check-eschewing platforms like Gab and Parler. The new social network Telepath, on the other hand, favors left-leaning users by aggressively blocking misinformation — often that from conservative news outlets — and banning identity-based attacks.

One other area Pew’s new study examined was the two parties’ use of hashtags on Twitter.

It found that no one hashtag was used by more than 5% of U.S. adults on Twitter during the study period. But there was a bigger difference when it came to the use of the #BlackLivesMatter hashtag, which was tweeted by 4% of Democrats on Twitter and just 1% of Republicans.

Other common hashtags used across both parties included #covid10, #coronavirus, @mytwitteranniversary, #newprofilepic, #sweepstakes, #contest and #giveaway.

It’s somewhat concerning, too, that hashtags were used in such a small percentage of tweets.

While their use has fallen out of favor somewhat — using a hashtag can seem “uncool” — the idea with hashtags was to allow users a quick way to tap into the global conversation around a given topic. But this decline in user adoption indicates there are now fewer tweets that can connect users to an expanded array of views.

Twitter today somewhat addresses this problem through its “Explore” section, highlighting trends, and users can investigate tweets using its keyword search tools. But if Twitter really wants to burst users’ bubbles, it may need to develop a new product — one that offers a different way to connect users to the variety of conversations taking place around a term, whether hashtagged or not.",Social Media,TechCrunch,https://techcrunch.com/2020/10/15/pew-most-prolific-twitter-users-tend-to-be-democrats-but-majority-of-users-still-rarely-tweet/,"Social media users are increasingly segregating themselves into bubbles of information, with Twitter users from both parties following accounts and using hashtags that reinforce their views while rarely engaging with opposing ideas. This reduces the ability of users to access a variety of perspectives and undermines the platform's original purpose of creating a shared public dialogue.","Information, Discourse & Governance"
252,Facebook says its new AI technology can detect ‘revenge porn’ – TechCrunch,"Facebook on Friday announced a new artificial intelligence-powered tool that it says will help the social network detect revenge porn — the nonconsensually shared intimate images that, when posted online, can have devastating consequences for those who appear in the photos. The technology will leverage both AI and machine learning techniques to proactively detect near-nude images or videos that are shared without permission across Facebook and Instagram.

The announcement follows on Facebook’s earlier pilot of a photo-matching technology, which had people directly submit their intimate photos and videos to Facebook. The program, which was run in partnership with victim advocate organizations, would then create a digital fingerprint of that image so Facebook could stop it from ever being shared online across its platforms. This is similar to how companies today prevent child abuse images from being posted to their sites.

The new AI technology for revenge porn, however, doesn’t require the victim’s involvement. This is important, Facebook explains, because victims are sometimes too afraid of retribution to report the content themselves. Other times, they’re simply unaware that the photos or videos are being shared.

While the company was short on details about how the new system itself works, it did note that it goes beyond simply “detecting nudity.”

After the system flags an image or video, a specially trained member of Facebook’s Community Operations team will review the image, then remove it if it violates Facebook’s Community Standards. In most cases, the company will also disable the account as a result. An appeals process is available if the person believes Facebook has made a mistake.

In addition to the technology and existing pilot program, Facebook says it also reviewed how its other procedures around revenge porn reporting could be improved. It found, for instance, that victims wanted faster responses following their reports and they didn’t want a robotic reply. Other victims didn’t know how to use the reporting tools or even that they existed.

Facebook noted that addressing revenge porn is critical as it can lead to mental health consequences like anxiety, depression, suicidal thoughts and sometimes even PTSD. There can also be professional consequences, like lost jobs and damaged relationships with colleagues. Plus, those in more traditional communities around the world may be shunned or exiled, persecuted or even physically harmed.

Facebook admits that it wasn’t finding a way to “acknowledge the trauma that the victims endure,” when responding to their reports. It says it’s now re-evaluating the reporting tools and process to make sure they’re more “straightforward, clear and empathetic.”

It’s also launching “Not Without My Consent,” a victim-support hub in the Facebook Safety Center that was developed in partnership with experts. The hub will offer victims access to organizations and resources that can support them, and will detail the steps to take to report the content to Facebook.

In the months ahead, Facebook says it also will build victim support toolkits with more locally and culturally relevant info by working with partners, including the Revenge Porn Helpline (U.K.), Cyber Civil Rights Initiative (U.S.), Digital Rights Foundation (Pakistan), SaferNet (Brazil) and Professor Lee Ji-yeon (South Korea).

Revenge porn is one of the many issues that results from offering the world a platform for public sharing. Facebook today is beginning to own up to the failures of social media across many fronts — which also include things like data privacy violations, the spread of misinformation and online harassment and abuse.

CEO Mark Zuckerberg recently announced a pivot to privacy, where Facebook’s products will be joined together as an encrypted, interoperable, messaging network — but the move has shaken Facebook internally, causing it to lose top execs along the way.

While changes are in line with what the public wants, many have already lost trust in Facebook. For the first time in 10 years Edison Research noted a decline in Facebook usage in the U.S., from 67 to 62 percent of Americans 12 and older. Still, Facebook is still a massive platform, with its more than 2 billion users. Even if users themselves opt out of Facebook, that doesn’t prevent them from ever becoming a victim of revenge porn or other online abuse by those who continue to use the social network.",Social Media,TechCrunch,https://techcrunch.com/2019/03/15/facebook-says-its-new-a-i-technology-can-detect-revenge-porn/,"Through initiatives such as its new AI-powered tool to detect revenge porn, Facebook is attempting to address the devastating consequences that victims of nonconsensual sharing of intimate images and videos can face, such as mental health issues, professional repercussions, and even physical harm.",Equality & Justice
253,Stop the Endless Scroll. Delete Social Media From Your Phone,"Most of the time, I navigate to my social media apps reflexively, as though my finger and the icons are magnets. I don't even realize I'm doing it until my thumb taps the Instagram icon on my screen. Again. And again. And again.

It’s a dirty digital habit, and it doesn’t make me happy. Maybe you can relate. Studies have repeatedly found that while social media connects us to one another, it also makes us feel bad. And yet, we do it anyway. We do it because we can’t stop.

So last month, I tried something new. I logged out of my social media accounts on my phone, and logged into them on my desktop. I set myself a goal to only engage with Facebook, Twitter, and Instagram on my laptop or work computer, hopefully offloading my biggest time suck to devices that I’m only around for part of the day.

And you know what? It’s actually sort of working. Slowly, and with some lapses, I’ve felt my compulsory need to look at social media fade. Instead of thumbing through Twitter while I’m bored waiting for the subway, I’ve picked up better, if still phone-dependent, habits—checking the New York Times app, listening to podcasts, playing mobile backgammon. Instead of waking up and scrolling through Instagram, then Twitter, then my email, I wake up and only check my email. Baby steps!

Addictive By Design

All social media is designed to keep us coming back, but that’s especially true of mobile apps. In recent years, there’s been pushback against the sticky interface design on platforms like Facebook, Twitter, and Instagram. Features like autoplay, endless scroll, reverse chronological timelines, and push notifications were once heralded as frictionless, user-friendly design. Now they’re described as manipulative.

Desktop applications aren't free of these hooks, but they’re less severe. Part of the reason is that social media is optimized for mobile. Facebook and Twitter might have started out as websites, but mobile usage quickly surpassed the number of people using the desktop versions. Today, a growing majority of people use Twitter and Facebook solely on their phone (54 percent for Facebook, 45 percent for Twitter). You can assume that Instagram, which didn’t introduce web profiles until two years after it launched, has even fewer desktop users.

As social media pivots to mobile, the desktop versions feel clunkier and less exciting. It’s not as fun to use social media on a computer—and that’s a good thing, says Adam Alter, a professor at NYU’s Stern School of Business and author of the book Irresistible, which charts the rise of addictive technology. “The feed doesn’t progress quite as neatly as when you're using a very simple finger swipe gesture,” he says. “You have to scroll the mouse or do something that requires a little extra effort, and even that little bit of extra effort can have an effect on how we experience the world.”

Now that I can't open Instagram or Twitter on my phone, I spend a lot less time on my phone in general. It’s also made me appreciate why these apps were so compelling in the first place. I used to rely on apps as a way to stave off boredom, and in the process, the apps themselves became boring, too. Now, I feel like I’m catching up on something I actually want to read. I'm not cured: Sometimes, I catch myself tapping on my phone’s Twitter icon only to find the login screen. And there’s still the problem of posting (good luck making an Instagram story on your laptop). But the point of my experiment isn’t to abolish social media; it’s to better understand why I’m using it in the first place.

“Part of the battle is teaching yourself you don't need these apps as much as you think you do,” Alter says. And it’s true. I no longer feel like I need the apps, but it’s nice to know I still want them.",Social Media,WIRED,https://www.wired.com/story/rants-and-raves-desktop-social-media/,"Social media has been linked to increased feelings of loneliness and depression, and has been designed to be as addictive as possible. This article discusses an experiment to reduce reliance on social media and better understand why it is used in the first place.",Social Norms & Relationships
254,Twitter finally boots hate group that Trump retweeted off its platform – TechCrunch,"Yesterday Twitter said it would begin enforcing new hate speech rules to shutter accounts that promote violence against citizens to further their causes.

The same day it suspended the accounts of the far right British hate group, Britain First, along with the accounts of its leader Paul Golding, and deputy Jayda Fransen.

Fransen, whose far right anti-muslim hate group has never achieved any sniff of electoral success, was nonetheless thrust into the mainstream limelight last month after president Trump retweeted three anti-muslim videos she had shared to his 44.8M followers — earning a personal rebuke from the UK prime minister, and condemnation from MPs across the domestic political spectrum for amplifying hate speech.

For a little wider context on Fransen, last year she was found guilty of religiously aggravated harassment after abusing a Muslim woman who was wearing a hijab.

Both her and Golding were also arrested in the UK earlier this week on charges relating to behavior intended to or likely to stir up hatred in Northern Ireland.

Curiously, a ‘Twitter People’ search for ‘Jayda Fransen’ which TechCrunch carried out today initially resulted in the first suggestion being @realDonaldTrump‘s account.

Some minutes later his account was no longer being algorithmically linked to Fransen — suggesting some human eyes at the company had locked onto the recommendation AI’s problem.

The three posts made by Fransen that Trump retweeted from his personal account last year also appear to now be gone from his feed — presumably as a result of the account being shut down.

Last month Twitter faced criticism for not removing the three tweets — but defended its action, telling CNN that there “may be the rare occasion when we allow controversial content or behavior which may otherwise violate our rules to remain on our service because we believe there is a legitimate public interest in its availability”.

“Each situation is evaluated on a case by case basis and ultimately decided upon by a cross-functional team,” the spokesperson added then.

Weeks on, the Britain First “case” appears to be closed — insofar as Twitter is concerned.

However it’s a different story on Facebook where the group’s Facebook page has more than 1.9M likes — and includes a shop where Facebook users can make a donation to the hate group or pay the hate group to become a member.

Facebook, where its page has 1.9m likes, remains the key social media platform for Britain First. And the company refuses to respond to my repeated requests to explain why it doesn’t contravene its rules. pic.twitter.com/MRlFfXoSzg — Rory Cellan-Jones (@ruskin147) December 19, 2017

On its Facebook page Britain First can also be found complaining that YouTube is now demonitizing and gating its videos behind warning notices.

Earlier this year the Google-owned platform faced an advertiser backlash when marketing was shown being displayed alongside hateful and offensive content — and said it would be taking “a tougher stance on hateful, offensive and derogatory content.”

Last month YouTube also confirmed a major policy shift regarding extremist content — saying it would be broadening takedowns to expand out from videos that directly incite violence to also removing non-violent extremist content.

Although, in this instance, it does not appear that Alphabet/Google is removing Britain First videos entirely.

Rather it’s making it harder for the group to use its mainstream platform to profit from their far right fringe activities.

We asked Facebook why it continues to allow the hate group to maintain a presence on its platform — which is demonstrably being used as a route for fundraising — but Facebook declined to comment.

Instead a spokeswoman pointed us to this company blog, from Facebook’s self-styled ‘Hard Questions‘ series, on the topic of hate speech.

In the post, Facebook’s Richard Allan, VP EMEA public policy, writes [emphasis mine]: “Our current definition of hate speech is anything that directly attacks people based on what are known as their “protected characteristics” — race, ethnicity, national origin, religious affiliation, sexual orientation, sex, gender, gender identity, or serious disability or disease.”",Social Media,TechCrunch,https://techcrunch.com/2017/12/19/twitter-finally-boots-hate-group-that-trump-retweeted-off-its-platform/,"Social media platforms are facing criticism for allowing hate groups to spread their messages and fundraise on their sites, without taking action to remove them. Twitter has begun to enforce new hate speech rules and has shut down accounts belonging to Britain First, but Facebook and YouTube have yet to take action.",Equality & Justice
255,US border officials are increasingly denying entry to travelers over others’ social media – TechCrunch,"Travelers are increasingly being denied entry to the United States as border officials hold them accountable for messages, images and video on their devices sent by other people.

It’s a bizarre set of circumstances that has seen countless number of foreign nationals rejected from the U.S. after friends, family or even strangers send messages, images or videos over social media sites like Facebook and Twitter, and encrypted messaging apps like WhatsApp, which are then downloaded to the traveler’s phone.

The latest case saw a Palestinian national living in Lebanon and would-be Harvard freshman denied entry to the U.S. just before the start of the school year.

Immigration officers at Boston Logan International Airport are said to have questioned Ismail Ajjawi, 17, for his religion and religious practices, he told the school newspaper The Harvard Crimson. The officers who searched his phone and computer reportedly took issue with his friends’ social media activity.

Ajjawi’s visa was canceled and he was summarily deported — for someone else’s views.

The United States border is a bizarre space where U.S. law exists largely to benefit the immigration officials who decide whether or not to admit or deny entry to travelers, and few protect the travelers themselves. Both U.S. citizens and foreign nationals alike are subject to unwarranted searches and few rights to free speech, and many have limited access to legal counsel.

That has given U.S. border officials a far wider surface area to deny entry to travelers — sometimes for arbitrary reasons.

On a typical day, U.S. Customs & Border Protection processes 1.13 million passengers by plane, sea and land and deny entry to more than 760 people. Sometimes a denial is clear, such as a past criminal conviction or the wrong documentation. But all too often, no specific reasons are given, and there are no grounds to appeal.

CBP also claims to have what critics say is broadly unconstitutional powers to search travelers’ phones — including those of U.S. citizens — at the border without needing a warrant. Last year, CBP searched 30,000 travelers’ devices — a four-times increase since 2015 — without any need for reasonable suspicion.

Complicating matters, the Trump administration in June began to demand that foreigners who apply for U.S. visas disclose their social media handles and profiles. Some 15 million are expected to fall under the new rule.

A spokesperson for Customs & Border Protection did not comment.

Summer Lopez, senior director of free expression programs at PEN America, a human rights nonprofit, said in a statement that the immigration policy on social media “demonstrates all too well the damage these ill-conceived policies can do.”

“That should not be the price of entrance to the U.S., let alone that one’s friends should have to censor themselves as well,” said Lopez.

But Ajjawi’s denied entry is not an isolated case.

Abed Ayoub, legal and policy director at the American-Arab Anti-Discrimination Committee, said device searches and subsequent denials of entry had become the “new normal” over the past year.

“We hear about this happening to Arab students and Muslim students coming into the U.S. today,” he told TechCrunch. Although all travelers are subject to having their devices searched, Ayoub said the government was “holding [the Arab and Muslim] community to a different level” than other backgrounds.

Ayoub said he’s had clients that have been turned away at the border for content found in their WhatsApp messages.

“It’s probably the most popular app in the Middle East,” he said. Because WhatsApp automatically downloads received images and videos to a user’s phone, any questionable content — even sent unsolicitedly — under a border official’s search could be enough to deny the traveler entry.

In one tweet, Ayoub posted a photo of an expedited removal form from one of his clients — also a student with U.S. visa — who was denied entry for an image he received in a WhatsApp group. The student strenuously denied any personal connection to the images and argued it had been automatically saved to his phone. The border official wrote that as a result of the device search the student was “inadmissible” to the U.S. The student was only a couple of semesters away from graduating, but a rejection meant the student can no longer return to the U.S.

“This is part of the backdoor ‘Muslim ban,’ ” Ayoub said, referring to a controversial executive order signed by President Trump in January 2017, which barred citizens from seven predominantly Muslim countries entry to the U.S.

“We don’t hear of other other individuals being denied because of WhatsApp or because of what’s on the social media,” he said.

Correction: an earlier version of this report said the Harvard student was Lebanese. He is a Palestinian national living in Lebanon.",Social Media,TechCrunch,https://techcrunch.com/2019/08/27/border-deny-entry-united-states-social-media/,"Travelers, including foreign nationals and U.S. citizens, are increasingly being denied entry to the U.S. as border officials hold them accountable for messages, images, and videos on their devices sent by other people, regardless of whether they had any personal connection to the content.",Security & Privacy
256,A Quiet War Rages Over Who Can Make Money Online,"Over the past year, two popular forums for men who identify as involuntary celibates, or incels, have been banned by Reddit and a domain registrar in response to members’ history of toxic misogyny and celebrating violence against women. Now some of these men are trying to turn the tables. Members of the incel community—including the official Twitter account for incels.is, a central hub for adherents—have joined with other men’s rights activists, using the content policies of online payment companies such as PayPal as weapons to block female pornographic actors and sex workers from making or spending money online.

The campaign is called the “ThotAudit,” in reference to the derogatory term “thot,” which stands for “that ho over there.” It began over the Thanksgiving holiday as a grassroots effort to intimidate sex workers and women who sell access to private pornographic social media accounts by reporting them to the Internal Revenue Service for tax evasion—without evidence of wrongdoing. But it quickly morphed into a battle over who has the right to make money on the internet.

The harassers are taking advantage of user reporting tools made available by companies like PayPal, Venmo, and CirclePay, in an attempt to force their targets offline and freeze their finances. The tactic has far-reaching implications beyond adult entertainment. Foreign governments and other groups have abused the policies to silence opponents on platforms like Twitter and Facebook for years. Attacking through the payment processors is a new wrinkle on that approach.

When Lily Adams, an actor and model who sells access to her pornographic photos and videos, first noticed the ThotAudit movement gaining traction Saturday evening, she took to Twitter, calling it a witch hunt. Within one minute, a ThotAuditor flagged her account and tweeted that she had been added “to the review list for Monday morning.” By Monday, Adams’ PayPal account had been terminated. In an email to WIRED, Adams said that she had approximately $526 in her PayPal account at the time, and that the company told her it would hold the funds “indefinitely.”

""The feeling of being attacked from all corners is definitely frustrating,"" said Adams. By the end of the day, she had been banned from every cash app, and she wasn't alone. Another pornographic actor and model, who goes by the name Haven Graye online, was banned by Venmo the day after the ThotAudit campaign began. ”118 men called me a prostijew, kike and sen[t] me pictures of people burning,” she told WIRED via direct message. On Twitter, dozens of other sex workers experienced similar harassment and expressed outrage over being banned from multiple online payment service providers, including PayPal, Cash App, and Circle Pay. Many of the women claimed to have lost access to hundreds of dollars, as most payment companies absorb whatever funds were stored in the reported account.

Participants in the harassment campaign openly discussed tactics and specific attacks in r/ThotAudit, a public subreddit. On Sunday, one user detailed a popular alternative to reporting sex workers to the IRS, which the user said was too slow and dangerous: “Find the thots paypal email, send them money, and then report them for selling goods against paypals services … It's against Paypal's rules to solicit digital sexual content. All of their funds will be locked pretty quickly.” Users also posted comprehensive “thot auditing field guides,” including a continually updated list of tens of thousands of potential targets, and detailed instructions for getting sex workers banned from PayPal, Venmo, Cash App, Amazon Pay, Stripe, Circle Pay, Snapchat, and Kik.",Social Media,WIRED,https://www.wired.com/story/quiet-war-rages-who-can-make-money-online/,"The ThotAudit, a campaign led by members of the incel community and other men's rights activists, targets female pornographic actors and sex workers by using the content policies of online payment companies to block them from making or spending money online, leaving many without access to hundreds of dollars.",Equality & Justice
257,Facebook whistleblower Frances Haugen testifies before the Senate – TechCrunch,"After revealing her identity on Sunday night, Frances Haugen — the whistleblower who leaked controversial Facebook documents to The Wall Street Journal — testified before the Senate Committee on Commerce, Science, and Transportation on Tuesday.

Haugen’s testimony came after a hearing last week, when Facebook global head of Safety Antigone Davis was questioned about the company’s negative impact on children and teens. Davis stuck to Facebook’s script, frustrating senators as she failed to answer questions directly. But Haugen, a former project manager on civic misinformation at Facebook, was predictably more forthcoming with information.

Haugen is an algorithm specialist, having served as a project manager at companies like Google, Pinterest and Yelp. While she was at Facebook, she addressed issues related to democracy, misinformation and counter-espionage.

“Having worked on four different types of social networks, I understand how complex and nuanced these problems are,” Haugen said in her opening statement. “However, the choices being made inside Facebook are disastrous — for our children, for our public safety, for our privacy and for our democracy — and that is why we must demand Facebook make changes.”

The algorithm

Throughout the hearing, Haugen made clear that she thinks that Facebook’s current algorithm, which rewards posts that generate meaningful social interactions (MSIs), is dangerous. Rolled out in 2018, this news feed algorithm prioritizes interactions (such as comments and likes) from the people who Facebook thinks you’re closest to, like friends and family.

But as the documents leaked by Haugen show, data scientists raised concerns that this system yielded “unhealthy side effects on important slices of public content, such as politics and news.”

Facebook also uses engagement-based ranking, in which an AI displays the content that it thinks will be most interesting to individual users. This means content that elicits stronger reactions from users will be prioritized, boosting misinformation, toxicity and violent content. Haugen said she thinks that chronological ranking would help mitigate these negative impacts.

“I’ve spent most of my career working on systems like engagement-based ranking. When I come to you and say these things, I’m basically damning 10 years of my own work,” Haugen said in the hearing.

As Haugen told “60 Minutes” on Sunday night, she was part of a civic integrity committee that Facebook dissolved after the 2020 election. Facebook implemented safeguards to reduce misinformation ahead of the 2020 U.S. presidential election. After the election, it turned off those safeguards. But after the attacks on the U.S. Capitol on January 6, Facebook switched them back on again.

“Facebook changed those safety defaults in the run up to the election because they knew they were dangerous. Because they wanted that growth back after the election, they returned to their original defaults,” Haugen said. “I think that’s deeply problematic.”

Haugen said that Facebook is emphasizing a false choice — that they can either use their volatile algorithms and continue their rapid growth, or they can prioritize user safety and decline. But she thinks that adopting more safety measures, like oversight from academics, researchers and government agencies, could actually help Facebook’s bottom line.

“The thing I’m asking for is a move [away] from short-term-ism, which is what Facebook is run under today. It’s being led by metrics and not people,” Haugen said. “With appropriate oversight and some of these constraints, it’s possible that Facebook could actually be a much more profitable company five or 10 years down the road, because it wasn’t as toxic, and not as many people quit it.”

Establishing government oversight

When asked as a “thought experiment” what she would do if she were in CEO Mark Zuckerberg’s shoes, Haugen said she would establish policies about sharing information with oversight bodies including Congress; she would work with academics to make sure they have the information they need to conduct research about the platform; and that she would immediately implement the “soft interventions” that were identified to protect the integrity of the 2020 election. She suggested requiring users to click on a link before they share it, since other companies like Twitter have found these interventions to reduce misinformation.

Haugen also added that she thinks Facebook as it’s currently structured can’t prevent the spread of vaccine misinformation, since the company is overly reliant on AI systems that Facebook itself says will likely never catch more than 10% to 20% of content.

Later on, Haugen told the committee that she “strongly encourages” reforming Section 230, a part of the United States Communications Decency Act that absolves social media platforms from being held liable for what their users post. Haugen thinks Section 230 should exempt decisions about algorithms, making it possible for companies to face legal consequences if their algorithms are found to cause harm.

“User-generated content is something companies have less control over. But they have 100% control over their algorithms,” Haugen said. “Facebook should not get a free pass on choices it makes to prioritize growth, virality and reactiveness over public safety.”

Sen. John Hickenlooper (D-CO) asked how Facebook’s bottom line would be impacted if the algorithm promoted safety. Haugen said that it would have an impact, because when users see more engaging content (even if it’s more enraging than engaging), they spend more time on the platform, yielding more ad dollars for Facebook. But she thinks the platform would still be profitable if it followed the steps she outlined for improving user safety.

International security

As reported in one of The Wall Street Journal’s Facebook Files stories, Facebook employees flagged instances of the platform being used for violent crime overseas, but the company’s response was inadequate, according to the documents Haugen leaked.

Employees raised concerns, for example, about armed groups in Ethiopia using the platform to coordinate violent attacks against ethnic minorities. Since Facebook’s moderation practices are so dependent on artificial intelligence, that means that its AI needs to be able to function in every language and dialect that its 2.9 billion monthly active users speak. According to the WSJ, Facebook’s AI systems don’t cover the majority of the languages spoken on the site. Haugen said that though only 9% of Facebook users speak English, 87% of the platform’s misinformation spending is devoted to English speakers.

“It seems that Facebook invests more in users who make the most money, even though the danger may not be evenly distributed based on profitability,” Haugen said. She added that she thinks Facebook’s consistent understaffing of the counter-espionage, information operations and counterterrorism teams is a national security threat, which she’s communicating with other parts of Congress about.

The future of Facebook

The members of the Senate committee indicated that they’re motivated to take action against Facebook, which is also in the midst of an antitrust lawsuit.

“I’m actually against the breaking up of Facebook,” Haugen said. “If you split Facebook and Instagram apart, it’s likely that most advertising dollars will go to Instagram, and Facebook will continue to be this ‘Frankenstein’ that is endangering lives around the world, only now there won’t be money to fund it.”

But critics argue that yesterday’s six-hour Facebook outage — unrelated to today’s hearing — showed the downside of one company having so much control, especially when platforms like WhatsApp are so integral to communication abroad.

In the meantime, lawmakers are drawing up legislation to promote safety on social media platforms for minors. Last week, Sen. Ed Markey (D-MA) announced that he would reintroduce legislation with Sen. Richard Blumenthal (D-CT) called the KIDS (Kids Internet Design and Safety) Act, which seeks to create new protections for online users under 16. Today, Sen. John Thune (R-SD) brought up a bipartisan bill he introduced with three other committee members in 2019 called the Filter Bubble Transparency Act. This legislation would increase transparency by giving users the option to view content that’s not curated by a secret algorithm.

Sen. Blumenthal even suggested that Haugen come back for another hearing about her concerns that Facebook is a threat to national security. Though Facebook higher-ups spoke against Haugen during the hearing, policymakers seemed moved by her testimony.",Social Media,TechCrunch,https://techcrunch.com/2021/10/05/facebook-whistleblower-frances-haugen-testifies-before-the-senate/,"The testimony of whistleblower Frances Haugen revealed that Facebook's current algorithm, which rewards posts that generate meaningful social interactions, can have negative impacts such as boosting misinformation and violent content. Haugen suggested that regulation and oversight could help address these issues, while also potentially increasing the platform's long-term profitability.",Security & Privacy
258,Security lapse exposed Republican voter firm’s internal app code – TechCrunch,"A voter contact and canvassing company, used exclusively by Republican political campaigns, mistakenly left an unprotected copy of its app’s code on its website for anyone to find.

The company, Campaign Sidekick, helps Republican campaigns canvas its districts using iOS and Android apps, which pull in names and addresses from voter registration rolls. Campaign Sidekick says it has helped campaigns in Arizona, Montana, and Ohio and contributed to the Brian Kemp campaign, which saw him narrowly win against Democratic rival Stacey Abrams in the Georgia gubernatorial campaign in 2018.

For the past two decades, political campaigns have ramped up their use of data to identify swing voters. This growing political data business has opened up a whole economy of startups and tech companies using data to help campaigns better understand their electorate. But that has led to voter records spilling out of unprotected servers and other privacy-related controversies — like the case of Cambridge Analytica obtaining private data from social media sites.

Chris Vickery, director of cyber risk research at security firm UpGuard, said he found the cache of Campaign Sidekick’s code by chance.

In his review of the code, Vickery found several instances of credentials and other app-related secrets, he said in a blog post on Monday, which he shared exclusively with TechCrunch. These secrets, such as keys and tokens, can typically be used to gain access to systems or data without a username or password. But Vickery did not test the password as doing so would be unlawful. Vickery also found a sampling of personally identifiable information, he said, amounting to dozens of spreadsheets packed with voter names and addresses.

Fearing the exposed credentials could be abused if accessed by a malicious actor, Vickery informed the company of the issue in mid-February. Campaign Sidekick quickly pulled the exposed cache of code offline.

One of the screenshots provided by Vickery showed a mockup of a voter profile compiled by the app, containing basic information about the voter and their past voting and donor history, which can be obtained from public and voter records. The mockup also lists the voter’s “friends.”

Vickery told TechCrunch he found “clear evidence” that the app’s code was designed to pull in data from its now-defunct Facebook app, which allowed users to sign-in and pull their list of friends — a feature that was supported by Facebook at the time until limits were put on third-party developers’ access to friends’ data.

“There is clear evidence that Campaign Sidekick and related entities had and have used access to Facebook user data and APIs to query that data,” Vickery said.

Drew Ryun, founder of Campaign Sidekick, told TechCrunch that its Facebook project was from eight years prior, that Facebook had since deprecated access to developers, and that the screenshot was a “digital artifact of a mockup.” (TechCrunch confirmed that the data in the mockup did not match public records.)

Ryun said after he learned of the exposed data the company “immediately changed sensitive credentials for our current systems,” but that the credentials in the exposed code could have been used to access its databases storing user and voter data.",Social Media,TechCrunch,https://techcrunch.com/2020/03/30/campaign-sidekick/,"The careless mistake of a Republican canvassing company, Campaign Sidekick, of leaving their app's code unprotected on their website has led to a potential breach of personally identifiable information, such as voter names and addresses, as well as the potential for malicious actors to access their databases storing user and voter data. This is yet another example of how",Security & Privacy
259,Where did social media go wrong? – TechCrunch,"For most of my life, the internet, particularly its social media — BBSes, Usenet, LiveJournal, blogosphere, even Myspace, early Twitter and Facebook — consistently made people happier. But roughly five years ago it began to consistently make people more miserable. What changed?

I posted that question to Twitter a week ago, and the most notable response was the response that did not exist: not a single person disputed the premise of the question. Yes, Twitter responses are obviously selection bias incarnate — but looking at the opprobrium aimed at social media from all sides today, I’d think that if anything it understates the current collective wisdom. Which of course can often be disjointed from factual reality … but still important. So, again: what changed?

Some argued that new, bad users flooded the internet then, a kind of ultimate Eternal September effect. I’m skeptical. Even five years ago Facebook was already ubiquitous in the West, and we were already constantly checking it on our smartphones. Others argue that it reflects happiness decreasing in society as a whole — but as far back as 2014? I remember that as, generally, a time of optimism, compared to today.

There was one really interesting response, from a stranger: “The nature of these social networks changed. They went from places where people debated to places where lonely people are trying to feel less lonely.” Relatedly, from a friend: “The algorithms were designed to make people spend more time on those sites. Interestingly, unhappy people spend more time on social sites. Is unhappiness the cause, or the result of algorithms surfacing content to make us unhappy?” That’s worth pondering.

Pretty much everyone else talked about money, basically buttressing the argument above. Modern social media algorithms drive engagement, because engagement drives advertising, and advertising drives profits, which are then used to hone the algorithms. It’s a perpetual motion engagement machine. Olden days social media, early Facebook and early Twitter, they had advertising, sure — but they didn’t have anything like today’s perpetual motion engagement.

Even that wouldn’t be so bad if it weren’t for the fact that there’s apparently a whole other perpetual motion machine at work in parallel, too: engagement drives unhappiness which drives engagement which drives unhappiness, because the kind of content which drives the most engagement apparently also drives anxiety and outrage — cf. Evan Williams’ notion that social media optimizes for car crashes — and arguably also, in the longer run, displace other activities, which do bring happiness and fulfillment.

I don’t want to sound like some sort of blood-and-thunder Luddite preacher. There’s nothing automatically wrong with maintaining a thriving existence on Facebook and Twitter, especially if you carefully prune your feeds such that they are asshole-free zones with minimal dogpiling and pointless outrage. (Some outrage is important. But most isn’t.) Social media has done a lot of excellent things, and still brings a lot of happiness to very many people.

But also, and increasingly, a lot of misery. Does it currently bring us net happiness? Five years ago I think that question would have seemed ridiculous to most: the answer would generally have been a quick yes-of-course. Nowadays, most would stop and wonder, and many would answer with an even faster hell-no. Five years ago, people who worked at Facebook (and to a lesser extent Twitter) were treated with respect and admiration by the rest of the tech industry. Nowadays, fairly or not, it’s something a lot more like disdain, and sometimes outright contempt.

The solution is obvious: change the algorithms. Which is to say: make less money. Ha.They could even remove the algorithms entirely, switch back to Strict Chronological, and still make money — Twitter was profitable before stock options before it switched to an algorithmic feed, and its ad offerings were way less sophisticated back then — but it’s not about making money, it’s about making the most money possible, and that means algorithmically curated, engagement-driven, misery-inducing feeds.

So: Social media is increasingly making us miserable. There’s an obvious solution, but financial realpolitik means we can’t get to it from here. So either we just accept this spreading misery as a normal, inescapable, fundamental part of our lives now — or some broader, more drastic solution is required. It’s a quandary.",Social Media,TechCrunch,https://techcrunch.com/2019/03/24/where-did-social-media-go-wrong/,"Social Media has become a perpetual motion machine of engagement-driven unhappiness, leading to anxiety, outrage and displacement of activities that bring happiness and fulfillment.",User Experience & Entertainment
260,How covid-19 conspiracy videos keep getting millions of views,"The ongoing battle between social-media companies and covid-19 misinformation pushers—including US president Donald Trump—stepped up again this week thanks to a new viral video. And it has exposed, once again, how difficult addressing conspiracy theories is for Facebook, Twitter, and others.

The latest video comes from a group called America’s Frontline Doctors, which is sponsored by the right-wing Tea Party Patriots. It features a professional-looking group of people in white lab coats advocating hydroxychloroquine, the malaria treatment previously pushed by the president. One doctor speaking at the press conference promoted the drug as a “cure” for the coronavirus, and said that people “don’t need” to wear masks. The FDA withdrew its emergency-use authorization for hydroxychloroquine for treating covid-19 patients in June, after establishing that the drug was not effective and potentially dangerous.

The video’s false claims led to removals on Facebook, YouTube, and Twitter for violating policies on health misinformation. Donald Trump Jr.’s Twitter account was temporarily limited after he shared it, and a retweet by the president himself was deleted. By the time that happened, however, the video had already been seen by millions of people.

The success of America’s Frontline Doctors and Plandemic, another covid-19 conspiracy video, which became a big hit in May, show how challenging it is to combat a misinformation ecosystem that has remained largely unchecked for years. Why is this happening now?

Misinformation peddlers are getting better at finding big audiences. Conspiracy theories are typically seen as thriving at the wild fringes of online thought. But that’s not really how some of the more successful videos have spread over the past few months. Prominent figures in the anti-vaccine movements began seeking out audiences with larger, more mainstream YouTubers to get their beliefs in front of larger audiences.

Also helping? Media coverage, whether sympathetic or outraged. The doctors’ press conference was livestreamed by Breitbart, with the caption, “BREAKING: American Doctors Address COVID-19 Misinformation with SCOTUS Press Conference” before the post was deleted. Breitbart has more than 4.5 million Facebook followers.

Takedowns can boost—not block—the cycle. Some far-right figures have, for years, claimed that technology platforms are secretly conspiring to silence conservative political thought. As soon as the press conference started disappearing from mainstream social media, supporters began to re-upload new copies of the video and share it. By then it had added appeal as a video that mainstream forces “don’t want you to see.” This makes it more likely to be seen by those who already don’t trust these institutions, exacerbating the problem.

Increased enforcement may be coming too late. Although the tactics used to slow down conspiracies may have some minor successes, they have happened so late that prevention may be impossible. We reported recently that experts believe it could be too late to stop the conspiracy group QAnon with fact checks and account bans.",Social Media,MIT,https://www.technologyreview.com/2020/07/28/1005738/hydroxychloroquine-covid-19-misinformation-video/,"Social Media companies' attempts to combat covid-19 misinformation have been unsuccessful due to the success of conspiracy theory peddlers, who have managed to reach larger audiences with sympathetic media coverage, and whose efforts to be silenced have only made them more appealing.","Information, Discourse & Governance"
261,"Facebook still full of groups trading fake reviews, says consumer group – TechCrunch","Facebook has failed to clean up the brisk trade in fake product reviews taking place on its platform, an investigation by the consumer association Which? has found.

In June both Facebook and eBay were warned by the UK’s Competition and Markets Authority (CMA) they needed to do more to tackle the sale of fake product reviews. On eBay sellers were offering batches of five-star product reviews in exchange for cash, while Facebook’s platform was found hosting multiple groups were members solicited writers of fake reviews in exchange for free products or cash (or both).

A follow-up look at the two platforms by Which? has found a “significant improvement” in the number of eBay listings selling five-star reviews — with the group saying it found just one listing selling five-star reviews after the CMA’s intervention.

But little appears to have been done to prevent Facebook groups trading in fake reviews — with Which? finding dozens of Facebook groups that it said “continue to encourage incentivised reviews on a huge scale”.

Here’s a sample ad we found doing a ten-second search of Facebook groups… (one of a few we saw that specify they’re after US reviewers)

Which? says it found more than 55,000 new posts across just nine Facebook groups trading fake reviews in July, which it said were generating hundreds “or even thousands” of posts per day.

It points out the true figure is likely to be higher because Facebook caps the number of posts it quantifies at 10,000 (and three of the ten groups had hit that ceiling).

Which? also found Facebook groups trading fake reviews that had sharply increased their membership over a 30-day period, adding that it was “disconcertingly easy to find dozens of suspicious-looking groups in minutes”.

We also found a quick search of Facebook’s platform instantly serves a selection of groups soliciting product reviews…

Which? says looked in detail at ten groups (it doesn’t name the groups), all of which contained the word ‘Amazon’ in their group name, finding that all of them had seen their membership rise over a 30-day period — with some seeing big spikes in members.

“One Facebook group tripled its membership over a 30-day period, while another (which was first started in April 2018) saw member numbers double to more than 5,000,” it writes. “One group had more than 10,000 members after 4,300 people joined it in a month — a 75% increase, despite the group existing since April 2017.”

Which? speculates that the surge in Facebook group members could be a direct result of eBay cracking down on fake reviews sellers on its own platform.

“In total, the 10 [Facebook] groups had a staggering 105,669 members on 1 August, compared with a membership of 85,647 just 30 days prior to that — representing an increase of nearly 19%,” it adds.



Across the ten groups it says there were more than 3,500 new posts promoting inventivised reviews in a single day. Which? also notes that Facebook’s algorithm regularly recommended similar groups to those that appeared to be trading in fake reviews — on the ‘suggested for you’ page.

It also says it found admins of groups it joined listing alternative groups to join in case the original is shut down.

Commenting in a statement, Natalie Hitchins, Which?’s head of products and services, said: ‘Our latest findings demonstrate that Facebook has systematically failed to take action while its platform continues to be plagued with fake review groups generating thousands of posts a day.

“It is deeply concerning that the company continues to leave customers exposed to poor-quality or unsafe products boosted by misleading and disingenuous reviews. Facebook must immediately take steps to not only address the groups that are reported to it, but also proactively identify and shut down other groups, and put measures in place to prevent more from appearing in the future.”

“The CMA must now consider enforcement action to ensure that more is being done to protect people from being misled online. Which? will be monitoring the situation closely and piling on the pressure to banish these fake review groups,” she added.

Responding to Which?‘s findings in a statement, CMA senior director George Lusty said: “It is unacceptable that Facebook groups promoting fake reviews seem to be reappearing. Facebook must take effective steps to deal with this problem by quickly removing the material and stop it from resurfacing.”

“This is just the start – we’ll be doing more to tackle fake and misleading online reviews,” he added. “Lots of us rely on reviews when shopping online to decide what to buy. It is important that people are able to trust they are genuine, rather than something someone has been paid to write.”

In a statement Facebook claimed it has removed 9 out of ten of the groups Which? reported to it and claimed to be “investigating the remaining group”.

“We don’t allow people to use Facebook to facilitate or encourage false reviews,” it added. “We continue to improve our tools to proactively prevent this kind of abuse, including investing in technology and increasing the size of our safety and security team to 30,000.”",Social Media,TechCrunch,https://techcrunch.com/2019/08/06/facebook-still-full-of-groups-trading-fake-reviews-says-consumer-group/,"Facebook has failed to take adequate measures to address the issue of fake product reviews being sold and traded on its platform, leaving customers exposed to the risk of being misled by false information.","Information, Discourse & Governance"
262,Black Mirror's third season opens with a vicious take on social media,"The third season of Charlie Brooker’s Black Mirror, a Twilight Zone-esque anthology TV series about technological anxieties and possible futures, was released on Netflix on October 21st. In this series, six writers will look at each of the third season’s six episodes to see what they have to say about current culture and projected fears.

If Black Mirror could be summed up in one sentence, it'd probably be ""Technology is exciting, but people are awful, and they keep finding the worst ways to apply it."" The anthology series had a seven-episode run on Britain's Channel 4 from 2011 to 2014, and Netflix is now producing new episodes, but the show hasn't changed much. It's still serving up cautionary tales about how people interact with media, the culture, communications, and technology, and how our obsessions and behaviors today might logically extend into horrors in the future. At its best, it's insightful, satirical, and bleak enough to shock even viewers jaded by the endless, pummeling violence of shows like Game of Thrones and The Walking Dead. At its worst, though, it can read like any other science fiction story processing our anxieties about the future: the hand-wringing fear of everything we take for granted can seem overblown and paranoid. As The Toast's Mallory Ortberg put it on Twitter, ""What if phones but too much?"" ""Nosedive,"" the first installment of Netflix's new six-episode season, replaces ""phones"" in that sentence with ""social media."" The script, written by Parks And Recreation's Michael Schur and Rashida Jones, turns social platforms' self-curation and validation-seeking into the backbone of a future society. But the issues at play go beyond the ways users project their best, most enviable selves onto their Instagrams and Facebook feeds. Almost exactly a year ago, Verge reports editor Josh Dzieza published a piece called ""The ratings game: How Uber and its peers turned us into horrible bosses."" The gist was that the combination of an on-demand economy and a new class of ratings-dependant employees was turning customers into entitled, hypersensitive critics. That article could have been the road map for ""Nosedive."" Just as Uber drives started offering riders free water, candy, and other perks to ingratiate themselves with increasingly demanding customers, the characters in ""Nosedive"" nervously tailor their lives to be ingratiating online, within certain very narrow guidelines. Black Mirror creator Charlie Brooker says ""Nosedive"" is intended as a satire, which helps explain its arch, bright, over-the-top tone. In the episode, augmented reality and a single ubiquitous social-media platform let users rate all their online and in-person interactions on a five-star scale. Everyone in this brave new world walks around with a user-generated score glowing in front of their faces, and that score determines their value in society, their access to services, and their employability. Protagonist Lacie Pound (Bryce Dallas Howard) has a respectable 4.2 score, but she envies the higher rating of her childhood friend Naomi (Alice Eve), and starts taking calculated steps to boost her ranking. The episode title hints sharply at what comes next.

The barrier for entry in ""Nosedive"" is fairly high. Viewers have to accept that there's no competition for the social-media app that rules everyone's lives, and no way to rate businesses or other institutions. In the real world, people who have bad experiences with airlines or car-rental companies often go straight to Twitter or Facebook with their complaints, and often rack up instant, insincere public apologies from online customer-service reps. Lacie has no such option; when a company screws her over, showing no remorse and giving her no recourse, she's expected to retreat meekly and politely. As judgmental as her society is, it doesn't turn any of that judgment on institutions. In the real world, there would be so many horrible ways to chase ratings The episode also suggests that the only way to approach the coveted 5.0 rating is to project a Martha Stewart Living illusion of graceful, plastic beauty. Pride and Prejudice director Joe Wright gives Lacie's pre-nosedive life a peach-and-pastel glow that's luminously pretty on-screen, but so flavorless and fake, it's impossible to imagine it's the only standard for five-star life. And yet there's no hint that anyone but the most low-rated outcasts in Lacie's world value any other form of achievement or art. In the real world, at minimum, there would be ways for people to degrade themselves for quick bursts of approval, like in the mesmerizing first season Black Mirror episode ""Fifteen Million Merits."" In ""Nosedive,"" the only option for people who like grit or grunge is to drop out of the system, and take the consequences. But ""Nosedive"" isn't out for reality. It's out to extrapolate the endgame of customer ratings systems, which turn the world into a lopsided giant prisoner's dilemma of applied power. And as exaggerated and unlikely as it is, it's also an effective story, because even in the broadness of its metaphor, it's relatable. Howard's performance goes a long way toward making the story work, because she projects such a fragile, brittle form of happiness when she's working hard for validation, and she's so rawly naked and afraid when her tricks stop working, and her real self starts pouring past the dams she's built. That feeling should be familiar to anyone who's censored their own image on social media out of fear of exposure, or just in hopes of a sparking a particular response from a particular person. It should be familiar to anyone who's ever had someone else post an unflattering photo of them, or fielded a hateful comment from a stranger. And it should also be familiar to anyone who's been in an online sphere where they can be rated and ranked — anyone who's owned a business that's featured on Yelp, or released a podcast or a YouTube video, or even just sold items on eBay or Amazon Marketplace. There's a personal bruising effect that comes with a bad ranking, especially when it's anonymous. It's the feeling of being judged not just by one jerk, but potentially by the entire world.",Social Media,Verge,https://www.theverge.com/2016/10/24/13379204/black-mirror-season-3-episode-1-nosedive-recap,"The main undesirable consequence of Social Media discussed in ""Nosedive"" is the pressure to conform to a narrow set of standards in order to gain approval and validation. Ratings systems transform the world into a lopsided giant prisoner's dilemma of applied power, where people must constantly censor their own image and try to project a Martha Stewart Living illusion",Social Norms & Relationships
263,People who mostly get news from social networks have some COVID-19 misconceptions – TechCrunch,"A new survey conducted by the Pew Research Center shows a COVID-19 information divide between people who mostly get their news from social networks and those who rely on more traditional news sources.

Pew surveyed 8,914 adults in the U.S. during the week of March 10, dividing survey respondents by the main means they use to consume political and election news. In the group of users that reports getting most of their news from social media, only 37% of respondents said that they expected the COVID-19 vaccine to be available in a year or more — an answer aligned with the current scientific consensus. In every other sample with the exception of the local TV group, at least 50% of those surveyed answered the question correctly. A third of social media news consumers also reported that they weren’t sure about the vaccine availability.

Among people who get most of their news from social media, 57% reported that they had seen at least some COVID-19 information that “seemed completely made up.” For people who consume most of their news via print media, that number was 37%.

Most alarmingly, people who primarily get their news via social media perceived the threat of COVID-19 to be exaggerated. Of the social media news consumers surveyed, 45% answered that the media “greatly exaggerated the risks” posed by the novel coronavirus. Radio news consumers were close behind, with 44% believing the media greatly exaggerated the threat of the virus, while only 26% of print consumers — those more likely to be paying for their news — believed the same.

The full results were part of Pew’s Election News Pathways project, which explores how people in the U.S. consume election news.

",Social Media,TechCrunch,https://techcrunch.com/2020/03/25/pew-coronavirus-covid-19-survey/,"The Pew survey found that people who mainly get their news from social media are less likely to have accurate information about the progress of the COVID-19 vaccine, with only 37% expecting it to be available within the next year. Additionally, 45% of social media news consumers perceived the threat of COVID-19 to be exaggerated.","Information, Discourse & Governance"
264,Move fast and break Facebook: A bull case for antitrust enforcement – TechCrunch,"This is the second post in a series on the Facebook monopoly. The first post explored how the U.S. Federal Trade Commission should define the Facebook monopoly. I am inspired by Cloudflare’s recent post explaining the impact of Amazon’s monopoly in its industry.

Perhaps it was a competitive tactic, but I genuinely believe it more a patriotic duty: guideposts for legislators and regulators on a complex issue. My generation has watched with a combination of sadness and trepidation as legislators who barely use email question the leading technologists of our time about products that have long pervaded our lives in ways we don’t yet understand.

I, personally, and my company both stand to gain little from this — but as a participant in the latest generation of social media upstarts, and as an American concerned for the future of our democracy, I feel a duty to try.

Mark Zuckerberg has reached his Key Largo moment.

In May 1972, executives of the era’s preeminent technology company — AT&T — met at a secret retreat in Key Largo, Florida. Their company was in crisis.

At the time, Ma Bell’s breathtaking monopoly consisted of a holy trinity: Western Electric (the vast majority of phones and cables used for American telephony), the lucrative long distance service (for both personal and business use) and local telephone service, which the company subsidized in exchange for its monopoly.

Over the next decade, all three government branches — legislators, regulators and the courts — parried with AT&T’s lawyers as the press piled on, battering the company’s reputation in the process. By 1982, a consent decree forced AT&T’s dismantling. The biggest company on earth withered to 30% of its book value and seven independent “Baby Bell” regional operating companies. AT&T’s brand would live on, but the business as the world knew it was dead.

Mark Zuckerberg is, undoubtedly, the greatest technologist of our time. For over 17 years, he has outgunned, outsmarted and outperformed like no software entrepreneur before him. Earlier this month, the U.S. Federal Trade Commission refiled its sweeping antitrust case against Facebook.

Its own holy trinity of Facebook Blue, Instagram and WhatsApp is under attack. All three government branches — legislators, regulators and the courts — are gaining steam in their fight, and the press is piling on, battering the company’s reputation in the process. Facebook, the AT&T of our time, is at the brink. For so long, Zuckerberg has told us all to move fast and break things. It’s time for him to break Facebook.

If Facebook does exist to “make the world more open and connected, and not just to build a company,” as Zuckerberg wrote in the 2012 IPO prospectus, he will spin off Instagram and WhatsApp now so that they have a fighting chance. It would be the ultimate Zuckerbergian chess move. Zuckerberg would lose voting control and thus power over all three entities, but in his action he would successfully scatter the opposition. The rationale is simple:

The United States government will break up Facebook. It is not a matter of if; it is a matter of when. Facebook is already losing. Facebook Blue, Instagram and WhatsApp all face existential threats. Pressure from the government will stifle Facebook’s efforts to right the ship. Facebook will generate more value for shareholders as three separate companies.

I write this as an admirer; I genuinely believe much of the criticism Zuckerberg has received is unfair. Facebook faces Sisyphean tasks. The FTC will not let Zuckerberg sneeze without an investigation, and the company has failed to innovate.

Given no chance to acquire new technology and talent, how can Facebook survive over the long term? In 2006, Terry Semel of Yahoo offered $1 billion to buy Facebook. Zuckerberg reportedly remarked, “I just don’t know if I want to work for Terry Semel.” Even if the FTC were to allow it, this generation of founders will not sell to Facebook. Unfair or not, Mark Zuckerberg has become Terry Semel.

The government will break up Facebook

It is not a matter of if; it is a matter of when.

In a speech on the floor of Congress in 1890, Senator John Sherman, the founding father of the modern American antitrust movement, famously said, “If we will not endure a king as a political power, we should not endure a king over the production, transportation and sale of any of the necessities of life. If we would not submit to an emperor, we should not submit to an autocrat of trade with power to prevent competition and to fix the price of any commodity.”

This is the sentiment driving the building resistance to Facebook’s monopoly, and it shows no sign of abating. Zuckerberg has proudly called Facebook the fifth estate. In the U.S., we only have four estates.

All three branches of the federal government are heating up their pursuit. In the Senate, an unusual bipartisan coalition is emerging, with Senators Amy Klobuchar (D-MN), Mark Warner (D-VA), Elizabeth Warren (D-MA) and Josh Hawley (R-MO) each waging a war from multiple fronts.

In the House, Speaker Nancy Pelosi (D-CA) has called Facebook “part of the problem.” Lina Khan’s FTC is likewise only getting started, with unequivocal support from the White House that feels burned by Facebook’s disingenuous lobbying. The Department of Justice will join, too, aided by state attorneys general. And the courts will continue to turn the wheels of justice, slowly but surely.

In the wake of Facebook co-founder Chris Hughes’ scathing 2019 New York Times op-ed, Zuckerberg said that Facebook’s immense size allows it to spend more on trust and safety than Twitter makes in revenue.

“If what you care about is democracy and elections, then you want a company like us to be able to invest billions of dollars per year like we are in building up really advanced tools to fight election interference,” Zuckerberg said.

This could be true, but it does not prove that the concentration of such power in one man’s hands is consistent with U.S. public policy. And the centralized operations could be rebuilt easily in standalone entities.

Time and time again, whether on Holocaust denial, election propaganda or vaccine misinformation, Zuckerberg has struggled to make quick judgments when presented with the information his trust and safety team uncovers. And even before a decision is made, the structure of the team disincentivizes it from even measuring anything that could harm Facebook’s brand. This is inherently inconsistent with U.S. democracy. The New York Times’ army of reporters will not stop uncovering scandal after scandal, contradicting Zuckerberg’s narrative. The writing is on the wall.

Facebook is losing

Facebook Blue, Instagram and WhatsApp all face existential threats. Pressure from the government will stifle Facebook’s efforts to right the ship.

For so long, Facebook has dominated the social media industry. But if you ask Chinese technology executives about Facebook today, they quote Tencent founder Pony Ma: “When a giant falls, his corpse will still be warm for a while.”

Facebook’s recent demise begins with its brand. The endless, cascading scandals of the last decade have irreparably harmed its image. Younger users refuse to adopt the flagship Facebook Blue. The company’s internal polling on two key metrics — good for the world (GFW) and cares about users (CAU) — shows Facebook’s reputation is in tatters. Talent is fleeing, too; Instacart alone recently poached 55 Facebook executives.

In 2012 and 2014, Instagram and WhatsApp were real dangers. Facebook extinguished both through acquisition. Yet today they represent the company’s two most promising, underutilized assets. They are the underinvested telephone networks of our time.

Weeks ago, Instagram head Adam Mosseri announced that the company no longer considers itself a photo-sharing app. Instead, its focus is entertainment. In other words, as the media widely reported, Instagram is changing to compete with TikTok.

TikTok’s strength represents an existential threat. U.S. children 4 to 15 already spend over 80 minutes a day on ByteDance’s TikTok, and it’s just getting started. The demographics are quickly expanding way beyond teenagers, as social products always have. For Instagram, it could be too little too late — as a part of Facebook, Instagram cannot acquire the technology and retain the talent it needs to compete with TikTok.

Imagine Instagram acquisitions of Squarespace to bolster its e-commerce offerings, or Etsy to create a meaningful marketplace. As a part of Facebook, Instagram is strategically adrift.

Likewise, a standalone WhatsApp could easily be a $100 billion market cap company. WhatsApp has a proud legacy of robust security offerings, but its brand has been tarnished by associations with Facebook. Discord’s rise represents a substantial threat, and WhatsApp has failed to innovate to account for this generation’s desire for community-driven messaging. Snapchat, too, is in many ways a potential WhatsApp killer; its young users use photography and video as a messaging medium. Facebook’s top augmented reality talents are leaving for Snapchat.

With 2 billion monthly active users, WhatApp could be a privacy-focused alternative to Facebook Blue, and it would logically introduce expanded profiles, photo-sharing capabilities and other features that would strengthen its offerings. Inside Facebook, WhatsApp has suffered from underinvestment as a potential threat to Facebook Blue and Messenger. Shareholders have suffered for it.

Beyond Instagram and WhatsApp, Facebook Blue itself is struggling. Q2’s earnings may have skyrocketed, but the increase in revenue hid a troubling sign: Ads increased by 47%, but inventory increased by just 6%. This means Facebook is struggling to find new places to run its ads. Why? The core social graph of Facebook is too old.

I fondly remember the day Facebook came to my high school; I have thousands of friends on the platform. I do not use Facebook anymore — not for political reasons, but because my friends have left. A decade ago, hundreds of people wished me happy birthday every year. This year it was 24, half of whom are over the age of 50. And I’m 32 years old. Teen girls run the social world, and many of them don’t even have Facebook on their phones.

Zuckerberg’s newfound push into the metaverse has been well covered, but the question remains: Why wouldn’t a Facebook serious about the metaverse acquire Roblox? Of course, the FTC would currently never allow it.

Facebook’s current clunky attempt at a hardware solution, with an emphasis on the workplace, shows little sign of promise. The launch was hardly propitious, as CNN reported, “While Bosworth, the Facebook executive, was in the middle of describing how he sees Workrooms as a more interactive way to gather virtually with coworkers than video chat, his avatar froze midsentence, the pixels of its digital skin turning from flesh-toned to gray. He had been disconnected.”

This is not the indomitable Facebook of yore. This is graying Facebook, freezing midsentence.

Facebook will generate more value for shareholders as three separate companies

Zuckerberg’s control of 58% of Facebook’s voting shares has forestalled a typical Wall Street reckoning: Investors are tiring of Zuckerberg’s unilateral power. Many justifiably believe the company is more valuable as the sum of its parts. The success of AT&T’s breakup is a case in point.

Five years after AT&T’s 1984 breakup, AT&T and the Baby Bells’ value had doubled compared to AT&T’s pre-breakup market capitalization. Pressure from Japanese entrants battered Western Electric’s market share, but greater competition in telephony spurred investment and innovation among the Baby Bells.

AT&T turned its focus to competing with IBM and preparing for the coming information age. A smaller AT&T became more nimble, ready to focus on the future rather than dwell on the past.

Standalone Facebook Blue, Instagram and WhatsApp could drastically change their futures by attracting talent and acquiring new technologies.

The U.K.’s recent opposition to Facebook’s $400 million GIPHY acquisition proves Facebook will struggle mightily to acquire even small bolt-ons.

Zuckerberg has always been one step ahead. And when he wasn’t, he was famously unprecious: “Copying is faster than innovating.” If he really believes in Facebook’s mission and recognizes that the situation cannot possibly get any better from here, he will copy AT&T’s solution before it is forced upon him.

Regulators are tying Zuckerberg’s hands behind his back as the company weathers body blows and uppercuts from Beijing to Silicon Valley. As Zuckerberg’s idol Augustus Caesar might have once said, carpe diem. It’s time to break Facebook.",Social Media,TechCrunch,https://techcrunch.com/2021/08/29/move-fast-and-break-facebook-a-bull-case-for-antitrust-enforcement/,"Facebook's monopoly over social media has led to an array of issues such as a tarnished brand image, underinvestment in Instagram and WhatsApp, and a lack of innovation. This has been recognized by both the public and the government, leading to increased pressure from the government and calls for Facebook to break up.","Information, Discourse & Governance"
265,Trump hints at stopping ‘powerful’ big tech in latest ‘get out the vote’ tweet – TechCrunch,"If there was any doubt that yesterday’s flogging of big tech CEOs by Senate Republicans was anything other than an electioneering stunt, President Trump has thumped the point home by tweeting a video message to voters in which he bashes “big tech” as (maybe) too powerful but certainly in need of being “spoken to” and (maybe) more.

The not-so-subtle suggestion being that a vote for Trump is a vote to break up the likes of Facebook, Google and Twitter.

In the video Trump signposts the DoJ’s antitrust suit against Google — ending with a call to his supporters to get out the vote. So the president is brandishing an anti-big tech message as the latest cudgel in his culture war, just a few days ahead of the 2020 US presidential election.

“For a long time I’ve been hearing about how powerful big tech is, whether it’s Facebook or Twitter or Google or any of them,” he begins the video, before making a quick vanity-dig about winning the 2016 election regardless of the “powerful” platforms being “totally against me”, as he glibly claims — entirely failing to mention that Facebook actually allowed its network to be a free and unfettered conduit for millions of pieces of anti-Clinton, pro-Trump propaganda cooked up in Russia.

Instead, he segues into a claim that the platforms have taken their power to a “a new level”, as he puts it — accusing them of “suppressing the corruption of Joe Biden” by “not letting the stories out”.

This is a direct reference to Trump’s Democrat challenger for the White House, and an indirect reference to a controversial New York Post story about a cache of emails purported to have been found on laptop hardware owned by Biden’s son Hunter — but which carry the distinct whiff of another election-focused political disinformation operation.

The big difference this time around is that “big tech” is rather more alive to the reputational risks to their platforms and companies if they’re found ignoring another orchestrated episode of election interference.

Hence both Facebook and Twitter limited the sharing of the Post’s story.

Twitter initially blocked links to it citing its hacked materials policy — though it later revised the policy after Republicans screamed “censorship”. And CEO Jack Dorsey got plenty more grilling on that theme at yesterday’s Senate hearing as Republican senators used the hearing as an opportunity to try to mint gotcha soundbites on bogus claims of big tech’s “anti-conservative bias & censorship”.

The tech CEOs mostly had to sit there and be bashed as it’s not politic for them to suggest Republicans might be experiencing more content moderation versus liberals because they break the rules more. Instead the electioneering pantomime ran on for hours.

Trump is just closing the loop on the politically biased soundbite fest by trying to turn tedious and trumped up claims of anti-conservative bias into a bald “get out the vote” message to his base.

“Big tech has to be spoken to and probably in some form has to be stopped,” is the closest he gets to an actual policy position here. So Trump voters shouldn’t get their hopes up that he might actually deliver a break up of Facebook et al. either.

The ironies are of course hot and heavy, given evidence shows social media algorithms’ baked-in preference for spreading controversial/outrageous content further and faster than the blander, more nuanced stuff that’s likely to be closer to the truth. Simply put, it’s human nature to click on the crazy stuff — and ad-funded platforms are fuelled by eyeball engagement. So lies have been great for big tech’s bottom lines.

That then means these very same “big tech” platforms tend to amplify Republican messaging — certainly of the Trumpian flavor, i.e. where trumped up claims, lacking in evidence and/or reality, are preferred. (Like, say, Trump calling Mexicans rapists or claiming the pandemic is over as thousands continue to die. Or that he has immunity from COVID-19 when the scientific consensus is we don’t know how long a person may be immune after fighting off the virus and we know some people have been reinfected with COVID-19, and so on.)

So the scale of the nonsense being peddled by Trump’s Republican party is indeed very strong and very powerful. But then, well, we haven’t been in Kansas for a long time.

At the time of writing, Twitter has also not placed any kind of contextual labelling on Trump’s tweet — despite the contents of the video arguably containing misinformation about big tech itself. But that’s just one more irony to add to the steaming pile.

And if you’re feeling a pang of pity for the tech CEOs caught in this partisan bind, it pays to remember they made their bed by claiming to operate community and content policies they didn’t — and still don’t — properly enforce. Which makes Trump their very own monster.",Social Media,TechCrunch,https://techcrunch.com/2020/10/29/trump-hints-at-stopping-powerful-big-tech-in-latest-get-out-the-vote-tweet/,"Social media algorithms tend to amplify Republican messaging, especially of the Trumpian flavor which is often inaccurate and lacking in evidence and reality, creating an environment where lies can be spread quickly and widely, leading to an increase in misinformation.","Information, Discourse & Governance"
266,Facebook failed to stop a child bride being auctioned on its platform – TechCrunch,"Facebook failed to prevent its platform being used to auction a 16-year-old girl off for marriage in South Sudan.

Child early and forced marriage (CEFM) is the most commonly reported form of gender-based violence in South Sudan, according to a recent Plan International report on the myriad risks for adolescent girls living in the war-torn region.

Now it seems girls in that part of the world have to worry about social media too.

Vice reported on the story in detail yesterday, noting that Facebook took down the auction post but not until after the girl had already been married off — and more than two weeks after the family first announced the attention to sell the child via its platform, on October 25.

Facebook said it first learned about the auction post on November 9, after which it says it took it down within 24 hours. It’s not clear how many hours out of the 24 it took Facebook to take the decision to remove the post.

A multimillionaire businessman from South Sudan’s capital city reportedly won the auction after offering a record “price” — of 530 cows, three Land Cruiser V8 cars and $10,000 — to marry the child, Nyalong Ngong Deng Jalang.

Plan International told Vice it’s the first known incident of Facebook being used to auction a child bride.

“It is really concerning because, as it was such a lucrative transaction and it attracted so much attention, we are worried that this could act as an incentive for others to follow suit,” the development organization told Vice.

A different human rights NGO posted to Twitter a screengrab of the deleted auction post, writing: “Despite various appeals made by human rights group, a 16 year old girl child became a victim to an online marriage auction post, which was not taken down by Facebook in South Sudan.”

Despite various appeals made by human rights group, a 16 year old girl child became a victim to an online marriage auction post, which was not taken down by Facebook in South Sudan. Sinking part is that people are now opting for social media for fulfilling orthodox rituals. pic.twitter.com/tj4cMADeFN — H4Human (@h4humanrights) November 20, 2018

We asked Facebook to explain how it failed to act in time to prevent the auction and it sent us the following statement, attributed to a spokesperson:

Any form of human trafficking — whether posts, pages, ads or groups is not allowed on Facebook. We removed the post and permanently disabled the account belonging to the person who posted this to Facebook. We’re always improving the methods we use to identify content that breaks our policies, including doubling our safety and security team to more than 30,000 and investing in technology.

The more than two-week delay between the auction post going live and the auction post being removed by Facebook raises serious questions about its claims to have made substantial investments in improving its moderation processes.

Human rights groups had directly tried to flag the post to Facebook. The auction had also reportedly attracted heavy local media attention. Yet it still failed to notice and act until weeks later — by which time it was too late because the girl had been sold and married off.

Facebook does not release country-level data about its platform so it’s not clear how many users it has in the South Sudan region.

Nor does it offer a breakdown of the locations of the circa 15,000 people it employs or contracts to carry out content review duties across its global content platform (which has 2 billion+ users).

Facebook admits that the content reviewers it uses do not speak every language in the world where its platform is used. Nor do they even speak every language that’s widely used in the world. So it’s highly unlikely it has any reviewers at all with a strong grasp of the indigenous languages spoken in the South Sudan region.

We asked Facebook how many moderators it employs who speak any of the languages in the South Sudan region (which is multilingual). A spokeswoman was unable to provide an immediate answer.

The upshot of Facebook carrying out retrospective content moderation from afar, relying on a tiny number of reviewers (relative to its total users), is that the company is failing to respond to human rights risks as it should.

Facebook has not established on-the-ground teams across its international business with the necessary linguistic and cultural sensitivities to be able to respond directly, or even quickly, to risks being created by its platform in every market where it operates. (A large proportion of its reviewers are sited in Germany — which passed a social media hate speech law a year ago.)

AI is not going to fix that very hard problem either — not in any human time-scale. And in the meanwhile, Facebook is letting actual humans take the strain.

But two weeks to notice and takedown a child bride auction is not the kind of metric any business wants to be measured by.

It’s increasingly clear that Facebook’s failure to invest adequately across its international business to oversee and manage the human rights impacts of its technology tools can have a very high cost indeed.

In South Sudan a lack of adequate oversight has resulted in its platform being repurposed as the equivalent of a high-tech slave market.

Facebook also continues to be on the hook for serious failings in Myanmar, where its platform has been blamed for spreading hate speech and accelerating ethnic violence.

You don’t have to look far to see other human rights abuses being aided and abetted by access to unchecked social media tools.",Social Media,TechCrunch,https://techcrunch.com/2018/11/21/facebook-failed-to-stop-a-child-bride-being-auctioned-on-its-platform/,"The lack of adequate oversight of Facebook's international business has led to its platform being used to facilitate human rights abuses, such as the auctioning off of a 16-year-old girl for marriage in South Sudan, and the spread of hate speech leading to ethnic violence in Myanmar.",Equality & Justice
267,Facebook grilled in Senate hearing over teen mental health – TechCrunch,"Last night, Facebook published two annotated slide decks in an attempt to contextualize the documents that The Wall Street Journal published this month, which reported evidence that the company is aware of its negative impact on teen mental health. These documents were released in anticipation of today’s Senate hearing on the mental health harms of Facebook and Instagram.

The Senate Committee on Commerce, Science, & Transportation questioned Facebook Global Head of Security Antigone Davis over two and a half hours, but lawmakers grew frustrated with Davis’ reticence to answer their questions directly, or provide much information that hasn’t been written in Facebook blog posts rebuking the WSJ reports.

Davis insisted that research from Facebook and Instagram has shown eight out of 10 young people say they have a neutral positive experience on the app, and that her team wants 10 out of 10 young users to have a good experience. But Senators pushed back with other findings from Facebook’s own data, like the fact that among teenagers with suicidal thoughts, 13% of British users and 6% of American users said they could trace those thoughts to Instagram. Senator Richard Blumenthal (who serves as Chair of the Subcommittee on Consumer Protection, Product Safety, and Data Security) said that his office did their own research by creating an account pretending to be a 13-year-old girl. Senator Blumenthal said they followed “easily findable accounts associated with extreme dieting and eating disorders.” Within a day, he said, the account’s recommendations were solely composed of accounts promoting self-harm and disordered eating.

“I congratulate you on a perfectly curated background,” Tennessee Senator Marsha Blackburn chided Davis. “It looks beautiful coming across the screen. I wish the messages that you were giving us were equally as attractive.”

“That is the perfect storm that Instagram has fostered and created. Facebook has asked us to trust it. But after these evasions and these revelations, why should we?” Senator Blumenthal asked.

But in the midst of filibustering tactics that fit right in on the Senate floor (“We’re pretty good at filibustering in the Senate, too,” Senator Klobuchar told Davis), the Facebook Global Head of Safety did elaborate on some of the company’s plans to improve young users’ experience, which Head of Instagram Adam Mosseri previously mentioned on Twitter.

“Young people indicated that when they saw uplifting content or inspiring content, that could move them away from some other issues that they’re struggling with,” Davis said at the hearing. “So one of the things that we’re actually looking at is called ‘nudges,’ where we would actually nudge someone who we saw potentially rabbit-holing down content towards more uplifting or inspiring content.”

In addition to a “nudges” feature, Davis said that the company is looking at a “take a break” feature, which would encourage users to stop looking at the app if they’ve been browsing certain content for too long. In 2018, Instagram introduced a “you’re all caught up” notice, which would appear when the user had scrolled through all posts from the last two days. This feature was introduced alongside “do not disturb” toggles, which helped users control when they wanted to receive notifications. These updates were part of “Time Well Spent” initiatives, designed to curb screen time and encourage healthier social media habits. But by 2020, the space beneath the “caught up” notice was turned into a feed of suggested posts and ads.

Big Tech continues to blatantly prioritize raking in revenue over protecting children and teens, and that must stop. We know that these companies won’t change their ways unless Congress forces them to. That’s why I am re-introducing the KIDS Act. pic.twitter.com/8rgZCevj15 — Ed Markey (@SenMarkey) September 30, 2021

At the hearing, Massachusetts Senator Ed Markey (a social media star in his own right) announced that he would reintroduce legislation with Senator Blumenthal called the KIDS (Kids Internet Design and Safety) Act, which seeks to create new protections for online users under 16. The bill would prohibit platforms directed at children from leveraging follower and like counts, push alerts that encourage users to use the app more, auto-play settings, badges that award elevated levels of engagement, or any design feature that unfairly encourages a user (“due to their age or inexperience,” the bill specifies) to make purchases, submit content, or spend more time on a platform.

Previously introduced in March 2020, Facebook has known about the proposed legislation for almost a year and a half.

“I think our company has made its position really well known that we believe it’s time for the update of internet regulations, and we’d be happy to talk to and work with you on that,” Davis told Senator Markey.

But when Markey directly asked if Facebook would support the KIDS Act, Davis said that Facebook would follow up on the question later.

“Well, your company has had this legislation in your possession for months. And you’re testifying here today before the committee that would have to pass this legislation,” said Senator Markey. “I just feel that delay and obfuscation is the legislative strategy of Facebook, especially since Facebook has spent billions of dollars on a marketing campaign calling on Congress to pass internet regulations, and Facebook purports to be committed to children’s well being.”

At the end of the hearing, Davis said that she hopes the Senate will have hearings with companies that have kid-focused apps, like TikTok and YouTube. Currently, Facebook has a Messenger Kids app, but the company but its Instagram for kids product on hold in light of WSJ’s reporting. Though WSJ has published six leaked documents from Facebook, the company itself only annotated and re-published two of them.",Social Media,TechCrunch,https://techcrunch.com/2021/09/30/facebook-grilled-in-senate-hearing-over-teen-mental-health/,"Social media has been linked to mental health issues in teens, and recent evidence from Facebook's own data has shown that 13% of British users and 6% of American users with suicidal thoughts traced them to Instagram. The Senate hearing discussed potential solutions, such as ""nudges"" and ""take a break"" features, but Senator Markey",Social Norms & Relationships
268,YouTube to reduce conspiracy theory recommendations in the UK – TechCrunch,"YouTube is expanding an experimental tweak to its recommendation engine that’s intended to reduce the amplification of conspiracy theories to the UK market.

In January, the video-sharing platform said it was making changes in the US to limit the spread of conspiracy theory content, such as junk science and bogus claims about historical events — following sustained criticism of how its platform accelerates damaging clickbait.

A YouTube spokeswoman confirmed to TechCrunch it is now in the process of rolling out the same update to suppresses conspiracy recommendations in the UK. She said it will take some time to take full effect — without providing detail on when exactly the changes will be fully applied.

The spokeswoman said YouTube acknowledges that it needs to do more to reform a recommendation system that has been shown time and again lifting harmful clickbait and misinformation into mainstream view. Though YouTube claims this negative spiral occurs only sometimes, and says on average its system points users to mainstream videos.

The company calls the type of junk content it’s been experimenting with recommending less often “borderline”, saying it’s stuff that toes the line of its acceptable content policies. In practice this means stuff like videos that make nonsense claims the earth is flat, or blatant lies about historical events such as the 9/11 terror attacks, or promote harmful junk about bogus miracle cures for serious illnesses.

All of which can be filed under misinformation ‘snake oil’. But for YouTube this sort of junk has been very lucrative snake oil as a consequence of Google’s commercial imperative being to keep eyeballs engaged in order to serve more ads.

More recently, though, YouTube has taken a reputational hit as its platform as been blamed for an extremist and radicalizing impact on young and impressionable minds by encouraging users to swallow junk science and worse.

A former Google engineer, Guillaume Chaslot, who worked on the YouTube recommendation algorithms went public last year to condemn what he described as the engine’s “toxic” impact which he said “perverts civic discussion” by encouraging users to create highly engaging borderline content.

Multiple investigations by journalists have also delved into instances where YouTube has been blamed for pushing people, including the young and impressionable, towards far right points of view via its algorithm’s radicalizing rabbit hole — which exposes users to increasingly extreme points of view without providing any context about what it’s encouraging them to view.

Of course it doesn’t have to be this way. Imagine if a YouTube viewer who sought out at a video produced by a partisan shock jock was suggested less extreme or even an entirely alternative political point of view. Or only saw calming yoga and mindfulness videos in their ‘up next’ feed.

YouTube has eschewed a more balanced approach to the content its algorithms select and recommend for commercial reasons. But it may also have been keen to avoid drawing overt attention to the fact that its algorithms are acting as de facto editors.

And editorial decisions are what media companies make. So it then follows that tech platforms which perform algorithmic content sorting and suggestion should be regulated like media businesses are. (And all tech giants in the user generated content space have been doing their level best to evade that sort of rule of law for years.)

That Google has the power to edit out junk is clear.

A spokeswoman for YouTube told us the US test of a reduction in conspiracy junk recommendations has led to a drop in the number of views from recommendations of more than 50%.

Though she also said the test is still ramping up — suggesting the impact on the viewing and amplification of conspiracy nonsense could be even greater if YouTube were to more aggressively demote this type of BS.

What’s very clear is the company has the power to flick algorithmic levers that determine what billions of people see — even if you don’t believe that might also influence how they feel and what they believe. Which is a concentration of power that should concern people on all sides of the political spectrum.

While YouTube could further limit algorithmically amplified toxicity the problem is its business continues to monetize on engagement, and clickbait’s fantastical nonsense is, by nature, highly engaging. So — for purely commercial reasons — it has a counter incentive not to clear out all YouTube’s crap.

How long the company can keep up this balancing act remains to be seen, though. In recent years some major YouTube advertisers have intervened to make it clear they do not relish their brands being associated with abusive and extremist content. Which does represent a commercial risk to YouTube — if pressure from and on advertisers steps up.

Like all powerful tech platforms, its business is also facing rising scrutiny from politicians and policymakers. And questions about how to ensure such content platforms do not have a deleterious effect on people and societies are now front of mind for governments in some markets around the world.

That political pressure — which is a response to public pressure, after a number of scandals — is unlikely to go away.

So YouTube’s still glacial response to addressing how its population-spanning algorithms negatively select for stuff that’s socially divisive and individually toxic may yet come back to bite it — in the form of laws that put firm limits on its powers to push people’s buttons.",Social Media,TechCrunch,https://techcrunch.com/2019/08/28/youtube-to-reduce-conspiracy-theory-recommendations-in-the-uk/,"The main undesirable consequence of Social Media discussed here is the spread of misinformation and conspiracy theories, which can be amplified by algorithms and have a negative effect on people and society.","Information, Discourse & Governance"
269,TikTok Waited Three Hours to Tell Cops About Livestreamed Suicide,"But it immediately began crafting a public relations strategy.

Wrong Priorities

In February 2019, a 19-year-old Brazilian man livestreamed his suicide on the popular video-sharing app TikTok. An hour and a half later, TikTok noticed and removed the video.

Three hours after that, TikTok finally contacted the police, a former employee of the Brazilian offices of TikTok parent company ByteDance has told The Intercept Brazil — and it spent those hours putting together a public relations strategy.

Clock’s Ticking

The employee, who spoke under the condition of anonymity, provided the Intercept with an image of an internal document detailing TikTok’s minute-by-minute response to the livestreamed suicide.

It suggests that TikTok found out about the stream via messages from influencers on WhatsApp — and not from the team in China tasked with monitoring content.

Advertisement

Advertisement

“The main issue was just how unprepared the Chinese team was for a situation like this,” the source told the Intercept, “where the app’s algorithm didn’t catch that it was a suicide, let alone bring down the livestream, even after so many complaints.”

Standard Statement

The Intercept wrote that TikTok didn’t answer any of its specific questions about the incident, but did provide a statement.

“We remain deeply saddened by this tragic incident and sympathize with the family,” the statement said. “We encourage anyone who needs support or is concerned about a friend or family member to contact a suicide hotline.”

READ MORE: TikTok Livestreamed a User’s Suicide — Then Got Its PR Strategy in Place Before Calling the Police [The Intercept]

Advertisement

Advertisement

More on TikTok: Researchers Find Major Security Flaws in Popular App TikTok

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Social Media,Futurism,https://futurism.com/the-byte/tiktok-waited-three-hours-tell-cops-livestreamed-suicide,"TikTok recently livestreamed a user's suicide, and instead of notifying the police immediately, the company prioritized developing a public relations strategy. This shows a disregard for the safety of users as well as the seriousness of the situation.",Security & Privacy
270,Where’s the accountability Facebook? – TechCrunch,"Facebook has yet again declined an invitation for its founder and CEO Mark Zuckerberg to answer international politicians’ questions about how disinformation spreads on his platform and undermines democratic processes.

But policymakers aren’t giving up — and have upped the ante by issuing a fresh invitation signed by representatives from another three national parliaments. So the call for global accountability is getting louder.

Now representatives from a full five parliaments have signed up to an international grand committee calling for answers from Zuckerberg, with Argentina, Australia and Ireland joining the UK and Canada to try to pile political pressure on Facebook.

The UK’s Digital, Culture, Media and Sport (DCMS) committee has been asking for Facebook’s CEO to attend its multi-month enquiry for the best part of this year, without success…

In its last request the twist was it came not just from the DCMS inquiry into online disinformation but also the Canadian Standing Committee on Access to Information, Privacy and Ethics.

This year policymakers on both sides of the Atlantic have been digging down the rabbit hole of online disinformation — before and since the Cambridge Analytica scandal erupted into a major global scandal — announcing last week they will form an ‘international grand committee’ to further their enquiries.

The two committees will convene for a joint hearing in the UK parliament on November 27 — and they want Zuckerberg to join them to answer questions related to the “platform’s malign use in world affairs and democratic process”, as they put it in their invitation letter.

Facebook has previously despatched a number of less senior representatives to talk to policymakers probing damages caused by disinformation — including its CTO, Mike Schroepfer, who went before the DCMS committee in April.

But both Schroepfer and Zuckerberg have admitted the accountability buck stops with Facebook’s CEO.

The company’s nine-month-old ‘Privacy Principles‘ also makes the following claim [emphasis ours]:

We are accountable In addition to comprehensive privacy reviews, we put products through rigorous data security testing. We also meet with regulators, legislators and privacy experts around the world to get input on our data practices and policies.

The increasingly pressing question, though, is to whom is Facebook actually accountable?

Zuckerberg went personally to the US House and Senate to face policymakers’ questions in April. He also attended a meeting of the EU parliament’s Conference of Presidents in May.

But the rest of the world continues being palmed off with minions. Despite some major, major harms.

Facebook’s 2BN+ user platform does not stop at the US border. And Zuckerberg himself has conceded the company probably wouldn’t be profitable without its international business.

Yet so far only the supranational EU parliament has managed to secure a public meeting with Facebook’s CEO. And MEPs there had to resort to heckling Zuckerberg to try to get answers to their actual questions.

“Facebook say that they remain “committed to working with our committees to provide any additional relevant information” that we require. Yet they offer no means of doing this,” tweeted DCMS chair Damian Collins today, reissuing the invitation for Zuckerberg. “The call for accountability is growing, with representatives from 5 parliaments now meeting on the 27th.”

The letter to Facebook’s CEO notes that the five nations represent 170 million Facebook users.

“We call on you once again to take up your responsibility to Facebook users, and speak in person to their elected representatives,” it adds.

Facebook say that they remain committed"" to working with our committees ""to provide any additional relevant information"" that we require. Yet they offer no means of doing this. The call for accountability is growing, with representatives from 5 parliaments now meeting on the 27th pic.twitter.com/VJFtpqUi0r — Damian Collins (@DamianCollins) November 7, 2018

The UK’s information commissioner said yesterday that Facebook needs to overhaul its business model, giving evidence to parliament on the “unprecedented” data investigation her office has been running which was triggered by the Cambridge Analytica scandal. She also urged policymakers to strengthen the rules on the use of people’s data for digital campaigning.

Last month the European parliament also called for Facebook to let in external auditors in the wake of Cambridge Analytica, to ensure users’ data is being properly protected — yet another invitation Facebook has declined.

Meanwhile an independent report assessing the company’s human rights impact in Myanmar — which Facebook commissioned but chose to release yesterday on the eve of the US midterms when most domestic eyeballs would be elsewhere — agreed with the UN’s damning assessment that Facebook did not do enough to prevent its platform from being used to incite ethical violence.

The report also said Facebook is still not doing enough in Myanmar.",Social Media,TechCrunch,https://techcrunch.com/2018/11/07/wheres-the-accountability-facebook/,"The international community is increasingly frustrated with Facebook's refusal to answer questions about how disinformation on its platform undermines democratic processes, with representatives from five parliaments now calling for the company's CEO to provide answers. The company has also declined invitations to let in external auditors and been criticized for not doing enough to prevent its platform from being used to","Information, Discourse & Governance"
271,Mozilla pulls ads off Facebook over data access concerns – TechCrunch,"Mozilla has announced it’s suspending its advertising on Facebook in the wake of the Cambridge Analytica privacy controversy — saying it has concerns the current default privacy settings remain risky, and having decided to take a fresh look at Facebook’s app permissions following the latest user data handling scandal.

This week the New York Times and The Observer of London reported that a researcher’s app had pulled personal information on about 270,000 Facebook users and 50 million of their friends back in 2015, and then passed that data haul to political consulting firm Cambridge Analytica in violation of Facebook’s policies.

Facebook’s policies previously allowed developers to siphon off app users’ Facebook friends data — though Facebook tightened up these permissions in 2014 — “to dramatically reduce data access”, as founder Mark Zuckerberg has now claimed — though evidently not dramatically enough for Mozilla.

Mozilla writes: “This news caused us to take a closer look at Facebook’s current default privacy settings given that we support the platform with our advertising dollars. While we believe there is still more to learn, we found that its current default settings leave access open to a lot of data – particularly with respect to settings for third party apps.”

It is also running a petition calling for Facebook to lock down app permission settings to ensure users’ privacy is “protected by default”, saying the current default settings “leave a lot of questions and a lot of data flying around”.

“Facebook’s current app permissions leave billions of its users vulnerable without knowing it,” it writes. “If you play games, read news or take quizzes on Facebook, chances are you are doing those activities through third-party apps and not through Facebook itself. The default permissions that Facebook gives to those third parties currently include data from your education and work, current city and posts on your timeline.

“We’re asking Facebook to change its policies to ensure third parties can’t access the information of the friends of people who use an app.”

Mozilla says it will “consider returning” to advertising on Facebook when — or presumably if — the company makes adequate changes to bolster default privacy settings.

“We are encouraged that Mark Zuckerberg has promised to improve the privacy settings and make them more protective. When Facebook takes stronger action in how it shares customer data, specifically strengthening its default privacy settings for third party apps, we’ll consider returning,” it writes. “We look forward to Facebook instituting some of the things that Zuckerberg promised today.”

We’ve reached out to Facebook for comment on Mozilla’s action and will update this story with any response.

At the time of writing Mozilla had not responded to questions about the move.

Even setting aside the current Facebook-Cambridge Analytica data handling scandal, big privacy-related changes are incoming to Facebook thanks to the European Union’s updated data protection framework, GDPR, which will apply from May 25 to any company that processes EU citizens’ personal data.

As part of those changes — and as Facebook tries to comply with the new EU privacy standard — in January the company announced it would be rolling out a new privacy center globally that would put core privacy settings in one place. That one-stop hub is yet to launch but must arrive before May 25.

Also in January Facebook published a set of privacy principles — including grand claims that: “We help people understand how their data is used”; “We design privacy into our products from the outset”; “We work hard to keep your information secure”; “You own and can delete your information”; and “We are accountable”.

Given the last of its published principles, it will be interesting to see which executive Facebook chooses to send to testify in front of Congress — to explain things like how it failed to protect the privacy of ~50M users nor even inform people their data had been siphoned off for illicit purposes.

Asked by CNN whether he will personally testify, Zuckerberg said he will do so “if it’s the right thing to do”. So we’ll soon find out how much that privacy accountability ‘principle’ is really worth.",Social Media,TechCrunch,https://techcrunch.com/2018/03/22/mozilla-pulls-ads-off-facebook-over-data-access-concerns/,"Mozilla has suspended its advertising on Facebook due to concerns over the current default privacy settings, which leave access open to a lot of data and can potentially expose users to exploitation. Facebook is now attempting to make changes to protect user privacy, but the efficacy of these changes will be put to the test when its CEO is asked to testify in",Security & Privacy
272,Study of political junk on Facebook raises fresh questions about its metrics – TechCrunch,"A midterms election study of political disinformation being fenced by Facebook’s platform supports the company’s assertion that a clutch of mostly right-leaning and politically fringe Pages it removed in October for sharing “inauthentic activity” were pulled for gaming its engagement metrics.

Though it remains unclear why it took Facebook so long to act against such prolific fakers — which the research suggests had been doping their metrics unchallenged on Facebook for up to five years.

The three-month research project carried out by Jonathan Albright of the Tow Center for Digital Journalism has largely focused on domestic political disinformation.

In a third and final blog detailing his findings he says some of the removed Pages had put up Facebook interaction numbers in the billions, and many of their videos consistently showed engagement in the tens of millions.

“I found that at least three of the Pages — removed less than a month ago — reported near-astronomical engagement numbers over the past five years,” he writes. “These are the kind of numbers that would be difficult to justify in almost any scenario — even in the case of a very large and sustained advertising spend on Facebook.”

One of the Pages with suspiciously high engagement flagged by Albright is a Page called Right Wing News.

“Less than a month before the 2018 midterm elections, when the Page was removed, Right Wing News had reported more engagement on Facebook over the past five years than the New York Times, The Washington Post, and Breitbart…combined,” he writes.

He also flags two other Pages that were removed by Facebook which had suspiciously high video views, called Silence is Consent and Daily Vine.

We’ve reached out to Facebook for a response.

The company is currently facing legal action from an unrelated group of advertisers who allege that Facebook knowingly misreported video metrics, inflating views for more than a year, and accusing the company of ad fraud. Facebook disputes the advertisers’ allegations.

In his blog, Albright also details how Facebook has seemingly failed to properly enforce a ban on conspiracy theorist and hate speech purveyor Alex Jones, whose personal Facebook Page and disinformation outlet, InfoWars, it pulled from its platform in August — writing: “Jones’ show and much of the removed InfoWars news content appears to have moved swiftly back onto the Facebook platform.”

How has Jones circumvented the ban on his main pages? By creating lots of similarly branded alternative Pages…

Albright writes that Facebook’s algorithms pushed Jones’ livestream show into search results when he was looking for Soros conspiracies: “And what did I get? The live high-definition stream of Jones’ show on Facebook — broadcast on one of the many InfoWars-branded Pages that is inconspicuously named “News Wars.”

According to his analysis, Jones’ InfoWars broadcasts appears to be almost back to where they were — in terms of views/engagement — before the Facebook ‘ban’ took down his two largest pages. Albright describes the “censorship” case as “a gross enforcement failure by Facebook”.

“Granular enforcement isn’t just reactive takedowns; it’s about proactive measures. This involves considering the factors — even the simple guerrilla marketing tactics — that play into how things like banned InfoWars live streams get further propagated,” he writes, summing up his findings.

“From what I’ve seen in this extensive look into Facebook’s platform, especially in regards to the company’s capacity to deal with the misuse of its platform as shown in the cases above — exactly two years after the end of the last election — I will argue that common sense approaches to platform integrity and manipulation still appear to be less of a priority for Facebook that automated detection and removal publicity.”

“The infinite gray area of information-sharing poses the real challenge: it’s the slippery soft conspiracy questions, the repetition of messages seen on shocking memes and statements like the “Soros Beto” caption [cited in the post], and the emotional clickbait that’s regularly shown in Jones’ InfoWars video cover stills. Without granular enforcement, the non-foreign bad actors will only get better, and refine their tactics to increase Americans’ exposure to [hyperpartisan junk news],” he adds.

“Information integrity is more than the scrutiny of provable statements or the linking of some data to shared content with an “i.” Transparency involves more than verifying one Page manager, putting it alongside a date and voluntary disclosure for a paid political campaign, and adding to an political “ad archive.”

Albright has posted additional findings from his three month trawl through the Facebook fake-o-sphere this week — including raising concerns about political Pages running ads targeting the US midterms which have changed moderator structure and included foreign-based administrators, as well as finding some running political ads that lacked a ‘Paid for’ disclosure label.

He also identifies a shift of tactics about political disinformation operators to sharing content in closed Facebook Groups where it’s less visible to outsiders trying to track junk news — yet can still be shared across Facebook’s platform to skew voters’ opinions.",Social Media,TechCrunch,https://techcrunch.com/2018/11/06/study-of-political-junk-on-facebook-raises-fresh-questions-about-its-metrics/,"Social media platforms are being used to spread political disinformation and has led to a shift of tactics among operators to share their content in closed Facebook Groups, making it less visible to outsiders but still able to influence voters' opinions.",Politics
273,Former YouTube star sentenced to ten years in prison for child porn – TechCrunch,"Former Youtube star Austin Jones has been sentenced to ten years in a US federal prison after pleading guilty to persuading underage girls to send him explicit videos of themselves.

Jones, who made a name for himself online singing covers of songs, was arrested and charged in 2017 with two counts of producing child pornography.

He later pled guilty to one charge of receiving child pornography — admitting in a plea agreement that in 2016 and 2017 he enticed six girls to to produce and send explicit videos to “prove” they were his “biggest fan”, per Buzzfeed.

“Production and receipt of child pornography are extraordinarily serious offenses that threaten the safety of our children and communities,” it quotes assistant U.S. Attorney Katherine Neff Welsh writing in a sentencing memo. “Jones’s actions took something from his victims and their families that they will never be able to get back.”

At the height of his YouTube fame Jones had around 540,000 subscribers to his channel and more than 20M video views.

In a 2015 apology vlog, after reports emerged of Jones asking young fans to send him twerking videos, he claimed it never went further than that. “There were never any nudes, never any physical contact, it never happened,” he said then.

But in his plea agreement Jones admitted to attempting to persuade more than thirty underage fans to send him explicit photos or videos.

YouTube removed Jones’ channel after he pled guilty in February — saying it had violated its community guidelines. But the Google-owned company initially refused to shutter it, telling the BBC a few days earlier that while it does have a policy of removing content when a person is convicted of a crime “in some cases” it does so only if the content is closely related to the crime committed.

Describing her experience in a vlog also posted to YouTube, one former fan she had received messages from Jones asking her for twerking videos prior to his 2015 apology video when she was 14-years-old.

“I just don’t understand how these people can let the fame get to their heads that much that they think it’s alright to do something to people like this,” she said. “It’s so messed up. But the fact that his fanbase was these vulnerable, insecure young girls makes it so much worse than it already is… He knew that that was his fanbase and he took advantage of that.”",Social Media,TechCrunch,https://techcrunch.com/2019/05/04/former-youtube-star-sentenced-to-ten-years-in-prison-for-child-porn/,"In this case, the main undesirable consequence of Social Media is that it can enable those with fame to take advantage of vulnerable young people, as Austin Jones did when he asked underage fans to send him explicit videos.",Social Norms & Relationships
274,"Facebook knows Instagram harms teens. Now, its plan to open the app to kids looks worse than ever – TechCrunch","Facebook knows Instagram harms teens. Now, its plan to open the app to kids looks worse than ever

Facebook is in the hot seat again.

The Wall Street Journal published a powerful multi-part series on the company this week, drawing from internal documents on everything from the company’s secretive practice of whitelisting celebrities to its knowledge that Instagram is taking a serious toll on the mental health of teen girls.

The flurry of investigative pieces makes it clear that what Facebook says in public doesn’t always reflect the company’s knowledge on known issues behind the scenes. The revelations still managed to shock even though Facebook has been playing dumb about the various social ills it has sown for years. (Remember when Mark Zuckerberg dismissed the notion that Facebook influenced the 2016 election as “crazy?”) Facebook’s longstanding PR playbook is to hide its dangers, denying knowledge of its darker impacts on society publicly, even as research spells them out internally.

That’s all well and good until someone gets ahold of the internal research.

One of the biggest revelations from the WSJ’s report: The company knows that Instagram poses serious dangers to mental health in teenage girls. An internal research slide from 2019 acknowledged that “We make body image issues worse for one in three teen girls” — a shocking admission for a company charging ahead with plans to expand to even younger and more vulnerable age groups.

As recently as May, Instagram’s Adam Mosseri dismissed concerns around the app’s negative impact on teens as “quite small.”

But internally, the picture told a different story. According to the WSJ, from 2019 to 2021, the company conducted a thorough deep dive into teen mental health, including online surveys, diary studies, focus groups and large-scale questionnaires.

According to one internal slide, the findings showed that 32% of teenage girls reported that Instagram made them have a worse body image. Of research participants who experienced suicidal thoughts, 13% of British teens and 6% of American teens directly linked their interest in killing themselves to Instagram.

“Teens blame Instagram for increases in the rate of anxiety and depression,” another internal slide stated. “This reaction was unprompted and consistent across all groups.”

Following the WSJ report, Senators Marsha Blackburn (R-TN) and Richard Blumenthal (D-CT) announced a probe into Facebook’s lack of transparency around internal research showing that Instagram poses serious and even lethal danger to teens. The Senate Subcommittee on Consumer Protection, Product Safety, and Data Security will launch the investigation.

“We are in touch with a Facebook whistleblower and will use every resource at our disposal to investigate what Facebook knew and when they knew it – including seeking further documents and pursuing witness testimony,” Senators Blackburn and Blumenthal wrote. “The Wall Street Journal’s blockbuster reporting may only be the tip of the iceberg.”

Blackburn and Blumenthal weren’t the only U.S. lawmakers alarmed by the new report. Sen. Ed Markey (D-MA), Rep. Kathy Castor (D-FL), and Lori Trahan (D-MA) sent Facebook their own letter demanding that the company walk away from its plan to launch Instagram for kids. “Children and teens are uniquely vulnerable populations online, and these findings paint a clear and devastating picture of Instagram as an app that poses significant threats to young people’s wellbeing,” the lawmakers wrote.

Big Tech has become the new Big Tobacco. Facebook is lying about how their product harms teens. https://t.co/85oo3B9oO0 — Rep. Ken Buck (@RepKenBuck) September 14, 2021

Facebook gobbled up Instagram because they were too chicken to compete against them fair & square for younger users. When there’s one big game in town, there’s a whole lot less pressure to offer the best service—or do the least damage. #BreakUpBigTechhttps://t.co/mIyHQ2iPs8 — Elizabeth Warren (@ewarren) September 16, 2021

In May, a group of 44 state attorneys general wrote to Instagram to encourage the company to abandon its plans to bring Instagram to kids under the age of 13. “It appears that Facebook is not responding to a need, but instead creating one, as this platform appeals primarily to children who otherwise do not or would not have an Instagram account,” the group of attorneys general wrote. They warned that an Instagram for kids would be “harmful for myriad reasons.”

In April, a collection of the same Democratic lawmakers expressed “serious concerns” about Instagram’s potential impact on the well-being of young users. That same month, a coalition of consumer advocacy organizations also demanded that the company reconsider launching a version of Instagram for kids.

According to the documents obtained by the WSJ, all of those concerns look extremely valid. In spite of extensive internal research and their deeply troubling findings, Facebook has downplayed its knowledge publicly, even as regulators regularly pressed the company for what it really knows.

Instagram’s Mosseri may have made matters worse Thursday when he made a less than flattering analogy between social media platforms and vehicles. “We know that more people die than would otherwise because of car accidents, but by and large, cars create way more value in the world than they destroy,” Mosseri told Peter Kafka on Recode’s media podcast. “And I think social media is similar.”

Mosseri dismissed any comparison between social media and drugs or cigarettes in spite of social media’s well-researched addictive effects, likening social platforms to the auto industry instead. Naturally, the company’s many critics jumped on the car comparison, pointing to their widespread lethality and the fact that the auto industry is heavily regulated — unlike social media.",Social Media,TechCrunch,https://techcrunch.com/2021/09/16/facebook-instagram-for-kids-mosseri-wsj-teen-girls/,"The Wall Street Journal's investigation into Facebook's internal practices has revealed that the company knew of the serious mental health risks of its Instagram platform, yet has downplayed this knowledge in public and even considered opening the service up to kids. This has been met with criticism from lawmakers and consumer advocacy groups, who have urged the company to reconsider.",Social Norms & Relationships
275,A guide to being an ethical online investigator,"But this activity raises some complex ethical and practical issues. How can you, an average person, be an ethical digital activist? What counts as going too far? How can you keep yourself safe? How can you participate in a way that doesn’t put anyone in danger? Below are some guidelines that might help.

Remember, you are not a hacker: There’s a big difference between accessing publicly available information, like a photo from a Facebook profile page that documents illegal activity, and hacking into a person’s otherwise private account to find that photo. That’s crossing the line.In the US, the Computer Fraud and Abuse Act (CFAA) limits the amount of access a person has to another’s information “without authorization,” which is undefined; this lack of clarity has frustrated lawyers who represent activists. “Those who do [violate CFAA] are breaking the law, and they’re criminals,” says Max Aliapoulios, a PhD student and cybersecurity researcher at New York University. It’s worth keeping in mind regional laws as well. In the European Union, “publicly identifying an individual necessarily means processing personally identifiable information; therefore individuals performing such activities need a legal basis to do so [under Article 6 of the GDPR],” says Ulf Buermeyer, the founder and legal director of Freiheitsrechte, a German-based civil rights organization.

Ethical issues abound: It’s not just legal issues that would-be amateur online investigators need to be aware of. Much of the online activity carried out in the wake of the Capitol riots raises ethical questions, too. Should a person who didn’t storm the Capitol but attended the rallies leading up to the riots be identified and risk punishment at work? Do those who were in and around the Capitol on January 6 automatically lose the right to privacy even if they weren’t involved in riots? It’s worth thinking through how you feel about some of these questions before you continue. Few are clear cut.

So, where does the information come from? “Our bread and butter is open source,” Fiorella says. “Open-source media” refers to information that is publicly available for use. Data archivists, or those who collect and preserve information online for historical purposes, accessed such open-source data to save posts before they disappeared as social media companies pushed President Donald Trump and many of his supporters off their platforms. “If you were at the Capitol storming and recorded video and took selfies that anyone can access, and it’s openly available on the internet, it’s fair game,” says Fiorella.

It’s your First Amendment right to access open-sourced information. Hacktivists and digital activists trawling social media alike will agree on this: they say it’s the most important aspect of their work. “Utilizing open-source intelligence isn’t a crime,” says Daly Barnett, an activist and staff technologist at the Electronic Frontier Foundation, a nonprofit digital rights group. “Archiving isn’t a crime. Freedom of information is good.”

Misidentification is a real danger. “Anyone with an internet connection and free time and willingness to do these things can be part of crowdsourcing efforts to clarify what happened,” Fiorella says. But crowdsourced efforts can be problematic, because people may zero in on the wrong individual. “There’s a fundamental tension here,” says Emmi Bevensee, a researcher and founder of the Social Media Analysis Toolkit, an open-source tool that tracks trends across mainstream and fringe social media platforms. “The more people you have working on a problem, the more likely you are to find the needle in the haystack. There’s a risk doing things like this, though. Not everyone has the same research skills or methodological accountability”—and mistakes can be devastating for the person misidentified. Misidentification carries potential legal risks, too.

You can join up with more established investigators instead of going it alone. There is, obviously, the FBI, which has collected images and is seeking the public’s help in identifying domestic terrorists. Bellingcat, one of the most respected, thorough investigatory sites devoted to this purpose, has created a Google spreadsheet for images of suspects that need identifying. Organizations also often have ethical standards put in place to guide new sleuths, like this one Bellingcat created in light of the Black Lives Matters protests.

Don’t doxx. Doxxing—or digging up personal information and sharing it publicly—is illegal. “The majority of doxxing has occurred from open-source intelligence,” Barnett says, and data hygiene is still something many people online struggle with. If you come across passwords, addresses, phone numbers, or any other similar identifier, do not share it—it’s a crime to do so. r/Datahoarder, a Reddit archiving group, notes that its members “do NOT support witch hunting.”

If you find something online that could be incriminating, ask, “Am I putting this person in danger?” Fiorella says he asks himself that question consistently, particularly in cases where a person might have few followers and is using social media just to share images with friends.

Show your methodology. Just like in middle school math class, show your work and how you got your results. Data researchers who do this work are famously diligent and exhaustive in how they record their work and triple-check their information. That sort of checking is especially important to ensure that people are properly identified and that others can learn from and retrace your steps for subsequent prosecution. (Methodology may take some technical expertise in some cases, and data researching organizations often run workshops and training sessions to help people learn how to do this.)

Do not share names online. Let’s say you see a picture of a possible suspect online and you recognize who it is. While you might be tempted to tag the person, or screenshot the image and put some commentary on your Instagram to get that addictive stream of likes, don’t. This work needs to be deliberate and slow, says Fiorella: “There’s a risk of misidentifying a person and causing harm.” Even if there’s no doubt that you have figured out who a person is, hold back and, at the most, submit your information to an organization like Bellingcat or the FBI to check your work and make sure it is correct.

You will run into situations where things are not clear. Theo shared the story of the viral video in which a Black Los Angeles woman is physically attacked by Trump supporters calling her the n-word. In the video, a man is seen with his arms around the woman amid the violent, jeering crowd. In initial reports, the man was described as part of the mob and harming the woman. Video footage seemed to show him putting her in the way of pepper spray, for example. Then police said the man was actually trying to protect the woman and that she had confirmed this version of events, though she later suggested to BuzzFeed that perhaps he ended up doing as much harm as good. Theo shared the image of the man in the immediate aftermath of the incident, and then he saw the account suggesting he was a good Samaritan. “I felt horrible,” he says. Theo points out that the man was also recorded using xenophobic and racist language, but “that got me to pause a little bit and think about what I’m doing that could impact people,” he says. “It’s a blurred line.” It doesn’t hurt to repeat it again: Do not share names online.

Your safety may be at risk. Theo says he has received death threats and has not felt safe in the past week, consistently looking over his shoulder if he steps out. Bevensee has received multiple death threats. Many digital activists have burner phones and backup computers, and work away from their families to protect them.

Keep your mental health in mind. This work can involve viewing violent images. Theo says he has been dealing with migraine headaches, sleep problems, paranoia, and the distress that comes with trying to keep up with his day job while handling his Instagram accounts and its sister Twitter account, @OutTerrorists. “I’m only one person, and I have to handle DMs and keep everything up to date,” he says, noting that he also updates posts with verified identifications from the FBI, goes through comments, and forwards information to the FBI himself. Take time to process and realize that it’s okay to feel upset. It’s one thing to use this as motivation to right the wrongs of the world, but nearly every expert and activist told me that having a way to deal with disturbing images is important.

Share your information with law enforcement—if it’s appropriate. Bevensee and Aliapoulios said the digital activism movement was a direct response to the perceived lack of official action. Many activists have a strong distrust of US law enforcement, pointing to the difference between how the Capitol rioters and Black Lives Matter protesters were treated. But in the case of the insurrection, which carries federal charges, experts and activists agree that the right thing to do is to take information to the authorities.",Social Media,MIT,https://www.technologyreview.com/2021/01/14/1015931/how-to-be-an-ethical-online-investigator-activist/,"The danger of misidentification and doxxing, as well as the risks of personal safety and mental health associated with digital activism, are all major concerns when engaging in activism on Social Media. It is important to remember that accessing private information without authorization is a crime, and that ethical and legal considerations must be taken into account.",Security & Privacy
276,Facebook partially documents its content recommendation system – TechCrunch,"Algorithmic recommendation systems on social media sites like YouTube, Facebook and Twitter have shouldered much of the blame for the spread of misinformation, propaganda, hate speech, conspiracy theories and other harmful content. Facebook, in particular, has come under fire in recent days for allowing QAnon conspiracy groups to thrive on its platform and for helping militia groups to scale membership. Today, Facebook is attempting to combat claims that its recommendation systems are at any way at fault for how people are exposed to troubling, objectionable, dangerous, misleading and untruthful content.

The company has, for the first time, made public how its content recommendation guidelines work.

In new documentation available in Facebook’s Help Center and Instagram’s Help Center, the company details how Facebook and Instagram’s algorithms work to filter out content, accounts, Pages, Groups and Events from its recommendations.

Currently, Facebook’s Suggestions may appear as Pages You May Like, “Suggested For You” posts in News Feed, People You May Know, or Groups You Should Join. Instagram’s suggestions are found within Instagram Explore, Accounts You May Like and IGTV Discover.

The company says Facebook’s existing guidelines have been in place since 2016 under a strategy it references as “remove, reduce, and inform.” This strategy focuses on removing content that violates Facebook’s Community Standards, reducing the spread of problematic content that does not violate its standards, and informing people with additional information so they can choose what to click, read or share, Facebook explains.

The Recommendation Guidelines typically fall under Facebook’s efforts in the “reduce” area, and are designed to maintain a higher standard than Facebook’s Community Standards, because they push users to follow new accounts, groups, Pages and the like.

Facebook, in the new documentation, details five key categories that are not eligible for recommendations. Instagram’s guidelines are similar. However, the documentation offers no deep insight into how Facebook actually chooses what to recommend to a given user. That’s a key piece to understanding recommendation technology, and one Facebook intentionally left out.

One obvious category of content that many not be eligible for recommendation includes those that would impede Facebook’s “ability to foster a safe community,” such as content focused on self-harm, suicide, eating disorders, violence, sexually explicit content, regulated content like tobacco or drugs or content shared by non-recommendable accounts or entities.

Facebook also claims to not recommend sensitive or low-quality content, content users frequently say they dislike and content associated with low-quality publishings. These further categories include things like clickbait, deceptive business models, payday loans, products making exaggerated health claims or offering “miracle cures,” content promoting cosmetic procedures, contests, giveaways, engagement bait, unoriginal content stolen from another source, content from websites that get a disproportionate number of clicks from Facebook versus other places on the web or news that doesn’t include transparent information about the authorship or staff.

In addition, Facebook claims it won’t recommend fake or misleading content, like those making claims found false by independent fact checkers, vaccine-related misinformation and content promoting the use of fraudulent documents.

It says it will also “try” not to recommend accounts or entities that recently violated Community Standards, shared content Facebook tries to not recommend, posted vaccine-related misinformation, engaged in purchasing “Likes,” has been banned from running ads, posted false information or are associated with movements tied to violence.

The latter claim, of course, follows recent news that a Kenosha militia Facebook Event remained on the platform after being flagged 455 times after its creation, and had been cleared by four moderators as non-violating content. The associated Page had issued a “call to arms” and hosted comments about people asking what types of weapons to bring. Ultimately, two people were killed and a third was injured at protests in Kenosha, Wisconsin when a 17-year old armed with an AR-15-style rifle broke curfew, crossed state lines and shot at protestors.

Given Facebook’s track record, it’s worth considering how well Facebook is capable of abiding by its own stated guidelines. Plenty of people have found their way to what should be ineligible content, like conspiracy theories, dangerous health content, COVID-19 misinformation and more by clicking through on suggestions at times when the guidelines failed. QAnon grew through Facebook recommendations, it’s been reported.

It’s also worth noting, there are many gray areas that guidelines like these fail to cover.

Militia groups and conspiracy theories are only a couple of examples. Amid the pandemic, U.S. users who disagreed with government guidelines on business closures can easily find themselves pointed toward various “reopen” groups where members don’t just discuss politics, but openly brag about not wearing masks in public or even when required to do so at their workplace. They offer tips on how to get away with not wearing masks, and celebrate their successes with selfies. These groups may not technically break rules by their description alone, but encourage behavior that constitutes a threat to public health.

Meanwhile, even if Facebook doesn’t directly recommend a group, a quick search for a topic will direct you to what would otherwise be ineligible content within Facebook’s recommendation system.

For instance, a quick search for the word “vaccines” currently suggests a number of groups focused on vaccine injuries, alternative cures and general anti-vax content. These even outnumber the pro-vax content. At a time when the world’s scientists are trying to develop protection against the novel coronavirus in the form of a vaccine, allowing anti-vaxxers a massive public forum to spread their ideas is just one example of how Facebook is enabling the spread of ideas that may ultimately become a global public health threat.

The more complicated question, however, is where does Facebook draw the line in terms of policing users having these discussions versus favoring an environment that supports free speech? With few government regulations in place, Facebook ultimately gets to make this decision for itself.

Recommendations are only a part of Facebook’s overall engagement system, and one that’s often blamed for directing users to harmful content. But much of the harmful content that users find could be those groups and Pages that show up at the top of Facebook search results when users turn to Facebook for general information on a topic. Facebook’s search engine favors engagement and activity — like how many members a group has or how often users post — not how close its content aligns with accepted truths or medical guidelines.

Facebook’s search algorithms aren’t being similarly documented in as much detail.",Social Media,TechCrunch,https://techcrunch.com/2020/08/31/facebook-partially-documents-its-content-recommendation-system/,"Social media sites have been heavily criticized for their algorithmic recommendation systems, which have been blamed for the spread of misinformation, propaganda, hate speech, conspiracy theories, and other harmful content. Facebook, in particular, has come under fire for its search algorithms, which prioritize engagement and activity rather than quality content.","Information, Discourse & Governance"
277,NFL teams will now be fined for posting videos and GIFs during games – TechCrunch,"NFL teams will now be fined for posting videos and GIFs during games

The NFL just announced a new social media policy that is about as anti-fan friendly as it gets.

According to league memos obtained by ESPN, the league’s social media policy (which takes effect tomorrow) effectively bans all video-based social media during games.

First, it restricts them from going live on apps like Facebook Live or Periscope during or after the game. Previously, teams have gone live to share behind-the-scenes experiences with fans, but this will no longer be allowed.

Perhaps more importantly, teams will also be banned from posting highlights of the game (either videos or GIFs) from kickoff until 60 minutes after a game ends.

The NFL’s official account will still post select video highlights during games, which teams are allowed to retweet or repost.

Violations will come with steep fines — $25,000 for a first offense, $50,000 for a second and $100,000 for a third.

So what does this look like?

Last night when the Panthers scored a touchdown they posted this:

While the NFL’s official account was able to post this:

Which do you think is more captivating to fans? And sure, while the Panthers could just retweet the NFL’s video, this won’t always be the case.

Last night there was only one game, so the NFL’s social media team could essentially post real-time video highlights for every play. But what happens on Sunday when there are seven games going on at once? Will the NFL have the bandwidth to capture every single highlight that teams want to share, and be able to do it fast enough that the teams can reshare it while it is still relevant?

A losing bet

These rules come at a time during the season when NFL viewership is down 14 percent from last year. Some blame this on the distracting presidential election, some blame it on NFL stars being hurt or suspended and I have blamed it on boring matchups and poor performances during the first five weeks.

So at a time when viewership is down, this is absolutely the worst policy for the NFL to adopt. The league wants to eliminate video shares so people can only watch the game on TV, but don’t they understand that social media drives viewership? People turn on the TV because they see an awesome, real-time highlight on Twitter that gets them excited about the game.

Want proof? Last season NBA viewership was up 10 percent. And the NBA has been extremely liberal and innovative in their social media strategy, essentially letting teams post whatever highlights they want. The result is a deluge of GIFs, Vines and video highlights that remind you there is an exciting NBA game on TV that you should watch right now.

The policy goes into effect tomorrow, and should make for an extra boring Football Sunday this weekend — at least when it comes to social media.",Social Media,TechCrunch,https://techcrunch.com/2016/10/11/nfl-teams-will-now-be-fined-for-posting-videos-and-gifs-during-games/,"The NFL's new social media policy, which takes effect tomorrow, bans teams from posting highlights of the game in video or GIF form from kickoff until 60 minutes after the game ends. This policy is likely to have a negative effect on viewership, as social media is often used to drive people to watch games live on TV.",User Experience & Entertainment
278,Snapchat Dysmorphia and the Real Dangers of Perceived Flaws,"Snapchat Dysmorphia

n. A fixation on perceived flaws in one’s appearance, caused by seeing too many filtered photos.

People used to show up in plastic surgeons’ offices with photos of movie stars, asking for Angelina’s lips or Jon Hamm’s chin. Today they come with selfies, asking to look like themselves. Not the human selves that mock us all in fitting-room mirrors, of course, but the sparkling, digitally embellished versions that increasingly populate our social feeds.

On platforms like Snapchat and Instagram, users now routinely deploy filters and tools like Facetune for selfie-improvement, fashioning reflections that better capture their true inner beauty. Swipe away acne or wrinkles. Swipe again for big soulful eyes, a thinner nose. You can even change the shape of your face.

Such fixes used to be just for glamour shots of celebrities. But nowadays, with flawless skin and symmetrical faces all over social media, the “beautiful people” are our peers. It’s enough to give you a complex. In fact, doctors have begun to speak of “Snapchat dysmorphia,” an obsession with normal imperfections that, for teens especially, can cause real harm. And it’s driving many to seek surgery, in hopes of editing their faces IRL like they do on their phones.

Snap Inc. can’t be thrilled to have its name on a new mental disorder (a brand hijacking almost as bad as the one Hormel suffered with spam). It’s response: Lighten up, filters are just a fun tool for personal expression. Yep, all good fun—until your kid comes home from the surgeon with permanent deer face.

This article appears in the November issue. Subscribe now.

More Great WIRED Stories",Social Media,WIRED,https://www.wired.com/story/snapchat-dysmorphia-real-dangers-perceived-flaws/,"The article discusses ""Snapchat Dysmorphia,"" a fixation on perceived flaws in one's appearance caused by seeing too many filtered photos on social media, and how it is driving teens especially to seek surgery in hopes of editing their faces IRL.",Social Norms & Relationships
279,How WhatsApp Could Worsen Brazil’s Yellow Fever Outbreak,"In remote areas of Brazil’s Amazon basin, yellow fever used to be a rare, if regular visitor. Every six to ten years, during the hot season, mosquitoes would pick it up from infected monkeys and spread it to a few loggers, hunters, and farmers at the forests’ edges in the northwestern part of the country. But in 2016, perhaps driven by climate change or deforestation or both, the deadly virus broke its pattern.

Yellow fever began expanding south, even through the winter months, infecting more than 1,500 people and killing nearly 500. The mosquito-borne virus attacks the liver, causing its signature jaundice and internal hemorrhaging (the Mayans called it xekik, or “blood vomit”). Today, that pestilence is racing toward Rio de Janeiro and Sao Paulo at the rate of more than a mile a day, turning Brazil’s coastal megacities into mega-ticking-timebombs. The only thing spreading faster is misinformation about the dangers of a yellow fever vaccine—the very thing that could halt the virus’s advance. And nowhere is it happening faster than on WhatsApp.

In recent weeks, rumors of fatal vaccine reactions, mercury preservatives, and government conspiracies have surfaced with alarming speed on the Facebook-owned encrypted messaging service, which is used by 120 million of Brazil’s roughly 200 million residents. The platform has long incubated and proliferated fake news, in Brazil in particular. With its modest data requirements, WhatsApp is especially popular among middle and lower income individuals there, many of whom rely on it as their primary news consumption platform. But as the country’s health authorities scramble to contain the worst outbreak in decades, WhatsApp’s misinformation trade threatens to go from destabilizing to deadly.

On January 25, Brazilian health officials launched a mass campaign to vaccinate 95 percent of residents in the 69 municipalities directly in the disease’s path—a total of 23 million people. A yellow fever vaccine has been mandatory since 2002 for any Brazilian born in regions where the virus is endemic. But in the last two years the disease has pushed beyond its normal range into territories where fewer than a quarter of people are immune, including the urban areas of Rio and Sao Paulo.

By the time of the announcement, the fake news cycle was already underway. Earlier in the month an audio message from a woman claiming to be a doctor at a well-known research institute began circulating on WhatsApp, warning that the vaccine is dangerous. (The institute denied that the recording came from any of its employees). A few weeks later it was a story linking the death of a university student to the vaccine. (That too proved to be a false report). In February, Igor Sacramento’s mother-in-law messaged him a pair of videos suggesting that the yellow fever vaccine was actually a scam aimed at reducing the world population. A health communication researcher at Fiocruz, one of Brazil’s largest scientific institutions, Sacramento recognized a scam when he saw one. And no, it wasn’t a global illuminati plot to kill off his countrymen. But he could understand why people would be taken in by it.",Social Media,WIRED,https://www.wired.com/story/when-whatsapps-fake-news-problem-threatens-public-health/,"Social media has become a platform for the spread of misinformation about a yellow fever vaccine, threatening to have deadly consequences in Brazil as the virus spreads. This misinformation is causing people to mistrust the vaccine, potentially leading to an increase in the disease's spread and resulting fatalities.","Information, Discourse & Governance"
280,Tech platforms called to support public interest research into mental health impacts – TechCrunch,"The tech industry has been called on to share data with public sector researchers so the mental health and psychosocial impacts of their service on vulnerable users can be better understood, and also to contribute to funding the necessary independent research over the next ten years.

The UK’s chief medical officers have made the call in a document setting out advice and guidance for the government about children’s and young people’s screen use. They have also called for the industry to agree a code of conduct around the issue.

Concerns have been growing in the UK about the mental health impacts of digital technologies on minors and vulnerable young people.

Last year the government committed to legislate on social media and safety. It’s due to publish a white paper setting out the detail of its plans before the end of the winter, and there have been calls for platforms to be regulated as publishers by placing a legal duty of care on them to protect non-adult users from harm. Though it’s not yet clear whether the government intends to go that far.

“The technology industry must share data they hold in an anonymised form with recognised and registered public sector researchers for ethically agreed research, in order to improve our scientific evidence base and understanding,” the chief medical officers write now.

After reviewing the existing evidence the CMOs say they were unable to establish a clear link between screen-based activities and mental health problems.

“Scientific research is currently insufficiently conclusive to support UK CMO evidence-based guidelines on optimal amounts of screen use or online activities (such as social media use),” they note, hence calling for platforms to support further academic research into public health issues.

Last week the UK parliament’s Science and Technology Committee made a similar call for high quality anonymized data to be provided to further public interest research into the impacts of social media technologies.

We asked Facebook-owned Instagram whether it will agree to provide data to public sector mental health and wellbeing researchers earlier this week. But at the time of writing we’re still waiting for a response. We’ve also reached out to Facebook for a reaction to the CMOs’ recommendations.

Update: A Facebook spokesperson said:

We want the time young people spend online to be meaningful and, above all, safe. We welcome this valuable piece of work and agree wholeheartedly with the Chief Medical Officers on the need for industry to work closely together with government and wider society to ensure young people are given the right guidance to help them make the most of the internet while staying safe.

Instagram’s boss, Adam Mosseri, is meeting with the UK health secretary today to discuss concerns about underage users being exposed to disturbing content on the social media platform.

The meeting follows public outrage over the suicide of a schoolgirl whose family said she had been exposed to Instagram accounts that shared self-harm imagery, including some accounts they said actively encouraged suicide. Ahead of the meeting Instagram announced some policy tweaks — saying it would no longer recommend self-harm content to users, and would start to screen sensitive imagery, requiring users click to view it.

In the guidance document the CMOs write that they support the government’s move to legislate “to set clear expectations of the technology industry”. They also urge the technology industry to establish a voluntary code of conduct to address how they safeguard children and young people using their platforms, in consultation with civil society and independent experts.

Areas that the CMOs flag for possible inclusion in such a code include “clear terms of use that children can understand”, as well as active enforcement of their own T&Cs — and “effective age verification” (they suggest working with the government on that).

They also suggest platforms include commitments to “remove addictive capabilities” from the UX design of their services, criticism so-called “persuasive” design.

They also suggest platforms commit to ensure “appropriate age specific adverts only”.

The code should ensure that “no normalisation of harmful behaviour (such as bullying and selfharming) occurs”, they suggest, as well as incorporate ongoing work on safety issues such as bullying and grooming, in their view.

In advice to parents and carers also included in the document, the CMOs encourage the setting of usage boundaries around devices — saying children should not be allowed to take devices into their bedrooms at bedtime to prevent disruption to sleep.

Parents also encourage screen-free meal time to allow families to “enjoy face-to-face conversation”.

The CMOs also suggest parents and guardians talk to children about device use to encourage sensible social sharing — also pointing out adults should never assume children are happy for their photo to be shared. “When in doubt, don’t upload,” they add.",Social Media,TechCrunch,https://techcrunch.com/2019/02/07/tech-platforms-called-to-support-public-interest-research-into-mental-health-impacts/,"The UK's Chief Medical Officers have called for the tech industry to share data with public sector researchers so the mental health and psychosocial impacts of their service on vulnerable users can be better understood. Concerns have been raised about the negative impacts of digital technologies on minors, such as self-harm, bullying and lack of sleep, and parents",Mental Health
281,Social Media Marathon Cheating: Con or Victimless Crime?,"Kara Bonneau had trained for months to run the 2014 Boston Marathon, one of the sport’s premier events. Excited about the challenge ahead of her, she posted a photo of her official race bib #14285 on her various social media accounts so that her friends and family could track her as she traversed the 26.2-mile course.

Mary Pilon is the author of The Monopolists and the forthcoming The Kevin Show. She primarily writes about sports, business, and politics. Sign up to get Backchannel's weekly newsletter, and follow us on Facebook, Twitter, and Instagram.

After crossing the finish line, Bonneau punched her bib number into the Boston Athletic Association’s website to look at the snapshots race photographers routinely take at various checkpoints of the race—both to sell as souvenirs and to verify results. She was surprised by what she saw. The photographs showed four different runners. None looked like her, but all sported bib #14285 as they trotted along the trails. Confused and outraged, Bonneau posted the images to her blog, asking for readers to help her solve the mystery of the other runners’ identities. The seeds of #BibGate2014 were sown.

Bonneau didn’t yet know that she was part of a scam that has become a trend in the racing world: social media banditing—where a runner uses social media to enter a race without officially registering. This typically means lifting a bib number off of someone’s Instagram ahead of a race, creating a duplicate, and then using the hoisted entry to run a race incognito. It’s a phenomenon born out of the current dual booms of technology and endurance events—and the very tools bib counterfeiters are using to circumvent race regulations are also being used to bust, ban, or—in some cases—publicly shame cheaters. And many are still divided about whether it’s a silly prank or a serious threat.

“I was just really shocked,” Bonneau tells me. “I think I would have been upset to see cheating in any arena, but because the Boston Marathon—and particularly this Boston Marathon, the year after the bombing—had such special meaning to me, I was really angry.”

For professional athletes, the motives for cheating generally are more obvious: money, fame, and often a low likelihood of being caught. But why would a middle- or back-of-the-pack runner lie or cheat in a race that doesn’t even matter?

Long before social media made things like bib replication easier, banditing at major races was viewed as a brave act. Rebellious runners like John Tarrant gatecrashed races as a political statement, in protest of rules about amateurism that limited how much money athletes could earn in appearance fees and endorsements. It was an underdog move against the proverbial Man. Darren Garnick, a filmmaker and runner in New Hampshire who bandited the Boston Marathon in 1986, said he laments this shift from those days to today’s anti-bandit culture.

“I think it’s sad there’s this crackdown,” Garnick says. “I remember after finishing the marathon then that I was beaming. You can’t take your car on the Indy 500 track and you can’t just walk on a PGA course. But the Boston Marathon is world-famous, and you could be a part of it. I don’t remember marathon bandits being considered a threat or a freeloader or a cockroach then.”

Organizers with New York Road Runners estimate that of the more than 50,000 runners who hit the starting line each year, the number of marathon bandits typically never exceeds 50. But the threat of banditing was large enough for the NYRR to create an anti-banditing campaign: “Respect the Run.” The campaign launched this spring, featuring amoeba-like cartoons stealing bibs and proclaiming it “risky business.”",Social Media,WIRED,https://www.wired.com/story/inside-the-weird-world-of-social-media-marathon-cheating/,"Social media has enabled an increase in marathon banditing, where runners use someone else's bib number to enter a race without officially registering, often leading to a crackdown from organizers who are concerned about the threat of banditing.",Social Norms & Relationships
282,"Facebook won’t ban political ads, prefers to keep screwing democracy – TechCrunch","It’s 2020 — a key election year in the U.S. — and Facebook is doubling down on its policy of letting people pay it to fuck around with democracy.

Despite trenchant criticism — including from U.S. lawmakers accusing Facebook’s CEO to his face of damaging American democracy — the company is digging in, announcing as much today by reiterating its defence of continuing to accept money to run microtargeted political ads.

Instead of banning political ads, Facebook is trumpeting a few tweaks to the information it lets users see about political ads — claiming it’s boosting “transparency” and “controls” while leaving its users vulnerable to default settings that offer neither.

Political ads running on Facebook are able to be targeted at individuals’ preferences as a result of the company’s pervasive tracking and profiling of internet users. And ethical concerns about microtargeting led the U.K.’s data protection watchdog to call in 2018 for a pause on the use of digital ad tools like Facebook by political campaigns — warning of grave risks to democracy.

Facebook isn’t for pausing political microtargeting, though. Even though various elements of its data-gathering activities are also subject to privacy and consent complaints, regulatory scrutiny and legal challenge in Europe, under regional data protection legislation.

Instead, the company made it clear last fall that it won’t fact-check political ads, nor block political messages that violate its speech policies — thereby giving politicians carte blanche to run hateful lies, if they so choose.

Facebook’s algorithms also demonstrably select for maximum eyeball engagement, making it simply the “smart choice” for the modern digitally campaigning politician to run outrageous BS on Facebook — as longtime Facebook exec Andrew Bosworth recently pointed out in an internal posting that leaked in full to the NYT.

Facebook founder Mark Zuckerberg’s defence of his social network’s political ads policy boils down to repeatedly claiming “it’s all free speech, man” (we paraphrase).

This is an entirely nuance-free argument that comedian Sacha Baron Cohen expertly demolished last year, pointing out that: “Under this twisted logic if Facebook were around in the 1930s it would have allowed Hitler to post 30-second ads on his solution to the ‘Jewish problem.’ ”

Facebook responded to the take-down with a denial that hate speech exists on its platform since it has a policy against it — per its typical crisis PR playbook. And it’s more of the same selectively self-serving arguments being dispensed by Facebook today.

In a blog post attributed to its director of product management, Rob Leathern, it expends more than 1,000 words on why it’s still not banning political ads (it would be bad for advertisers wanting to reach “key audiences,” is the non-specific claim) — including making a diversionary call for regulators to set ad standards, thereby passing the buck on “democratic accountability” to lawmakers (whose electability might very well depend on how many Facebook ads they run…), while spinning cosmetic, made-for-PR tweaks to its ad settings and what’s displayed in an ad archive that most Facebook users will never have heard of as “expanded transparency” and “more control.”

In fact these tweaks do nothing to reform the fundamental problem of damaging defaults.

The onus remains on Facebook users to do the leg work on understanding what its platform is pushing at their eyeballs and why.

Even as the “extra” info now being drip-fed to the Ad Library is still highly fuzzy (“We are adding ranges for Potential Reach, which is the estimated target audience size for each political, electoral or social issue ad so you can see how many people an advertiser wanted to reach with every ad,” as Facebook writes of one tweak.)

The new controls similarly require users to delve into complex settings menus in order to avail themselves of inherently incremental limits — such as an option that will let people opt into seeing “fewer” political and social issue ads. (Fewer is naturally relative, ergo the scale of the reduction remains entirely within Facebook’s control — so it’s more meaningless “control theatre” from the lord of dark pattern design. Why can’t people switch off political and issue ads entirely?)

Another incremental setting lets users “stop seeing ads based on an advertiser’s Custom Audience from a list.”

But just imagine trying to explain WTF that means to your parents or grandparents — let alone an average internet user actually being able to track down the “control” and exercise any meaningful agency over the political junk ads they’re being exposed to on Facebook.

It is, to quote Baron Cohen, “bullshit.”

Nor are outsiders the only ones calling out Zuckerberg on his BS and “twisted logic”: A number of Facebook’s own employees warned in an open letter last year that allowing politicians to lie in Facebook ads essentially weaponizes the platform.

They also argued that the platform’s advanced targeting and behavioral tracking tools make it “hard for people in the electorate to participate in the public scrutiny that we’re saying comes along with political speech” — accusing the company’s leadership of making disingenuous arguments in defence of a toxic, anti-democratic policy.

Nothing in what Facebook has announced today resets the anti-democratic asymmetry inherent in the platform’s relationship to its users.

Facebook users — and democratic societies — remain, by default, preyed upon by self-interested political interests thanks to Facebook’s policies which are dressed up in a self-interested misappropriation of “free speech” as a cloak for its unfettered exploitation of individual attention as fuel for a propaganda-as-service business.

Yet other policy positions are available.

Twitter announced a total ban on political ads last year — and while the move doesn’t resolve wider disinformation issues attached to its platform, the decision to bar political ads has been widely lauded as a positive, standard-setting example.

Google also followed suit by announcing a ban on “demonstrably false claims” in political ads. It also put limits on the targeting terms that can be used for political advertising buys that appear in search, on display ads and on YouTube.

Still, Facebook prefers to exploit “the absence of regulation,” as its blog post puts it, to not do the right thing and keep sticking two fingers up at democratic accountability — because not applying limits on behavioral advertising best serves its business interests. Screw democracy.

“We have based [our policies] on the principle that people should be able to hear from those who wish to lead them, warts and all, and that what they say should be scrutinized and debated in public,” Facebook writes, ignoring the fact that some of its own staff already pointed out the sketchy hypocrisy of trying to claim that complex ad targeting tools and techniques are open to public scrutiny.",Social Media,TechCrunch,https://techcrunch.com/2020/01/09/facebook-wont-ban-political-ads-prefers-to-keep-screwing-democracy/,"Facebook's policy of allowing people to pay to run microtargeted political ads, combined with its lack of fact-checking and its algorithms which select for maximum eyeball engagement, has created an environment which puts democracy at risk. It allows politicians to spread false information and hateful lies, while giving them the ability to target specific individuals and groups with","Information, Discourse & Governance"
283,Twitter is getting rid of the egg avatar (because that will totally fix the abuse problem) – TechCrunch,"Twitter is getting rid of the egg avatar (because that will totally fix the abuse problem)

Everyone knows that Twitter has a harassment problem. And while the service has tried things like banning abusive users (both on a temporary and permanent basis) it hasn’t really fixed the problem.

Today, they’re announcing another sweeping change that fails to address the real problem: they’re cracking the egg.

In a long-winded post the service announced they are replacing the default egg with a unisex profile picture resembling a head and shoulders silhouette.

Beginning in 2010, all new accounts started with their default profile picture as an egg. Since then it has become an integral part of the Twitter brand. Everyone, even non-hardcore Twitter users, know the egg. Even CNN frequently features the egg when they show tweets from politicians and other celebrities.

Twitter gave a bunch of reasons for the switch. Some, like the fact that a more generic profile picture should encourage new users to actually upload a real profile picture of themselves, make sense.

But one reason for the switch was related to harassment.

Specifically, Twitter said that since abusive accounts often have the egg as a profile picture, there is now “an association between the default egg profile photo and negative behavior.”

Gee, ya think?

What Twitter isn’t understanding is that abusive tweets sent from an egg account will now just be abusive tweets sent from a silhouette account. Switching up the profile picture may be putting a band-aid on the problem, but it does nothing to fix harassment in the long run.

An abusive tweet is an abusive tweet, whether it’s next to an egg, a silhouette or a real person’s avatar.

Plus, as a byproduct, Twitter is killing off yet another part of Twitter’s unique internal language and identity — just like they did with @replies and favorites.

Don’t blame the egg, just fix the problem.",Social Media,TechCrunch,https://techcrunch.com/2017/03/31/twitter-is-getting-rid-of-the-egg-avatar-because-that-will-totally-fix-the-abuse-problem/,Twitter's replacement of the egg avatar does nothing to address the real problem of harassment and only serves to kill off the unique internal language and identity of Twitter.,User Experience & Entertainment
284,Facebook’s got 99 problems but Trump’s latest ‘bias’ tweet ain’t one – TechCrunch,"By any measure, Facebook hasn’t had the best of years in 2018.

But while toxic problems keep piling up and, well, raining acidly down on the social networking giant — from election interference, to fake accounts, faulty metrics, security flaws, ethics failures, privacy outrages and much more besides — the silver lining of having a core business now widely perceived as hostile to democratic processes and civilized sentiment, and the tool of choice for shitposters agitating for hate and societal division, well, everywhere in the world, is that Facebook has frankly far more important things to worry about than the latest anti-tech-industry salvo from President Trump.

In an early morning tweet today, Trump (again) attacked what he dubbed anti-conservative “bias” in the digital social sphere — hitting out at not just Facebook but tech’s holy trinity of social giants, with a claim that “Facebook, Twitter and Google are so biased towards the Dems it is ridiculous!”

Facebook, Twitter and Google are so biased toward the Dems it is ridiculous! Twitter, in fact, has made it much more difficult for people to join @realDonaldTrump. They have removed many names & greatly slowed the level and speed of increase. They have acknowledged-done NOTHING! — Donald J. Trump (@realDonaldTrump) December 18, 2018

Time was when Facebook was so sensitive to accusations of internal anti-conservative bias that it fired a bunch of journalists it had contracted and replaced them with algorithms — which almost immediately pumped up a bunch of fake news. RIP irony.

Not today, though.

When asked if it had a response to Trump’s accusation of bias a Facebook spokesperson told us: “We don’t have anything to add here.”

The brevity and alacrity of the response suggested the spokesperson had a really cheerful expression on their face when they typed it.

The relief of Facebook not having to give a shit this time was kinda palpable, even in pixel form.

It was also a far cry from the screeds the company routinely dispenses these days to try to muffle journalistic — and indeed political — enquiry.

Trump evidently doesn’t factor “bigly” on Facebook’s oversubscribed risk-list.

Even though Facebook was the first name on the president’s (non-alphabetical) tech giant hit-list.

Still, Twitter appeared to have irked Trump more, as his tweet singled out the short-form platform — with an accusation that Twitter has made it “much more difficult for people to join [sic] @realDonaldTrump”. (We think by “join” he means follow. But we’re speculating wildly.)

This is perhaps why Twitter felt moved to provide a response to the claim of bias, albeit also without wasting a lot of words.

Here’s its statement:

Our focus is on the health of the service, and that includes work to remove fake accounts to prevent malicious behavior. Many prominent accounts have seen follower counts drop, but the result is higher confidence that the followers they have are real, engaged people.

Presumably the president failed to read our report, from July, when we trailed Twitter’s forthcoming spam purge, warning it would result in users with lots of followers taking a noticeable hit in the coming days. In a word: Sad.

Of course, we also asked Google for a response to Trump’s bias claim. But just got radio silence.

In similar “bias” tweets from August the company got a bigger Trump-lashing. And in a response statement then it told us: “We never rank search results to manipulate political sentiment.”

Google CEO Sundar Pichai has also just had to sit through some three hours of questions from Republicans in Congress on this very theme.

So the company probably feels it’s exhausted the political bias canard.

Even while, as the claims drone on and on, it might truly come to understand what it feels like to be stuck inside a filter bubble.

In any case, there are far more pressing things to accuse Google’s algorithms of than being “anti-Trump.”

So it’s just as well it didn’t waste time on another presidential sideshow intended to distract from problems of Trump’s own making.",Social Media,TechCrunch,https://techcrunch.com/2018/12/18/facebooks-got-99-problems-but-trumps-latest-bias-tweet-aint-one/,"Social media has been accused of having a negative impact on democracy, with its platforms being used to spread hate and division, as well as enabling election interference and other toxic problems. In addition, the algorithms used by social media giants like Facebook, Twitter and Google have been accused of being biased towards one political party or another, although the companies deny",Politics
285,Is Our Addiction To Tragedy On Social Media Inspiring Violence? – TechCrunch,"If terrorism requires an audience, then the recent mainstream adoption of social media may be giving violent actors a bigger stage than ever before. There are many reasons people lash out at the world, but I don’t think it’s unreasonable to suggest that becoming the center of attention could be a factor pushing some to commit atrocities. Our retweets could be delivering their messages of fear.

This is not to say social media infamy is the cause of any of the recent tragedies in Boston, or Sandy Hook, [Update: or Santa Barbara] or anywhere else. But watching the world feverishly tweet and Facebook post about the manhunt unfolding in Watertown last night frightened me. I couldn’t help but wonder if other angry, disturbed, or mentally ill individuals might be watching, too, and craving that same notoriety. I shuddered to think of a future where a terrorist in hiding laughs as they see their actions trigger millions of mentions.

Some believe that social media’s role is no different than that of traditional media years ago — that terrorists and killers in the 1920s would have just been just as attracted to becoming a newspaper headline as the subject of a sea of tweets. I disagree. Those old outlets were broadcast mediums; they weren’t participatory. Listening to reports of catastrophe on the radio and discussing them with people nearby doesn’t internalize the fear the same way as personally re-sharing and reacting to them in a real-time global forum. Social media instills emotions deeper.

According to Max Abrahms, PhD, a counter-terrorism research fellow at Johns Hopkins and author of “What Terrorists Really Want” from the International Security journal, “One thing counter-terrorism researchers want to know is what is the motive of the terrorist. They want to know that because they want to deprive terrorism of any utility. If we could remove the value of committing terrorism they wouldn’t do it.” He tells me, “One of the main goals of terrorists is to get attention. By it’s very definition, terrorism requires an audience, so it’s no surprise the advent of terrorism came alongside the growth of mass media in the 1880s. Social media today no doubt spreads the message of terrorists even quicker and to more people.”

We’ve already seen perpetrators make use of social media to promote their point of view, like Christopher Dorner who was caught in an extended manhunt after killing several police officers in February. He may have wanted the manifesto he posted to Facebook accusing police of corruption to be widely shared. And we shared it. His message hit closer to home because it was our friends distributing it, rather than a newspaper like the letters from the Zodiac Killer.

If killing sprees are a cry for attention or help or an attempt to show the world someone’s pain, then our fixation on tragedy, our willingness to pause our lives and spread the news bit by bit could be playing into their hands.

In fact, when video footage of Osama Bin Laden in his compound was recovered, Abrahms says one of the most striking things we found was that Osama was sitting on his sofa watching himself on television. It suggests terrorists do derive utility from knowing millions of people are paying attention to them.

Our real-time updates can even have real negative consequences for the safety of those involved in pursuits of criminals. In February during the Dorner manhunt, the San Bernardino District Attorney tweeted that “The sheriff has asked all members of the press to stop tweeting immediately. It is hindering officer safety. And last night, the Boston Police Department discouraged people from posting what they heard on police scanners:

#MediaAlert: WARNING: Do Not Compromise Officer Safety by Broadcasting Tactical Positions of Homes Being Searched. — Boston Police Dept. (@bostonpolice) April 19, 2013

UPDATE: Boston Police are asking social media users not to post information they hear on police frequencies/scanner channels. — CBS News (@CBSNews) April 19, 2013

But does this kind of attention inspire violence? When I asked Abrahms who studied at Oxford and has spoken on Al Jazeera, he explained “In a sense, yes. Ideally we would completely ignore terrorism.” That’s not to say discussion is bad, and Abrahms notes social media’s potential to surface evidence in investigations. Still, propagating the fear and grief caused by acts of violence has the potential to satisfy those who commit them.

This isn’t a call for guilt, or even abrupt change, but for mindfulness. When the Internet crowds around tragedy, do we think about the impact of feverishly sharing the latest gruesome details? There is a difference between distributing actionable news and trumpeting fear, and being aware of that difference is critical now that we each have our own audience.

Abrahms concludes, and I agree, “if people are sharing news stories they find intellectually interesting, there’s nothing wrong with that. The problem is if people internalize it and come to overestimate the chance of they themselves being victim of a terrorist attack. You don’t want society to overreact.”",Social Media,TechCrunch,https://techcrunch.com/2013/04/19/terrorism-requires-an-audience/,"Social Media's real-time updates have the potential to satisfy violent actors, as the fear and grief caused by their acts can be propagated quickly, increasing their notoriety and desirability. This can lead to an overestimation of the chance of being victim of a terrorist attack, causing society to overreact.",Security & Privacy
286,Delete your account – TechCrunch,"We love to Tweet. We love to share. It appears to be a strategy for change but it isn’t. And we have to accept that.

These are strange times. We are able to reach millions with a single Medium post but each day a new post supplants the last. Today’s impassioned jeremiad – like this one – is tomorrow’s digital fish wrapper. A CEO can Tweet something noble today and it is gone a moment later. We can spread the word through Facebook but that site’s pernicious systems keeps it from spreading outside of your immediate zone of friends. In the end all we do on social media is akin to a fart in a crowded room – sure to annoy someone nearby but dissipated by the time it reaches the edges.

So I’m saying delete your account and do something.

If you are a programmer and you haven’t contacted your favorite investigative journalists, the folks at ProPublica, or anyone at your local paper, delete your account and do it. Journalists need your technical expertise to secure their devices, set up secure drops, and understand the data coming out of the countless leaks that are sure to come. You are vitally important.

If you’re running a startup delete your account and look up from your laptop. This can be your first effort at corporate philanthropy. Donate to the ACLU. Volunteer to help immigrants assimilate. Send some cash to Trump. I don’t care. Get political. We had a solid decade of inaction. It’s time to delete your account and do something.

If you’re a VC cut ties from members of your class that actively destroy free speech and rant about the coming dystopia. It makes no sense to invest when the world is coming to an end so perhaps you should protect your investments and prevent it?

If you think about tweeting something witty, don’t. Say that witty thing at a protest march or in a city council meeting or at a school board election. Run for local office. This is the moment for insurgent democracy and if you’re at Apple or Google and truly believe in your Burning-Man-equality-for-all fever dream of Silicon Valley then make it happen locally. If you live in a small city lead the hundreds of amazing entrepreneurs in an effort to enact change. If you’re international reach out. It will be individual citizens who change things since most of our governments are now at odds. This second decade of the 21st century has gone aground. We need to tow it to open waters and keep sailing. Delete your account and do something.

Delete your account and build something if you want to fight back. Delete your account and do something if you want to fight back. Delete your account and run for office if you want to fight back. Facebook posts are meaningless. Twitter is a way for us to feel smug at misspelled rants from the White House. Ignore their false produce and read real news from a real source you trust. Pay for media. Understand that journalists are as baffled as you but, thanks to experience, they have the means to tease out truth. But they can’t do it alone.

No matter what side you’re on, no matter how much glee you get in funny memes and clever retorts, delete your account. The world doesn’t exist on your backlit high-resolution screen. It exists a few degrees up and out, out where people are marching, deals are being made, and the world is changing without you.

Delete your account. The world needs you.

Image via IconFinder",Social Media,TechCrunch,https://techcrunch.com/2017/01/28/delete-your-account/,"Social media can give us a false sense of accomplishment, as posts are quickly forgotten and replaced by new ones. We need to delete our accounts and take tangible, meaningful action to enact change.",Social Norms & Relationships
287,Germany’s social media hate speech law is now in effect – TechCrunch,"A new law has come into force in Germany aimed at regulating social media platforms to ensure they remove hate speech within set periods of receiving complaints — within 24 hours in straightforward cases or within seven days where evaluation of content is more difficult.

The name of the law translates to ‘Enforcement on Social Networks’. It’s also referred to as NetzDG, an abbreviation of its full German name.

Fines of up to €50 million can be applied under the law if social media platforms fail to comply, though as Spiegal Online reports there is a transition period for companies to gear up for compliance — which will end on January 1, 2018. However the Ministry in charge has started inspections this month.

Social platform giants such as Facebook, YouTube and Twitter were couched as the initial targets for the law, but Spiegal Online suggests the government is looking to apply the law more widely — including to content on networks such as Reddit, Tumblr, Flickr, Vimeo, VK and Gab.

The usage bar for complying with the takedown timeframes is being set at a service having more than two million registered users in Germany.

While Spiegal Online reports that the German government is intending to have 50 people assigned to the task of implementing and policing the law.

It also says all social media platforms, regardless of size, must provide a contact person in Germany for user complaints or requests for information from investigators. Recent queries will need to be answered within 48 hours or risk penalties, it adds.

One obvious question here is how any fines could be applied across international borders if a social media firm has no bricks-and-mortar presence in Germany, though.

The law does also require social media firms operating in Germany to appoint a contact person in the country. But again, those companies that are outside Germany may be rather hard to police — unless the government intends to start trying to block access to non-compliance services which would only invite further controversy.

The Germany cabinet backed the proposal for the law back in April. At the time, justice minister Heiko Maas said: “Freedom of expression ends where criminal law begins.”

The country has specific hate speech laws which criminalize certain types of speech, such as incitement to racial and religious violence, and the NetzDG law cites sections of the existing German Criminal Code — applying itself specifically to social media platforms.

Germany not alone in Europe at seeking to clamp down on illegal content being spread and amplified via social media, either.

The UK has also been active recently in leading a push by several G7 nations against online extremism — with the apparent aim of reducing takedown times for this type of content to an average of just two hours.

Germany has also been pushing for a European Union wide response to tackling the spread of hate speech across online platforms.

And last week the European Commission put out new guidance for social media platforms urging them to be more pro-active about removing “illegal content” — including by developing tools to automative identification and prevent re-uploading of problem content.

It warned social giants that it might seek to draft a legislative proposal if they do not improve takedown performance within six months.

However the executive body appears to be seeking to bundle up various types of “illegal” content into the same problem bucket — and quickly drew criticism it risks encouraging algorithmic censorship by seeking to create one set of rules to apply to copyrighted content and terrorist propaganda, for example. Which does underline the risks around broad efforts to regulate the types of content that can and can’t be viewed online.

Critics of Germany’s NetzDG law argue it will encourage tech platforms to censor controversial content to avoid the risk of big fines. And while speedy social media takedowns of offensive hate speech might enjoy mainstream backing in Germany, it remains to be seen how the law will operate in practice.

Meanwhile, if overly expansive rules end up being fashioned to try to regulate all sorts of “illegal” content online that could also result in a wider chilling effect on online expression, and reduced support for broad regulatory efforts.",Social Media,TechCrunch,https://techcrunch.com/2017/10/02/germanys-social-media-hate-speech-law-is-now-in-effect/,"The new NetzDG law in Germany has been met with criticism as it may encourage censorship of controversial content to avoid the risk of financial penalties, leading to a wider chilling effect on online expression.","Information, Discourse & Governance"
288,"TikTok removed 81 million videos for violations in Q2, representing 1% of uploads – TechCrunch","TikTok released a transparency update about content that was removed from the platform between April 1 and June 30, 2021. The platform says it removed 81,518,334 videos for violating its community guidelines or terms of service during that time period, which represents less than 1% of the total videos posted. This also means that during that quarter, 8.1 billion+ videos were posted on TikTok, averaging out to about 90 million videos posted each day.

TikTok rolled out technology in July that automatically removes content that violates its policies on minor safety, adult nudity and sexual activities, violent and graphic content, and illegal activities. The app uses automation for these kinds of content in particular, because it says these are the areas where its technology has the highest degree of accuracy. TikTok also has human content moderators on its safety team that reviews reported content.

When the automated technology was announced, TikTok said its false-positive rate for removals — when it removes content that actually doesn’t violate rules — is 5%. This number is reflected in the transparency report released today, as about 4.6 million of the 81 million removed videos were reinstated. The most common reason for content to be removed, encompassing 41.3% of removed videos, was for violation of minor safety guidelines. TikTok says that 94.1% of all removed content was identified and taken down before a user reported it.

Still, within that 5% of falsely removed content lies some troubling examples of failed content moderation, like when popular Black creator Ziggi Tyler pointed out that the app would flag his account bio as “inappropriate” when he included phrases like “Black Lives Matter” or “I am a black man.” Meanwhile, phrases like “supporting white supremacy” weren’t immediately censored.

As platforms like YouTube and Facebook move to ban content with vaccine misinformation, other prominent social media companies like TikTok must decide how to tackle similar issues. TikTok’s community guidelines ban content that’s false or misleading about COVID-19, vaccines and more broad anti-vaccine disinformation.

In Q2, TikTok removed 27,518 videos due to COVID-19 misinformation. TikTok says that 83% of these videos were removed before they were reported by users, 87.6% of these videos were removed within 24 hours of posting, and 69.7% of the videos had zero views. Meanwhile, TikTok’s COVID-19 information hub, developed with information from the WHO and CDC, was viewed over 921 million times globally.

Numbers-wise, it’s no secret that TikTok continues to grow, as the app crossed the 1 billion monthly active user landmark last month. The numbers in these transparency reports also reflect that growth — in the first half of 2020, TikTok removed 104 million videos, which represented 1% of all content posted. That 1% removal rate is consistent with the data published today, but today’s report accounts for 81 million videos posted in just one quarter, rather than two quarters.",Social Media,TechCrunch,https://techcrunch.com/2021/10/13/tiktok-removed-81-million-videos-for-violations-in-q2-representing-1-of-uploads/,"Social media platforms like TikTok have seen a huge rise in content posted, but this can lead to a lack of adequate moderation, resulting in content such as vaccine misinformation, or false-positives when it comes to flagged content, being spread.","Information, Discourse & Governance"
289,"Facebook’s ex-CSO, Alex Stamos, defends its decision to inject ads in WhatsApp – TechCrunch","Alex Stamos, Facebook’s former chief security officer, who left the company this summer to take up a role in academia, has made a contribution to what’s sometimes couched as a debate about how to monetize (and thus sustain) commercial end-to-end encrypted messaging platforms in order that the privacy benefits they otherwise offer can be as widely spread as possible.

Stamos made the comments via Twitter, where he said he was indirectly responding to the fallout from a Forbes interview with WhatsApp co-founder Brian Acton — in which Acton hit at out at his former employer for being greedy in its approach to generating revenue off of the famously anti-ads messaging platform.

Both WhatsApp founders’ exits from Facebook has been blamed on disagreements over monetization. (Jan Koum left some months after Acton.)

In the interview, Acton said he suggested Facebook management apply a simple business model atop WhatsApp, such as metered messaging for all users after a set number of free messages. But that management pushed back — with Facebook COO Sheryl Sandberg telling him they needed a monetization method that generates greater revenue “scale”.

And while Stamos has avoided making critical remarks about Acton (unlike some current Facebook staffers), he clearly wants to lend his weight to the notion that some kind of trade-off is necessary in order for end-to-end encryption to be commercially viable (and thus for the greater good (of messaging privacy) to prevail); and therefore his tacit support to Facebook and its approach to making money off of a robustly encrypted platform.

Stamos’ own departure from the fb mothership was hardly under such acrimonious terms as Acton, though he has had his own disagreements with the leadership team — as set out in a memo he sent earlier this year that was obtained by BuzzFeed. So his support for Facebook combining e2e and ads perhaps counts for something, though isn’t really surprising given the seat he occupied at the company for several years, and his always fierce defence of WhatsApp encryption.

(Another characteristic concern that also surfaces in Stamos’ Twitter thread is the need to keep the technology legal, in the face of government attempts to backdoor encryption, which he says will require “accepting the inevitable downsides of giving people unfettered communications”.)

I don't want to weigh into the personal side of the WhatsApp vs Facebook fight, as there are people I respect on both sides, but I do want to use this as an opportunity to talk about the future of end-to-end encryption. (1/13) — Alex Stamos (@alexstamos) September 26, 2018

This summer Facebook confirmed that, from next year, ads will be injected into WhatsApp statuses (aka the app’s Stories clone). So it is indeed bringing ads to the famously anti-ads messaging platform.

For several years the company has also been moving towards positioning WhatsApp as a business messaging platform to connect companies with potential customers — and it says it plans to meter those messages, also from next year.

So there are two strands to its revenue generating playbook atop WhatsApp’s e2e encrypted messaging platform. Both with knock-on impacts on privacy, given Facebook targets ads and marketing content by profiling users by harvesting their personal data.

This means that while WhatsApp’s e2e encryption means Facebook literally cannot read WhatsApp users’ messages, it is ‘circumventing’ the technology (for ad-targeting purposes) by linking accounts across different services it owns — using people’s digital identities across its product portfolio (and beyond) as a sort of ‘trojan horse’ to negate the messaging privacy it affords them on WhatsApp.

Facebook is using different technical methods (including the very low-tech method of phone number matching) to link WhatsApp user and Facebook accounts. Once it’s been able to match a Facebook user to a WhatsApp account it can then connect what’s very likely to be a well fleshed out Facebook profile with a WhatsApp account that nonetheless contains messages it can’t read. So it’s both respecting and eroding user privacy.

This approach means Facebook can carry out its ad targeting activities across both messaging platforms (as it will from next year). And do so without having to literally read messages being sent by WhatsApp users.

As trade offs go, it’s a clearly a big one — and one that’s got Facebook into regulatory trouble in Europe.

It is also, at least in Stamos’ view, a trade off that’s worth it for the ‘greater good’ of message content remaining strongly encrypted and therefore unreadable. Even if Facebook now knows pretty much everything about the sender, and can access any unencrypted messages they sent using its other social products.

In his Twitter thread Stamos argues that “if we want that right to be extended to people around the world, that means that E2E encryption needs to be deployed inside of multi-billion user platforms”, which he says means: “We need to find a sustainable business model for professionally-run E2E encrypted communication platforms.”

On the sustainable business model front he argues that two models “currently fit the bill” — either Apple’s iMessage or Facebook-owned WhatsApp. Though he doesn’t go into any detail on why he believes only those two are sustainable.

He does say he’s discounting the Acton-backed alternative, Signal, which now operates via a not-for-profit (the Signal Foundation) — suggesting that rival messaging app is “unlikely to hit 1B users”.

In passing he also throws it out there that Signal is “subsidized, indirectly, by FB ads” — i.e. because Facebook pays a licensing fee for use of the underlying Signal Protocol used to power WhatsApp’s e2e encryption. (So his slightly shade-throwing subtext is that privacy purists are still benefiting from a Facebook sugardaddy.)

Then he gets to the meat of his argument in defence of Facebook-owned (and monetized) WhatsApp — pointing out that Apple’s sustainable business model does not reach every mobile user, given its hardware is priced at a premium. Whereas WhatsApp running on a cheap Android handset ($50 or, perhaps even $30 in future) can.

Other encrypted messaging apps can also of course run on Android but presumably Stamos would argue they’re not professionally run.

“I think it is easy to underestimate how radical WhatsApp’s decision to deploy E2E was,” he writes. “Acton and Koum, with Zuck’s blessing, jumped off a bridge with the goal of building a monetization parachute on the way down. FB has a lot of money, so it was a very tall bridge, but it is foolish to expect that FB shareholders are going to subsidize a free text/voice/video global communications network forever. Eventually, WhatsApp is going to need to generate revenue.

“This could come from directly charging for the service, it could come from advertising, it could come from a WeChat-like services play. The first is very hard across countries, the latter two are complicated by E2E.”

“I can’t speak to the various options that have been floated around, or the arguments between WA and FB, but those of us who care about privacy shouldn’t see WhatsApp monetization as something evil,” he adds. “In fact, we should want WA to demonstrate that E2E and revenue are compatible. That’s the only way E2E will become a sustainable feature of massive, non-niche technology platforms.”

Stamos is certainly right that Apple’s iMessage cannot reach every mobile user, given the premium cost of Apple hardware.

Though he elides the important role that second hand Apple devices play in helping to reduce the barrier to entry to Apple’s pro-privacy technology — a role Apple is actively encouraging via support for older devices (and by its own services business expansion which extends its model so that support for older versions of iOS (and thus secondhand iPhones) is also commercially sustainable).

Robust encryption only being possible via multi-billion user platforms essentially boils down to a usability argument by Stamos — which is to suggest that mainstream app users will simply not seek encryption out unless it’s plated up for them in a way they don’t even notice it’s there.

The follow on conclusion is then that only a well-resourced giant like Facebook has the resources to maintain and serve this different tech up to the masses.

There’s certainly substance in that point. But the wider question is whether or not the privacy trade offs that Facebook’s monetization methods of WhatsApp entail, by linking Facebook and WhatsApp accounts and also, therefore, looping in various less than transparent data-harvest methods it uses to gather intelligence on web users generally, substantially erodes the value of the e2e encryption that is now being bundled with Facebook’s ad targeting people surveillance. And so used as a selling aid for otherwise privacy eroding practices.

Yes WhatsApp users’ messages will remain private, thanks to Facebook funding the necessary e2e encryption. But the price users are having to pay is very likely still their personal privacy.

And at that point the argument really becomes about how much profit a commercial entity should be able to extract off of a product that’s being marketed as securely encrypted and thus ‘pro-privacy’? How much revenue “scale” is reasonable or unreasonable in that scenario?

Other business models are possible, which was Acton’s point. But likely less profitable. And therein lies the rub where Facebook is concerned.

How much money should any company be required to leave on the table, as Acton did when he left Facebook without the rest of his unvested shares, in order to be able to monetize a technology that’s bound up so tightly with notions of privacy?

Acton wanted Facebook to agree to make as much money as it could without users having to pay it with their privacy. But Facebook’s management team said no. That’s why he’s calling them greedy.

Stamos doesn’t engage with that more nuanced point. He just writes: “It is foolish to expect that FB shareholders are going to subsidize a free text/voice/video global communications network forever. Eventually, WhatsApp is going to need to generate revenue” — thereby collapsing the revenue argument into an all or nothing binary without explaining why it has to be that way.",Social Media,TechCrunch,https://techcrunch.com/2018/09/27/facebooks-ex-cso-alex-stamos-defends-its-decision-to-inject-ads-in-whatsapp/,"The main undesirable consequence of social media discussed here is the erosion of user privacy in order to generate revenue. Facebook's monetization methods, such as linking accounts across its product portfolio and targeting ads, have a knock-on impact on user privacy, despite the fact that their messages remain strongly encrypted. This has caused disagreement between Facebook and former WhatsApp",Security & Privacy
290,"Facebook, Snapchat and the Dawn of the Post-Truth Era","Many, including me, have cited parallels between Gutenberg's invention of the printing press and the internet. As with blogs and Facebook posts, the printing press meant written thought and communication, and its wide distribution, was no longer the exclusive province of an anointed clergy. The voiceless gained a voice, sparking the violent and centuries-long turmoil of the Reformation, the Counter-Reformation, and the Thirty Years’ War, the sort of existential fractures we seem to be teetering on the verge of ourselves.

But how did 17th- and 18th-century Europeans manage to muddle through such disruptive change? How did something so potentially dangerous give birth to the Enlightenment and all its trappings of democracy and human rights? That required a centuries-long elaboration of norms around editorship, the protocols of scholarly and journalistic truth, and a publishing industry of gatekeepers. The New York Times and HarperCollins (and dare we say WIRED) of our day are the contemporary remnants of that coping mechanism.

Antonio García Martínez (@antoniogm) is an Ideas contributor for WIRED. Before turning to writing he headed Facebook's early ad-targeting efforts. His 2016 memoir, Chaos Monkeys, was a New York Times best seller.

When it comes to the internet, the technologists’ response is very different: They collectively swear by the algorithm, fancy talk for a recipe of logical steps and maybe some math. Our brains can’t parse the jumble of content—part art, part trash—our friends generate on Facebook or the wider web, so an algorithm sorts it for us. Mark Zuckerberg, or really, his News Feed algorithm, is now editor-in-chief of the world’s content (for better or worse).

For a consumer, the difference between Gutenbergian editorial curation and Facebook’s algorithm is that between idealistic prescription and amoral prediction. An editor tells you to eat your vegetables and presents you with an expansive piece on the convoluted political and ethnic logic of the Yemen war. That piece will also be (relatively) balanced---within the self-awareness envelope of the media outlet---and probably fact-checked for objective truths like names, places and quotes.

Conversely, the algorithm will deliver as much sugar and fat as you like, offering up daily doses of french fries like Kim Kardashian’s posterior and the latest contretemps in the Trump telenovela, plus whatever viral, unsubstantiated BS emerged from the internet ooze that day.

In many ways, we are rushing forward to the past. To understand why, consider life before books and mass literacy. There was no notion of “looking something up”—the only knowledge resided in collective memory, or maybe in the head of an elder or shaman. There was no recorded, linear timeline of events beyond nature's cycles of seasons or lunar phases; society lived in an eternal present. Mediated experience was a swirl of word-of-mouth ephemera and tribal mythology that persisted through unfaithful oral repetition. This was every human’s world until a few centuries ago, nothing on an evolutionary timescale.

How does our brave, new smartphone world compare?

What we politely call “fake news”---a formulation that presupposes some antecedent credible truth called “news” that we're now abandoning---is just the tribal folklore of a certain (and usually opposing) tribe. Our exhausting and constant absorption in a transitory but completely overwhelming media cycle is our own preliterate eternal present. Who thinks now of Cecil the Lion and the villainous dentist who shot him, whose practice was promptly ruined by an online mob? We’re too busy dealing with the third huge Trump scandal this week, which we’ll forget in due course thanks to next week’s school shooting. Could any of us, if pressed, even construct a chronological ordering of Trump media cycles, or would we have only episodic memories of highlights, as we do when trying to reconstruct some long-ago period of life from memory? Twitter’s Moments product is a constant stream of just such transitory and disordered reactions to context-free events. A history-less forgetfulness is the overarching product vision for Snapchat, whose posts—atomized and textless morsels of personal experience—are designed to disappear and never be consulted or searched.",Social Media,WIRED,https://www.wired.com/story/facebook-snapchat-and-the-dawn-of-the-post-truth-era/,"Social media has created an ""eternal present"" where news cycles and events quickly become forgotten, and where fake news can easily spread and be accepted as truth. This lack of historical context and awareness leads to a dangerous culture of forgetfulness and confusion.","Information, Discourse & Governance"
291,Papua New Guinea threatens to close Facebook for a month to investigate its harmful impact – TechCrunch,"Facebook is proving problematic for many governments worldwide, but few would think to shut it down entirely.

That’s exactly the approach that Papua New Guinea, the Pacific sea island nation located near Australia, is proposing to take with a new measure that could see the social network closed off for a month. During that period, the government plans to investigate the impact of fake accounts, pornography and false news and information which it said are rife on the social network in the country.

The prospect of a month-long ban was announced by Papua New Guinea’s communications minister Sam Basil who told Post Courier that the government “cannot allow the abuse of Facebook to continue in the country.”

Internet penetration in the country is thought to be less than 15 percent, which suggests at face value that Facebook isn’t particularly mainstream. However, that may not be an accurate measure of how many of the country’s eight million population use the social network since mobile is the primary access point in many parts of Asia Pacific. Still, the ban is unlikely to be welcomed by the population.

Post Courier reported that Basil even floated the idea of a dedicated social network to replace Facebook in the country.

At this point, the Facebook ban — however delicious it may sound given recent events — is not confirmed for Papua New Guinea. It remains a possibility once Basil has liaised with police, according to the media report.

Our attempts to reach Basil via phone and email to confirm the plan were not successful.

Facebook has been under fierce pressure around the way it handles data for its 1.5 billion users after it emerged that Cambridge Analytica, a consulting firm that worked on the successful Trump election campaign, hijacked data on nearly 90 million users of the social network.

The aftermath of the scandal has seen Facebook CEO Mark Zuckerberg testify on data security and processes in front of Congress and the House in the U.S., as well as the EU parliament in Europe.

Meanwhile, and of equal importance, Facebook has also been engaged in controversies in the emerging world. The UN has accused it of accelerating racial violence in Myanmar, while the service was closed for three days in Sri Lanka to stop anti-muslim violence. In the Philippines, it has been scrutinized for helping controversial President Rodrigo Duterte into power, while Vietnamese activists have expressed concern that it is helping the government crack down on people in the country.",Social Media,TechCrunch,https://techcrunch.com/2018/05/29/papua-new-guinea-threatens-to-close-facebook-for-a-month/,"Social Media has been linked to a number of undesirable consequences worldwide, including accelerated racial violence in Myanmar, helping controversial leaders into power, and the spread of false news and information. Papua New Guinea is now considering a complete ban on Facebook to investigate these issues.",Equality & Justice
292,"UPDATED: Machine learning can fix Twitter, Facebook, and maybe even America – TechCrunch","UPDATE: A previous version of this post went up under the editor’s byline. The author of the piece is Chris Nicholson.

Quitting Twitter is easy — I’ve done it a hundred times. Someone called it “a clown car that drove into a gold mine,” and like all clown cars, Twitter makes the passengers get out once in awhile.

If I go back, it’s because I’m addicted. The tight news cycle, tweetstorms, gossip mongers, insight, argument, factoids, snark and one-liners. For an information junkie, that little bubble is hard to resist.

But Twitter — and Facebook, for that matter — is desperately broken in ways that alienate users, spread hate and endanger us as a species. The elections have revealed how broken they are better than anything else could have.

First, let’s talk about what’s broken. One set of problems are the collisions between unlike users, and the offense, outrage, and remorse that follow. Another, much larger set of problems arise from the falsehood, hate and lies that go viral on social media, and their electoral consequences.

These two sets of problems are interrelated. We’re getting too much trolling and not enough facts: we need to screen out one and let in the other. The right filters can address both problems.

People who have left Twitter in the last year, at least temporarily, include Leslie Jones of the Ghostbusters remake, the British comic Stephen Fry, and Marc Andreessen of A16Z. Other notable recent departures include Zelda Williams, who was attacked after the suicide of her father Robin.

That’s right: She was attacked on Twitter after the suicide of her father. They sent her fake mortuary photos of him. That’s an example of the first problem.

Offense and Open Communities

A lot of people, especially in San Francisco, think that open communities are great and that social media should be all about connecting people.

But not everybody should be connected. Umberto Eco said that television gave us the village idiot so that we could feel superior, while the Internet gave us the village idiot as a source of truth. Nobody wants to argue with the village idiot, let alone millions of them.

On Twitter, you have to block them one at a time. That’s a lot of work, and by then it’s too late. Their trolling idiocy has infected your life. Their work is done before the rules can be enforced.

Perfectly open communities always go sour. You need filters. Every functional community has them. And that’s where machine learning comes in.

The natural-language processing to detect trolls, racism and insults isn’t hard, and Tweets as a data genre have been analyzed to death. We can build filters that work. (If you want to know how, you should read about neural nets and deep learning.) Deep learning is setting new records in accuracy for a lot of difficult problems, including image and voice recognition. It will achieve similar gains with text classification using algorithms like Word2vec and Doc2vec.

If you can detect trolls, you can protect the people they’re trolling by muting or putting a warning over the trolls’ posts. Twitter could even figure out who likes a few threats of violence now and then and personalize the masking.

Personally, I go on Twitter to learn new things and hear new voices. But there have been some interesting studies from liberal scholars that certain kinds of diversity can hurt civic life and erode trust. At the very least, it’s something that online communities should pay attention to, if they want to keep people coming back.

There’s a radical openness to Twitter, which is cool some of the time, and uncool other times. It’s the uncool times that stick with you. You can’t unsee morgue shots of your father, like those that were Tweeted at Zelda.

Twitter can do something about it, and they should. They already have a way of screening out porn. Why don’t they do the same thing with ethnic slurs, death threats and other kinds of trolling? Just draw a curtain over them. After a while, people will figure out that they don’t really want to see what someone said, if Twitter masks it. And their day will be better. And they will keep using Twitter.

Tweeting to the Choir in a Post-Fact Bubble

“A lie gets halfway around the world before the truth has a chance to get its pants on.” Winston Churchill said that. With social media, a lie probably circles the world a couple times…

The algorithms of platforms such as Facebook and Twitter may not shield us from hate, but they do encourage the spread of emotion-rich content among like-minded people, especially when that content triggers outrage. The more shares a post gets, the more it will be promoted to similar individuals because Facebook et al optimizes for engagement, period.

Unfortunately, a lot of that content is false, and its popularity has consequences.

One of the main problems with U.S. politics is a yearslong shift away from facts and science. It’s the replacement of a reality-based community, as formulated in the years of George W. Bush, with a platform of wishful thinking … backed by nukes.

That’s problematic for a lot of different reasons, notably the way it breaks our ability to understand cause and effect, trade, war and indifferent nature. It’s particularly harmful to how we relate to each other. Because facts are something that can unite very disparate people, while beliefs are endlessly divergent. Without them; bubbles all the way down.

We live in an age of self-reinforcing beliefs, and the reinforcement of groupthink happens in a feedback loop with the media, especially social media.

We need to stop the flow of hate and lies and help the spread of facts, because words matter.

What do I mean by lies? I mean this fake news. And the weird way Macedonian teenagers pumped out disinformation about Trump during this election cycle. Not only are our social media channels filled with garbage, but Americans are being gamed by foreigners. It should be illegal, but even if it’s not against the law, it’s something tech companies could control, if they wanted to…

They can control it because we now have the ability to detect hidden patterns in text to — say — identify a book’s true author. (The pseudonymous author Elena Ferrante was outed by a statistical textual analysis of her work before an investigative journalist doxed her this year.) Just like Google can build a highly accurate spam filter to keep you from wasting time on the pleas of Nigerian princes, deep learning can classify text by many measures, including its degree of factuality, falsehood or truthiness.

Algorithms can do that because we know how to “vectorize” text. That is, we can turn any text into a column of numbers, and those are called neural embeddings. It’s a simple, yet unlikely, translation to represent language in numbers.

Doing that makes natural language computer-readable. Then we can perform powerful mathematical operations on text to detect patterns and similarities, make predictions and apply categories to it. Those categories might be: “probably false” or “probably true.” And once we know the likelihood of a text’s factualness, we can decide how far it should spread.

We have fact-checkers at organizations like Snopes, Politifact or Media Matters applying judgments to news stories already. Those could be turned into labeled datasets to train algorithms to categorize text they’ve never seen before. If that’s not neutral enough, Facebook could build its own team of fact-checkers.

The real question is, do the tech companies want to control it?

Mark Zuckerberg is still thinking about that one. Facebook and Twitter flattered themselves that they played a role in the Arab Spring, but Zuckerberg said this weekend that it’s a “pretty crazy idea” that fake news on Facebook affected this tight election. You can’t have it both ways.

A smug, amoral response from the people at the top of powerful tech companies isn’t what we need. They have a responsibility to the public, to the species and to themselves to promote the facts and to mute the hate and lies, even if that responsibility is not enshrined in law. Not least because Mark Zuckerberg is Jewish, and Donald Trump rode a wave of anti-Semitism and white nationalism to power. It doesn’t matter how you identify when they start handing out the yellow stars.

While media endorsements meant diddly squat this election cycle, the way that media and social media promoted false stories week after week to increase their eyeballs and mindshare had a huge effect. This presidential election tipped on a couple percentage points in a few key states. Or to be precise, 107,330 votes in Wisconsin, Michigan and Pennsylvania handed America to Trump. That’s the equivalent of the population in Boulder, Colorado; West Palm Beach, Florida; or Daly City, California.

Do you think Comey’s untimely announcements, the months of Russian hacking, or Wisconsin’s vote-suppressing ID laws might have accounted for a town’s worth of ballots? If so, why wouldn’t the algorithms of a powerful social media platform used by tens of millions of US voters?

Broadcast media spent much more airtime covering the non-scandal of Hillary’s emails as it did covering the issues, or her policies, and social media amplified instead of remedied that distortion.

Some great journalists wrote some great stories this year. They reported on Trump’s mafia connections, his virtually non-existent philanthropy, and his extensive ties to Russia.

But those stories didn’t have legs. They never reached the larger audience that needed to hear them most, because we have become polarized. We go looking for opinions that agree with ours. Each of us needs some windows opened onto the disagreeable facts and inconvenient truths that will slap us in the face no matter what we wish for.

The tech platforms powering social media can help reconcile us with reality in many quiet ways, or they can join the indifferent and venal attention merchants that ushered a conman, a bigot and a sexual predator into the White House for the sake of an earnings report.

Until then, what we read on Twitter and Facebook will add nothing to our understanding of the world. It will just be our own breath backing up on us. And on that note, I think I need a Tic Tac.",Social Media,TechCrunch,https://techcrunch.com/2016/11/26/machine-learning-can-fix-twitter-facebook-and-maybe-even-america/,"The negative effects of Social Media are numerous, from the spread of hate and lies to the alienation of users, and even the impact of false information on electoral outcomes. To address these issues and ensure a better user experience, Social Media should implement filters and algorithms to screen out and detect trolls, racism, death threats, and other forms of hate",Equality & Justice
293,Border agents are checking entrants’ Facebook and Twitter profiles — but we still don’t know how closely,"Earlier this week, incoming Harvard freshman Ismail B. Ajjawi found himself blocked from entering the US. Ajjawi, a Palestinian resident of Lebanon, had landed in Boston before the start of classes. But The Harvard Crimson reported that after hours of questioning, US Customs and Border Protection agents revoked his visa. Ajjawi said a CBP agent searched his phone and laptop while asking questions about his friends’ social media activity. Then, she “started screaming at me,” Ajjawi said. “She said that she found people posting political points of view that oppose the US on my friend[s] list.”

CBP hasn’t revealed what actually got Ajjawi’s visa revoked. “Specific information on individual travelers cannot be released due to the Privacy Act requirements and for law enforcement purposes,” an agency spokesperson told The Verge. “This individual was deemed inadmissible to the United States based on information discovered during the CBP inspection.”

Social media posts might be public, but surveillance still has a chilling effect

But his case is just one incident in a troubling and well-established trend of expanding social media surveillance at the border. The Obama-era Department of Homeland Security initially suggested an “online presence” field for people requesting visa waivers, and the Trump administration quickly forged ahead with asking for social media data. Some border agents have aggressively pushed visitors to disclose their account handles, even when the practice was optional. Earlier this year, the State Department started requiring most visa applicants to list their social media accounts.

This week has offered a nightmare scenario for this vetting process. Ajjawi’s account suggests that digital surveillance goes far beyond checking whether a potential immigrant is a criminal threat — and that border officials are treating tenuous social media connections like close, meaningful relationships.

In some ways, checking social media posts isn’t as invasive as looking through private files on a device, something the CBP has done for years to find child pornography or other illegal material. Even so, it can have a chilling effect on speech, making people afraid to express their political views online — or say anything at all, since social media posts can be easily misinterpreted. “There are lots of free speech and freedom of association issues with looking at someone’s social media, even to vet them to come to the US,” says Sophia Cope, senior staff attorney at the Electronic Frontier Foundation.

While people have been detained or physically assaulted for refusing to unlock electronic devices, there have been relatively few major incidents involving social media specifically. It’s not clear that monitoring visa applicants’ posts is very helpful either. The DHS proposed social media rules after reports that San Bernardino shooter Tashfeen Malik had openly posted about jihad online. But the FBI said those reports weren’t true. A 2019 Brennan Center for Justice report noted that DHS pilot programs involving social media surveillance were “notably unsuccessful” at finding national security threats. Instead, co-author Faiza Patel argued on Twitter, it’s most useful for “targeting political and religious views — or even assumptions about them based on what your friends say.”

‘We need to know what structure is in place to ensure that travelers don’t face abuse.’

So is CBP really judging visa applicants on their Twitter feed or Facebook friend list? Cases like Ajjawi’s — where someone was apparently punished for things they didn’t even say — certainly seem to be rare. But we’re also relying on very incomplete data, and more vulnerable visitors could be far more hesitant to tell their stories. “I think there’s something very unique about this set of circumstances,” says Center for Democracy and Technology policy counsel Mana Azarmi, because the incident involves a student at a prestigious university with a prominent university paper that could pick up the story. “That kind of high visibility — we shouldn’t expect that of every interaction with CBP.”

A CBP spokesperson emphasized to The Verge that device searches affect “less than one-hundredth of one percent of travelers arriving to the United States.” But Azarmi points out that we don’t really know how these travelers are chosen — including whether they’re singled out for unfair reasons, and whether they’re regularly asked about troubling details like their friends’ opinions. “Are CBP personnel instructed to do that? Is that something that they’re looking out for?” she says. “We need to know what structure is in place to ensure that travelers don’t face abuse. And we don’t have that kind of information.”

Civil liberties advocates aren’t even sure how border agents might have found Ajjawi’s friends’ posts. Per a rule from 2017, officers are supposed to avoid accessing any data that’s stored outside the device, which includes full social media feeds on a phone or laptop. In this case, an officer could have seen cached posts, they could have looked his friends up on a separate computer, or they could have simply broken the rules — but there’s little outside oversight that would help us figure out which of those scenarios is most likely. The Verge asked CBP to confirm that it would have put devices in airplane mode before searching them; a spokesperson referred us to an information sheet that doesn’t mention the policy.

Unlike texts or emails, it’s hard to make social media truly private without defeating some of its purpose. And other many other law enforcement agencies use publicly available data for surveillance; police, for example, have scraped Facebook and Twitter data to monitor protests. There’s still no consensus on how deeply the government can mine this kind of data. But when there’s so much of it easily accessible, it can be used in deeply troubling ways.

Right now, the best way to protect social media data at the border is simply to uninstall apps and close browser tabs — but that doesn’t settle the question of when the government should be able to look up your friends online. “We should only expect that stories like this will become more common,” says Azarmi of this week’s events. “Because this collection has become more routine.”",Social Media,Verge,https://www.theverge.com/2019/8/31/20837448/social-media-dhs-cbp-surveillance-us-border-ismail-ajjawi-harvard,"Social media surveillance at the border has a chilling effect, making people afraid to express their political views online, and can be used to target individuals based on what their friends post.",Security & Privacy
294,Instagram and Facebook will start censoring ‘graphic images’ of self-harm – TechCrunch,"In light of a recent tragedy, Instagram is updating the way it handles pictures depicting self-harm. Instagram and Facebook announced changes to their policies around content depicting cutting and other forms of self-harm in dual blog posts Thursday.

The changes come about in light of the suicide of a 14-year-old girl named Molly Russell, a U.K. resident who took her own life in 2017. Following her death, her family discovered that Russell was engaged with accounts that depicted and promoted self-harm on the platform.

As the controversy unfolded, Instagram Head of Product Adam Mosseri penned an op-ed in the Telegraph to atone for the platform’s at times high-consequence shortcomings. Mosseri previously announced that Instagram would implement “sensitivity screens” to obscure self-harm content, but the new changes go a step further.

Starting soon, both platforms will no longer allow any “graphic images of self-harm” most notably those that depict cutting. This content was previously allowed because the platforms worked under the assumption that allowing people to connect and confide around these issues was better than the alternative. After a “comprehensive review with global experts and academics on youth, mental health and suicide prevention,” those policies are shifting.

“… It was advised that graphic images of self-harm – even when it is someone admitting their struggles – has the potential to unintentionally promote self-harm,” Mosseri said.

Instagram will also begin burying non-graphic images about self-harm (pictures of healed scars, for example) so they don’t show up in searches, relevant hashtags or on the explore tab. “We are not removing this type of content from Instagram entirely, as we don’t want to stigmatize or isolate people who may be in distress and posting self-harm related content as a cry for help,” Mosseri said.

According to the blog post, after consulting with groups like the Centre for Mental Health and Save.org, Instagram tried to strike a balance that would still allow users to express their personal struggles without encouraging others to hurt themselves. For self-harm, like disordered eating, that’s a particularly difficult line to walk. It’s further complicated by the fact that not all people who self-harm have suicidal intentions, and the behavior has its own nuances apart from suicidality.

“Up until now, we’ve focused most of our approach on trying to help the individual who is sharing their experiences around self-harm. We have allowed content that shows contemplation or admission of self-harm because experts have told us it can help people get the support they need. But we need to do more to consider the effect of these images on other people who might see them. This is a difficult but important balance to get right.”

Mental health research and treatment teams have long been aware of “peer influence processes” that can make self-destructive behaviors take on a kind of social contagiousness. While online communities can also serve as a vital support system for anyone engaged in self-destructive behaviors, the wrong kind of peer support can backfire, reinforcing the behaviors or even popularizing them. Instagram’s failure to sufficiently safeguard for the potential impact this kind of content can have on a hashtag-powered social network is fairly remarkable considering that both Instagram and Facebook claim to have worked with mental health groups to get it right.

These changes are expected in the “coming weeks.” For now, a simple search of Instagram’s #selfharm hashtag still reveals a huge ecosystem of self-harmers on Instagram, including self-harm-related memes (some hopeful, some not) and many very graphic photos of cutting.

“It will take time and we have a responsibility to get this right,” Mosseri said. “Our aim is to have no graphic self-harm or graphic suicide related content on Instagram… while still ensuring we support those using Instagram to connect with communities of support.”",Social Media,TechCrunch,https://techcrunch.com/2019/02/07/instagram-self-harm-cutting-facebook/,"The recent tragedy of a teen's suicide caused by her engagement with accounts depicting and promoting self-harm, has prompted Instagram to update their policies around content related to self-harm. In order to reduce the potential of other people being influenced by such content, Instagram will no longer allow graphic images of self-harm, and will bury non-",Social Norms & Relationships
295,Here's How the US Should Fight ISIS With Social Media,"The Islamic State wants to rule the world. It murders enemies—sometimes in mass, sometimes individually, always brutally. It enslaves and abuses women. It jails everyday joes for smoking, drinking, trading, or speaking their minds. It is a brutal, dead-end regime cloaked in a perverted medieval understanding of one of the world's great religious faiths.

But one of the scariest things, Westerners seem to agree, is that ISIS is really good at Twitter.

Adam Weinstein About Adam Weinstein runs the Fortress America blog at Gawker dot com. A former social media editor and military public affairs consultant in Iraq, he wrote his master's thesis on winning hearts and minds in the war on terror. It was completely wrong.

The U.S. diplomatic corps is fully engaged in a battle to beat ISIS in the electronic talkosphere. But what does that mean, exactly?

So far, the government's social media campaign against ISIS has been, like most governmental campaigns, long on bureaucracy and short on details. The State Department's chief of ""public diplomacy,"" former Time managing editor Rick Stengel, oversees a Center for Strategic Counterterrorism Communications, whose portfolio includes social media culture-jamming. Its tasks include ""creating communities of interest, supporting positive voices, narrowing the space violent extremists have to work in, repeatedly and aggressively presenting the reality of what is going on on the ground,"" according to former CSCC head Alberto Fernandez.

How that happens is fairly opaque. Part of that is because of government inside baseball–intrigues on who should run the State Department's anti-ISIS team, and how tightly it should coordinate with other agencies. But there are deeper structural problems with the whole endeavor, too. My experience in wartime military social media is: There are a multitude of ways to screw it up, and not many ways to get a positive result.

With that in mind, here are a few strategies the U.S. can use to combat ISIS. Like any strategy in counterinsurgency, none is a panacea, but all have advantages worth considering. Some of these the government is already doing, though I have some notes for how to improve their reach. Others are my own humble suggestions. Take ‘em or leave ‘em.

Play Some Mood Music

The first and most obvious strategy is to use social media the way a government agency or corporation uses any other PR tool: To tell a coherent story about yourself and your competitors. This the government is attempting to do.

""More important than ideologies and ideas are how those elements are packaged, delivered and digested for wider audiences,"" Fernandez says. ""More than fully formed ideologies, we are all prodded and driven by narratives – by stories, images, slogans, memes, and stereotypes.""

This means performing some typical Twitter feats—fact-checking and #realkeeping when dullards spout detestable dumbness. Beyond that, it also means humanizing Americans and their allies, as well as the victims of ISIS aggression. ""Given some of the language you sometimes hear from lone wolf attacks in the West or al-Qaeda propaganda videos,"" Fernandez says, ""you would be shocked to know that 85 percent of their victims are Muslims and that that percentage has risen even higher in the past few years.""

One achievable goal, then, is ""to reclaim the stories and lives of these forgotten victims"" and articulate the damage ISIS actually does, including to its own adherents. It also means retweeting emotionally resonant messages like this:

How much does this approach accomplish? Very little among jihadis, who will easily counter that Western policies are just as responsible for death and mayhem. And it does nothing to dispel overly simplistic accounts of the ISIS-U.S. struggle as a ""clash of civilizations."" But it has fewer drawbacks than more targeted, aggressive strategies–unless, of course, it deteriorates into bombastic propaganda and scaremongering stereotypes.

Target Specific Users With Smarm

One way the State Department uses its mood music is to blast it out at individuals who toe the jihadi line. Its ""Think Again, Turn Away"" program aims to deter them from joining the global jihad by pointing out its costs and the hypocrisy of its leaders.

How effective are these tweets and videos? Good damn question. ""If you're talking about would-be extremists reading a tweet and turning away from violence as a result, it's hard to tell how much that is happening,"" CSCC official Will McCants told Mother Jones early last year, before leaving for the Brookings Institution. ""So if you measure success that way, it's hard to know.""

One recent study says such campaigns are dismissed as disingenuous ""spin"" by target audiences and ""generate more negativity"" toward the U.S. and its social media operators. And when a campaign goes bad, brother, it goes comically bad. The State Department became the butt of multiple jokes in 2013 after releasing a bizarre YouTube video spoofing Al Qaeda leader Ayman Al-Zawahiri. As the Daily Dot observed: ""Dying is easy. Comedy is hard.""",Social Media,WIRED,https://www.wired.com/2015/03/heres-us-fight-isis-social-media/,"The use of social media as a counterinsurgency tool can be counterproductive, resulting in dismissive reactions from target audiences and creating more negative views of the U.S. and its social media operators. It can also easily become a platform for bombastic propaganda, scaremongering, and failed attempts at humor.","Information, Discourse & Governance"
296,Activists push back on Facebook’s decision to remove a DC protest event – TechCrunch,"A number of activists and organizers in the Washington, DC area are disputing Facebook’s decision to remove a counter-protest event for a rally organized by Jason Kessler, the white nationalist figure who planned the deadly 2017 rally in Charlottesville, Va.

Facebook removed the event, “No Unite the Right 2-DC,” after discovering that one account connected to the event exhibited what Facebook calls “coordinated inauthentic behavior.” The company defines this activity as “people or organizations creating networks of accounts to mislead others about who they are, or what they’re doing.”

The Facebook page at the center of the controversy was called “Resisters.” TechCrunch confirmed that the Resisters page was created by “bad actors,” as defined by the company, who coordinated fake accounts to deceive users. Facebook ultimately removed the No Unite the Right 2-DC event due to its known interaction and engagement with the Resisters page and maintains that Resisters was an illegitimate page from its inception.

As the company explained in its blog post:

The “Resisters” Page also created a Facebook Event for a protest on August 10 to 12 and enlisted support from real people… Inauthentic admins of the “Resisters” Page connected with admins from five legitimate Pages to co-host the event.

The company also observed that a known Internet Research Agency (IRA) account joined the counter-protest event as an admin, though it only served as an admin for seven minutes. (The IRA has been assessed by the U.S. intelligence community as a content farm likely funded by a close Putin ally with ties to Russian intelligence.) On top of that, Facebook noted that an IRA account the company was aware of shared a Facebook event hosted by Resisters in 2017.

Here’s where things get even more tricky. The event that Facebook deleted had been taken over by a handful of real DC area activist groups. These groups, including Smash Racism DC, Black Lives Matter DC, Black Lives Matter Charlottesville and other local groups, worked together under the coalition name “Shut It Down DC” and their actions and plans were not inspired by the “No Unite the Right 2” event, they just happened to cross paths. (Since then, the coalition has recreated the Facebook event as “Hate Not Welcome: No Unite The Right 2.”)

We've since created a new Facebook event but we know real organizing comes from talking with our neighbors, and that this is a real protest in Washington, DC. It is not George Soros, it is not Russia, it is just us. — ShutItDownDC (@shutitdowndc) July 31, 2018

TechCrunch spoke with a handful of DC-based organizers, including Andrew Batcher, a Washington, DC-based activist involved with Shut It Down DC, to clarify how the local coalition of organizers became connected to an event and an account deemed illegitimate by Facebook.

“It was grassroots organizing from a lot of different groups who were interested in this,” Batcher said. “A lot of groups went down to Charlottesville last year. Charlottesville is only two hours south of DC.”

He explains that the group’s impetus was Kessler’s own event, not a Facebook event that organizers stumbled onto.

“When we started organizing we talked about making a Facebook page and saw that this already existed,” Batcher said. “It happens pretty regularly in DC knowing how many major events take a place here.”

“We asked to be made co-hosts of the event and we put our stuff up on it basically,” Batcher said. That included video calls to action, photos and other content, including the event description. “Everything that was taken down was ours.”

Beyond creating the initial placeholder page, Batcher says that the Resisters page had “absolutely no involvement” in the event. “This is really outrageous for us,” Batcher said. “[It] makes it look like we’re Russian pawns. We know that we’re not, and we know that we’ve been doing this organizing.”

He and other activists on the left have expressed concerns that this depiction could undermine their efforts in the mainstream and even lead to conspiracy theories like Pizzagate that spill over into real life violence.

Facebook says that it reached out to the legitimate organizers of No Unite The Right 2-DC with the following message:

We haven’t been able to connect on the phone yet, but I did want to make sure you know that earlier today we removed a Facebook event that you are listed as a co-host of, “No Unite the Right 2 – DC”, because one the Pages that created the event, “Resisters”, has been removed from Facebook because [it] was created by someone establishing an inauthentic account that has been associated with coordinated inauthentic behavior. I understand this may be surprising or frustrating. We are reaching out to make sure you have the relevant information and understand that this has nothing to do with you or your Page. Later today, we’ll begin providing information about the event deletion to the approximately 2,600 users who indicated their interest in the event, and the 600 plus users that said they’d attend. If you are interested in setting up another event, we would be happy to include details about it in our public communications.

According to Batcher, most of the event’s organizers with Shut It Down DC did not receive any correspondence and others received an email “two lines long,” which he provided.

The group is dismayed that Facebook went ahead and removed the event before making contact with more of its organizers. In interviews with TechCrunch, he and other organizers expressed a deep distrust of Facebook and a desire to see more evidence from the company that supports its recent actions. One organizer connected to the DC groups expressed concern that Facebook might be flagging activists working together using VPNs for suspicious coordinated activity. When asked about that concern, Facebook explained that VPN use and common privacy measures would not be sufficient, by Facebook’s standards, to cause an account or page to be removed.

“If there was an account that did something bad, get rid of that account. It doesn’t seem to me like it would have to spread to all of this legitimate organizing,” Batcher said. “What we would like is a public apology and them letting people know that we are real people doing real organizing.” He added that Facebook did not show consideration for the potential damage to the people who organized the event together in real life.

That distrust is reflected on both sides of the political spectrum. Concerns that Facebook is censoring content made by right-wing figures have bubbled up in congressional hearings and been floated among many users on the right, though there is little evidence supporting recent claims. On the left, the company has a checkered history of sometimes unwitting censorship in incidents that have affected everyone from Black Lives Matter supporters to parts of the LGBTQ community. In some of those cases, Facebook users were abusing the platform’s reporting tools for targeted harassment, but the company was slow to address concerns or to change its policy.

Facebook has also dragged its feet in confronting openly abusive, racist content on its platform and recently faced criticism for internal policies that allow white nationalism while forbidding white supremacism, drawing what is widely considered to be an artificial distinction between the two. These woes don’t just affect Facebook, but the platform does appear to be a perfect storm for anyone acting in bad faith.

While the counter-protest organizers have since created a new Facebook event and intend to continue their efforts, the situation is a fairly unsettling cautionary tale of a rising form of manipulation on the world’s biggest social platform. Recent revelations and those from 2017 show that a new breed of “blended” social media influence campaign — fake accounts leveraging the efforts of real, regular people — proves particularly insidious.

So-called “bad actors” are infiltrating legitimate causes, creating chaos and throwing everything into question. Even when these efforts are exposed, it’s a winning formula for anyone seeking to sow further discord and doubt in the U.S. political landscape. For everyone else, the odds aren’t looking good.",Social Media,TechCrunch,https://techcrunch.com/2018/08/01/facebook-organizers-protest-no-unite-the-right-2/,"So-called ""bad actors"" are infiltrating legitimate causes on Social Media, creating chaos and sowing discord and doubt in the US political landscape, which is a major problem for users of the platform.",Discourse & Governance
297,TikTok called out for lack of ads transparency and for failing to police political payola – TechCrunch,"TikTok called out for lack of ads transparency and for failing to police political payola

TikTok announced a ban on political advertising all the way back in 2019. So you’d be forgiven for thinking the ugly problem of democracy-denting political disinformation doesn’t apply inside its walled garden of dancing Gen Zers. But you’d be wrong.

New research by Mozilla suggests that policy loopholes and lax oversight, especially around influencer marketing, coupled with an ongoing lack of ads transparency by TikTok — which offers no publicly searchable ad archive — are making its video-sharing platform vulnerable to passing off political ads as organic content.

Mozilla says it found more than a dozen instances of TikTok influencers across the political spectrum who were being paid (or otherwise compensated) by a variety of political organizations to promote partisan messages without disclosing that these posts were sponsored.

“Our research found that TikTok influencers across the political spectrum had undisclosed paid relationships with various political organizations in the U.S.,” it writes. “Several right-wing TikTok influencers appear to be funded by conservative

organizations like Turning Point USA, a tax-exempt nonprofit which has a dedicated influencer program specifically targeted at funding young conservative content creators on social media.”

It similarly found evidence of left-leaning sponsored political messaging being spread without proper disclosures by TikTok influencers, noting that: “We found some evidence that progressive influencers supported by left-leaning political organizations were posting pro-Biden messages prior to the U.S. presidential election. For instance, The 99 Problems created and funded the Hype House account House of US, where influencers post political messaging.”

In the report, “Th€se Are Not Po£itical Ad$: How Partisan Influencers Are Evading TikTok’s Weak Political Ad Policies”, Mozilla calls out the platform for not offering adequate tools for “influencers” — aka users who have amassed a large enough number of followers to become attractive targets for advertisers to target for making paid postings — to report sponsorships, pointing out that other major social media platforms (like Facebook/Instagram) do offer such tools and can flag influencer content if they’re found failing to properly report ads.

“Of course, it’s hard to know exactly how self-disclosure ad policies are being enforced across platforms but TikTok is significantly far behind Instagram and YouTube when it comes to providing tools and enacting clear, strict, and transparent policies,” Mozilla writes in the report.

Per TikTok’s rules, content creators are supposed to self-identify any paid content (typically by using the hashtag #ad or #sponsored), in keeping with U.S. Federal Trade Commission guidelines for the disclosure of paid influence.

But, as Mozilla points out, if TikTok isn’t actively monitoring or scrutinizing influencer ads (as the report suggests), it raises an obvious concern over how the platform can claim to be enforcing its “trust and safety” protocols.

Mozilla’s report also points to rumours that TikTok is testing features that will allow influencers to pay to further promote specific posts — which could dial up the “dark money” political disinformation problem further, i.e. if not combined with active policing and enforcement of sponsorship disclosures.

“There do not appear to be any safeguards preventing creators from using this feature to promote paid political messages,” it warns. “It is unclear how TikTok is monitoring this content to ensure that it complies with their political ad policy.”

Another major criticism in the report is the general lack of ads transparency by TikTok versus other social platforms — with Mozilla’s report pointing out that it does not offer public, searchable ad databases as others (including Facebook/Instagram, Snap and Google/YouTube) do. Twitter has also had a searchable ads archive since 2018.

“Mozilla believes Facebook and Google are doing a poor job on ad transparency, so the fact that TikTok can’t match even them is troubling,” the report notes.

In recommendations to TikTok (or to policymakers shaping laws aimed at preventing abuse of such platforms) Mozilla suggests that it needs to develop specific mechanisms for content creators to disclose partnerships; invest in comprehensive advertising transparency, including launching an ad database which includes paid partnerships (not just native platform ads); and update its policies and enforcement processes to cover all the ways that paid political influence can happen on its platform.

TikTok was contacted with questions on its approach to ads transparency and sponsored content. It sent this statement:

Political advertising is not allowed on TikTok , and we continue to invest in people and technology to consistently enforce this policy and build tools for creators on our platform. As we evolve our approach we appreciate feedback from experts, including researchers at the Mozilla Foundation, and we look forward to a continuing dialogue as we work to develop equitable policies and tools that promote transparency, accountability, and creativity.

There are signs that TikTok is trying to get ahead of criticisms in the report — as Mozilla’s researcher, Becca Ricks, notes that the company has very recently (“within the past week”) created a branded content policy.

“It includes mention of a ‘branded content toggle‘ to help influencers disclose paid partnerships,” she went on, adding: “We’re currently analyzing the feature to learn more. But we’re cautiously optimistic that this could be a (small) step in the right direction, especially after we raised these issues directly with TikTok two weeks ago in the course of our research.

“That said, Mozilla’s other recommendations — and the entirety of the problems we uncovered in the research — remain. So TikTok has a long road to being truly transparent.”

Mozilla’s report is just the latest black cloud to fall over TikTok’s platform, which is under pressure on a variety of fronts related to its content and wider policies, including around ad disclosures.

Last week, EU regulators kicked off what they couched as a formal “dialogue” with TikTok following a number of complaints by consumer protection groups, which have accused the platform of hidden marketing, aggressive advertising techniques targeted at children and misleading and confusing contractual terms.

Other regional complaints have called out TikTok’s approach to privacy and user data. And it’s being sued in the U.K. over its handling of children’s data.

Concerns over weak age verification also led to an intervention by Italy’s data protection regulator earlier this year — acting on concerns for the safety of underage users. In that case TikTok was forced to remove more than half a million accounts that were suspected of being used by children younger than 13.

In recent months TikTok has been trying to burnish its image with policymakers, announcing what it bills as a “Transparency Center” in the U.S. last year — and another for Europe this April — saying these centers would provide a space for outside experts to access information about its content moderation and security policies.

However Mozilla said the centers suffer from a lack transparency vis-à-vis ads, writing in the report that they “do not provide detailed transparency regarding advertisements”, and specifying TikTok does not disclose specific data about “how many or which ads were rejected under TikTok’s ban on political advertisements”, for example.

TikTok’s opacity arounds ads looks to be on borrowed time as the issue of online political ads transparency is coming into sharper focus around the world.

In the U.S. a bipartisan bill to try to regulate online platforms that sell ads was introduced in 2017 — although progress stalled as the bill failed to pass ahead of the 2019 U.S. presidential election.

In Europe lawmakers are expected to put forward a regulatory proposal this fall that will tighten ad disclosure and reporting requirements on platforms as part of a wider package of digital reforms that aim to drive safety, transparency and accountability.",Social Media,TechCrunch,https://techcrunch.com/2021/06/03/tiktok-called-out-for-lack-of-ads-transparency-and-for-failing-to-police-political-payola/,"Mozilla's research found that TikTok is vulnerable to passing off political ads as organic content due to its policy loopholes and lack of ads transparency, making it difficult to enforce its self-disclosure ad policies and to police political payola.","Information, Discourse & Governance"
298,Can data science save social media? – TechCrunch,"The unfettered internet is too often used for malicious purposes and is frequently woefully inaccurate. Social media — especially Facebook — has failed miserably at protecting user privacy and blocking miscreants from sowing discord.

That’s why CEO Mark Zuckerberg was just forced to testify about user privacy before both houses of Congress. And now governmental regulation of Facebook and other social media appears to be a fait accompli.

At this key juncture, the crucial question is whether regulation — in concert with Facebook’s promises to aggressively mitigate its weaknesses — will correct the privacy abuses and continue to fulfill Facebook’s goal of giving people the power to build transparent communities, bringing the world closer together?

The answer is maybe.

What has not been said is that Facebook must embrace data science methodologies initially created in the bowels of the federal government to help protect its two billion users. Simultaneously, Facebook must still enable advertisers — its sole source of revenue — to get the user data required to justify their expenditures.

Specifically, Facebook must promulgate and embrace what is known in high-level security circles as homomorphic encryption (HE), often considered the “Holy Grail” of cryptography, and data provenance (DP). HE would enable Facebook, for example, to generate aggregated reports about its user psychographic profiles so that advertisers could still accurately target groups of prospective customers without knowing their actual identities.

Meanwhile, data provenance — the process of tracing and recording true identities and the origins of data and its movement between databases — could unearth the true identities of Russian perpetrators and other malefactors, or at least identify unknown provenance, adding much-needed transparency in cyberspace.

Both methodologies are extraordinarily complex. IBM and Microsoft, in addition to the National Security Agency, have been working on HE for years, but the technology has suffered from significant performance challenges. Progress is being made, however. IBM, for example, has been granted a patent on a particular HE method — a strong hint it’s seeking a practical solution — and last month proudly announced that its rewritten HE encryption library now works up to 75 times faster. Maryland-based ENVEIL, a startup staffed by the former NSA HE team, has broken the performance barriers required to produce a commercially viable version of HE, benchmarking millions of times faster than IBM in tested use cases.

How homomorphic encryption would help Facebook

HE is a technique used to operate on and draw useful conclusions from encrypted data without decrypting it, simultaneously protecting the source of the information. It is useful to Facebook because its massive inventory of personally identifiable information is the foundation of the economics underlying its business model. The more comprehensive the data sets about individuals, the more precisely advertising can be targeted.

HE could keep Facebook information safe from hackers and inappropriate disclosure, but still extract the essence of what the data tells advertisers. It would convert encrypted data into strings of numbers, do math with these strings, then decrypt the results to get the same answer it would if the data wasn’t encrypted at all.

A particularly promising sign for HE emerged last year, when Google revealed a new marketing measurement tool that relies on this technology to allow advertisers to see whether their online ads result in in-store purchases.

Unearthing this information requires analyzing data sets belonging to separate organizations, notwithstanding the fact that these organizations pledge to protect the privacy and personal information of the data subjects. HE skirts this by generating aggregated, non-specific reports about the comparisons between these data sets.

In pilot tests, HE enabled Google to successfully analyze encrypted data about who clicked on an advertisement in combination with another encrypted multi-company data set that recorded credit card purchase records. With this data in hand, Google was able to provide reports to advertisers summarizing the relationship between the two databases to conclude, for example, that five percent of the people who clicked on an ad wound up purchasing in a store.

Data provenance

Data provenance has a markedly different core principle. It’s based on the fact that digital information is atomized into 1s and 0s with no intrinsic truth. The dual digits exist only to disseminate information, whether accurate or widely fabricated. A well-crafted lie can easily be indistinguishable from the truth and distributed across the internet. What counts is the source of these 1s and 0s. In short, is it legitimate? What is the history of the 1s and 0s?

The art market, as an example, deploys DP to combat fakes and forgeries of the world’s greatest paintings, drawings and sculptures. It uses DP techniques to create a verifiable, chain-of-custody for each piece of the artwork, preserving the integrity of the market.

Much the same thing can be done in the online world. For example, a Facebook post referencing a formal statement by a politician, with an accompanying photo, would have provenance records directly linking the post to the politician’s press release and even the specifics of the photographer’s camera. The goal — again — is ensuring that data content is legitimate.

Companies such as Walmart, Kroger, British-based Tesco and Swedish-based H&M, an international clothing retailer, are using or experimenting with new technologies to provide provenance data to the marketplace.

Let’s hope that Facebook and its social media brethren begin studying HE and DP thoroughly and implement it as soon as feasible. Other strong measures — such as the upcoming implementation of the European Union’s General Data Protection Regulation, which will use a big stick to secure personally identifiable information — essentially should be cloned in the U.S. What is best, however, are multiple avenues to enhance user privacy and security, while hopefully preventing breaches in the first place. Nothing less than the long-term viability of social media giants is at stake.",Social Media,TechCrunch,https://techcrunch.com/2018/04/18/can-data-science-save-social-media/,"Social media giants such as Facebook have failed in protecting user privacy and blocking miscreants from sowing discord, leading to governmental regulation of social media and the need to embrace data science methodologies to protect users.",Security & Privacy
299,Facebook’s latest account purge exposes Africa’s misinformation problem – TechCrunch,"Facebook last week purged a network of hundreds of pages, groups and Instagram accounts it labeled as producing “coordinated inauthentic behavior” toward Africa.

The activity originated in Israel and was largely targeted toward Nigeria, Senegal, Togo, Angola, Niger, and Tunisia.

It was mostly political in nature and primarily paid for by Archemedes Group, a global political consulting firm, Facebook said.

This isn’t the first case of social media platforms used as vehicles for political manipulation in Africa. Cambridge Analytica, the controversial big-data actor employed in Brexit and Donald Trump’s 2016 presidential victory, was active on the continent before and after both events.

On its recent Africa related deletions, Facebook said:

The people behind this network used fake accounts to run Pages, disseminate their content and artificially increase engagement. They also represented themselves as locals, including local news organizations, and published allegedly leaked information about politicians. The Page administrators and account owners frequently posted about political news, including topics like elections in various countries, candidate views and criticism of political opponents.

The activity took place over 65 Facebook accounts, 161 Pages, 23 Groups, 12 events and four Instagram accounts. There were 2.8 million accounts that followed one or more of these pages and 5,500 accounts joined at least one of these Groups.

Facebook said more than $800,000 was spent on ads associated with these accounts starting in December 2012 and running as recently as April this year.

Facebook declined to offer TechCrunch additional information on the account deletions beyond their release. But the Atlantic Council’s Digital Forensic Research Lab (DFRL) has been digging deeper and released some initial findings in a Medium Post. In addition to connecting the accounts to activity in Ghana — a country not named in FB’s release — DFRL shed some light on fake news targeted at Nigeria’s February 2019 elections.

Examples included a “Make Nigeria Worse Again” trolling initiative aimed at the campaign of Atiku Abubakar, who was challenger to Nigeria’s incumbent president Muhammadu Buhari — who won a second-term.

DFRL also shared examples connected to the deleted Facebook accounts aimed at elections in Mali, Tunisia, Niger, Togo, Algeria, and Angola. It noted the ads related to this nexus of activity were paid for in U.S. dollars, Israeli Shekels, and Brazilian reals. “The spending in different currencies suggests how vast the operation was, encompassing multiple regions around the world,” said DFRL’s reporting.

Fake news on social media platforms has reared its head in Africa several times. Cambridge Analytica, backed by U.S. big-data billionaire Robert Mercer, was found to have been involved in elections in Kenya and Nigeria before its controversial role directing pro-Brexit and pro-Trump online activity in 2016. Facebook later banned Cambridge Analytica from its platform.

Social media driven fake news — primarily on Facebook and WhatsApp — became such an issue in Kenya’s 2017 elections the country’s parliament passed a bill in 2018, with specific punitive measures, to combat it. An investigation by the UK’s Channel 4 later revealed that Cambridge Analytica had advised the 2017 presidential campaign of Kenyan incumbent president Uhuru Kenyatta, who won in a disputed run-off vote.

Facebook has prioritized growth in Africa, particularly since Mark Zuckerberg visited the continent’s tech scene in 2016.

The U.S. social media company has grown Africa users to over 200 million and Facebook owned chat-tool, WhatsApp, is the most downloaded messenger app on the continent.

But Facebook’s recent Africa account purge shows when Facebook travels, so too does its list of pros and cons, including the ability of global actors to use it for nefarious uses in local settings.",Social Media,TechCrunch,https://techcrunch.com/2019/05/20/facebook-account-purge-africa-fake-news/,"Facebook's recent purge of accounts related to Africa shows that Social Media platforms, such as Facebook and WhatsApp, are being used by global actors for nefarious purposes in local settings, including spreading fake news and manipulating elections.","Information, Discourse & Governance"
300,"Facebook to block new political ads 1 week before Nov 3, adds more tools and rules for fair elections – TechCrunch","We’re now 61 days away from the U.S. presidential election, and Facebook is once more ramping up its efforts to level the playing field and attempt to keep its platform from being manipulated to influence how people vote.

CEO Mark Zuckerberg today announced a series of new measures, including the news that it will block new political and issue ads in the final week of the campaign — although campaigns can still run ads to encourage people to vote, and they can still run older political ads. Other announcements today detailed more work to counter misinformation, and stronger rules to counter voter suppression, including misleading references to COVID-19 at the polls.

The news today is significant not just because it’s a sign of how Facebook continues to work on more proactive measures around the election, but that it is definitely past the point of trying to present itself as an innocent bystander to forces that would have been in play even if Facebook didn’t exist.

“This election is not going to be business as usual,” he wrote in the post. “We all have a responsibility to protect our democracy. That means helping people register and vote, clearing up confusion about how this election will work, and taking steps to reduce the chances of violence and unrest.”

Other measures will include placing its Voter Information Center — a hub for voting information, with deadlines and guides on how to vote by mail and other related details that it announced in August — at the top of Facebook and Instagram “almost every day until the election.” (Originally, the hub was going to be accessible — and somewhat hidden — in the menu; now it’s being moved into a more prominent slot.)

Zuckerberg said that the political ad blocking is being put in place because Facebook — another admission — doesn’t believe that there would be enough time to contest any new claims that might come in the ads.

But while blocking those last-minute political ads is an important move, it’s not a complete block of all political ads. Facebook said that political ads posted more than a week before the election can still stay up, and targeting for those ads can still be adjusted. In other words, they can essentially be re-run, or run as new campaigns.

Zuck’s explanation is that the older ads have time to be researched. “Those ads will already be published transparently in our Ads Library so anyone, including fact-checkers and journalists, can scrutinize them,” he noted.

But others are already coming out with criticism against the measures, saying they do not go far enough.

“This is a first step, but it doesn’t address the myriad of issues that could actually influence election outcomes,” said Lisa Kaplan, founder and CEO, Alethea Group, a specialist consultant on disinformation, in a statement. “The fact that you can alter advertising takes the teeth out of the proposed policy changes, and political advertising is only one piece of the disinformation puzzle. What’s more pressing is addressing that individuals have been fed a steady misinformation and disinformation diet for months, and one week of allegedly limiting advertising cannot be expected to make much of a difference. While this is a nice gesture — it’s unlikely to make an impact and leaves voters vulnerable to disinformation.”

The company said that its efforts so far have driven 24 million clicks to voter registration sites, but how those translate into actual registrations is not clear. The company has set a goal of helping 4 million people register to vote, and Zuckerberg himself has donated $300 million to organizations working on that effort.

Other efforts announced today include a number of moves to try to combat misinformation — one of the key ways that Facebook has been leveraged in past elections to influence voting.

Specifically, Facebook said it is extending the window beyond 72 hours — its original timescale — where it’s going to try to identify and remove false claims about polling conditions, given that many may try to vote early this time around.

And given how a lot of misinformation is also shared through direct channels off Facebook itself, it’s also going to limit how things can be forwarded on Messenger to stem how content goes viral on there. “You’ll still be able to share information about the election, but we’ll limit the number of chats you can forward a message to at one time,” Zuckerberg noted. This will, of course, cut both ways (those trying to put out accurate information might also get dinged) but ultimately is a direct result of how Facebook has altered forwarding on WhatsApp around elections in other countries, such as India.

One of the other issues that has been highlighted previously has been how the high percentage of people voting by mail might be exploited to the advantage of candidates that take strong early leads in live voting: the worry is that the live results get called as early victories, before other votes are tallied, which could, for example, dissuade people from going to polling stations and voting. Facebook now says that it will be adding labels to candidates and campaigns that try to declare victory before the official calls (but won’t be removing those posts). It’s working with Reuters and the National Election Pool to determine more accurate results, it said.

Another big theme in misinformation has been around COVID-19 and how scare tactics around this are used to dissuade people from voting. Facebook said it will “remove posts with claims that people will get Covid-19 if they take part in voting,” with links to more accurate information. The rule will also include ads with this message.

Misinformation also comes through Facebook by way of sending false details about how polling stations or how voting works, for example not just trying to discourage people from going to polls, but also intentionally giving specific groups of voters the wrong information about how to vote, for example telling them that it’s okay to send in their ballots past the deadline.

All of these policies will work in tandem with how Facebook deals with a completely different threat, coming not from candidates and their campaigns but other actors intent on destabilising how democratic processes work, or simply to influence how they go.

Just this week, Facebook took down a network of 13 accounts and two pages sending out misleading claims about political candidates. The company says that it’s investing more into its security to continue fighting this, but it’s a huge problem, stretching back years now to the previous U.S. presidential election, and apparently not going away anytime soon. While originally the threats were identified as coming from countries like Russia, Zuckerberg now admitted that “We’re increasingly seeing attempts to undermine the legitimacy of our elections from within our own borders.”

It’s going to be a long 61 days….",Social Media,TechCrunch,https://techcrunch.com/2020/09/03/facebook-to-block-new-political-ads-1-week-before-nov-3-adds-more-tools-and-rules-for-fair-elections/,"Social media platforms are proving to be a powerful tool to manipulate and undermine the democratic process, with Facebook taking measures to block new political and issue ads in the final week of the US presidential election and placing its Voter Information Center at the top of its platforms to help inform voters, while attempting to counter misinformation and voter suppression.",Politics
301,Facebook upgrades its AI to better tackle COVID-19 misinformation and hate speech – TechCrunch,"Facebook’s AI tools are the only thing standing between its users and the growing onslaught of hate and misinformation the platform is experiencing. The company’s researchers have cooked up a few new capabilities for the systems that keep the adversary at bay, identifying COVID-19-related misinformation and hateful speech disguised as memes.

Detecting and removing misinformation relating to the virus is obviously a priority right now, as Facebook and other social media become breeding grounds not just for ordinary speculation and discussion, but malicious interference by organized campaigns aiming to sow discord and spread pseudoscience.

“We have seen a huge change in behavior across the site because of COVID-19, a huge increase in misinformation that we consider dangerous,” said Facebook CTO Mike Schroepfer in a call with press earlier today.

The company contracts with dozens of fact-checking organizations around the world, but — leaving aside the question of how effective the collaborations really are — misinformation has a way of quickly mutating, making taking down even a single image or link a complex affair.

Take a look at the three example images below, for instance: In some ways they’re nearly identical, with the same background image, colors, typeface and so on. But the second one is slightly different — it’s the kind of thing you might see when someone takes a screenshot and shares that instead of the original. The third is visually the same but the words have the opposite meaning.

An unsophisticated computer vision algorithm would either rate these as completely different images due to those small changes (they result in different hashes) or all the same due to overwhelming visual similarity. Of course we see the differences right away, but training an algorithm to do that reliably is very difficult. And the way things spread on Facebook, you might end up with thousands of variations rather than a handful.

“What we want to be able to do is detect those things as being identical because they are, to a person, the same thing,” said Schroepfer. “Our previous systems were very accurate, but they were very fragile and brittle to even very small changes. If you change a small number of pixels, we were too nervous that it was different, and so we would mark it as different and not take it down. What we did here over the last two and a half years is build a neural net-based similarity detector that allowed us to better catch a wider variety of these variants again at very high accuracy.”

Fortunately analyzing images at those scales is a specialty of Facebook’s. The infrastructure is there for comparing photos and searching for features like faces and less desirable things; it just needed to be taught what to look for. The result — from years of work, it should be said — is SimSearchNet, a system dedicated to finding and analyzing near-duplicates of a given image by close inspection of their most salient features (which may not be at all what you or I would notice).

SimSearchNet is currently inspecting every image uploaded to Instagram and Facebook — billions a day.

The system is also monitoring Facebook Marketplace, where people trying to skirt the rules will upload the same image of an item for sale (say, an N95 face mask) but slightly edited to avoid being flagged by the system as not allowed. With the new system, the similarities between recolored or otherwise edited photos are noted and the sale stopped.

Hateful memes and ambiguous skunks

Another issue Facebook has been dealing with is hate speech — and its more loosely defined sibling hateful speech. One area that has proven especially difficult for automated systems, however, is memes.

The problem is that the meaning of these posts often results from an interplay between the image and the text. Words that would be perfectly appropriate or ambiguous on their own have their meaning clarified by the image on which they appear. Not only that, but there’s an endless number of variations in images or phrasings that can subtly change (or not change) the resulting meaning. See below:

Each individual piece of the puzzle is fine in some contexts, insulting in others. How can a machine learning system learn to tell what’s good and what’s bad? This “multimodal hate speech” is a non-trivial problem because of the way AI works. We’ve built systems to understand language, and to classify images, but how those two things relate is not so simple a problem.

The Facebook researchers note that there is “surprisingly little” research on the topic, so theirs is more an exploratory mission than a solution. The technique they arrived at had several steps. First, they had humans annotate a large collection of meme-type images as hateful or not, creating the Hateful Memes data set. Next, a machine learning system was trained on this data, but with a crucial difference from existing ones.

Almost all such image analysis algorithms, when presented with text and an image at the same time, will classify the one, then the other, then attempt to relate the two together. But that has the aforementioned weakness that, independent of context, the text and images of hateful memes may be totally benign.

Facebook’s system combines the information from text and image earlier in the pipeline, in what it calls “early fusion,” to differentiate it from the traditional “late fusion” approach. This is more akin to how people do it — looking at all the components of a piece of media before evaluating its meaning or tone.

Right now the resultant algorithms aren’t ready for deployment at large — at around 65-70% overall accuracy, though Schroepfer cautioned that the team uses “the hardest of the hard problems” to evaluate efficacy. Some multimodal hate speech will be trivial to flag as such, while some is difficult even for humans to gauge.

To help advance the art, Facebook is running a “Hateful Memes Challenge” as part of the NeurIPS AI conference later this year; this is commonly done with difficult machine learning tasks, as new problems like this one are like catnip for researchers.

AI’s changing role in Facebook policy

Facebook announced its plans to rely on AI more heavily for moderation in the early days of the COVID-19 crisis. In a press call in March, Mark Zuckerberg said that the company expected more “false positives” — instances of content flagged when it shouldn’t be — with the company’s fleet of 15,000 moderation contractors at home with paid leave.

YouTube and Twitter also shifted more of their content moderation to AI around the same time, issuing similar warnings about how an increased reliance on automated moderation might lead to content that doesn’t actually break any platform rules being flagged mistakenly.

In spite of its AI efforts, Facebook has been eager to get its human content reviewers back in the office. In mid-April, Zuckerberg gave a timeline for when employees could be expected to get back to the office, noting that content reviewers were high on Facebook’s list of “critical employees” marked for the earliest return.

While Facebook warned that its AI systems might remove content too aggressively, hate speech, violent threats and misinformation continue to proliferate on the platform as the coronavirus crisis stretches on. Facebook most recently came under fire for disseminating a viral video discouraging people from wearing face masks or seeking vaccines once they are available — a clear violation of the platform’s rules against health misinformation.

The video, an excerpt from a forthcoming pseudo-documentary called “Plandemic,” initially took off on YouTube, but researchers found that Facebook’s thriving ecosystem of conspiracist groups shared it far and wide on the platform, injecting it into mainstream online discourse. The 26-minute-long video, peppered with conspiracies, is also a perfect example of the kind of content an algorithm would have a difficult time making sense of.

On Tuesday, Facebook also released a community standards enforcement report detailing its moderation efforts across categories like terrorism, harassment and hate speech. While the results only include a one month span during the pandemic, we can expect to see more of the impact of Facebook’s shift to AI moderation next time around.

In a call about the company’s moderation efforts, Zuckerberg noted that the pandemic has made “the human review part” of its moderation much harder, as concerns around protecting user privacy and worker mental health make remote work a challenge for reviewers, but one the company is navigating now. Facebook confirmed to TechCrunch that the company is now allowing a small portion of full-time content reviewers back into the office on a volunteer basis and, according to Facebook Vice President of Integrity Guy Rosen, “the majority” of its contract content reviewers can now work from home. “The humans are going to continue to be a really important part of the equation,” Rosen said.",Social Media,TechCrunch,https://techcrunch.com/2020/05/12/facebook-upgrades-its-ai-to-better-tackle-covid-19-misinformation-and-hate-speech/,"The rise of social media has brought along an increase in hate speech, misinformation, and other malicious interference by organized campaigns aiming to sow discord. To combat this, Facebook has developed AI technologies such as SimSearchNet and a ""Hateful Memes"" detector to identify and remove such content, however, the system is not yet perfect and human","Information, Discourse & Governance"
302,Europe’s top court sharpens guidance for sites using leaky social plug-ins – TechCrunch,"Europe’s top court has made a ruling that could affect scores of websites that embed the Facebook ‘Like’ button and receive visitors from the region.

The ruling by the Court of Justice of the EU states such sites are jointly responsible for the initial data processing — and must either obtain informed consent from site visitors prior to data being transferred to Facebook, or be able to demonstrate a legitimate interest legal basis for processing this data.

The ruling is significant because, as currently seems to be the case, Facebook’s Like buttons transfer personal data automatically, when a webpage loads — without the user even needing to interact with the plug-in — which means if websites are relying on visitors’ ‘consenting’ to their data being shared with Facebook they will likely need to change how the plug-in functions to ensure no data is sent to Facebook prior to visitors being asked if they want their browsing to be tracked by the adtech giant.

The background to the case is a complaint against online clothes retailer, Fashion ID, by a German consumer protection association, Verbraucherzentrale NRW — which took legal action in 2015 seeking an injunction against Fashion ID’s use of the plug-in which it claimed breached European data protection law.

Like ’em or loath ’em, Facebook’s ‘Like’ buttons are an impossible-to-miss component of the mainstream web. Though most Internet users are likely unaware that the social plug-ins are used by Facebook to track what other websites they’re visiting for ad targeting purposes.

Last year the company told the UK parliament that between April 9 and April 16 the button had appeared on 8.4M websites, while its Share button social plug-in appeared on 931K sites. (Facebook also admitted to 2.2M instances of another tracking tool it uses to harvest non-Facebook browsing activity — called a Facebook Pixel — being invisibly embedded on third party websites.)

The Fashion ID case predates the introduction of the EU’s updated privacy framework, GDPR, which further toughens the rules around obtaining consent — meaning it must be purpose specific, informed and freely given.

Today’s CJEU decision also follows another ruling a year ago, in a case related to Facebook fan pages, when the court took a broad view of privacy responsibilities around platforms — saying both fan page administrators and host platforms could be data controllers. Though it also said joint controllership does not necessarily imply equal responsibility for each party.

In the latest decision the CJEU has sought to draw some limits on the scope of joint responsibility, finding that a website where the Facebook Like button is embedded cannot be considered a data controller for any subsequent processing, i.e. after the data has been transmitted to Facebook Ireland (the data controller for Facebook’s European users).

The joint responsibility specifically covers the collection and transmission of Facebook Like data to Facebook Ireland.

“It seems, at the outset, impossible that Fashion ID determines the purposes and means of those operations,” the court writes in a press release announcing the decision.

“By contrast, Fashion ID can be considered to be a controller jointly with Facebook Ireland in respect of the operations involving the collection and disclosure by transmission to Facebook Ireland of the data at issue, since it can be concluded (subject to the investigations that it is for the Oberlandesgericht Düsseldorf [German regional court] to carry out) that Fashion ID and Facebook Ireland determine jointly the means and purposes of those operations.”

Responding the judgement in a statement attributed to its associate general counsel, Jack Gilbert, Facebook told us:

Website plugins are common and important features of the modern Internet. We welcome the clarity that today’s decision brings to both websites and providers of plugins and similar tools. We are carefully reviewing the court’s decision and will work closely with our partners to ensure they can continue to benefit from our social plugins and other business tools in full compliance with the law.

The company said it may make changes to the Like button to ensure websites that use it are able to comply with Europe’s GDPR.

Though it’s not clear what specific changes these could be, such as — for example — whether Facebook will change the code of its social plug-ins to ensure no data is transferred at the point a page loads. (We’ve asked Facebook and will update this report with any response.)

Facebook also points out that other tech giants, such as Twitter and LinkedIn, deploy similar social plug-ins — suggesting the CJEU ruling will apply to other social platforms, as well as to thousands of websites across the EU where these sorts of plug-ins crop up.

“Sites with the button should make sure that they are sufficiently transparent to site visitors, and must make sure that they have a lawful basis for the transfer of the user’s personal data (e.g. if just the user’s IP address and other data stored on the user’s device by Facebook cookies) to Facebook,” Neil Brown, a telecoms, tech and internet lawyer at U.K. law firm Decoded Legal, told TechCrunch.

“If their lawful basis is consent, then they’ll need to get consent before deploying the button for it to be valid — otherwise, they’ll have done the transfer before the visitor has consented

“If relying on legitimate interests — which might scrape by — then they’ll need to have done a legitimate interests assessment, and kept it on file (against the (admittedly unlikely) day that a regulator asks to see it), and they’ll need to have a mechanism by which a site visitor can object to the transfer.”

“Basically, if organisations are taking on board the recent guidance from the ICO and CNIL on cookie compliance, wrapping in Facebook ‘Like’ and other similar things in with that work would be sensible,” Brown added.

Luca Tosoni, a research fellow at the University of Oslo’s Norwegian Research Center for Computers and Law who has been following the case, said the court has not clarified what interests may be considered ‘legitimate’ in this context — only that both the website operator and the plug-in provider must pursue a legitimate interest.

“After today’s judgment, all website operators that insert third-party plug-ins (such as Facebook ‘Like’ buttons) in their websites should carefully reassess their compliance with EU data protection law,” he agreed. “In particular, they should verify whether their privacy policies cover data processing operations involving the collection and transmission of visitors’ personal data by means of third-party plug-ins. Many of today’s policies are unlikely to cover such operations.

“Website operators should also assess what is the appropriate legal basis for the collection and transmission of personal data by means of the plug-ins embedded in their websites, and if consent applies, they should ensure that they obtain the user’s consent before the data collection takes place, which may often prove challenging in practice. In this regard, the use of pre-ticked checkboxes is not advisable, as it tends to be considered insufficient to fulfil the criteria for valid consent under European data protection law.”

Also commenting on the judgement, Michael Veale, a UK-based researcher in tech and privacy law/policy, said it raises questions about how Facebook will comply with Europe’s data protection framework for any further processing it carries out of the social plug-in data.

“The whole judgement to me leaves open the question ‘on what grounds can Facebook justify further processing of data from their web tracking code?'” he told us. “If they have to provide transparency for this further processing, which would take them out of joint controllership into sole controllership, to whom and when is it provided?

“If they have to demonstrate they would win a legitimate interests test, how will that be affected by the difficulty in delivering that transparency to data subjects?’

“Can Facebook do a backflip and say that for users of their service, their terms of service on their platform justifies the further use of data for which individuals must have separately been made aware of by the website where it was collected?

“The question then quite clearly boils down to non-users, or to users who are effectively non-users to Facebook through effective use of technologies such as Mozilla’s browser tab isolation.”

How far a tracking pixel could be considered a ‘similar device’ to a cookie is another question to consider, he said.

The tracking of non-Facebook users via social plug-ins certainly continues to be a hot-button legal issue for Facebook in Europe — where the company has twice lost in court to Belgium’s privacy watchdog on this issue. (Facebook has continued to appeal.)

Facebook founder Mark Zuckerberg also faced questions about tracking non-users last year, from MEPs in the European Parliament — who pressed him on whether Facebook uses data on non-users for any other uses vs the security purpose of “keeping bad content out” that he claimed requires Facebook to track everyone on the mainstream Internet.

MEPs also wanted to know how non-users can stop their data being transferred to Facebook? Zuckerberg gave no answer, likely because there’s currently no way for non-users to stop their data being sucked up by Facebook’s servers — short of staying off the mainstream Internet.

This report was updated with additional comment",Social Media,TechCrunch,https://techcrunch.com/2019/07/29/europes-top-court-sharpens-guidance-for-sites-using-leaky-social-plug-ins/,"This ruling by the Court of Justice of the EU could affect scores of websites that embed the Facebook 'Like' button, as they may now need to obtain informed consent from site visitors prior to their data being transferred to Facebook, or be able to demonstrate a legitimate interest legal basis for processing this data. This means that non-users may have",Security & Privacy
303,How To Turn Off Snapchat’s Stalkerish Snap Map Feature,"Snapchat has always known exactly where you are. What, you thought everyone saw that “Greetings from the Brooklyn Bridge” filter? Until recently, Snapchat didn’t do much with your location data beyond serving up geofilters and pushing location-specific stories. Sure, the app uses your whereabouts to help marketers sell you stuff, but what social media behemoth doesn't?

This week, though, Snapchat unveiled a feature that leverages your location data in a whole new way, and it’s got a lot of people freaked out. Snap Map tracks your current location and places your Bitmoji avatar on a map like a pin. Others can zoom in and find exactly where you are, down to the street address. It’s sort of like Apple’s Find My Friends and Facebook’s Live Location feature—only, you know, on Snapchat.

Obviously, Snapchat sees Snap Map as a fun and convenient way to connect with friends. But for plenty of people, the new feature is just plain creepy. Some worry about the ""stalker factor,"" particularly for Snapchat's younger users who might not fully grasp the implications of a technology that constantly broadcasts their location.

How to Opt Out

The good news: Snap Maps is technically opt-in. It doesn't take effect until you update the app and go through the feature's on-boarding flow. Once you do that, though, it's fairly straightforward (as far as Snapchat goes) to opt-out of sharing your location with others. If you’re opening the app for the first time after the update, Snapchat will walk you through a step-by-step tutorial on how to use the Snap Map. First, it’ll show you how to pinch and zoom in the camera tool to access the map. Next, it’ll ask who you want to see your location.

You get three choices: all your friends, select friends, or only me. Choosing “only me” activates what Snapchat calls Ghost Mode. This makes your avatar disappear from others’ maps; like true poltergeist, you can see others but they can’t see you. Great for lurkers! But remember, Snapchat’s all seeing, all knowing eyes (ahem, your phone’s sensors) can still track your location. To turn off location data altogether, you’ll need to visit your phone's settings where you can scroll down to Snapchat, click on “location,” and choose to never share.

If you’ve already gone through the tutorial and want to change who can see you on their map, that's easy, too. Open Snap Map and click on the setting menu in the right hand corner, where you can decide who gets the privilege of tracking your every move.",Social Media,WIRED,https://www.wired.com/story/how-to-turn-off-snapchat-snap-maps/,"The main undesirable consequence of Snapchat's Snap Map feature is that it allows others to track your exact location, raising safety and privacy concerns for users, particularly for younger users who may not understand the implications of the feature.",Security & Privacy
304,"On social media, Harvey Weinstein’s downfall engenders fragile hope in a sea of hypocrisy","It has been two weeks since The New York Times published a report detailing three decades of sexual misconduct and abuse by one of Hollywood’s most powerful players, producer Harvey Weinstein. That piece centered on eight women who spoke up about his behavior, and later settled with Weinstein out of court. Five days later, a second report followed in The New Yorker, presenting revelations from 13 more women who shared stories of harassment, assault, and rape.

In a follow-up from The New York Times, seven more women, including Angelina Jolie and Gwyneth Paltrow, came forward to tell their stories about Weinstein trying to trick, manipulate, or outright force them into sex in exchange for his professional assistance. Heather Graham told her story in Variety. Cara Delevingne and Kate Beckinsale told theirs on Instagram. Rose McGowan and Lena Headey shared theirs on Twitter. Last night, Lupita Nyong’o published her story in The New York Times, detailing years of Weinstein bullying and harassing her. At the end, she writes, “I did not know that things could change. I did not know that anybody wanted things to change.”

So far, more than 40 women have spoken up about Weinstein. His downfall has been quick and decisive: he’s been fired from the Weinstein Company, ousted from the Academy of Motion Pictures, and is currently under investigation by New York City and London police.

social media’s broad role in Weinstein’s unmasking is a mixed bag

Earlier this week, Kathryn Bigelow — the only woman to win a Best Director Oscar in the Academy’s 90-year history — was quoted in The New York Times in a piece about how the Weinstein allegations “opened a floodgate” in Hollywood, unearthing dozens of old stories, and laying bare its toxic culture. She told the paper, “The democratization of the spread of information can finally move faster than a powerful media mogul’s attempts to bury it.”

In some ways, this is obviously true. It’s hard to imagine an incident like the 1999 event Rebecca Traister detailed for The Cut — where Weinstein called her a cunt, and assaulted her co-worker in full view of dozens of reporters and photographers — staying a secret in the age of the two-second upload. However, social media’s broad role in Weinstein’s unmasking is a mixed bag. Women rallied on Twitter, with impressive speed and single-mindedness, sharing stories of harassment and assault in other industries and their personal lives. The hashtag #MeToo created a stream of solidarity that was unavoidable on the platform. But the same week, women were moved to boycott Twitter for 24 hours, protesting Rose McGowan’s temporary suspension, and the company’s generally negligent handling of harassment and abuse.

In the era of social media and democratized information, the first and largest batch of allegations still came out through two traditional news outlets, both of which are paywalled. Subsequent reports were amplified in other large publications, generally in proportion to the fame of the person speaking. We aren’t done with gatekeepers deciding which stories will make the leap from social media to broad cultural conversation. We certainly aren’t ready to say Twitter is going to save the world.

a tweet can technically reach as many people as The New York Times

But a tweet can technically reach as many people as The New York Times, if it goes sufficiently viral. Unbound by the strictures of journalism, it can add context that traditional outlets can’t or won’t. For the last two weeks, despite all its other flaws, Twitter has allowed for a backchannel where the silver lining in the Weinstein revelation — a tremendous hope that the culture of silence and complicity around sexual abuse might finally change — has been tempered by ruthless, methodical exposure of the sea of hypocrisy that still surrounds it. Ben Affleck posted a statement to Facebook and tweeted a screenshot of it, writing, “I am saddened and angry that a man who I worked with used his position of power to intimidate, sexually harass, and manipulate many women over decades.” Minutes later, writer Pilot Viruet quote-tweeted it with the caption “how’s your brother doing,” referencing Casey Affleck’s sexual harassment lawsuits, and it was retweeted nearly 7,000 times. Old videos of Ben Affleck harassing female journalists were dug up and circulated widely. Kate Winslet told Variety that she found the Weinstein story “disgraceful and appalling,” and a few hours later, a quip about her upcoming collaboration with Woody Allen went viral. The Academy of Motion Pictures released a statement about Weinstein’s dismissal from its voting body and was quickly skewered for neglecting to give Allen, Bill Cosby, or Roman Polanski the same treatment.

Good but Mel Gibson said ""if you get raped by a pack of n****** it's your fault"" on tape & @TheAcademy nominated him 4 Best Director LAST yr https://t.co/IaRL2rVurV — Marcia Belsky (@MarciaBelsky) October 14, 2017

The revelations about Weinstein have already had a ripple effect in other industries, including mine, and I doubt we are anywhere near done hearing stories about, as a viral spreadsheet put it, the “shitty men” in media — much less the shitty men in Hollywood and everywhere else. There’s a tension in this, though; it feels good to pull up the floorboards and expose the rot, to finally have disgusting, publicly acknowledged evidence of a problem women have always insisted was there. And at the same time, there’s so much rot that it looks unmanageable. There’s so much rot that it’s tempting to burn the whole place down. The women writing about these allegations have to ask terrible questions: “Women who make public claims about sexual abuse face shaming and disbelief, not to mention professional blackballing,” Stephanie Merry wrote in The Washington Post. “And for what?” Precious little, even in recent memory. We live in a country with a president accused of similar crimes dozens of times over: “Does it matter, though? Does it ever matter?”

There is so much complicity to call out, we have no choice but to crowdsource the task. The same publications that are publishing the words of Weinstein’s victims have been complicit for decades in the power structures that persist nearly unchecked in Hollywood. (Full-page portraits of Casey Affleck were printed in The New York Times three times leading up to the Oscars.) Famous men who are releasing statements expressing disgust and horror that this occurred right under their noses are — as many have already pointed out — complicit, because they chose not to listen until now. The Academy is complicit, not only because it welcomed Mel Gibson and Casey Affleck to this year’s awards, but because it has spent nine decades behaving as if only men are capable of making exceptional art, as if only men have something worthwhile to say about the human experience.

the hollywood reporter is very bad pic.twitter.com/YNL1yKAD7o — Kaitlyn Tiffany (@kait_tiffany) December 14, 2016

This isn’t irrelevant. It’s part of the same knotty, awful truth. Hollywood would still be a hostile and dangerous place for women even if no one there was being sexually harassed, assaulted, or intimidated. If you are a woman who wants to make movies, you will see your ambition questioned and your livelihood threatened at every turn. Last year, the pool of directors making the industry’s top 250 films was 93 percent male. The screenwriting pool was 87 percent male. Executive producers: 83 percent. Overall, San Diego State's Center for the Study of Women in Television and Film found that women held only 17 percent of all major positions on the year’s top 250 films. In her Times piece, Nyong’o also wrote that she hadn’t experienced anything like Weinstein’s gross misconduct since she cut ties with him, speculating, “I think it is because all the projects I have been a part of have had women in positions of power[.]” Finding those projects was a feat of will and good luck.

On Twitter, a common reaction to the Weinstein revelations was pointing out the tragedy of loss of talent; many of the people he harassed or assaulted left the film industry altogether, rather than face a lifetime of evading more men like him. It’s not a leap to say there are thousands of voices missing from our movies, thanks to the actions of men like Weinstein. Women in the film industry aren’t just missing out on opportunity because they’re literally being passed over for jobs, but also because — as I tried to argue just before Casey Affleck was handed that golden statue — they’re sacrificing time, energy, and creativity to the task of protecting their bodies from harm. Time, energy, and creativity is wasted because women have to spend these resources figuring out who to avoid, how to avoid them, who to protect, how to protect them, when to speak, what to say, and how to say it so someone believes it.

This is all there in the accounts of the women who have spoken up; some 500 words of Nyong’o’s essay are about how she strategized to protect her career from Weinstein’s spite.

So what can we do, other than get angry? Last December, before Casey Affleck slalomed around questions about his two sexual harassment lawsuits and won his first Oscar, I wrote that I didn’t think it could possibly happen. How could the Academy give him that silly-but-consequential prize, knowing how angry we were? I wasn’t the only person talking about it; it was all over the internet, and women were loudly, articulately angry. At the time, I received several comments and emails about a question I posed: “Who knows how many stories like these have gone without comment because they’re de rigueur, and their time in the limelight has not yet come?” People wrote to tell me I was undercutting my own argument with this wild speculation and bitter rhetorical flourish.

It would feel good to see Harvey Weinstein’s downfall as an unambiguous tipping point for the reign of horrible men in Hollywood. It is exhausting to read tweet after tweet about the specific ways in which it may not be, raining down our timelines all day long. I would prefer to think the extraordinary women who have broadcasted their nightmares have eked out an unqualified triumph on behalf of all of us. But I can honor them better by being grateful for the cacophony of voices — the people calling out more bad men, more weak allies, more hypocritical statements, and more shitty half-truths, over and over until it feels like too much.

These thousands of tweets, and the online “outrage culture” they could be uncharitably lumped with, are easy to tear down as redundant and self-righteous, and maybe a lot of them are. But what do we have, other than our anger, and each other’s anger? Righteous anger is worth expressing, worth sustaining, and worth stoking until it burns your belly like Lupita Nyong’o’s “flare of rage.” I don’t have the power to dismantle Hollywood’s power structure, or to banish its bullies and idiots to obscurity. I only have the power to be angry, and talk about it, and help you stay angry, too. There’s hope. It feels very fragile.",Social Media,Verge,https://www.theverge.com/2017/10/20/16503276/harvey-weinstein-sexual-assault-hollywood-sexism-hypocrisy-social-media,"Social media has allowed for a backchannel where many stories of sexual misconduct and harassment can be shared, but it has also revealed the complicity of powerful figures in the industry and the entrenched sexism that still exists. Despite the power of tweets and viral hashtags, traditional outlets and gatekeepers still largely decide which stories get featured, and women who speak",Equality & Justice
305,Senators Take to Facebook to Criticize Facebook's Russian Ads,"For roughly three hours Tuesday, senators laid into executives from Facebook, Google, and Twitter about Russia’s attempts to spread disinformation using social media. Lawmakers pressed the tech giants for failing to stop voter suppression, not detecting Russian operatives paying for ads in rubles, and the terrifying power they possess over American politics. Then, lawmakers touted their efforts---on Facebook and Twitter, of course.

Many didn’t wait until the end of the Senate subcommittee hearing, with aides who are social-media whizzes tweeting and retweeting their bosses’ tough stands on big tech in real time. Republican Sen. John Kennedy of Louisiana pointedly asked Facebook’s general counsel, “Do you have a profile on me?” and then posted a clip of him asking the question on his Facebook page. Sen. Lindsey Graham, the committee chairman who called the hearing, tweeted a link to a YouTube video of his opening remarks, “ICYMI.”

This inescapable power dynamic between dominant social networks and their users is sometimes mistaken for hypocrisy or irony, rather than what it is: an economic reality. Facebook and Twitter have become essential tools of political communication. When Russia Today and Sputnik were banned from advertising on Twitter as agents of Russian propaganda, they complained about the ruling on Twitter. When Gab, the right-wing social network, wanted to point out the hypocrisy of Twitter’s testimony today, it also did so on Twitter.

Those are companies, and the senators who grilled social-media executives on Tuesday are powerful political actors who are trying to prevent bad actors from undermining democracy again. Yet when Graham found out that 126 million people on Facebook saw election-rated disinformation from Russian operatives, not 10 million as Facebook reported earlier, he shared the news on his Facebook page, not in a press release or on his Senate webpage.

Graham is well aware of the issue. He began his remarks quoting Donald Trump saying he wouldn’t be president without social media. “I would dare say that every politician up here asking you questions uses your service,” said Graham, “and we find it invaluable to communicate with our constituents and get our message out.” Sure they do. What other choice do they have?",Social Media,WIRED,https://www.wired.com/story/senators-take-to-facebook-to-criticize-facebooks-russian-ads/,"The main consequence discussed here is the inescapable power dynamic between dominant social networks and their users. Politicians rely on these networks to communicate with their constituents, but bad actors, such as Russian operatives, can use these networks to spread disinformation and undermine democracy.",Politics
306,Apple CEO Tim Cook says social media is being used to manipulate and divide us,"Apple CEO Tim Cook doesn’t have a high opinion of platform-owning social media and advertising companies, most of which are now embroiled in an ongoing controversy about Russian interference in the 2016 US election. Cook, in an interview with Lester Holt on NBC Nightly News this evening, said the big issue isn’t the advertisements targeting groups and individuals, but the overall nature of social media as a tool for misinformation and manipulation.

“I don’t believe the big issue are ads from foreign governments. I believe that’s like .1 percent of the issue,” Cook says. “The bigger issue is that some of these tools are used to divide people, to manipulate people, to get fake news to people in broad numbers so as to influence their thinking. This to me is the No. 1 through 10 issue.”

Following a second day of Capitol Hill testimony from members of Google, Facebook, and Twitter, Congress has taken an increasingly aggressive and adversarial stance on Silicon Valley’s ability to reign in the abuse of its products and keep them free of bad actors and foreign governments. In the past couple of months, it’s been revealed that Russia-linked advertising was prevalent in Google search and YouTube ads, as well on both Facebook and Twitter’s ad platforms. And in the past week, following the disclosure of new reports from all three tech companies, we know these ads reached many more users than previously thought, in some cases having more than 10 times the initial reported impact.

Cook addressed the congressional hearings, saying that the social media companies have “learned along the way a lot,” and that “we'll probably learn more in those hearings as to the particulars.” He went on to say that just as all New York companies aren’t all the same — and all media companies aren’t the same — well, all technology companies aren’t the same, either. They have different values, different principles, different business models.”

Those business models include compiling volumes of user data, something that Cook reasserted isn’t something that Apple is interested in. “We take a very pro-privacy view,” Cook says. “Apple doesn’t know what the content of your messages are. We encrypt FaceTime end to end. We don’t know what you’re saying.” Holt brought up that the company’s upcoming iPhone X raises privacy concerns with its FaceID system. Cook explained that the facial recognition data is stored on the phone and encrypted, and that “Apple doesn’t have that: your device has that.”",Social Media,Verge,https://www.theverge.com/2017/11/1/16595596/apple-ceo-tim-cook-fake-news-russia-election-ad-interference,"The main issue raised by Apple CEO Tim Cook is the potential for social media to be used to divide people, manipulate them, and spread false information, which could lead to negative impacts on individuals and society at large.","Information, Discourse & Governance"
307,How Pro-Eating Disorder Posts Evade Filters on Social Media,"For almost as long as the internet has existed, so too have pro-eating disorder communities: blogs, groups, forums, and social media profiles where users share stories and photos related to disordered eating and body image. Some members simply want a judgment-free place to express their feelings about a complicated illness, but others promote more dangerous behavior, like encouraging extreme diets or dissuading people from getting help.

And as with other kinds of harmful content on the internet, platforms hosting pro-ED communities have long struggled with how to moderate them. In 2001, Yahoo removed more than 100 pro-ED sites from its servers, saying they violated its terms of service. A decade later, a HuffPost exposé about teenage girls creating “thinspiration” blogs on Tumblr prompted that site and others to ban explicitly pro-ED communities.

New research published last month in the peer-reviewed journal New Media & Society highlights how pro-ED groups continue to evade attempts at moderation. The study also found that sites like Pinterest and Instagram sometimes suggest more pro-ED content to users via their recommendation algorithms. It isn’t an isolated problem—researchers have found that recommendation engines on platforms like YouTube also suggest problematic content, like conspiracy theories. But unlike fake news, users who share pro-eating disorder content could be suffering from a serious illness like anorexia or bulimia. Companies need to weigh not just the content itself, but also the effect that removing it might have on the vulnerable people who share it.

“It’s very, very difficult to tease out what would fit under the category of toxic, or pro-eating disorder content,” says Claire Mysko, the CEO of the National Eating Disorders Association, which has worked with social media sites to help moderate pro-ED communities. “The people who are posting it and who are engaged in these communities are really struggling. You don’t want to set it up as this is good and bad, demonizing the users who are posting this content.”

The Hashtag Dilemma

The New Media & Society study focuses on hashtags as a tool for content moderation, and underscores how difficult it can be for tech companies to find problematic content and to decide what should and shouldn’t be removed.

After the HuffPost article was published in 2012, Instagram, Pinterest, and Tumblr began moderating pro-ED hashtags and search terms.

Now when users search for tags related to eating disorders, such as #bulimia, the sites either block results entirely or surface a pop-up message asking if they want to seek help.

What it looks like when a user searches for ""bulimia"" on Tumblr. New Media & Society

It’s an understandable strategy: hashtags, unlike images, can be easily categorized and flagged by automated detection systems and human moderators. They’re also how many people find new content, so blocking certain hashtags limits how many people a post may reach. Historically, tagged content has also been over-emphasized by social science researchers for many of the same reasons. It’s easier to analyze a large data set of specific hashtags than to manually comb through untagged content.

But specific hashtags aren’t permanent fixtures of social media, and they can easily morph to suit the communities’ needs. A separate 2016 study from the Georgia Institute of Technology found that pro-ED users simply began to intentionally misspell or alter terms: “#thinspiration” became “#thynspiration,” “#thinspire,” or “#thinspirational,” for example. In order to evade moderation, many pro-ED accounts and blogs don’t use any hashtags at all, making them harder for platforms to find and researchers to study.",Social Media,WIRED,https://www.wired.com/story/how-pro-eating-disorder-posts-evade-social-media-filters/,"The pro-eating disorder communities on social media platforms have been difficult to moderate, as users continually find new ways to avoid detection, such as misspelling or omitting hashtags. This makes it difficult for tech companies to detect and remove problematic content, as well as for researchers to understand the impact of these communities.",Social Norms & Relationships
308,The Great Hack tells us data corrupts – TechCrunch,"This week professor David Carroll, whose dogged search for answers to how his personal data was misused plays a focal role in The Great Hack: Netflix’s documentary tackling the Facebook-Cambridge Analytica data scandal, quipped that perhaps a follow up would be more punitive for the company than the $5BN FTC fine released the same day.

The documentary — which we previewed ahead of its general release Wednesday — does an impressive job of articulating for a mainstream audience the risks for individuals and society of unregulated surveillance capitalism, despite the complexities involved in the invisible data ‘supply chain’ that feeds the beast. Most obviously by trying to make these digital social emissions visible to the viewer — as mushrooming pop-ups overlaid on shots of smartphone users going about their everyday business, largely unaware of the pervasive tracking it enables.

Facebook is unlikely to be a fan of the treatment. In its own crisis PR around the Cambridge Analytica scandal it has sought to achieve the opposite effect; making it harder to join the data-dots embedded in its ad platform by seeking to deflect blame, bury key details and bore reporters and policymakers to death with reams of irrelevant detail — in the hope they might shift their attention elsewhere.

Data protection itself isn’t a topic that naturally lends itself to glamorous thriller treatment, of course. No amount of slick editing can transform the close and careful scrutiny of political committees into seat-of-the-pants viewing for anyone not already intimately familiar with the intricacies being picked over. And yet it’s exactly such thoughtful attention to detail that democracy demands. Without it we are all, to put it proverbially, screwed.

The Great Hack shows what happens when vital detail and context are cheaply ripped away at scale, via socially sticky content delivery platforms run by tech giants that never bothered to sweat the ethical detail of how their ad targeting tools could be repurposed by malign interests to sew social discord and/or manipulate voter opinion en mass.

Or indeed used by an official candidate for high office in a democratic society that lacks legal safeguards against data misuse.

But while the documentary packs in a lot over an almost two-hour span, retelling the story of Cambridge Analytica’s role in the 2016 Trump presidential election campaign; exploring links to the UK’s Brexit leave vote; and zooming out to show a little of the wider impact of social media disinformation campaigns on various elections around the world, the viewer is left with plenty of questions. Not least the ones Carroll repeats towards the end of the film: What information had Cambridge Analytica amassed on him? Where did they get it from? What did they use it for? — apparently resigning himself to never knowing. The disgraced data firm chose declaring bankruptcy and folding back into its shell vs handing over the stolen goods and its algorithmic secrets.

There’s no doubt over the other question Carroll poses early on the film — could he delete his information? The lack of control over what’s done with people’s information is the central point around which the documentary pivots. The key warning being there’s no magical cleansing fire that can purge every digitally copied personal thing that’s put out there.

And while Carroll is shown able to tap into European data rights — purely by merit of Cambridge Analytica having processed his data in the UK — to try and get answers, the lack of control holds true in the US. Here, the absence of a legal framework to protect privacy is shown as the catalyzing fuel for the ‘great hack’ — and also shown enabling the ongoing data-free-for-all that underpins almost all ad-supported, Internet-delivered services. tl;dr: Your phone doesn’t need to listen to you if it’s tracking everything else you do with it.

The film’s other obsession is the breathtaking scale of the thing. One focal moment is when we hear another central character, Cambridge Analytica’s Brittany Kaiser, dispassionately recounting how data surpassed oil in value last year — as if that’s all the explanation needed for the terrible behavior on show.

“Data’s the most valuable asset on Earth,” she monotones. The staggering value of digital stuff is thus fingered as an irresistible, manipulative force also sucking in bright minds to work at data firms like Cambridge Analytica — even at the expense of their own claimed political allegiances, in the conflicted case of Kaiser.

If knowledge is power and power corrupts, the construction can be refined further to ‘data corrupts’, is the suggestion.

The filmmakers linger long on Kaiser which can seem to humanize her — as they show what appear vulnerable or intimate moments. Yet they do this without ever entirely getting under her skin or allowing her role in the scandal to be fully resolved.

She’s often allowed to tell her narrative from behind dark glasses and a hat — which has the opposite effect on how we’re invited to perceive her. Questions about her motivations are never far away. It’s a human mystery linked to Cambridge Analytica’s money-minting algorithmic blackbox.

Nor is there any attempt by the filmmakers to mine Kaiser for answers themselves. It’s a documentary that spotlights mysteries and leaves questions hanging up there intact. From a journalist perspective that’s an inevitable frustration. Even as the story itself is much bigger than any one of its constituent parts.

It’s hard to imagine how Netflix could commission a straight up sequel to The Great Hack, given its central framing of Carroll’s data quest being combined with key moments of the Cambridge Analytica scandal. Large chunks of the film are comprised from capturing scrutiny and reactions to the story unfolding in real-time.

But in displaying the ruthlessly transactional underpinnings of social platforms where the world’s smartphone users go to kill time, unwittingly trading away their agency in the process, Netflix has really just begun to open up the defining story of our time.",Social Media,TechCrunch,https://techcrunch.com/2019/07/27/the-great-hack-tells-us-that-data-corrupts/,"The documentary The Great Hack highlights the consequences of unregulated surveillance capitalism, which risks individuals' and society's privacy, and is facilitated by a data-free-for-all enabled by the lack of legal privacy safeguards. It also reveals the staggering value of digital data, and how this attracts bright minds to data firms that are ultimately corrupted by their",Security & Privacy
309,Steve Bannon’s show pulled off Twitter and YouTube over calls for violence – TechCrunch,"Former presidential advisor and right-wing pundit Steve Bannon had his show suspended from Twitter and an episode removed by YouTube after calling for violence against FBI director Christopher Wray and the government’s leading pandemic expert, Dr. Anthony Fauci.

Bannon, speaking with co-host Jack Maxey, was discussing what Trump should do in a hypothetical second term. He suggested firing Wray and Fauci, but then went further, saying “I’d actually like to go back to the old times of Tudor England, I’d put the heads on pikes, right, I’d put them at the two corners of the White House as a warning to federal bureaucrats.”

This may strike one at first as mere hyperbole — one may say “we want his head on a platter” and not really be suggesting they actually behead anyone. But the conversation continued and seemed to be more in earnest than it first appeared:

Maxey: Just yesterday there was the anniversary of the hanging of two Tories in Philadelphia. These were Quaker businessmen who had cohabitated, if you will, with the British while they were occupying Philadelphia. These people were hung. This is what we used to do to traitors. Bannon: That’s how you won the revolution. No one wants to talk about it. The revolution wasn’t some sort of garden party, right? It was a civil war. It was a civil war.

Whether one considers this only nostalgia for the good old days of mob justice or an actual call to bring those days back, the exchange seems to have been enough for moderators at YouTube and Twitter to come down hard on the pair’s makeshift broadcast.

Twitter confirmed that it has “permanently suspended” (i.e. it can be appealed but won’t be restored automatically) the account for violating the rule against glorifying violence.

YouTube removed the episode from “Steve Bannon’s War Room” channel Wednesday afternoon after it was brought to their attention. A representative for the platform said “We’ve removed this video for violating our policy against inciting violence. We will continue to be vigilant as we enforce our policies in the post-election period.”

Online platforms have struggled with finding the line between under and over-moderation. Facebook, Twitter, YouTube, TikTok, Instagram and others have all taken different measures, from preemptively turning off features to silently banning hashtags. Facebook today took down a group with more than 300,000 members that was acting as an amplifier for misinformation about the election.

While the platforms have been vigorous in at least some ways in the labeling and isolation of misinformation, it’s more difficult for video platforms. Just minutes ago Trump took to YouTube to detail a variety of unfounded conspiracy theories about mail-in voting, but the platform can’t exactly do a live fact-check of the president and shut down his channel. More than with text-based networks, video tends to spread before it is caught and flagged due to the time it takes to review it.",Social Media,TechCrunch,https://techcrunch.com/2020/11/05/steve-bannons-show-pulled-off-twitter-and-youtube-over-calls-for-violence/,"Social media platforms have struggled to find a balance between under and over-moderation. This has led to a wide range of measures, from shutting off features to banning hashtags, but the most concerning issue is in video platforms, where misinformation can spread faster than it can be flagged and removed.","Information, Discourse & Governance"
310,Study: Russia-linked fake Twitter accounts sought to spread terrorist-related social division in the UK – TechCrunch,"A study by UK academics looking at how fake social media accounts were used to spread socially divisive messages in the wake of a spate of domestic terrorists attacks this year has warned that the problem of hostile interference in public debate is greater than previously thought.

The researchers, who are from Cardiff University’s Crime and Security Research Institute, go on to assert that the weaponizing of social media to exacerbate societal division requires “a more sophisticated ‘post-event prevent’ stream to counter-terrorism policy”.

“Terrorist attacks are designed as forms of communicative violence that send a message to ‘terrorise, polarise and mobilise’ different segments of the public audience. These kinds of public impacts are increasingly shaped by social media communications, reflecting the speed and scale with which such platforms can make information ‘travel’,” they write.

“Importantly, what happens in the aftermath of such events has been relatively neglected by research and policy-development.”

The researchers say they collected a dataset of ~30 million datapoints from various social media platforms. But in their report they zero in on Twitter, flagging systematic use of Russian linked sock-puppet accounts which amplified the public impacts of four terrorist attacks that took place in the UK this year — by spreading ‘framing and blaming’ messaging around the attacks at Westminster Bridge, Manchester Arena, London Bridge and Finsbury Park.

They highlight eight accounts — out of at least 47 they say they identified as used to influence and interfere with public debate following the attacks — that were “especially active”, and which posted at least 427 tweets across the four attacks that were retweeted in excess of 153,000 times. Though they only directly name three of them: @TEN_GOP (a right-wing, anti-Islam account); @Crystal1Jonson (a pro-civil rights account); and @SouthLoneStar (an anti-immigration account) — all of which have previously been shuttered by Twitter. (TechCrunch understands the full list of accounts the researchers identified as Russia-linked has not currently been shared with Twitter.)

Their analysis found that the controllers of the sock puppets were successful at getting information to ‘travel’ by building false accounts around personal identities, clear ideological standpoints and highly opinionated views, and by targeting their messaging at sympathetic ‘thought communities’ aligned with the views they were espousing, and also at celebrities and political figures with large follower bases in order to “‘boost’ their ‘signal’” — “The purpose being to try and stir and amplify the emotions of these groups and those who follow them, who are already ideologically ‘primed’ for such messages to resonate.”

The researchers say they derived the identities of the 47 Russian accounts from several open source information datasets — including releases via the US Congress investigations pertaining to the spread of disinformation around the 2016 US presidential election; and the Russian magazine РБК — although there’s no detailed explanation of their research methodology in their four-page policy brief. They claim to have also identified around 20 additional accounts which they say possess “similar ‘signature profiles’” to the known sock puppets — but which have not been publicly identified as linked to the Russian troll farm, the Internet Research Agency, or similar Russian-linked units. While they say a number of the accounts they linked to Russia were established “relatively recently”, others had been in existence for a longer period — with the first appearing to have been set up in 2011, and another cluster in the later part of 2014/early 2015. The “quality of mimicry” being used by those behind the false accounts makes them “sometimes very convincing and hard to differentiate from the ‘real’ thing”, they go on to assert, further noting: “This is an important aspect of the information dynamics overall, inasmuch as it is not just the spoof accounts pumping out divisive and ideologically freighted communications, they are also engaged in seeking to nudge the impacts and amplify the effects of more genuine messengers.” ‘Genuine messengers’ such as a Nigel Farage — aka one of the UK politicians directly cited in the report as having had messages addressed to him by the fake accounts in the hopes he would then apply Twitter’s retweet function to amplify the divisive messaging. (Farage was leader of UKIP, one of the political parties that campaigned for Brexit and against immigration.) Far right groups have also used the same technique to spread their own anti-immigration messaging via the medium of president Trump’s tweets — in one recent instance earning the president a rebuke from the UK’s Prime Minister, Theresa May. Last month May also publicly accused Russia of using social media to “weaponize information” and spread socially divisive fake news on social media, underscoring how the issue has shot to the top of the political agenda this year.

“The involvement of overseas agents in shaping the public impacts of terrorist attacks is more complex and troubling than the journalistic coverage of this story has implied,” the researchers write in their assessment of the topic.

They go on to claim there’s evidence for “interventions” involving a greater volume of fake accounts than has been documented thus far; spanning four of the UK terror attacks that took place earlier this year; that measures were targeted to influence opinions and actions simultaneously across multiple positions on the ideological spectrum; and that activities were not just being engaged by Russian units — but with European and North American right-wing groups also involved.

They note, for example, having found “multiple examples” of spoof accounts trying to “propagate and project very different interpretations of the same events” which were “consistent with their particular assumed identities” — citing how a photo of a Muslim woman walking past the scene of the Westminster bridge attack was appropriate by the fake accounts and used to drive views on either side of the political spectrum:

The use of these accounts as ‘sock puppets’ was perhaps one of the most intriguing aspects of the techniques of influence on display. This involved two of the spoof accounts commenting on the same elements of the terrorist attacks, during roughly the same points in time, adopting opposing standpoints. For example, there was an infamous image of a Muslim woman on Westminster Bridge walking past a victim being treated, apparently ignoring them. This became an internet meme propagated by multiple far-right groups and individuals, with about 7,000 variations of it according to our dataset. In response to which the far right aligned @Ten_GOP tweeted: She is being judged for her own actions & lack of sympathy. Would you just walk by? Or offer help? Whereas, @ Crystal1Johnson’s narrative was: so this is how a world with glasses of hate look like – poor woman, being judged only by her clothes.

The study authors do caveat that as independent researchers it is difficult for them to guarantee ‘beyond reasonable doubt’ that the accounts they identified were Russian-linked fakes — not least because they’ve been deleted (and the study is based off of analysis of digital traceries left from online interactions).",Social Media,TechCrunch,https://techcrunch.com/2017/12/18/study-russia-linked-fake-twitter-accounts-sought-to-spread-terrorist-related-social-division-in-the-uk/,"Social media have been weaponized to exacerbate societal division, with independent research revealing that Russian-linked sock-puppet accounts were used to spread divisive messages in the wake of domestic terrorist attacks this year. This highlights the need for a more sophisticated ""post-event prevent"" counter-terrorism policy.",Security & Privacy
311,"Instagram launches tools to filter out abusive DMs based on keywords and emojis, and to block people, even on new accounts – TechCrunch","Facebook and its family of apps have long grappled with the issue of how to better manage — and eradicate — bullying and other harassment on its platform, turning both to algorithms and humans in its efforts to tackle the problem better. In the latest development, today, Instagram is announcing some new tools of its own.

First, it’s introducing a new way for people to further shield themselves from harassment in their direct messages, specifically in message requests by way of a new set of words, phrases and emojis that might signal abusive content, which will also include common misspellings of those key terms, sometimes used to try to evade the filters. Second, it’s giving users the ability to proactively block people even if they try to contact the user in question over a new account.

The blocking account feature is going live globally in the next few weeks, Instagram said, and it confirmed to me that the feature to filter out abusive DMs will start rolling out in the U.K., France, Germany, Ireland, Canada, Australia and New Zealand in a few weeks’ time before becoming available in more countries over the next few months.

Notably, these features are only being rolled out on Instagram — not Messenger, and not WhatsApp, Facebook’s other two hugely popular apps that enable direct messaging. The spokesperson confirmed that Facebook hopes to bring it to Messenger later this year (no word on WhatsApp). Instagram and others have regularly issued updates on single apps before considering how to roll them out more widely.

Instagram said that the feature to filter DMs for abusive content — not scan, the company was clear to point out — will be based on a list of words and emojis that Facebook compiles with the help of anti-discrimination and anti-bullying organizations (it did not specify which), along with terms and emoji’s that you might add in yourself. And to be clear, it has to be turned on proactively, rather than being made available by default.

Why? More user license, it seems, and to keep conversations private if users want them to be. “We want to respect peoples’ privacy and give people control over their experiences in a way that works best for them,” a spokesperson said, pointing out that this is similar to how its comment filters also work. It will live in Settings>Privacy>Hidden Words for those who want to turn on the control.

There are a number of third-party services out there in the wild now building content moderation tools that sniff out harassment and hate speech — they include the likes of Sentropy and Hive — but what has been interesting is that the larger technology companies up to now have opted to build these tools themselves. That is also the case here, the company confirmed.

The system is completely automated, although Facebook noted that it reviews any content that gets reported. While it doesn’t keep data from those interactions, it confirmed that it will be using reported words to continue building its bigger database of terms that will trigger content getting blocked, and subsequently deleting, blocking and reporting the people who are sending it.

On the subject of those people, it’s been a long time coming that Facebook has started to get smarter on how it handles the fact that the people with really ill intent have wasted no time in building multiple accounts to pick up the slack when their primary profiles get blocked. People have been aggravated by this loophole for as long as DMs have been around, even though Facebook’s harassment policies had already prohibited people from repeatedly contacting someone who doesn’t want to hear from them, and the company had already also prohibited recidivism, which as Facebook describes it, means “if someone’s account is disabled for breaking our rules, we would remove any new accounts they create whenever we become aware of it.”

The company’s approach to direct messages has been something of a template for how other social media companies have built these out.

In essence, they are open-ended by default, with one inbox reserved for actual contacts, but a second one for anyone at all to contact you. While some people just ignore that second box altogether, the nature of how Instagram works and is built is for more, not less, contact with others, and that means people will use those second inboxes for their DMs more than they might, for example, delve into their spam inboxes in email.

The bigger issue continues to be a game of whack-a-mole, however, and one that not just its users are asking for more help to solve. As Facebook continues to find itself under the scrutinizing eye of regulators, harassment — and better management of it — has emerged as a very key area that it will be required to solve before others do the solving for it.",Social Media,TechCrunch,https://techcrunch.com/2021/04/21/instagram-launches-tools-to-filter-out-abusive-dms-based-on-keywords-and-emojis-and-to-block-people-even-on-new-accounts/,"Social media platforms like Facebook, Instagram and WhatsApp have long struggled to manage bullying and other forms of harassment on their platforms, prompting the introduction of new tools to filter out abusive content and give users more control over their experiences. Despite these efforts, the issue of harassment remains a major problem, with regulators increasingly demanding further steps be taken to effectively tackle",Social Norms & Relationships
312,Cambridge Analytica’s Nix recalled by fake news probe – TechCrunch,"Stock up on the popcorn — the currently suspended CEO of the firm at the center of a data handling and political ad-targeting storm currently embroiling Facebook, Cambridge Analytica, has been recalled by a UK parliamentary committee that’s running a probe into the impact of fake news because it’s unhappy with the quality of his prior answers.

The committee also says it has fresh questions for Alexander Nix in light of revelations that hit the headlines at the weekend about how a researcher’s app was used to gather personal information on about 270,000 Facebookers and 50 million of their friends, back in 2015 — data that was passed to CA in violation of Facebook’s policies.

Nix gave evidence to the DCMS committee on February 27, when he claimed: “We do not work with Facebook data, and we do not have Facebook data. We do use Facebook as a platform to advertise, as do all brands and most agencies, or all agencies, I should say. We use Facebook as a means to gather data. We roll out surveys on Facebook that the public can engage with if they elect to.”

That line is one of the claims the committee says it’s keen to press him on now. In a letter to Nix, it writes: “[T]here are a number of inconsistencies in your evidence to us of 27 February, notably your denial that your company received data from the Global Science Research company [aka the firm behind the survey app used by CA to harvest data on 50M Facebook users, according to The Observer].”

“We are also interested in asking you again about your claim that you “do not work with Facebook data, and […] do not have Facebook data,” it continues, warning: “Giving false statements to a Select Committee is a very serious matter.”

The self-styled ‘not a political consultancy’ but “technology-driven marketing firm” (and sometime “campaign consultancy and communication services” company) — which Nix also described in his last evidence session as “not a data miner… a data analytics company” — had its Facebook account suspended late last week for violating Facebook’s platform policies.

While Nix was suspended as CEO by CA’s board on Tuesday, following a Channel 4 News report aired a series of secret recordings that appeared to show Nix advocating the use of proxy organisations to feed untraceable messages onto social media to influence voters. The recordings were made by an undercover reporter posing as a potential client.

The UK’s data protection watchdog, the ICO, has also applied for a warrant to gain access to CA’s offices and servers — accusing the company of failing to hand over information the regulator had requested as part of a wider investigation it’s carrying out into the use of data analytics for political purposes.

CA is also now facing several legal challenges from Facebook users angry about how their data appears to have been misused.

We reached out to the company for comment on the DCMS recall. At the time of writing it had not responded.

Below are a few choice segments from Nix’s last evidence session in from of the committee — which we expect he will be asked to revisit should he agree to make a repeat appearance…

The committee has also asked Mark Zuckerberg to give evidence in person — albeit, don’t bet the farm on that coming to pass. CA ‘data whistleblower’ Chris Wylie has agreed to testify and is due to given evidence on March 27.

Q698 Rebecca Pow:… Could you expand a bit more on what those surveys are, what you are asking people and how you are gathering the data? Do you keep that data on surveys carried out on Facebook or does Facebook keep it? Alexander Nix: I cannot speak to Facebook, but as far as I am aware the process works a bit like an opinion survey. If I want to find out how many people prefer red cars or yellow cars, I can post that question on Facebook and people can agree. They can opt in to answer a survey and they give their consent and they say, “I prefer a yellow car” and then we can collect that data. That is no different to running a telephone poll or a digital poll or a mail poll or any other form of poll. It is just a platform that allows you to engage with communities. Q699 Rebecca Pow: Are they a big part of your data-gathering service? Alexander Nix: When we work for brands, whether it is in the UK or in the US or elsewhere, we often feel the need to probe their customers and find out what they think about particular products or services. We might use Facebook as a means to engage with the general public to gather this data. Q700 Simon Hart: Let me ask a very quick question on the Facebook survey opt-in option that you were describing. If you are asking somebody what kind of car they prefer and they opt in, does that facilitate access to other data that may be held by Facebook, which is irrelevant to car colour, or is it only the data you collect on car colour that is relevant?Nothing else that is part of the data held by Facebook would be available to you. Alexander Nix: You are absolutely right—no other data. As far as I am aware, Facebook does not share any of its data. It is what is known as a walled garden, which keep its data— Q701 Simon Hart: People are not in any way accidently giving you consent to access data other than that that you specifically asked for. Alexander Nix: That is correct. People are not giving us consent and Facebook does not have a mechanism that allows third parties such as us to access its data on its customers. Q702 Simon Hart: Even with its customers’ consent. Alexander Nix: Even with its customers’ consent. Chair: You said in your letter to me that, “Cambridge Analytica does not gather” data from Facebook. Alexander Nix: From Facebook? Chair: Yes. Alexander Nix: That is correct.

*

Q718 Chair: The actual quote from the letter is: “On 8 February 2018 Mr Matheson implied that Cambridge Analytica ‘gathers data from users on Facebook.’ Cambridge Analytica does not gather such data.” But from what you said you do, do you not, through the surveys? Alexander Nix: Yes, I think I can see what has happened here. What we were trying to say in our letter is that we do not gather Facebook data from Facebook users. We can use Facebook as an instrument to go out and run large-scale surveys of the users, but we do not gather Facebook data. Q719 Chair: By that do you mean that you do not have access to data that is owned by Facebook? Alexander Nix: Exactly. Q720 Chair: You acquire data from Facebook users through them engaging with surveys and other things. Alexander Nix: Exactly right. Q721 Chair: Is your engagement, either directly or through any associate companies you may have, just through the placing of surveys or are there other tools or games or thingsthat are on Facebook that you use to gather data from Facebook users? Alexander Nix: No, simply through surveys.

*",Social Media,TechCrunch,https://techcrunch.com/2018/03/22/cambridge-analyticas-nix-recalled-by-fake-news-probe/,"The CEO of Cambridge Analytica, the firm at the center of a data handling and political ad-targeting storm currently embroiling Facebook, has been recalled by a UK parliamentary committee due to inconsistencies in his prior evidence. The committee is probing the impact of fake news and investigating the company's alleged misuse of personal data on about 270,",Security & Privacy
313,Getting Banned From The App Store Was The Best Thing That Happened To Us – TechCrunch,"Getting Banned From The App Store Was The Best Thing That Happened To Us

“Brace yourself, Marco…we’ve just been removed from the App Store.” With those words, I visualized everything we’d slaved over for months crumple itself into a ball of trash and — ironically — flinging itself into the bin beside me. But it hardly came as a surprise.

Despite stellar traction at over 4 million users and 15 million flings opened per day after launching less than a year ago, Fling had morphed into something entirely different from what I initially conceived.

Fling’s Conception

It was October 2013, and I was on a plane from Hong Kong to London. It’s a 13-hour journey, so I had plenty of time to kill. But instead of tuning out to in-flight movies, I found myself oddly drawn to watching the plane’s flight path.

It was the same dull, slow-moving animation I’d seen countless times before, but this time was different. I’d spent a lot of time thinking about what the next big app in social messaging was going to be, and as I flipped through British Airway’s in-flight magazine that showed its hundreds of routes around the world, a vision started to crystallize.

“I need to make an emergency call,” I said.

There was apprehension, and possibly a faked medical emergency involved, but finally I managed to reach our COO.

“Emerson, I’ve got an idea, and it’s either gonna be worth zero or a billion.”

Despite the skepticism, I got to work anyway, pulling up Photoshop and completing Fling’s designs by the end of the flight.

The vision was clear: Fling was going to be a platform that allowed you to send any real-time message to 50 random strangers in the world. We built the app in a matter of weeks, and within a month we had nearly half a million downloads and incredibly active users. They were sharing snippets of their lives all over the globe, from America to Zambia.

Fling’s vision was coming to life without any of the roadblocks I’d expected. It seemed too good to be true…and it was.

Fling’s Content Minefield

As more and more users began flocking to Fling, we were fortunate enough to receive increasing PR and coverage from various publications. Unfortunately, this didn’t come without its drawbacks.

Vanity metrics can be toxic if you’re not careful with them.

As with any platform that allows any degree of anonymity, we had to deal with another very different type of PR — something we jokingly (kind of, but not really) dubbed ”the penis rate.” Hopefully that illuminates the problem sufficiently and I don’t need to go into more detail.

Unless you’re working on something that’s prone to this issue, I don’t think it’s possible to appreciate the frustration. It was like a perverse game of Whack-A-Mole — we would ban users and delete inappropriate flings, only to have them reappear more creatively named and obscenely photographed than ever.

We eventually had a full-time team devoted to keeping dicks at bay. But despite our efforts, we couldn’t decrease the rate of reported flings to a number lower than 10 percent.

Knowing we had to fundamentally change the app in order to fix the problem, we began devising the next iteration. Fling 2.0 would be the most viral platform in existence for discovering authentic, unfiltered content.

But making this move wasn’t easy. The sexual nature of Fling had become the fuel of our vanity metrics, and with billions of flings delivered per month it was difficult to bring ourselves to tamper with this incredible traction.

One of the most humbling learnings I’ve ever faced as a CEO is that vanity metrics are as addictive — and dangerous — as any hard drug. We knew we had to change the culture of our user base; we even completed the designs of Fling 2.0. But like any self-described addict will tell you, change isn’t easy. We needed an intervention.

Why We Were Removed From The App Store

To be clear, Apple officially removed us because they implemented a policy against apps with randomized messaging. But there’s no doubt in my mind that the reason they pulled the trigger was because much of Fling had become a playground for men to harass women. Take a look at these two graphs:

The more flings that women sent on their first day, the more unlikely they were to come back. Compare this to the analogous graph for men, which is quite normal:

To give you some context about the ridiculousness: Imagine a restaurant where those who eat the most food are the least likely to return.

Sadly, we knew about this gender dichotomy earlier, but we didn’t move fast enough. Apple did the right thing by removing us, and was kind enough to work closely with us around the clock in order to get back in; I’ll always be grateful for that.

How We Handled The Removal

In the short term, we had to relaunch as quickly as possible. This meant identifying the absolute minimum set of features required for relaunch. We expected user backlash, but also considered it an opportunity to rid the app of inappropriate users. We whittled the required core features down to two: following users and sharing flings (reflinging).

Startups are an art, not a science.

The next priority was removing all roadblocks until relaunch. This meant things like ensuring someone from each team was always on-call so communication wouldn’t be bottlenecked. As far as my role went, it meant things like booking Airbnb stays and late-night Uber rides to reduce travel hassle. It also meant smaller things, like volunteering for coffee runs when I had downtime.

As it turns out, there’s nothing that will motivate and mobilize a team like fighting to get your app approved again, and the vibe in the room was not unlike going to war with the clock. Fling 2.0 was ready for release within a fortnight.

The longer-term solution required a bit more introspection. Truth be told, our surging vanity metrics made everyone, including (especially) me, a bit complacent. But even with billions of flings sent per month, something seemed amiss.

That “something” was that our original vision had been lost along the way. We created Fling to create connections powerful enough to disrupt one’s social graph. How many of these connections could truly be meaningful when the norm was to troll for sexual conversation?

We knew we had to ensure two things:

Never run into a content issue again.

Make sure that the metrics we tracked were aligned with success.

We tightened our rules around moderation and tweaked our design anywhere users had the opportunity to create inappropriate content. For example, we de-emphasized the usernames of strangers, which sometimes contained terms that slipped through our filters, and instead focused on their country of origin. We also started tracking report incidents and implemented tools like Periscope to display these rates company-wide.

But this was also a wake-up call to improve good content, and we put a content team in place to surface the best flings. We also designed a quality score based on characteristics of amazing flings and tracked their related metrics. One of them happened to be average fling engagement from female users.

Content quality changed almost overnight. Selfies were quickly replaced by glimpses into people’s lives that would normally go unseen. The best flings started traveling at an incredible speed in the form of reflings, which today make up 50 percent of all content (for comparison, retweets made up only 2 percent of tweets in Twitter’s early days).

Lessons Learned

Change is difficult. Sometimes it requires a cataclysmic event. Getting banned from the app store was the scare that we needed, and it could have been much worse. As CEO, I never should have let it get to that point, but I’m glad things turned out the way they did.

Here’s what I learned:

Vanity metrics can be toxic if you’re not careful with them. They don’t necessarily reflect the health of your product or company. If you only focus on vanity metrics, you’ll hit a local maxima sooner or later, and it’s worth sacrificing them to raise your app’s true ceiling.

Startups are an art, not a science. Metrics (and quantitative data in general) should be taken with context. You need to continually assess the qualitative side, as well. For example, vanity metrics state we have lost some users, but that needed to happen. Imagine what would have happened if we simply added new features without purging original harassers. (“Great, I’ll refling and follow a bunch of people…now back to harassing people over chat.”)

Everyone at your company cares even if they don’t tell you something like this happens. The first thing that our head of design told me was, “I love what we do here; I love this place, and I love my job…I’m willing to do whatever it takes to keep it, so let’s get to work.” If you’re a startup, it’s unlikely people would be there unless they’re passionate.

Your product needs to be aligned with your vision at all times. If there’s a moment where you find it’s not, then move quickly to fix it. Don’t wait until it’s out of your control.

I wish I could offer up a Cinderella ending here. It would be nice to end this by saying that Fling has twenty million active users and we are now one of the world’s most successful startups.

We’re not there yet, but I know 100 percent that we will be. The same gut that first told me that we needed to change now says we’re on the right path, as long as we stay true to what we’ve learned. And this time, I am listening to it.",Social Media,TechCrunch,https://techcrunch.com/2015/08/30/getting-banned-from-the-app-store-was-the-best-thing-that-happened-to-us/,"Getting banned from the App Store was the best thing that happened to Fling, as it forced them to address the issue of inappropriate content and harassment, which had become a problem due to their surging vanity metrics. Fling was able to relaunch with a focus on discovering authentic, unfiltered content and tracking metrics that aligned with success",Social Norms & Relationships
314,UK parliament seizes cache of internal Facebook documents to further privacy probe – TechCrunch,"Facebook founder Mark Zuckerberg may yet regret underestimating a UK parliamentary committee that’s been investigating the democracy-denting impact of online disinformation for the best part of this year — and whose repeat requests for facetime he’s just as repeatedly snubbed.

In the latest high gear change, reported in yesterday’s Observer, the committee has used parliamentary powers to seize a cache of documents pertaining to a US lawsuit to further its attempt to hold Facebook to account for misuse of user data.

Facebook’s oversight — or rather lack of it — where user data is concerned has been a major focus for the committee, as its enquiry into disinformation and data misuse has unfolded and scaled over the course of this year, ballooning in scope and visibility since the Cambridge Analytica story blew up into a global scandal this April.

The internal documents now in the committee’s possession are alleged to contain significant revelations about decisions made by Facebook senior management vis-a-vis data and privacy controls — including confidential emails between senior executives and correspondence with Zuckerberg himself.

This has been a key line of enquiry for parliamentarians. And an equally frustrating one — with committee members accusing Facebook of being deliberately misleading and concealing key details from it.

The seized files pertain to a US lawsuit that predates mainstream publicity around political misuse of Facebook data, with the suit filed in 2015, by a US startup called Six4Three, after Facebook removed developer access to friend data. (As we’ve previously reported Facebook was actually being warned about data risks related to its app permissions as far back as 2011 — yet it didn’t full shut down the friends data API until May 2015.)

The core complaint is an allegation that Facebook enticed developers to create apps for its platform by implying they would get long-term access to user data in return. So by later cutting data access the claim is that Facebook was effectively defrauding developers.

Since lodging the complaint, the plaintiffs have seized on the Cambridge Analytica saga to try to bolster their case.

And in a legal motion filed in May Six4Three’s lawyers claimed evidence they had uncovered demonstrated that “the Cambridge Analytica scandal was not the result of mere negligence on Facebook’s part but was rather the direct consequence of the malicious and fraudulent scheme Zuckerberg designed in 2012 to cover up his failure to anticipate the world’s transition to smartphones”.

The startup used legal powers to obtain the cache of documents — which remain under seal on order of a California court. But the UK parliament used its own powers to swoop in and seize the files from the founder of Six4Three during a business trip to London when he came under the jurisdiction of UK law, compelling him to hand them over.

According to the Observer, parliament sent a serjeant at arms to the founder’s hotel — giving him a final warning and a two-hour deadline to comply with its order.

“When the software firm founder failed to do so, it’s understood he was escorted to parliament. He was told he risked fines and even imprisonment if he didn’t hand over the documents,” it adds, apparently revealing how Facebook lost control over some more data (albeit, its own this time).

In comments to the newspaper yesterday, DCMS committee chair Damian Collins said: “We are in uncharted territory. This is an unprecedented move but it’s an unprecedented situation. We’ve failed to get answers from Facebook and we believe the documents contain information of very high public interest.”

Collins later tweeted the Observer’s report on the seizure, teasing “more next week” — likely a reference to the grand committee hearing in parliament already scheduled for November 27.

But it could also be a hint the committee intends to reveal and/or make use of information locked up in the documents, as it puts questions to Facebook’s VP of policy solutions…

More next week – Parliament seizes cache of Facebook internal papers https://t.co/129OkaGe7k — Damian Collins (@DamianCollins) November 25, 2018

That said, the documents are subject to the Californian superior court’s seal order, so — as the Observer points out — cannot be shared or made public without risk of being found in contempt of court.

A spokesperson for Facebook made the same point, telling the newspaper: “The materials obtained by the DCMS committee are subject to a protective order of the San Mateo Superior Court restricting their disclosure. We have asked the DCMS committee to refrain from reviewing them and to return them to counsel or to Facebook. We have no further comment.”

Although Collins later tweeted again, this time emphasizing: “Under UK law and parliamentary privilege we can publish papers if we choose to as part of our inquiry.”

The @CommonsCMS has received the documents it ordered from Six4Three relating to Facebook. I have reviewed them and the committee will discuss how we will proceed early next week. Under UK law & parliamentary privilege we can publish papers if we choose to as part of our inquiry — Damian Collins (@DamianCollins) November 25, 2018

Facebook’s spokesperson added that Six4Three’s “claims have no merit”, further asserting: “We will continue to defend ourselves vigorously.”

Earlier on Sunday, Facebook also sent a response to Collins, which Guardian reporter Carole Cadwalladr posted soon after.

NEW: Facebook responds to UK parliament's seizure of internal docs. It is getting its lines of attack out there. This is copy of its letter to @DamianCollins that it has just sent me… pic.twitter.com/lfSeoM1j2l — Carole Cadwalladr (@carolecadwalla) November 25, 2018

With the response, Facebook seems to be using the same tactics which were responsible for the latest round of criticism against the company — deny, delay, and dissemble.

And, well, the irony of Facebook asking for its data to remain private also shouldn’t be lost on anyone at this point…

Collins later wrote back to Allan, publishing a copy of his letter on social media:

I have written back to Richard Allan at Facebook following their email to me today regarding the documents ordered by @CommonsCMS from Six4Three. You can read a copy of it here pic.twitter.com/lXWS2gOPBM — Damian Collins (@DamianCollins) November 25, 2018

Another irony: In July, the Guardian reported that as part of Facebook’s defence against Six4Three’s suit the company had argued in court that it is a publisher — seeking to have what it couched as ‘editorial decisions’ about data access protected by the US’ first amendment.

Which is — to put it mildly — quite the contradiction, given Facebook’s long-standing public characterization of its business as just a distribution platform, never a media company.

So expect plenty of fireworks at next week’s public hearing as parliamentarians once again question Facebook over its various contradictory claims.

It’s also possible the committee will have been sent an internal email distribution list by then, detailing who at Facebook knew about the Cambridge Analytica breach in the earliest instance.

This list was obtained by the UK’s data watchdog, over the course of its own investigation into the data misuse saga. And earlier this month information commissioner Elizabeth Denham confirmed the ICO has the list and said it would pass it to the committee.

The accountability net does look to be closing in on Facebook management.

Even as Facebook continues to deny international parliaments any face-time with its founder and CEO (the EU parliament remains the sole exception).

Last week the company refused to even have Zuckerberg do a video call to take the committee’s questions — offering its VP of policy solutions, Richard Allan, to go before what’s now a grand committee comprised of representatives from seven international parliaments instead.

The grand committee hearing will take place in London on Tuesday morning, British time — followed by a press conference in which parliamentarians representing Facebook users from across the world will sign a set of ‘International Principles for the Law Governing the Internet’, making “a declaration on future action”.

So it’s also ‘watch this space’ where international social media regulation is concerned.

As noted above, Allan is just the latest stand-in for Zuckerberg. Back in April the DCMS committee spend the best part of five hours trying to extract answers from Facebook CTO, Mike Schroepfer.

“You are doing your best but the buck doesn’t stop with you does it? Where does the buck stop?” one committee member asked him then.

“It stops with Mark,” replied Schroepfer.

But Zuckerberg definitely won’t be stopping by on Tuesday.

This report was updated with additional reactions from Facebook and Collins",Social Media,TechCrunch,https://techcrunch.com/2018/11/25/uk-parliament-seizes-cache-of-internal-facebook-documents-to-further-privacy-probe/,"The UK Parliament has seized documents allegedly containing significant revelations about decisions made by Facebook senior management vis-a-vis data and privacy controls, in a move to hold Facebook accountable for misuse of user data.",Security & Privacy
315,President Obama Is Waging a War on Hackers,"In next week's State of the Union address, President Obama will propose new laws against hacking that could make either retweeting or clicking on the above (fictional) link illegal. The new laws make it a felony to intentionally access unauthorized information even if it's been posted to a public website. The new laws make it a felony to traffic in information like passwords, where ""trafficking"" includes posting a link.

You might assume that things would never become that bad, but it’s already happening even with the current laws. Prosecutors went after Andrew “weev” Auernheimer for downloading a customer list AT&T negligently made public. They prosecuted Barrett Brown for copying a URL to the Stratfor hack from one chatroom to another. A single click is all it takes. Prosecutors went after the PayPal-14 for clicking on a single link they knew would flood PayPal’s site with traffic. The proposed changes make such prosecutions much easier.

Robert Graham About Robert Graham is an inventor of cybersecurity technologies. He is CEO of Errata Security. This article first appeared on Errata's blog.

Even if you don’t do any of this, you can still be guilty if you hang around with people who do. Obama proposes upgrading hacking to a “racketeering” offense, means you can be guilty of being a hacker by simply acting like a hacker (without otherwise committing a specific crime). Hanging out in an IRC chat room giving advice to people now makes you a member of a “criminal enterprise”, allowing the FBI to sweep in and confiscate all your assets without charging you with a crime. If you innocently clicked on the link above, and think you can defend yourself in court, prosecutors can still use the 20-year sentence of a racketeering charge in order to force you to plea bargain down to a 1-year sentence for hacking. (Civil libertarians hate the police-state nature of racketeering laws).

Obama’s proposals come from a feeling in Washington D.C. that more needs to be done about hacking in response to massive data breaches of the last couple years. But they are blunt political solutions which reflect no technical understanding of the problem.

Most hacking is international and anonymous. They can’t catch the perpetrators no matter how much they criminalize the activities. This War on Hackers is likely to be no more effective than the War on Drugs, where after three decades the prison population has sky rocketed from 0.1% of the population to a staggering 1%. With 5% the world’s population, we have 25 percent of the world’s prisoners—and this has done nothing to stop drugs. Likewise, while Obama’s new laws will dramatically increase hacking prosecutions, they’ll be of largely innocent people rather than the real hackers that matter.",Social Media,WIRED,https://www.wired.com/2015/01/president-obama-waging-war-hackers/,"The new laws proposed by President Obama against hacking could have a drastic effect on social media users, potentially making a harmless action like clicking a link a felony offense and allowing prosecutors to target people for participating in online chatrooms with lengthy sentences.",Security & Privacy
316,Social Media Is Reshaping Sex Work—But Also Threatening It,"If you’re an aspiring high-end escort like Estelle Lucas, Instagram is the only place to connect with the most coveted of clients: wealthy men of Silicon Valley and Wall Street. Lucas, who bills herself as “the premier escort in Melbourne,” has a few thousand followers and a quirky vibe. She frequently posts ribald memes alongside her lingerie shots—“Come for the boobs, stay for the cat videos,” says her bio—most with her face either turned away from the camera or blurred out.

“I'm actually surprised my account hasn't been taken down,” Lucas says. “Every now and then I log in and I have a notification that my account has breached Community Guidelines and a photo has been removed, but I have absolutely no idea which photo it is or what the breach was. Perhaps a slither of a nipple? Shield your eyes!”

'Every now and then I log in and I have a notification that my account has breached Community Guidelines and a photo has been removed, but I have absolutely no idea which photo it is or what the breach was.' -Estelle Lucas, escort

Not that Instagram is the promised land. According to Sage, the platform’s “pretty explicit antagonism” toward sex workers means that an account can be suspended at any time. “You have to treat your Instagram account like one of those burner phones,” he says. “A lot of people always have a few backup accounts with some content posted. On the other hand, you go where the audience is.”

The needle-threading at issue here is that marketing yourself necessitates not only posting photos of yourself to attract new clients and keep old ones engaged, but also cracking jokes, connecting with and amplifying other sex workers, and developing relationships with lingerie makers and photographers—all without explicitly acknowledging that you’re a sex worker, which (along with photos of genitalia) is almost sure to get you a suspension at the very least. Many workers use their profiles to link back to more explicit personal websites, but that’s a risky move as well, even if your business is legal in your home country. “I think they’re deleting us for who we are and not what we post,” Kush says.

Safety in Social

But if social media platforms like Instagram are so hostile, why do sex workers like Lucas, Sage, and Kush do business there? Partly to find the largest audience, and partially because it beats the traditional method: meeting clients IRL. Sociologist Angela Jones has identified five “affordances” the internet gives sex workers—and chief among them is “reduced risk of bodily harm.” Sex workers often face extreme bodily risk, and the internet provides life-saving distance.

“When I'm on the internet, there's a lot of space and room to think and I don't feel pressured to make a decision before I'm ready,” Lucas says. It also allows her to keep clients in their place: “Sometimes my clients talk to me via social media but I keep it courteous and curt, otherwise I feel like it's free labour on my part.” (WIRED reached out to numerous people who had commented on sex workers’ Instagram accounts, but received either silence or denials in return.)

Still, social media comes with its own set of dangers—all of which force sex workers to become ever more tech-savvy. There are the obvious ones like doxxing and dealing with undercover law enforcement. But because sex workers aren’t exactly the users social media companies have in mind, unexpected tech updates can inadvertently put them at risk. Snapchat’s Snap Map outed the locations of many a sex worker, a far darker version of the drama it created for relationship infidelities or teenagers sneaking out at night.

And then there are other privacy concerns. Kush was forced to shut down her third Instagram model account because, despite having all privacy settings cranked up to the maximum, Facebook kept linking it to her personal Facebook account—which uses her real name and is what she calls “my last personal saving grace.” One day after she woke up, she fired up Instagram and saw that the app had suggested her own mother as someone to follow. “I disabled that account with one click,” she says. (After combing through security forums, Kush fixed the problem by deleting the Facebook app from her phone and only using it on browsers.)

The Ever-Changing Rules

So what happens now? Prostitution, as the adage goes, is the oldest profession in the world. It’s certainly not going anywhere, and it’s unlikely to leave the internet unless people suddenly stop living their lives more online than off. Hunting down individual accounts makes little sense, especially since sex workers are such a savvy bunch: According to Sage, they gravitated to livestreamed content about five years before Instagram stories and Facebook Lives became the norm.

Their security measures and algorithm manipulations are equally sophisticated—and if you ask them, that’s because they helped build the platforms that are now so keen to jettison them. “They’re trying to change the rules. Twitter allows nudity, but I didn’t add a ‘sensitive content’ filter, they did,” Kush says. “Patreon is another one. They used sex workers to create their following and now we’re no longer to post pornographic content. Vine allowed nudity at first, but no longer after it became a really popular platform.”",Social Media,WIRED,https://www.wired.com/story/sex-work-social-media/,"Social media platforms pose a unique set of risks for sex workers, from doxxing and undercover law enforcement to unexpected tech updates that can inadvertently put them at risk. Despite their sophisticated security measures and algorithm manipulations, sex workers are still subject to the whims of ever-changing rules, leaving them to navigate the platforms with trepidation.",Security & Privacy
317,I Spent a Week Using Only the Alt-Right's Vision of the Internet,"Scroll with me here. Somebody named BeatlesBaby makes “a very badass chicken curry.” Look, there’s a nice sepia-tinted pencil drawing of Ned Stark from Game of Thrones. Apparently, “Walking is the new smoking #Health #Fitness,” and some guy’s wife loves her treadmill desk. Read this: A Marine gives his beloved bomb-sniffing dog a hero’s farewell.

You could find these posts anywhere, on Facebook or Instagram or some homey subreddit. But that’s not how they ended up on my screen. I saw them on Gab, a Twitter-­like social media platform catering to the so-called alt-right, the web-incubated white-­nationalist movement that shot to prominence during the last election and made international headlines for its violent rally in Charlottesville, Virginia, earlier this year. I was on Gab because, not long ago, I spent a week of my online life exclusively in the alt-right’s domain, a network of copycat sites collectively known as Alt-Tech.

On Gab, when people aren’t chatting about exercise equipment, they swap jokes, revel in the camaraderie of the expanding #GabFam, and complain about the “normies” on other social media sites. I spot Alex Jones, host of the far-right radio show Infowars, using his (active, verified) account to sell merchandise and plug his website. Media commentator Mike Cernovich is there too, pushing his YouTube channel, Medium articles, and T-shirts. Gabbers love promotion. If they’re not elevating themselves, they’re supporting the cause, buying stuff and sending followers to Fox News and Breitbart—but also to places I don’t recognize, like Voat and Infogalactic and WeSearchr.

I click. Voat is Reddit with different fonts—and is in fact home to many communities banned from Reddit. Clicking “Random” brings me to pages like /v/manspreading, then /v/MasculinePhilosophy (“A space to discuss the nature of masculinity and the condition of both contemporary men and men throughout history”), and then /v/SwedenYes (“A place to discuss all the problems of multiculturalism in Sweden and Europe in general”). On Infogalactic, the alt-right’s corrective to Wikipedia, I read that the extremist movement has been “widely adopted by mainstream conservative and center-right parties in the USA and Europe”; the entry for the widely debunked Pizzagate conspiracy theory calls it a “crowdsourced investigation.” And WeSearchr? Users there ask for money (known as “posting bounty”) to track down anti-Trump protesters. The site also operates as a sort of GoFundMe. Thousands of users contributed more than $150,000 to help neo-Nazi news site The Daily Stormer with its many legal bills.

Since many of these people have been excommunicated from mainstream websites, Alt-Tech serves as something like a safe space, where they can just “be themselves.” The smirking cartoon Pepe the Frog is, of course, everywhere—in usernames, posts, even lurking in Gab’s logo. Sometimes he’s advocating for a balanced diet; other times he’s part of a goose-stepping army headed off to fight in what the alt-right has dubbed the “meme war.” Are memes weapons? In Alt-Tech, very much so. “Memes,” writes self-described white nationalist Christopher Cantwell, “are just a gateway drug to the alt-right.” (In late August, Cantwell surrendered to police to face felony charges of attacking counterprotesters in Charlottesville.)

Last September, the Anti-­Defamation League declared Pepe the Frog a hate symbol, and Alt-Tech had a conniption. Posters responded with furious, red-eyed, screaming Pepes and “DON'T TREAD ON MEME” flags, complete with Pepe-headed snakes. One user said he was “just another apathetic individual” until he heard Hillary Clinton badmouth Pepe. “Now all I want to do is laugh at the #LeftyLunacy,” he wrote. That’s what Cant­well means by gateway drug. Memes focus people’s attention. When I mention to Alice Marwick, a social media scholar at UNC Chapel Hill, that extremists are using memes for recruitment, she suggests that the technique “is not that different from Islamic radicalization.” People come for the edgy aesthetic; some stay for the ideology.",Social Media,WIRED,https://www.wired.com/story/alt-tech-social-media/,"Social Media is being used by the far-right for recruitment, often through the use of memes. Through sites such as Gab, Voat, Infogalactic and WeSearchr, extremists are able to spread their message and have been compared to radical Islamic recruitment tactics. This has the potential to radicalize people who may have initially been",Discourse & Governance
318,"For Trump and Facebook, judgment day is around the corner – TechCrunch","For Trump and Facebook, judgment day is around the corner Facebook's experimental Oversight Board is on the cusp of its biggest decision

Facebook unceremoniously confiscated Trump’s biggest social media megaphone months ago, but the former president might be poised to snatch it back.

Facebook’s Oversight Board, an external Supreme Court-like policy decision making group, will either restore Trump’s Facebook privileges or banish him forever on Wednesday. Whatever happens, it’s a huge moment for Facebook’s nascent experiment in outsourcing hard content moderation calls to an elite group of global thinkers, academics and political figures and allowing them to set precedents that could shape the world’s biggest social networks for years to come.

Facebook CEO Mark Zuckerberg announced Trump’s suspension from Facebook in the immediate aftermath of the Capitol attack. It was initially a temporary suspension, but two weeks later Facebook said that the decision would be sent to the Oversight Board. “We believe the risks of allowing the President to continue to use our service during this period are simply too great,” Facebook CEO Mark Zuckerberg wrote in January.

Facebook’s VP of Global Affairs Nick Clegg, a former British politician, expressed hope that the board would back the company’s own conclusions, calling Trump’s suspension an “unprecedented set of events which called for unprecedented action.”

Trump inflamed tensions and incited violence on January 6, but that incident wasn’t without precedent. In the aftermath of the murder of George Floyd, an unarmed Black man killed by Minneapolis police, President Trump ominously declared on social media “when the looting starts, the shooting starts,” a threat of imminent violence with racist roots that Facebook declined to take action against, prompting internal protests at the company.

The former president skirted or crossed the line with Facebook any number of times over his four years in office, but the platform stood steadfastly behind a maxim that all speech was good speech, even as other social networks grew more squeamish.

In a dramatic address in late 2019, Zuckerberg evoked Martin Luther King Jr. as he defended Facebook’s anything goes approach. “In times of social turmoil, our impulse is often to pull back on free expression,” Zuckerberg said. “We want the progress that comes from free expression, but not the tension.” King’s daughter strenuously objected.

A little over a year later, with all of Facebook’s peers doing the same and Trump leaving office, Zuckerberg would shrink back from his grand free speech declarations.

In 2019 and well into 2020, Facebook was still a roiling hotbed of misinformation, conspiracies and extremism. The social network hosted thousands of armed militias organizing for violence and a sea of content amplifying QAnon, which moved from a fringe belief on the margins to a mainstream political phenomenon through Facebook.

Those same forces would converge at the U.S. Capitol on January 6 for a day of violence that Facebook executives characterized as spontaneous, even though it had been festering openly on the platform for months.

How the Oversight Board works

Facebook’s Oversight Board began reviewing its first cases last October. Facebook can refer cases to the board, like it did with Trump, but users can also appeal to the board to overturn policy decisions that affect them after they exhaust the normal Facebook or Instagram appeals process. A five member subset of its 20 total members evaluate whether content should be allowed to remain on the platform and then reach a decision, which the full board must approve by a majority vote. Initially, the Oversight Board was only empowered to reinstate content removed on Facebook and Instagram, but in mid-April began accepting requests to review controversial content that stayed up.

Last month, the Oversight Board replaced departing member Pamela Karlan, a Stanford professor and voting rights scholar critical of Trump, who left to join the Biden administration. Karlan’s replacement, PEN America CEO Suzanne Nossel, wrote an op-ed in the LA Times in late January arguing that extending a permanent ban on Trump “may feel good” but that decision would ultimately set a dangerous precedent. Nossel joined the board too late to participate in the Trump decision.

The Oversight Board’s earliest batch of decisions leaned in the direction of restoring content that’s been taken down — not upholding its removal. While the board’s other decisions are likely to touch on the full spectrum of frustration people have with Facebook’s content moderation preferences, they come with far less baggage than the Trump decision. In one instance, the Oversight Board voted to restore an image of a woman’s nipples used in the context of a breast cancer post. In another, the board decided that a quote from a famous Nazi didn’t merit removal because it wasn’t an endorsement of Nazi ideology. In all cases, the Oversight Board can issue policy recommendations, but Facebook isn’t obligated to implement them — just the decisions.

Befitting its DNA of global activists, political figures and academics, the Oversight Board might have ambitions well beyond one social network. Earlier this year, Oversight Board co-chair and former Prime Minister of Denmark Helle Thorning-Schmidt declared that other social media companies would be “welcome to join” the project, which is branded in a conspicuously Facebook-less way. (The group calls itself the “Oversight Board” though everyone calls it the “Facebook Oversight Board.”)

“For the first time in history, we actually have content moderation being done outside one of the big social media platforms,” Thorning-Schmidt declared, grandly. “That in itself… I don’t hesitate to call it historic.”

Facebook’s decision to outsource some major policy decisions is indeed an experimental one, but that experiment is just getting started. The Trump case will give Facebook’s miniaturized Supreme Court an opportunity to send a message, though whether the takeaway is that it’s powerful enough to keep a world leader muzzled or independent enough to strike out from its parent and reverse the biggest social media policy decision ever made remains to be seen.

If Trump comes back, the company can shrug its shoulders and shirk another PR firestorm, content that its experiment in external content moderation is legitimized. If the board doubles down on banishing Trump, Facebook will rest easy knowing that someone else can take the blowback this round in its most controversial content call to date. For Facebook, for once, it’s a win-win situation.",Social Media,TechCrunch,https://techcrunch.com/2021/05/04/facebook-trump-oversight-board-decision/,"Social media has been a source of tension and misinformation, with companies like Facebook allowing users to post inflammatory and potentially dangerous content without consequence. This has culminated in the Facebook Oversight Board's decision on the suspension of former President Donald Trump, which could set a precedent for the future of social media.","Information, Discourse & Governance"
319,"UK police secretly monitoring 9,000 political campaigners using social media surveillance","An obscure unit within London's Metropolitan Police Service has been monitoring and keeping records on almost 9,000 political campaigners and activists using social media surveillance and other methods. A freedom of information request filed by the The Guardian reveals that the National Domestic Extremism Unit (NDEU) stores dossiers on 8,931 individuals labeled as ""domestic extremists,"" many of which do not have any criminal record, according to a senior officer familiar with the unit's operations.

The NDEU has been monitoring the campaigners with a 24/7, 17-person social media surveillance team, which uses a technique called ""Socmint"" (Social Media Intelligence). The technique scrapes and analyzes Facebook profiles, Tweets, and other public data, using geolocation tracking and ""sentiment analysis"" tools to predict future crimes by determining targets' moods. The unit uses the strategy, along with undercover agents and paid informants, to monitor a wide range of individuals spanning the political spectrum, from the far-right English Defense League to animal rights advocates and anti-war protestors.

Regulations meant to prevent excessive police surveillance are in place under the UK's Regulatory Investigation Powers Act (RIPA), but the law, which passed in 2000, doesn't include any controls for police use of social media for investigations.

The documents come a day after it was reported that Metropolitan Police secretly eavesdropped on meetings between an attorney and the eyewitness of the 1993 hate crime murder of British teenager Stephen Lawrence. The eyewitness, Duwayne Brooks, had been targeted in a police smear campaign aimed at discrediting his testimony in the case, which finally ended last year with the conviction of two men after new evidence came to light.",Social Media,Verge,https://www.theverge.com/2013/6/26/4467056/uk-police-monitoring-9000-political-activists-using-social-media,"The Metropolitan Police Service's National Domestic Extremism Unit has been storing dossiers on 8,931 individuals labeled as ""domestic extremists"" with no criminal records, and using Social Media Intelligence (Socmint) to monitor their activities. This calls into question the lack of regulations around police use of social media for investigations, and highlights how",Security & Privacy
320,Blistering civil rights audit raises alarms about Facebook’s ongoing policy failures – TechCrunch,"The results of a multiyear investigation into Facebook’s policies and their consequences for the civil liberties of its more than 2.5 billion users are out.

The audit, conducted by former ACLU director Laura W. Murphy and lawyers from law firm Relman Colfax set out by collecting concerns from a broad swath of civil rights organizations concerned about Facebook’s growing power and its potentially harmful reverberations through marginalized communities in particular and democratic society more broadly. The auditors also used concerns from some lawmakers, who have become increasingly critical of Facebook since the 2016 U.S. election, to steer their investigation. The ultimate goal of the project was to “make sure important civil rights laws and principles are respected, embraced and robustly incorporated” into the social network.

As the report notes, the audit doesn’t situate Facebook’s decisions in the context of it competitors, instead evaluating the company’s behavior on its own. The approach is useful, because social media companies often get a pass for behavior that’s standard in the industry, an approach that lowers standards across the board rather than looking at real-world impacts. The auditors make a point of giving Facebook credit for its cooperation in the audit, which the company itself undertook with pressure from outside groups concerned about its failings on issues like race-based hate, misinformation, voter suppression and extremism.

While the report has its positive moments of giving credit where credit is due, the auditors found that Facebook’s progress on civil rights issues has had many one step forward, two steps back moments over the last two years. Any sense of optimism about the company’s progress is tempered by frustration about Facebook’s policy missteps at the very top.

“While the audit process has been meaningful and has led to some significant improvements in the platform, we have also watched the company make painful decisions over the last nine months with real world consequences that are serious setbacks for civil rights,” the auditors wrote.

As far as positive decisions go, they cite Facebook’s progress on changing policy in discriminatory housing and employment ads, expanded voter suppression policies, census interference prevention measures, more frequent meetings with civil rights leaders and changes to content moderation policies, like its prohibition of praise for white nationalism that went into effect last year.

In spite of some progress, the auditors say they still have a number of concerns. Specifically, they called for Facebook to implement its voter suppression policies more aggressively leading into the 2020 U.S. election, citing President Trump’s ominous false claims about voting in the 2020 election, which went untouched on the platform.

Facebook’s enforcement of its policies against white nationalism and white separatism (terms mostly synonymous with white supremacy) were also an area of concern, with the auditors calling for the company to forbid this kind of content even if it doesn’t use those terms specifically. The chalked some of these failures up to the company’s policies being “too reactive and piecemeal” rather than coherent, a statement that anyone paying attention to the company’s recent decisions can certainly relate to.

The audit cites a number of specific moments as failures for Facebook’s policies, including Nick Clegg’s deeply controversial assertion last year that politicians wouldn’t be subject to the company’s already shaky fact-checking program, a decision widely denounced by civil rights leaders and Facebook’s other critics for giving people in power “more freedom on the platform to make false, voter suppressive and divisive statements than the average user.”

Mark Zuckerberg’s Georgetown speech last October enshrining a very narrow and contentious conception of free speech at the expense of everything else was another major moment of departure for Facebook from its stated commitment to civil rights, the auditors write. “The prioritization of free expression over all other values, such as equality and nondiscrimination, is deeply troubling.”

Facebook’s decision to this day to not reverse its course on posts from Trump that intentionally mislead the public about voting (in one he baselessly claimed vote-by-mail systems are “fraudulent”) also remains an ongoing source of frustration, undermining the company’s progress. The auditors wrote that they are confounded about why the company “has failed to grasp the urgency” of robust policy enforcement with fewer than five months to go before the U.S. presidential election, concluding that Facebook’s decision to keep the Trump posts up shows that civil rights and voting integrity fall far behind its expedient interpretation of free expression on the company’s list of values.

“This report outlines a number of positive and consequential steps that the company has taken, but at this point in history, the Auditors are concerned that those gains could be obscured by the vexing and heartbreaking decisions Facebook has made that represent significant setbacks for civil rights,” the report states.

The bits of the audit we’ve touched on here only scratch the surface of a fairly comprehensive and often clarifying examination of a complex company’s often troubling policy decisions, so we’ve embedded the full Facebook civil rights audit document below.",Social Media,TechCrunch,https://techcrunch.com/2020/07/08/facebook-civil-rights-audit/,"The audit found that Facebook’s decisions often prioritize free expression over civil rights, leading to significant setbacks such as insufficient enforcement of voter suppression policies and allowing posts from powerful figures that intentionally mislead the public.",Equality & Justice
321,Facebook Didn't Create Donald Trump—The Click Economy Did,"In mid-October I wandered into a Trump field office in Youngstown, Ohio and met Coni Kessler, a kind 75-year-old Youngstown native with penciled-on eyebrows and a Women for Trump button on her Trump 2016 t-shirt. She sat me down in a chair just beside her, and for more than an hour, explained why she detested Hillary Clinton and was ecstatic to vote for Trump this year.

Clinton, she told me, is an atheist who wears an earpiece during debates so billionaire George Soros can feed her talking points. The day Clinton collapsed into the back of her van when she was sick with pneumonia? According to Kessler, the Clintons hired a young actress to run up and give Clinton a hug for a staged photo after the collapse. Kessler also said she’d seen videos of Bill Clinton raping an underage girl but that the video had mysteriously disappeared. She wondered why no one was talking about Bill Clinton’s illegitimate, half-black son. And she said that whenever she talks negatively about Clinton online, “they”---presumably the technology overlords---shut her phone down.

At some point, I stopped Kessler to ask her where she’d gotten all these stories, stories I knew were false Clinton conspiracy theories. Her answer: “It was on my Facebook page.”

Kessler's stories were extreme, yes, but I should have known then that Trump enjoyed the support of enough people like Kessler, being fed an entirely different narrative about the two candidates, to help carry this election. But I missed it entirely. Because of her conviction that these stories were true, I considered Kessler part of a fringe that could not possibly gain the backing of enough voters to win. I missed this because, well, I was living in my own bubble, too.

I’ll stop here to say that, no, this isn’t another story about how Facebook gave us President-elect Trump. By now, there are plenty of solid stories like that, and they’re not wrong. To be sure, Facebook played an instrumental role in this election by allowing the kind of fake news stories Kessler described to me to proliferate and by giving all of us the option of only seeing the content we “like.” But it would be wrong to lay the election results entirely at Facebook’s feet. Trump won in large part because he mastered this particular moment in social media’s evolution in a way that no presidential candidate---not even Barack Obama, whose embrace of social media was historic in its own way---has ever done before.

We may not know exactly how rich our soon-to-be president actually is. But in an economy where clicks are currency, Trump is King Midas.

The Popularity Vote

Nicco Mele was as clueless as anyone that Trump was going to win the election. This is strange, since Mele actually predicted just that in his 2012 book, The End of Big. Mele describes how the internet enabled insurgent political candidates like Barack Obama in 2008 to succeed by corroding traditional top-down political power structures. That corrosion is in many ways a good thing, he wrote, because the political class is corrupt and divisive. “Yet in hastening the demise of parties and empowering upstarts, radical connectivity also paves the way for a dangerous populism to take hold of our political system,” he says. “We get exciting candidates like Barack Obama who can shake up the system, but also extremist or fringe candidates who if elected could bring the whole house down.”

“That’s exactly what’s happened,” Mele says today.",Social Media,WIRED,https://www.wired.com/2016/11/facebook-alone-didnt-create-trump-click-economy/,"Social Media has made it easier for populist, extremist candidates to gain a foothold in politics, as seen in the 2016 presidential election. This has created a dangerous situation as fringe candidates could bring down the whole system.",Politics
322,Twitter Didn't Suspend Hope Hicks,"Wednesday morning, conservative corners of the internet were furious to discover that @HopeHicks45, the Twitter account belonging to Donald Trump's newly official communications director, had been suspended. The only problem? That’s not Hope Hicks. @HopeHicks45 was nothing more than an imposter.

The ordeal kicked off when journalist Jules Suzdaltsev noted that what appeared to be Hicks' account was not verified by the service, surprising for such a prominent public figure. Less than a day later, Twitter suspended that same account:

Not long after, Pizzagate champion and Trump retweet recipient Jack Posobiec started stirring up an outrage campaign. ""Twitter has suspended Trump's communications director @HopeHicks45 one day after she was appointed,"" he said in a tweet over a screenshot of the suspended account. At the time of publication, Posobiec's tweet has been retweeted more than 1,500 times.

That was just the beginning. ""Alt-left targeted her account and mass-reported it. Twitter took it down,"" he said shortly after. Posobiec then followed up with, ""This is a horrific sign for our democracy when @Twitter refuses to allow our President's own communications channel with the people."" As well as, ""Twitter must issue an apology to Ms. Hicks, as well as the United States of America. Immediately.""

The Political Insider, a conservative DC-focused blog, at least remained somewhat skeptical: ""Can you believe it? Not even 24 hours after being put in charge of White House communications, a Twitter account that allegedly belongs to Hicks has been suspended. It should be noted that it’s not verified if the account, which has the handle @HopeHicks45, really belongs to Hicks. But if you google 'Hope Hicks twitter,' it’s the very first result you get.""

Others, however, showed less caution and also introduced Nazis. ""This is the NAZI behavior of the Left, and it's very exposed today...As Hope Hicks has been deleted by Twitter with no reasoning behind the suspension,"" wrote the inimitable PuppetStringNews.com.",Social Media,WIRED,https://www.wired.com/story/hope-hicks-twitter-suspended/,"The incident of @HopeHicks45, an imposter account of Donald Trump's newly official communications director, being suspended on Twitter has sparked outrage from conservative corners of the internet and accusations of ""Nazi behavior"" from the left. This event has highlighted the dangerous consequences of Social Media, such as false information, mass-reporting and lack of",Social Norms & Relationships
323,Facebook and Twitter to provide Brexit disinformation reports soon – TechCrunch,"A UK parliamentary committee that’s investigating fake news has been told by Facebook and Twitter they will provide information relating to Russian interference during the UK’s 2016 Brexit referendum vote in the coming weeks.

With election disinformation being publicly interrogated in the US, questions have increasingly been asked in the UK about whether foreign government agents also sought to use social channels to drive Brexit propaganda and sway voters.

Last month Damian Collins, the chair of the digital, culture, media and sport committee, wrote to Facebook and Twitter asking them to look into whether Russian-backed accounts had been used to try to influence voters in the June 2016 in/out EU referendum.

The Guardian reports that Collins has also asked senior representatives from the two companies to give evidence on the reach of fake news at the British embassy in Washington in February.

Earlier this month, the UK prime minister cranked up the political pressure by publicly accused the Russian government of seeking to “weaponize information” by planting fake stories and photoshopped images to try to meddle in elections and sow discord in the West.

In a letter sent to Collins on Friday, Twitter confirmed it would be divulging its own findings soon, writing: “We are currently undertaking investigations into these questions and intend to share our findings in the coming weeks.”

Also responding to the committee last week, Facebook noted it had been contacted by the UK’s Electoral Commission about the issue of possible Russian interference in the referendum, as part of enquiries it’s making into whether the use of digital ads and bots on social media broke existing political campaigning rules.

“We are now considering how we can best respond to the Electoral Commission’s request for information and expect to respond to them by the second week of December. Given that your letter is about the same issue, we will share our response to the Electoral Commission with you,” Facebook writes.

We understand that Google has also been asked by the Electoral Commission to provide it with information pertaining to this probe.

Meanwhile, the UK’s data protection watchdog is conducting a parallel investigation into what it describes as “the data-protection risks arising from the use of data analytics, including for political purposes”.

Where Brexit is concerned, it’s not yet clear how significant the impact of political disinformation amplified via social media was to the outcome of the vote. But there clearly was a disinformation campaign of sorts.

And one that prefigured what appears to have been an even more major effort by Kremlin agents to deflect voters in the US presidential election, just a few months later.

After downplaying the impact of ‘fake news’ on the election for months, Facebook recently admitted that Russian-backed content could have reached as many as 126 million US users over the key political period.

Earlier this month it also finally admitted to finding some evidence of Brexit disinformation being spread via its platform. Though it claimed it had not found what it dubbed “significant coordination of ad buys or political misinformation targeting the Brexit vote”.

Meanwhile, research conducted by a group of academics using Twitter’s API to look at how political information diffused on the platform around the Brexit vote — including looking at how bots and human users interacted — has suggested that more than 156,000 Russian accounts mentioned #Brexit.

The researchers also found that Russian accounts posted almost 45,000 messages related to the EU referendum in the 48 hours around the vote (i.e. just before and just after).

While another academic study reckoned to have identified 400 fake Twitter accounts being run by Kremlin trolls.

Twitter has claimed that external studies based on tweet data pulled via its API cannot represent the full picture of how information is diffused on its platform because the data stream does not take account of any quality filters it might also be applying, nor any controls individual users can use to shape the tweets they see.

It reiterates this point in its letter to Collins, writing:

… we have found studies of the impact of bots and automation on Twitter necessarily and systematically underrepresent our enforcement actions because these defensive actions are not visible via our APIs, and because they take place shortly after content is created and delivered via our streaming API. Furthermore, researchers using an API often overlook the substantial in-product features that prioritize the most relevant content. Based on user interests and choices, we limit the visibility of low-quality content using tools such as Quality Filter and Safe Search — both of which are on by default for all of Twitter’s users and active for more than 97% of users.

It also notes that researchers have not always correctly identified bots — flagging media reports which it claims have “recently highlighted how users named as bots in research were real people, reinforcing the risks of limited data being used to attribute activity, particularly in the absence of peer review”.

Although there have also been media reports of the reverse phenomenon: i.e. Twitter users who were passing themselves off as ‘real people’ (frequently Americans), and accruing lots of retweets, yet who have since been unmasked as Kremlin-controlled disinformation accounts. Such as @SouthLoneStar.

Twitter’s letter ends by seeking to play down the political influence of botnets — quoting the conclusion of a City University report that states “we have not found evidence supporting the notion that bots can substantively alter campaign communication”.

But again, that study would presumably have been based on the partial view of information diffusion on its platform that Twitter has otherwise complained does not represent the full picture (i.e. in order to downplay other studies that have suggested bots were successfully spreading Brexit-related political disinformation).

So really, it can’t have it both ways. (See also: Facebook selling ads on its platform while trying to simultaneously claim the notion that fake news can influence voters is “crazy”.)

In its letter to Collins, Twitter does also say it’s “engaged in dialogue with academics and think tanks around the world, including those in the UK, to discuss potential collaboration and to explore where our own efforts can be better shared without jeopardizing their effectiveness or user privacy”.

And at least now we don’t have too much longer to wait for its official assessment of the role Russian agents using its platform played in Brexit.

Albeit, if Twitter provided full and free access to researchers so that the opinion-influencing impact of its platform could be more robustly studied the company probably still wouldn’t like all the conclusions being drawn. But nor would it so easily be able to downplay them.",Social Media,TechCrunch,https://techcrunch.com/2017/11/28/facebook-and-twitter-to-provide-brexit-disinformation-reports-soon/,"With the investigation of election disinformation in the US, questions are being raised in the UK about whether foreign government agents used social channels to sway the 2016 Brexit referendum vote. Facebook and Twitter have been asked to provide information on potential Russian interference and both companies have confirmed they will share their findings in the coming weeks.","Information, Discourse & Governance"
324,How digital beauty filters perpetuate colorism,"That harm can involve bleaching or other risky body treatments: the skin-lightening industry has grown rapidly and is now worth more than $8 billion worldwide each year. But beyond physical risks, researchers and activists have also begun documenting troubling emotional and psychological effects of online colorism.

Amy Niu researches selfie-editing behavior as part of her PhD in psychology at the University of Wisconsin, Madison. In 2019, she conducted a study to determine the effect of beauty filters on self-image for American and Chinese women. She took pictures of 325 college-aged women and, without telling them, applied a filter to some photos. She then surveyed the women to measure their emotions and self-esteem when they saw edited or unedited photos. Her results, which have not yet been published, found that Chinese women viewing edited photos felt better about themselves, while American women (87% of whom were white) felt about the same whether their photos were edited or not.

Niu believes that the results show there are huge differences between cultures when it comes to “beauty standards and how susceptible people are to those beauty filters.” She adds, “Technology companies are realizing it, and they are making different versions [of their filters] to tailor to the needs of different groups of people.”

This has some very obvious manifestations. Niu, a Chinese woman living in America, uses both TikTok and Douyin, the Chinese version (both are made by the same company, and share many of the same features, although not the same content.) The two apps both have “beautify” modes, but they are different: Chinese users are given more extreme smoothing and complexion lightening effects.

She says the differences don’t just reflect cultural beauty standards—they perpetuate them. White Americans tend to prefer filters that make their skin tanner, teeth whiter, and eyelashes longer, while Chinese women prefer filters that make their skin lighter.

Niu worries that the vast proliferation of filtered images is making beauty standards more uniform over time, especially for Chinese women. “In China, the beauty standard is more homogeneous,” she says, adding that the filters “erase lots of differences to our faces” and reinforce one particular look.",Social Media,MIT,https://www.technologyreview.com/2021/08/15/1031804/digital-beauty-filters-photoshop-photo-editing-colorism-racism/,"The proliferation of heavily filtered images on Social Media is perpetuating and reinforcing a homogeneous beauty standard for Chinese women, which can have emotional and psychological effects and encourage dangerous body treatments.",Equality & Justice
325,Clinton and Trump Can't Unite On Orlando Because Twitter Won’t Let Them,"The letter reads like a work of science fiction. Former president George H.W. Bush wrote it after Bill Clinton defeated him in the 1992 presidential election. He left it in the Oval Office for Clinton. It reads in part:

I’m not a very good one to give advice; but just don’t let the critics discourage you or push you off course. You will be our President when you read this note. I wish you well. I wish your family well. Your success is now our country’s success. I am rooting hard for you. Good luck, George

Can you remember a world in which members of rival parties addressed one another in such an amicable way? Can you even imagine that such civility ever existed in politics at all?

The letter is making the rounds online these days because, since the horrific shooting in Orlando, presumptive Democratic nominee Hillary Clinton has been mentioning it in stump speeches and sharing it on social media. She says it reminds her of the “America we love,” the subtext being: Look at how far we've strayed.

Cynics will say that Bush was handing over the keys in a much simpler world, that in 20-plus years, global threats have grown more complex, and that current politics are a symptom of that complexity. But there’s something else that’s changed: the way the country talks to itself. The loudest, angriest voices have turned social media—the platforms of first response in times of crisis—into venues where antagonism is the only mode. The country can't come together in crises because the moment they happen, the shouting online begins.

Carolyn Cole/Los Angeles Times/Getty Images

Not Coming Together

Time was when national tragedies would unite the country. After 9/11, a sense of collective resolve and compassion seemed to prevail over the nation, if only for a moment. Flash forward to Sunday morning just after the Orlando shooting. Most of us woke up to a Twitter and Facebook frenzy that had already become deeply politicized. We woke up in a country that had already chosen sides via ready-made hashtags. A country that had already set about attacking politicians on both sides of the aisle. A country that had already decided the enemy was not the shooter himself, but the Democratic leaders who aren't tough enough on national security or the Republicans who are overly lax about gun control.

The country can't come together in crises because the moment they happen, the shouting on Twitter begins.

Studies show it only takes a matter of days for social media discourse about a tragedy to morph from a relatively neutral national conversation to a series of politicized echo chambers. If Orlando is any indication, that timeframe has narrowed from days to hours. Each passing catastrophe serves as another reminder of the country's seeming collective helplessness to stop it. And so Americans get angry. We the people demand answers---and thanks to social media, we can demand them loudly and publicly.

That's a long way from 1993, when Bush left the letter to Clinton. No online army thirsted for instant outrage from its political leaders, demanding that they pick a side. Furious masses didn't start online petitions or dig up politicians' old tweets to call them out should they ever contradict themselves or dare to compromise. The conversations that once occurred behind closed doors in Washington, DC, now happen out in the open, every minute of every day, often in 140 characters or less. The reason our politicians can no longer agree on these life or death issues is because, well, we the people probably wouldn't let them if they tried.",Social Media,WIRED,https://www.wired.com/2016/06/clinton-trump-cant-unite-orlando-twitter-wont-let/,"Social media has allowed for a faster and more politicized response to national tragedies, creating an atmosphere of anger and distrust that makes it difficult for politicians to come together and find common ground on important issues.",Discourse & Governance
326,Why Has BlackBerry Been Blamed for the London Riots?,"The whole of London is taking stock after outbreaks of violence and looting across the capital. Ealing, Camden, Peckham, Clapham, Lewisham, Woolwich and Hackney have all provided a location for young rioters intent on stealing and vandalizing. So what has the humble BlackBerry got anything to do with it?

Here's an FAQ:

Doesn't BlackBerry only make phones for business people?

BlackBerry, owned by Research In Motion (RIM), is not just the smartphone of choice for business executives, it's also much sought after among the yoof -- according to Ofcom, some 37 percent of British teens have a BlackBerry handset. One of the brand's main draws is BlackBerry Messenger (BBM) -- a free instant messaging service that allows BlackBerry owners to communicate with each other on a one-to-one or one-to-many basis. So popular is the service that it's largely replaced text messaging between BlackBerry users because it's free, instant and allows you to quickly communicate with all of the contacts with whom you have shared your BBM PIN.

But there are lots of instant messaging services, what makes BBM so special?

[partner id=""wireduk""]With the protests in the Middle East that dominated the headlines earlier this year, Twitter and Facebook were credited with rallying the masses. They provided an easy way to rally people very quickly, broadcasting details about the protests. Unlike Twitter and Facebook, however, BBM messages are largely untraceable by the authorities. BlackBerry automatically encrypts messages sent to another person's BlackBerry when using their PIN -- this means that the messages cannot be intercepted by a government or mobile network. As such the service has become very popular in the Middle East where it is used to criticize authorities, which explains why Saudi Arabia and the United Arab Emirates tried to block BBM and other functions last year.

So how has BBM been used during the London riots?

It's been used in fundamentally the same way as one might use Twitter, with widespread calls to action being broadcast to groups of contacts, except under PIN protection. One BBM broadcast sent on Sunday and shown to the Guardian called for people from all over London to vandalize shops on Oxford Street and to attack the police. It read: ""Everyone from all sides of London meet up at the heart of London (central) OXFORD CIRCUS!!, Bare SHOPS are gonna get smashed up so come get some (free stuff!!!) fuck the feds we will send them back with OUR riot! >:O Dead the ends and colour war for now so if you see a brother... SALUT! if you see a fed... SHOOT!"". Another message read: ""Everyone in edmonton enfield wood green everywhere in north link up at enfield town station at 4 o clock sharp!!!!""

It has been reported that BBM was also used to rally those elsewhere in London and in Birmingham.

What is BlackBerry doing about it?

Clearly the riots are not BlackBerry's fault, but given the high penetration of BlackBerry phones among the age group that is largely causing the disturbances and the fact that BBM cannot be monitored in the way that Facebook and Twitter can be, BlackBerry has been under pressure to respond. Before the trouble flared up yesterday afternoon, the UK BlackBerry account tweeted: ""We feel for those impacted by this weekend's riots in London. We have engaged with the authorities to assist in any way we can.""

RIM also released an extended statement where Patrick Spence, managing director of global sales and regional marketing, added: ""As in all markets around the world where BlackBerry is available, we co-operate with local telecommunications operators, law enforcement and regulatory officials. Similar to other technology providers in the UK we comply with the Regulation of Investigatory Powers Act and co-operate fully with the Home Office and UK police forces.""",Social Media,WIRED,https://www.wired.com/2011/08/blackberry-london-riots/,"The London riots have highlighted the potential issues with using instant messaging services such as BlackBerry Messenger, which cannot be monitored by authorities in the same way as other social media applications. This has led to calls for greater regulation of such services in order to prevent them from being used for criminal activity.",Security & Privacy
327,Writer Anand Giridharadas on tech’s billionaires: ‘Are they even on the same team as us?’ – TechCrunch,"Since the start of the coronavirus pandemic, America’s roughly 640 billionaires have seen their fortunes soar by $845 billion in combined assets or 29% collectively, widening the already yawning gap between the very richest and the rest of the U.S.

Many of those billions were made by tech founders, including Mark Zuckerberg, Jeff Bezos, and Elon Musk, whose companies have soared in value and, in tandem, their net worth. In fact, so much has been made so fast and by so few relatively, that it’s easy to wonder if greater equality is now forever out of reach.

To talk about that question, we reached out earlier this week to Ananad Giridharadas, a former New York Times correspondent whose 2018 book, “Winners Take All: The Elite Charade of Changing the World,” became a best-seller.

Giridharadas’s message at the time was largely that the generosity of the global elite is somewhat laughable — that many of the same players who say they want to help society are creating its most intractable problems. Think, for example, of Bezos, whose company paid zero in federal tax in 2017 and 2018 and who is now on the cusp of opening a tuition-free preschool for underserved children called the Bezos Academy.

Given the aggressive escalation over the last six months of the same trends Giridharadas has tracked for years, we wondered how he views the current situation. Our chat has been edited for length and clarity below. You can hear our longer conversation here.

TC: You have a weekly newsletter where you make the point that Jeff Bezos could give every one of Amazon’s 876,000 employees a ‘pandemic’ bonus of $105,000, and he would still have as much money as he did in March.

AG: There’s this way in which these crises are not merely things that the rich and powerful survive. They’re things that they leverage and exploit, and it starts to raise the question of: are they even on the same team as us? Because when you have discussions about stimulus relief, around what kind of policy responses you could have to something like the 2008 financial crisis or the pandemic, there’s initially some discussion and clamor for universal basic income, or substantial monthly checks for people, or even the French approach of nationalizing people salaries. . .and those things usually die. And they die thanks to corporate lobbyists and advocates of the rich and powerful, and are replaced by forms of relief that are upwardly redistributive that essentially exploit a crisis to transfer wealth and power to the top.

TC: Earlier in the 20th century, there was this perception that industry would contribute to solving a crisis with government. In this economy, we didn’t see a lot of the major tech companies, or a lot of the companies that were benefiting from this crisis, really sacrificing something to help the U.S. Do you see things that way?

AG: I think that’s right. I’m always wary of idealizing certain periods in the past, and I think there were a lot of problems in that time. But I think there’s no question that it was not as difficult back then, as it is today, to summon some kind of sense of common purpose and even the need to sacrifice values like profit-seeking for other values.

I mean, after 9/11, President George W. Bush told us all to go shopping as the way to advance the common good. Donald Trump is now 18 levels of hell further down that path, not even telling us that we need to do anything for each other and [instead describing earlier this week] a pandemic that has killed 200,000 people as being something that doesn’t really affect most people.

So there’s just been a coarsening. And that kind of selfish trajectory of our culture, after 40 years of being told that what we do alone is better than what we do together, that what we do to create wealth is more important than what we do to advance shared goals — that quite dismal, dull message has had its consequences. And when you get a pandemic like this, and you suddenly need to be able to summon people to all socially distance at a minimum or, more ambitiously, pull for the common good or pay higher taxes or things that might even cost them a little bit, it’s very hard to do because the groundwork isn’t there.

TC: You’ve talked quite a bit over the years about “fake change.”

AG: Silicon Valley is the new Rome of our time, meaning a place in the world that ends up deciding how a lot of the rest of the world lives. No matter where you lived on the planet Earth, when the Roman Empire started to rise, it had plans for you one way or another, through your legal system, or your language, or culture, or something else. The Roman Empire was coming for you.

Silicon Valley is that for our time. It’s the new Rome [in] that you can’t live on planet Earth and be unaffected, directly or indirectly, by the decisions made in this relatively small patch of [the world]. So the question then becomes, what does that new Rome want? And my impression of having reported on that world is that it’s an incredibly homogeneous world of people at the top of this new Rome. It’s white-male-dominated in a way that even other white-male-dominated sectors of the American economy are not . . . and it’s a lot of a certain kind of man who often is actually more obtuse about understanding human society and sociological dynamics and human beings than the average person.

Maybe they didn’t spend a lot of time negotiating human dynamics at sleepovers, which is fine. But when you end up with a new Rome and it’s hyper dominated by people of one race and one gender, many of whom are disproportionately socially unintelligent, running the platforms through which most human sociality now occurs — democratic discourse, family community, so on and so forth — we all start to live in a world created by people who are just quite limited. They are smart at the thing they’re smart at, and they’ve become in charge of a lot of how the world works. And they are simply not up to the task. And we see evidence of that every day.

TC: Are you speaking about empathy?

AG: Empathy is absolutely one of [the factors]. The ability to understand the more amorphous, non technological, non quantifiable things . . . it’s so interesting, because it’s people who are clearly very smart in a certain area but just honestly do not understand democratic theory. There’s just so much work that’s been done — deep, complicated thinking going back to Plato and Aristotle, but also modern sociological work, including why a safety net and welfare is complicated. And there’s a certain kind of personality type that I have found very dominant in Silicon Valley, where it’s these men who just don’t really have a lens for that.

They’re often geniuses. It’s a certain kind of particular personality type where you care a lot about one thing and you go deep on that one thing, and it’s probably the same personality type that Beethoven had. It’s a great thing, actually. It’s just not great for governing us, and what these people are doing is privately governing us, and they have no humility about the limitations of their worldview.

TC: We’re talking largely about social media here. Is it reasonable to expect some kind of government action. Do you think that’s naive?

AG: It’s absolutely essential that the tech industry be brought into the same kind of sensible regulatory regime. I mean, you have kids, I have kids. If you’ve ever read the side of their car seats or any of the other products in their lives, you understand how much regulation there is for our benefit. . . I often say that the government at its best is like a lawyer for all of us. The government is like ‘Why don’t we check out these car seats for you and create some rules around them, and then you can just buy a car seat and not have to wonder whether it’s the kind that protects your child or crumbles?’ That’s what the government does for all kinds of things.

TC: You’ve talked about billionaires who don’t want to pay taxes yet don’t hesitate to make a donation because they have control over where their money is spent and they get their name on a building, and it’s true. Many companies whose founders also consider themselves philanthropists, like Salesforce and Netflix, paid no federal tax in 2018, which amounts to billions of dollars lost. If you had to prioritize between taking antitrust action or closing the tax loopholes, which would you choose?

AG: They’re both important. But I think I would prioritize taxation.

One way to think about it is this pre distribution and redistribution. The monopoly issue in a way is pre distribution, which is how much money you get to make in the first place. If you get to be a monopoly because we don’t enforce antitrust laws, you’re going to end up making, pre tax, a lot more money than you would otherwise have made if you had to compete in an actual free market.

Once you’ve made that money, the tax question comes up. So both are important, but I think you can’t overestimate the extent to which the tax thing is just totally foundational. If you look at the report that the 400 richest families in America pay a lower effective tax rate than the bottom half of families, it’s appalling.

We live in a complicated world. A lot of different things have been going on, including just in the last few months. But if you have to really summarize the drift and the shift of the last 40 years, it’s been a war on taxation. And it’s been a massive redistribution of wealth from the bottom to the top of American life through taxation. Since the ’80s, the top 1% has gained $21 trillion of wealth, and the bottom half of Americans have lost $900 billion of wealth on average — and much of that was prosecuted through the tax code.

Awkward! Above, Giridharadas shaking hands with Amazon founder Jeff Bezos at a Wired event in 2018.",Social Media,TechCrunch,https://techcrunch.com/2020/09/24/writer-anand-giridharadas-on-techs-billionaires-are-they-even-on-the-same-team-as-us/,"The rise of Silicon Valley as the ""new Rome"" of the modern world has led to a hyper-dominant white male population of tech giants who are often socially unintelligent, governing the platforms through which most human sociality now occurs. This has resulted in an exploitative socioeconomic situation, where those in power have used crises to transfer wealth",Equality & Justice
328,Seven types of election misinformation to watch out for,"“Misinformation” and “disinformation” aren’t even great terms for what’s going on anymore, as the same words can include systemic campaigns by governments to suppress voters, harmful conspiracy theories with connections to real-world violence, and dumb hoaxes that are pushed into the river of online information for no good reason.

But we do know that there will be bad information, coordinated campaigns, and attempts to amplify harmful content as far as possible through the election and beyond. So here’s a list of some of the things you might encounter this week.

Unsourced rumors from polling locations

At the start of Election Day, bad actors will be trying to keep voters away from the polls. There’s already a decent amount of misinformation out there about poll watchers, but experts are worried about the effect those recruited by the Republican Party to watch for “fraud” could have on minority voters in Democratic-leaning districts. Angelo Carusone, the president of Media Matters for America, a left-leaning media watchdog group that has been tracking far-right misinformation campaigns, said last week that he was particularly worried about what happens if those poll watchers start issuing misleading or false reports from polling locations themselves. “The whole idea of being a poll watcher is that you are credentialed in some way to sound the alarm,” he said. “It has a better chance of breaking through the media, especially local media.”

Lyric Jain, the founder of the UK-based verification app Logically, said their researchers will be tracking a few types of rumors about polling locations, including those related to the pandemic.

“What we’re likely seeing is reports of fake covid outbreaks at polling stations,” he said. He also expects unsourced rumors of inadequate safety procedures at local polling places. Other experts we spoke to were a bit more skeptical of the likelihood of these rumors having much impact on Election Day, in part because these rumors have been present during early voting without really catching a bigger audience. However, Jain guessed that we’re “likely to see a lot more of that on Election Day itself,” and they’ll be harder to fight in real time if they do catch some online traction.

Fake and out-of-context screen shots, videos, and images

Misinformation doesn’t have to be entirely fabricated. Experts have warned voters to keep an eye out for decontextualized media too. Videos, images, and even news articles can be removed from their original context and deployed to make a questionable narrative sound more credible.

So, for instance, a recent tweet from Richard Grenell—a former ambassador to Germany and former acting director of national intelligence under Trump’s administration—claimed to show Joe Biden wearing a mask outside, but not wearing one on a plane, in order to prove his hypocrisy. The two photos tweeted by Grenell do indeed show Biden on a plane without a mask … in 2019, before the pandemic.

Other uses of decontextualized media include sharing article after article of voter fraud stories from years ago as if they’re happening now, in order to give the impression that the election results are compromised by widespread fraud that doesn’t exist. Or videos of long lines and unrest at polling stations when the images are from different elections, or from incidents that are unrelated to elections at all. Screen shots are also easy to fabricate and repurpose, so be skeptical of friends posting screen grabs of conversations with their friend of a friend of a friend who is a poll worker.

If you’re hell-bent on sharing something, there are a few questions you can ask yourself about a video or photo you’re seeing on Election Day: Is this video from a reporter from an outlet you trust, and is the reporter at the polling place in question? Can you find other videos or reports of the same incident from people who appear to actually be there? Have news sources or fact-checking organizations you trust been able to verify that this happened?

For photos in particular, try running the content through a reverse image search service like Google Images, which allows you to figure out whether it’s been used before elsewhere.

Rumors from private groups

In 2016, people were primarily concerned with misinformation’s ability to “go viral.” But in this election, private online spaces are a popular place for misinformation to spread, and much harder to track.

“It makes for very different problems that a lot of apps, a lot of platforms aren’t quite ready to deal with,” Jain says. Private Facebook groups have helped health misinformation find an audience for years before the election, and those groups were still popular into the pandemic.

Nina Jankowicz, a disinformation fellow at the Wilson Center, told NPR over the summer that private groups are particularly susceptible to breeding misinformation because of the ways in which those groups succeed as communities. “The moderators of groups use the community that they build there to create a sense of trust,” she said. “In some cases, these are really polarizing environments,” full of “content that is really indoctrinating there.”

QAnon’s success over the summer in reaching the mainstream demonstrated just how well this can work in spaces that are unrelated to politics: mom groups and wellness communities were particularly susceptible to some of QAnon’s more mainstream-friendly campaigns.

At the end of October, Facebook announced that it was suspending recommendations to users to join groups themed around political or social issues. Researchers have long warned that algorithmic group recommendations—basically, suggesting you join x or y group based on your interest in z— play a part in bringing users deeper into conspiratorial and extreme thinking.

Repeat offenders with big followings

The Election Integrity Partnership, a coalition of researchers working to combat election-related misinformation and disinformation in real time, has identified a number of “repeat offender” Twitter accounts with large followings that have regularly shared or engaged with misleading narratives about the election. Those accounts, which include @realdonaldtrump, are largely familiar faces in the pro-Trump universe, like Charlie Kirk and Sean Hannity. It also includes the handles of several figures connected to pro-Trump media with a history of amplifying misinformation, and Breaking911, a viral news account with a weird history that no one should trust.

The repeat offenders identified by EIP are particularly adept at reframing things that were originally true by quote-tweeting them and adding their own narrative, the report notes. That effort is aided by reframing rewrites from outlets like Gateway Pundit and Breitbart, which can pick up on and help spread misleading narratives about real incidents to larger audiences.

Local news, kind of

As local newspapers diminish their coverage or shut down altogether, there’s some pretty compelling evidence that bad actors and PR firms are exploiting the news vacuum that creates. The New York Times reported in October on a large network of sites that, at first glance, appear to be local papers with names like “Maine Business Daily” or “Ann Arbor Times.” In fact, however, those sites are part of a “proliferation of partisan local-news sites funded by political groups associated with both parties,” the report said.

So know the reliable local news sources in your area—and watch out for new sources that suddenly appear. If you’re curious about whether a hyperpartisan site is pretending to be a newspaper in your area, Nieman Lab made this map over the summer. While it doesn’t appear totally up to date, it’s a good starting point.

Media Matters said it is also concerned about Sinclair Broadcast Group, the conservative company that owns local television stations across the United States. The company has, in the past, ordered anchors to read a script on air about the mainstream media that echoes Trump’s views about “fake news.” Eric Bolling used an episode of his Sinclair show to spread misinformation about covid. The show, which had been posted online to multiple Sinclair station websites, was edited to remove some of the claims before it aired.

Trump’s campaign

You shouldn’t rely on any political campaign to give you the results of the election, but Axios reported this weekend that Trump’s campaign may be planning on declaring victory on election night if the results at that moment are favorable to him, despite knowing that uncounted ballots in key states could shift the results.

Is saying this partisan? As a journalist, whenever you write about political misinformation, someone’s going to tell you you’re showing your bias by scrutinizing the misinformation coming from the Trump campaign more than the Biden campaign. But the situations are plainly not equivalent. One campaign in this race has embraced misinformation as a political tactic and repeatedly spread falsehoods about mail-in voting, and the other has not.

Your well-meaning friends

Well-meaning people are also fully capable of spreading misinformation. Whether it’s re-sharing a racist or otherwise awful meme in order to condemn it, or getting caught up in the hype of believing that every glitching voter machine is a sign of a hacked election or every slightly off thing on Twitter is a Russian bot, it’s possible that some of the people sharing bad information in your feed today will be people you know and trust.

We’ve gone into some of these traps (and how to avoid them) here. While it’s depressing to see people who should know better share misinformation on the internet, the good news is that, experts believe, this is exactly the sort of online terribleness that you, the individual, can do something about. If you feel up to it, respectfully providing context or skepticism in response to one of your friends’ suspicious posts can help to slow the spread of a false narrative.",Social Media,MIT,https://www.technologyreview.com/2020/11/02/1011543/election-2020-sources-of-misinformation/,"Misinformation and disinformation are running rampant on social media, and can include fabricated stories, out-of-context images and videos, private group rumors, and accounts with large followings spreading false information. This can lead to confusion and distrust amongst users, and can even be used to discourage voting.","Information, Discourse & Governance"
329,"Lawmakers confront TikTok, Snapchat and YouTube about eating disorder content – TechCrunch","Representatives from TikTok, Snapchat and YouTube testified before the Senate Subcommittee on Consumer Protection, Product Safety, and Data Security today to discuss how to protect kids online. This hearing follows Facebook whistleblower Frances Haugen‘s document leaks to the Wall Street Journal, which — among many things — exposed Facebook’s knowledge that Instagram is toxic for teenage girls. According to Facebook’s own research, 32% of teen girls said that when they felt bad about their bodies, Instagram made them feel worse.

But as the Senate tries to hold Facebook accountable for its influence on teen girls, lawmakers understand that this problem doesn’t begin and end with Mark Zuckerberg. Though the companies that testified today each have policies prohibiting content that promotes eating disorders, Senators cited evidence from constituents about teenagers on these platforms who have still suffered from illnesses like anorexia and bulimia.

“On YouTube, my office created an account as a teenager. We watched a few videos about extreme dieting and eating disorders. They were easy to find,” Senator Blumenthal (D-CT), the committee chair, said in his opening statement. He said that then, the account was fed related eating disorder content in its recommendations. “There’s no way out of this rabbit hole.”

Blumenthal’s staff also found troubling content on TikTok. The Wall Street Journal conducted an investigation like this into the platform, creating 31 bot accounts — registered as users — between the ages of 13 and 15. The publication reported that while content glorifying eating disorders is banned on TikTok, the accounts in its investigation were still served several such videos.

Senator Amy Klobuchar (D-MN) confronted Michael Beckerman, TikTok’s head of Public Policy for the Americas, asking if TikTok has stopped promoting content that glorifies eating disorders, drugs and violence to teens.

Beckerman noted that he doesn’t agree with the Wall Street Journal’s methodology for that experiment — the users were bots programmed to search for and linger on certain content — but affirmed that TikTok has made improvements to the way users can control the algorithm and see age-appropriate content on TikTok.

Beckerman said that content related to drugs violates community guidelines and that 97% of content violating policies about minor safety is removed proactively. These numbers track with a recently released transparency report, outlining information about how content was removed on the platform between April and June 2021. Per the report, 97.6% of content violating minor safety policies were removed proactively before being reported by users, and 93.9% of those videos were removed at zero views. In the category of “suicide, self-harm and dangerous acts” — which is inclusive of content glorifying eating disorders — 94.2% were removed proactively, and 81.8% of videos had zero views.

Senator Klobuchar continued by asking Beckerman if TikTok has conducted any research about how the platform might push content promoting eating disorders to teens, and if Beckerman personally had asked for any internal studies on eating disorders before testifying. He said no to both questions, but reaffirmed that TikTok works with outside experts on these issues.

Senator Tammy Baldwin (D-WI) asked each company to outline the steps each company is taking to remove “content that promotes unhealthy body image and eating disorders and direct users to supportive resources instead.” In particular, Baldwin’s question was geared toward how these companies are focusing on these issues among younger users.

Beckerman reiterated that TikTok “aggressively” removes content that promotes eating disorders and works with outside organizations to support users who might need help. He may have been referring to TikTok’s recent expansion of its mental health resources. Right after Instagram was blasted for its harm to teen girls, TikTok rolled out a brief memo about the impact of eating disorders in its Safety Center, developed in collaboration with the National Eating Disorders Association (NEDA). NEDA has a long track record of collaborating with social media platforms and worked with Pinterest to prohibit ads promoting weight loss this year.

Beckerman added that TikTok doesn’t allow ads that target people based on weight loss. The app updated its policies in September 2020 to ban ads for fasting apps and weight loss supplements, and increase restrictions on ads that promote a negative body image. This update came soon after Rolling Stone reported that TikTok was advertising fasting apps to teenage girls. Still, TikTok allows weight management product ads for users above the age of 18.

Snapchat’s Vice President of Global Public Policy Jennifer Stout answered Klobuchar’s question by saying that content promoting eating disorders violates community guidelines. Snapchat directs users who search terms like “anorexia” or “eating disorder” to expert resources that might be able to help them.

Per Snap’s ad policies, diet and weight loss ads aren’t banned, but certain content in that realm is. Ads can’t promote weight loss supplements, contain exaggerated or unrealistic claims, or show “before and after” pictures related to weight loss.

Leslie Miller, YouTube’s vice president of Government Affairs and Public Policy, also said that YouTube prohibits content glorifying eating disorders. YouTube’s ad policy says that it allows ads for weight loss as long as the imagery isn’t disturbing.

But TikTok and YouTube’s representatives both pointed out how some users can find solace on social media, for instance, in a video about how someone overcame an eating disorder. This content can be uplifting and help teens know that they’re not alone in what they’re experiencing.

Miller claimed that when users search for eating disorder content, its algorithms “raise up” content that might offer positive support to someone who is struggling with an eating disorder. She said more than 90% of content that violates guidelines is spotted through technology, but human moderators contribute as well.

Toward the end of the hearing, Senator Blumenthal circled back to the points he made in his opening statement — his office made fake TikTok accounts for teenage girls and was quickly able to find content that is supposedly banned from the platform.

“How do you explain to parents why TikTok is inundating their kids with these kinds of videos of suicide, self-injury and eating disorders?” Senator Blumenthal asked.

“I can’t speak to what the examples were from your staff, but I can assure you that’s not the normal experience that teens or people that use TikTok would get,” Beckerman said.

Though the representatives from TikTok, Snapchat and YouTube used their ad policy and content moderation guidelines as evidence that their companies are moving in the right direction, Senators still seemed hesitant about how cooperative the platforms would be in passing legislation to make social media safer for children.

As the hearing closed, Senator Blumenthal observed that he wouldn’t be taking the day’s testimony at face value. “The time for platitudes and bromides is over,” Blumenthal said.",Social Media,TechCrunch,https://techcrunch.com/2021/10/26/lawmakers-confront-tiktok-snapchat-and-youtube-about-eating-disorder-content/,"The Senate Subcommittee on Consumer Protection, Product Safety, and Data Security heard testimony from representatives from TikTok, Snapchat and YouTube concerning how to protect kids online and how their platforms have been known to promote unhealthy body image and eating disorders in teens. It was also revealed that content glorifying eating disorders and other dangerous activities is easy to find on these",Social Norms & Relationships
330,Facebook Sketches a Future With a Diminished News Feed,"For most of the past year, Mark Zuckerberg has been trying to convince the world that Facebook was fast becoming a very different company—one that accepted its enormous role in shaping public opinion worldwide and would spend what it took to exercise its power responsibly. Many still have trouble believing him, and it's easy to understand why.

Every time it seems as if Facebook is making progress against the hackers, spammers, and trolls hell-bent on turning it into a cesspool of hate speech and fake news, new problems surface. It's gone to huge lengths, for example, to tout its work to bring more transparency and reduced fraud to political ads. But Tuesday, just before Facebook released its results, Vice News reported that it had attempted to place ads on Facebook while posing as each of the 100 US senators. Facebook approved them all.

Screwups like this have felt like a twice-a-month event for the past year, seemingly unending. Zuckerberg and his executives and engineers at Facebook are some of the smartest, most experienced and well-funded talent in the world tackling these problems. Yet they keep looking like villagers who are using their fingers to keep their dam from leaking.

Holding Facebook accountable is critical. But that discussion obscures an equally important transformation going on at the company: For most of its existence, users associated Facebook with News Feed—that scrolling list of stories and ads that appears when you first visit the site. That's changing, and fast, Zuckerberg said as the company reported third-quarter financial results on Tuesday.

He said traffic to new platforms like Stories on both Facebook and Instagram, which Facebook owns, and its messaging platforms WhatsApp and Messenger, is growing so fast that they are diluting News Feed's cultural force and may eventually challenge it as the company’s dominant revenue generator. ""People feel more comfortable being themselves when their content is seen by smaller groups, and (the posts) don’t stick around forever, "" Zuckerberg said, referring to two Stories features he said users like.

Facebook is still a financial juggernaut. It reported profit of $5.1 billion in the third quarter on revenue of $13.7 billion. But as it telegraphed when it reported earnings three months ago, its growth has slowed precipitously. Investors had gotten used to double-digit percentage growth in everything at Facebook every quarter. But in the third quarter, Facebook’s profit increased 9 percent over the same quarter a year earlier, the slowest rate of increase in more than three years; moreover, third-quarter profit increased a scant 1 percent from the second quarter.

Content View Iframe URL

Why is this happening? In addition to the billions it is spending to better police its platforms, more of its users' time is being spent outside of News Feed. Facebook doesn't yet show as many ads on those newer platforms, so those eyeballs generate less money. Investors seemed prepared for, and willing to accept, the explanation: Facebook shares rose 3 percent in after-hours trading.

Content View Iframe URL

Zuckerberg said the self-imposed splintering of Facebook’s audience is as big as the shift the company underwent about six years ago when it realized users primarily wanted to interact with Facebook on a smartphone, not a laptop or desktop. ""If the last 10 years at Facebook have been about connecting friends and family, the next 10 will be about building communities,"" he said. He talked about how Facebook would soon be a bigger player in helping people find jobs, buy and sell things, create events, fund-raise and find a date. He said that the shift was happening so fast that in the coming months user engagement with Facebook Stories would surpass that of News Feed.

He also said now that users were gravitating to video on Facebook’s Watch platform, rather than News Feed, that he'd changed his position about video's negative impact on the Facebook user experience. In News Feed, he said, too much video promoted passivity and did not promote social behaviors. He said that when video is distributed in its own section—where users actively seek it out—it did not have that impact as much.

It all sounded positively prosaic coming from the man who previously sketched grand—now despised—visions like ""making the world more open and connected"" and ""move fast and break things."" That's a good thing, because the new Facebook won’t matter much if it can't convince people that the old Facebook—the place where hackers, spammers and trolls run wild—is dead.

More Great WIRED Stories",Social Media,WIRED,https://www.wired.com/story/facebook-sketches-future-with-diminished-news-feed/,"The consequences of Social Media are becoming more apparent as hackers, spammers, and trolls use the platforms to spread hate speech and fake news. Facebook is attempting to mitigate these issues, however, mistakes still happen, resulting in a lack of trust in the company.",Security & Privacy
331,Mark Zuckerberg still won’t address the root cause of Facebook’s misinformation problem,"As Hao wrote, a study from New York University of partisan publishers’ Facebook pages found “those that regularly posted political misinformation received the most engagement in the lead-up to the 2020 US presidential election and the Capitol riots.”

Zuckerberg, after saying that “a bunch of inaccurate things” about Facebook’s incentives for allowing and amplifying misinformation and polarizing content had been shared at the hearing by members of Congress, added:

“People don’t want to see misinformation or divisive content on our services. People don’t want to see clickbait and things like that. While it may be true that people may be more likely to click on it in the short term, it’s not good for our business or our product or our community for it to be there.”

His answer is a common Facebook talking point and skirts the fact that the company has not undertaken a centralized, coordinated effort to examine and reduce the way its recommendation systems amplify misinformation. To learn more, read Hao’s reporting.

Zuckerberg’s comments came during the House Committee on Energy and Commerce hearing on disinformation, where members of Congress asked Zuckerberg, Google CEO Sundar Pichai, and Twitter CEO Jack Dorsey about the spread of misinformation about the US election in November, the January 6 attack on the Capitol building, and covid vaccines, among other things.

As has become common in these hearings, conservative legislators also questioned the CEOs about perceived anti-conservative bias on their platforms, a longtime right-wing claim that data doesn’t support.",Social Media,MIT,https://www.technologyreview.com/2021/03/25/1021293/mark-zuckerberg-root-cause-facebook-misinformation/,"The hearing focused on the consequences of Social Media platforms such as Facebook, Google, and Twitter allowing and amplifying misinformation and polarizing content, which has been found to receive the most engagement leading up to the 2020 US election and the Capitol riots. This has been a major concern, as it has the potential to lead to confusion and unrest among users","Information, Discourse & Governance"
332,How the next generation is reshaping political discourse,"The ways young people use such tools are already changing the look of political campaigns and grassroots organizing. Many nonprofits and other groups are now recruiting more and more young people to play larger roles within their organizations.

The key to making sure young people stay engaged is including them in more political conversations, says Beth Simone Noveck, director of New York University’s Governance Lab and New Jersey’s first chief innovation officer. Noveck leads a project called CrowdLaw, which studies ways lawmakers can use technology to incorporate the opinions of citizens, especially young ones, into the legislative process. She also heads a GovLab program called ReinventED, which centers on using technology to engage students, educators, and caregivers, especially from marginalized communities, in efforts to solve education issues.

Exercises completed by ReinventED show that students’ priorities even in the midst of a pandemic lean toward solving real-world problems and improving nontraditional academic subjects. Policymakers, on the other hand, are more concerned with public health and school reopening plans.

At a time when our trust in government is nearing historic lows, the future of political participation is at stake.

“The people who are most expert in education—mainly students and teachers, and to a lesser extent the parents of those students—are rarely, if ever, consulted in how we design our schools,” Noveck says. “My hope is that by using tools like this, by laying bare what people really care about, that can help to change the direction of what we’re focusing on.”

Digital platforms, however, may be a double-edged sword. Participating in online movements may not translate into offline engagement—some experts warn it could have the opposite effect. “On social media, you can get a burst of interest, sometimes a burst of activity, because it’s so easy to feel like you’ve participated just by clicking a link or retweeting something or using a hashtag,” says Nicholas Carr, a sociology professor at Williams College. “What’s unclear is whether social media will help or hurt the ability of activists to sustain interests in a long-term campaign of change.”

Instead, the result may be “slacktivism,” a term coined during the rise of the internet for the practice of publicly supporting a cause in ways that take little effort, often to make yourself look good. “That can diminish or even demean the seriousness of political discourse in a way that can kind of hinder our ability to solve big problems,” says Carr.

People who engage in this performative activism are still spreading political messages, though, says William Golub, a junior at Stanford University who volunteered with the texting team on Joe Biden’s presidential campaign last year. “I think that there certainly are people who will just post about something on social media and that’s the end of the chain, but lots of those people are people who wouldn’t have done anything at all,” he says.",Social Media,MIT,https://www.technologyreview.com/2021/06/30/1026350/gen-z-reshaping-political-discourse-digital-democracy/,"Social media can lead to ""slacktivism,"" where people publicly support a cause in ways that take little effort, often to make themselves look good, potentially diminishing the seriousness of political discourse and hindering our ability to solve big problems.",Discourse & Governance
333,"Social media firms should face fines for hate speech failures, urge UK MPs – TechCrunch","Social media giants Facebook, YouTube and Twitter have once again been accused of taking a “laissez-faire approach” to moderating hate speech content on their platforms.

This follows a stepping up of political rhetoric against social platforms in recent months in the UK, following a terror attack in London in March — after which Home Secretary Amber Rudd called for tech firms to do more to help block the spread of terrorist content online.

In a highly critical report looking at the spread of hate, abuse and extremism on Facebook, YouTube and Twitter, a UK parliamentary committee has suggested the government looks at imposing fines on social media forms for content moderation failures.

It’s also calling for a review of existing legislation to ensure clarity about how the law applies in this area.

“Social media companies currently face almost no penalties for failing to remove illegal content. There are too many examples of social media companies being made aware of illegal material yet failing to remove it, or to do so in a timely way. We recommend that the government consult on a system of escalating sanctions to include meaningful fines for social media companies which fail to remove illegal content within a strict timeframe,” the committee writes in the report.

Last month, the German government backed a draft law which includes proposals to fine social media firms up to €50 million if they fail to remove illegal hate speech within 24 hours after a complaint is made.

A Europe Union-wide Code of Conduct on swiftly removing hate speech, which was agreed between the Commission and social media giants a year ago, does not include any financial penalties for failure — but there are signs some European governments are becoming convinced of the need to legislate to force social media companies to improve their content moderation practices.

The UK Home Affairs committee report describes it as “shockingly easy” to find examples of material intended to stir up hatred against ethnic minorities on all three of the social media platforms it looked at for the report.

It urges social media companies to introduce “clear and well-funded arrangements for proactively identifying and removing illegal content — particularly dangerous terrorist content or material related to online child abuse”, calling for similar co-operation and investment to combat extremist content as the tech giants have already put into collaborating to tackle the spread of child abuse imagery online.

The committee’s investigation, which started in July last year following the murder of a UK MP by a far right extremist, was intended to be more wide-ranging. However, because the work was cut short by the UK government calling an early general election the committee says it has published specific findings on how social media companies are addressing hate crime and illegal content online — having taken evidence for this from Facebook, Google and Twitter.

“It is very clear to us from the evidence we have received that nowhere near enough is being done. The biggest and richest social media companies are shamefully far from taking sufficient action to tackle illegal and dangerous content, to implement proper community standards or to keep their users safe. Given their immense size, resources and global reach, it is completely irresponsible of them to fail to abide by the law, and to keep their users and others safe,” it writes.

“If social media companies are capable of using technology immediately to remove material that breaches copyright, they should be capable of using similar content to stop extremists re-posting or sharing illegal material under a different name. We believe that the government should now assess whether the continued publication of illegal material and the failure to take reasonable steps to identify or remove it is in breach of the law, and how the law and enforcement mechanisms should be strengthened in this area.”

The committee flags multiple examples where it says extremist content was reported to the tech giants but these reports were not acted on adequately — calling out Google, especially, for “weakness and delays” in response to reports it made of illegal neo-Nazi propaganda on YouTube.

It also notes the three companies refused to tell it exactly how many people they employ to moderate content, and exactly how much they spend on content moderation.

The report makes especially uncomfortable reading for Google with the committee directly accusing it of profiting from hatred — arguing it has allowed YouTube to be “a platform from which extremists have generated revenue”, and pointing to the recent spate of advertisers pulling their marketing content from the platform after it was shown being displayed alongside extremist videos. Google responded to the high-profile backlash from advertisers by pulling ads from certain types of content.

“Social media companies rely on their users to report extremist and hateful content for review by moderators. They are, in effect, outsourcing the vast bulk of their safeguarding responsibilities at zero expense. We believe that it is unacceptable that social media companies are not taking greater responsibility for identifying illegal content themselves,” the committee writes.

“If social media companies are capable of using technology immediately to remove material that breaches copyright, they should be capable of using similar content to stop extremists re-posting or sharing illegal material under a different name. We believe that the government should now assess whether the continued publication of illegal material and the failure to take reasonable steps to identify or remove it is in breach of the law, and how the law and enforcement mechanisms should be strengthened in this area.”

The committee suggests social media firms should have to contribute to the cost to the taxpayer of policing their platforms — pointing to how football teams are required to pay for policing in their stadiums and the immediate surrounding areas under UK law as an equivalent model.

It is also calling for social media firms to publish quarterly reports on their safeguarding efforts, including —

analysis of the number of reports received on prohibited content

how the companies responded to reports

what action is being taken to eliminate such content in the future

“It is in everyone’s interest, including the social media companies themselves, to find ways to reduce pernicious and illegal material,” the committee writes. “Transparent performance reports, published regularly, would be an effective method to drive up standards radically and we hope it would also encourage competition between platforms to find innovative solutions to these persistent problems. If they refuse to do so, we recommend that the government consult on requiring them to do so.”

The report, which is replete with pointed adjectives like “shocking”, “shameful”, “irresponsible” and “unacceptable”, follows several critical media reports in the UK which highlighted examples of moderation failures on social media platforms, and showed extremist and paedophilic content continuing to be spread on social media platforms.

Responding to the committee’s report, a YouTube spokesperson told us: “We take this issue very seriously. We’ve recently tightened our advertising policies and enforcement; made algorithmic updates; and are expanding our partnerships with specialist organisations working in this field. We’ll continue to work hard to tackle these challenging and complex problems”.

In a statement, Simon Milner, director of policy at Facebook, added: “Nothing is more important to us than people’s safety on Facebook. That is why we have quick and easy ways for people to report content, so that we can review, and if necessary remove, it from our platform. We agree with the Committee that there is more we can do to disrupt people wanting to spread hate and extremism online. That’s why we are working closely with partners, including experts at Kings College, London, and at the Institute for Strategic Dialogue, to help us improve the effectiveness of our approach. We look forward to engaging with the new Government and parliament on these important issues after the election.”

Nick Pickles, Twitter’s UK head of public policy, provided this statement: “Our Rules clearly stipulate that we do not tolerate hateful conduct and abuse on Twitter. As well as taking action on accounts when they’re reported to us by users, we’ve significantly expanded the scale of our efforts across a number of key areas. From introducing a range of brand new tools to combat abuse, to expanding and retraining our support teams, we’re moving at pace and tracking our progress in real-time. We’re also investing heavily in our technology in order to remove accounts who deliberately misuse our platform for the sole purpose of abusing or harassing others. It’s important to note this is an ongoing process as we listen to the direct feedback of our users and move quickly in the pursuit of our mission to improve Twitter for everyone.”

The committee says it hopes the report will inform the early decisions of the next government — with the UK general election due to take place on June 8 — and feed into “immediate work” by the three social platforms to be more pro-active about tackling extremist content.

Commenting on the publication of the report yesterday, Home Secretary Amber Rudd told the BBC she expected to see “early and effective action” from the tech giants.",Social Media,TechCrunch,https://techcrunch.com/2017/05/02/social-media-firms-should-face-fines-for-hate-speech-failures-urge-uk-mps/,"The UK Home Affairs committee report has exposed the laissez-faire approach to moderating hate speech content on Facebook, YouTube, and Twitter, and is calling for the government to impose fines on these social media companies for content moderation failures.",Equality & Justice
334,White House refuses to endorse the ‘Christchurch Call’ to block extremist content online – TechCrunch,"The United States will not join other nations in endorsing the “Christchurch Call” — a global statement that commits governments and private companies to actions that would curb the distribution of violent and extremist content online.

“While the United States is not currently in a position to join the endorsement, we continue to support the overall goals reflected in the Call. We will continue to engage governments, industry, and civil society to counter terrorist content on the Internet,” the statement from the White House reads.

The “Christchurch Call” is a non-binding statement drafted by foreign ministers from New Zealand and France meant to push internet platforms to take stronger measures against the distribution of violent and extremist content. The initiative originated as an attempt to respond to the March killings of 51 Muslim worshippers in Christchruch and the subsequent spread of the video recording of the massacre and statements from the killer online.

By signing the pledge, companies agree to improve their moderation processes and share more information about the work they’re doing to prevent terrorist content from going viral. Meanwhile, government signatories are agreeing to provide more guidance through legislation that would ban toxic content from social networks.

Already, Twitter, Microsoft, Facebook and Alphabet — the parent company of Google — have signed on to the pledge, along with the governments of France, Australia, Canada and the United Kingdom.

The “Christchurch Call” is consistent with other steps that government agencies are taking to address how to manage the ways in which technology is tearing at the social fabric. Members of the Group of 7 are also meeting today to discuss broader regulatory measures designed to combat toxic combat, protect privacy and ensure better oversight of technology companies.

For its part, the White House seems more concerned about the potential risks to free speech that could stem from any actions taken to staunch the flow of extremist and violent content on technology platforms.

“We continue to be proactive in our efforts to counter terrorist content online while also continuing to respect freedom of expression and freedom of the press,” the statement reads.”Further, we maintain that the best tool to defeat terrorist speech is productive speech, and thus we emphasize the importance of promoting credible, alternative narratives as the primary means by which we can defeat terrorist messaging.”

Signatories are already taking steps to make it harder for graphic violence or hate speech to proliferate on their platforms.

Last night, Facebook introduced a one-strike policy that would ban users who violate its live-streaming policies after one infraction.

The Christchurch killings are only the latest example of how white supremacist hate groups and terrorist organizations have used online propaganda to create an epidemic of violence at a global scale. Indeed, the alleged shooter in last month’s attack on a synagogue in Poway, Calif., referenced the writings of the Christchurch killer in an explanation for his attack, which he published online.

Critics are already taking shots at the White House for its inability to add the U.S. to a group of nations making a non-binding commitment to ensure that the global community can #BeBest online.",Social Media,TechCrunch,https://techcrunch.com/2019/05/15/white-house-rejects-calls-to-endorse-the-christchurch-call-to-block-extremist-content-online/,"The spread of violent and extremist content online is a growing concern, with the Christchurch Call providing a non-binding statement by which governments and private companies commit to actions to curb its distribution. The United States has not joined the call, citing the potential risks to free speech, and instead emphasizing the importance of promoting alternative, non-violent narratives",Security & Privacy
335,Manipulating an Indian politician’s tweets is worryingly easy to do – TechCrunch,"Here’s a concerning story from India, where the upcoming election is putting the use of social media in the spotlight.

While the Indian government is putting Facebook, Google and other companies under pressure to prevent their digital platforms from being used for election manipulation, a journalist has demonstrated just how easy it is to control the social media messages published by government ministers.

Pon Radhakrishnan, India’s minister of state for finance and shipping, published a series of puzzling tweets today after Pratik Sinha, a co-founder of fact-checking website Alt News, accessed a Google document of prepared statements and tinkered with the content.

Among the statements tweeted out, Radhakrishnan said Prime Minister Modi’s government had failed the middle classes and had not made development on improving the country’s general welfare. Sinha’s edits also led to the official BJP Assam Pradesh account proclaiming that the prime minister had destroyed all villages and made women slaves to cooking.

And then you can get a Union Minister to tweet that ""working for the middle class is low on the agenda of Modi Govt"" CC @PonnaarrBJP 2/n pic.twitter.com/79WH2r73C5 — Pratik Sinha (@free_thinker) February 13, 2019

These are the opposite of the partisan messages that the accounts intended to send.

The messages were held in an unlocked Google document that contained a range of tweets compiled for the Twitter accounts. Sinha managed to access the document and doctor the messages into improbable statements — which he has done before — in order to show the shocking lack of security and processes behind the social media content.

How do you get a Union Minister to tweet what you want? Well, you go and edit the trending document made by BJP IT cell, and then you control what they tweet. Thread. Here's the video of this morning when their trending document got automagically updated :-) 1/n pic.twitter.com/6DLwDPg2CV — Pratik Sinha (@free_thinker) February 13, 2019

Sinha said he made the edits “to demonstrate how dangerous this is from the security standpoint for this country.”

“I had fun but it could have disastrous consequences,” he told TechCrunch in a phone interview. “This is a massive security issue from the point of view of a democracy.”

Sinha said he was able to access the document — which was not restricted or locked to prevent changes — through a WhatsApp group that is run by members of the party. Declining to give specifics, he said he had managed to infiltrate the group and thus gain access to a flow of party and government information and, even more surprisingly, get right into the documents and edit them.

What’s equally as stunning is that, even with the message twisted 180 degrees, their content didn’t raise an alarm. The tweets were still loaded and published without any realization. It was only after Sinha went public with the results that Radhakrishnan and BJP Assam Pradesh account begin to delete them.

The Indian government is rightly grilling Facebook and Google to prevent its platform being abused around the election, as evidence suggested happened in the U.S. presidential election and the U.K.’s Brexit vote, but members of the government themselves should reflect on the security of their own systems, too. It would be too easy for these poor systems to be exploited.",Social Media,TechCrunch,https://techcrunch.com/2019/02/13/india-politician-tweets/,"The poor security of Indian government's Social Media systems could easily be exploited to manipulate the upcoming election, as demonstrated by a journalist who accessed and edited a Google document of prepared statements for government ministers. This highlights the importance of the government grilling Facebook and Google to prevent its platform being abused.",Security & Privacy
336,Twitter claims more progress on squeezing terrorist content – TechCrunch,"Twitter has put out its latest Transparency Report providing an update on how many terrorist accounts it has suspended on its platform — with a cumulative 1.2 million+ suspensions since August 2015.

During the reporting period of July 1, 2017 through December 31, 2017 — for this, Twitter’s 12th Transparency Report — the company says a total of 274,460 accounts were permanently suspended for violations related to the promotion of terrorism.

“This is down 8.4% from the volume shared in the previous reporting period and is the second consecutive reporting period in which we’ve seen a drop in the number of accounts being suspended for this reason,” it writes. “We continue to see the positive, significant impact of years of hard work making our site an undesirable place for those seeking to promote terrorism, resulting in this type of activity increasingly shifting away from Twitter.”

Six months ago the company claimed big wins in squashing terrorist activity on its platform — attributing drops in reports of pro-terrorism accounts then to the success of in-house tech tools in driving terrorist activity off its platform (and perhaps inevitably rerouting it towards alternative platforms — Telegram being chief among them, according to experts on online extremism).

At that time Twitter reported a total of 299,649 pro-terrorism accounts had been suspended — which it said was a 20 per cent drop on figures reported for July through December 2016.

So the size of the drops are also shrinking. Though it’s suggesting that’s because it’s winning the battle to discourage terrorists from trying in the first place.

For its latest reporting period, ending December 2017, Twitter says 93% of the accounts were flagged by its internal tech tools — with 74% of those also suspended before their first tweet, i.e. before they’d been able to spread any terrorist propaganda.

Which means that around a quarter of the pro-terrorist accounts did manage to get out at least one terror tweet.

This proportion is essentially unchanged since the last report period (when Twitter reported suspending 75% before their first tweet) — so whatever tools it’s using to automate terror account identification and blocking appear to be in a steady state, rather than gaining in ability to pre-filter terrorist content.

Twitter also specifies that government reports of violations related to the promotion of terrorism represent less than 0.2% of all suspensions in the most recent reporting period — or 597 to be exact.

As with its prior transparency report, a far larger number of Twitter accounts are being reported by governments for “abusive behavior” — which refers to long-standing problems on Twitter’s platform such as hate speech, racism, misogyny and trolling.

And in December a Twitter policy staffer was roasted by UK MPs during a select committee session after the company was again shown failing to remove violent, threatening and racist tweets — which committee staffers had reported months earlier in that case.

Twitter’s latest Transparency Report specifies that governments reported 6,254 Twitter accounts for abusive behavior — yet the company only actioned a quarter of these reports.

That’s still up on the prior reporting period, though, when it reported actioning a paltry 12% of these type of reports.

The issue of abuse and hate speech on online platforms generally has rocketed up the political agenda in recent years, especially in Europe — where Germany now has a tough new law to regulate takedowns.

Platforms’ content moderation policies certainly remain a bone of contention for governments and lawmakers.

Last month the European Commission set out a new rule of thumb for social media platforms — saying it wants them to take down illegal content within an hour of it being reported.

This is not legislation yet, but the threat of EU-wide laws being drafted to regulate content takedowns remains a discussion topic — to encourage platforms to improve performance voluntarily.

Where terrorist content specifically is concerned, the Commission has also been pushing for increased used by tech firms of what it calls “proactive measures”, including “automated detection”.

And in February the UK government also revealed it had commissioned a local AI firm to build an extremist content blocking tool — saying it could decide to force companies to use it.

So political pressure remains especially high on that front.

Returning to abusive content, Twitter’s report specifies that the majority of the tweets and accounts reported to it by governments which it did remove violated its rules in the following areas: impersonation (66%), harassment (16%), and hateful conduct (12%).

This is an interesting shift on the mix from the last reported period when Twitter said content was removed for: harassment (37%), hateful conduct (35%), and impersonation (13%).

It’s difficult to interpret exactly what that development might mean. One possibility is that impersonation could cover disinformation agents, such as Kremlin bots, which Twitter has being suspending in recent months as part of investigations into election interference — an issue that’s been shown to be a problem across social media, from Facebook to Tumblr.

Governments may also have become more focused on reporting accounts to Twitter that they believe are wrappers for foreign agents to spread false information to try to meddle with democratic processes.

In January, for example, the UK government announced it would be setting up a civil service unit to combat state-led disinformation campaigns.

And removing an account that’s been identified as a fake — with the help of government intelligence — is perhaps easier for Twitter than judging whether a particular piece of robust speech might have crossed the line into harassment or hate speech.

Judging the health of conversations on its platform is also something the company recently asked outsiders to help it with. So it doesn’t appear overly confident in making those kind of judgement calls.",Social Media,TechCrunch,https://techcrunch.com/2018/04/05/twitter-transparency-report-12/,"Social Media platforms have been struggling with issues of hate speech, racism, trolling and terrorism on their sites, leading to government pressure to take down illegal content quickly. This has prompted the use of automated tools to identify and block terrorist accounts and the proposal of further legislation to regulate content takedowns.",Security & Privacy
337,"Facebook denies its algorithms are a problem, but launches a tool to more easily view a non-algorithmic News Feed – TechCrunch","Following years of backlash over its algorithms and their ability to push people to more extreme content, which Facebook continues to deny, the company today announced it would give its users new tools to more easily switch over to non-algorithmic views of their News Feed. This includes the recently launched “Favorites,” which shows you posts from up to 30 of your favorite friends and Pages, as well as the “Most Recent” view, which shows posts in chronological order. It also introduced new controls for adjusting who can comment on your posts, and other changes.

The features themselves aren’t entirely new, in some cases, but they’ve been made easier to get to with the addition of a Feed Filter Bar on mobile for changing the view of the News Feed, and an option menu on your posts to control who can comment.

The “Most Recent” view of the News Feed has long existed but has been buried in the extended “more” menu (the three-bar hamburger icon) on the Facebook mobile app. It’s not as useful as it sounds because it shows you all the posts from both friends and Pages in a single chronological view. If you’ve been on Facebook for many years, then you’ve probably “Liked” a number of Facebook Pages for brands, businesses and public figures. These Pages tend to post with more frequency than your friends, so the feed has become largely a long scroll through Page updates.

However, if you still prefer the “Most Recent” view, the Feed Filter Bar will give you a tool to easier switch back and forth between this and other views. The feature will launch on Android first, then roll out to iOS.

Meanwhile, Facebook has offered a way to prioritize who you see in your News Feed through a “See First” setting, but the newer “Favorites” feature rebrands this effort and gives you a single destination under Settings to select and deselect your Favorites, including favorite Pages.

The updated commenting controls are a new take on a habit many Facebook users have already adopted — when they share a post only to a given audience, like family or friends, while excluding other groups like work colleagues or even specific people. Now, users will have the option to instead share their posts but control who can engage in conversations. Public figures, for example, may choose to adopt the feature to restrict their audience to only those brands and profiles they’ve tagged.

Facebook says it will also show more context around suggestions it displays in the News Feed with its “Why am I seeing this?” feature that will explain how its algorithmic suggestions work. It says several factors may be at work here, in terms of what’s shown and why — including your location, whether you or people like you have engaged with related topics, groups or Pages, and more.

The changes arrive at a time when Facebook, along with other tech giants, has come under fire for its role in spreading misinformation leading to deadly events, like the storming of the U.S. Capitol, and serious public health crises, like vaccine hesitancy during a pandemic. Facebook CEO Mark Zuckerberg last week testified before the House’s Subcommittee on Communications and Technology about its failures to remove dangerous misinformation and its allowing of extremists to become more radicalized and to organize online.

Facebook’s official position, however, is that it doesn’t play a role in directing people towards problematic content — they seek it out. And people’s News Feeds are only a reflection of their own choices, in that way.

These thoughts and more were detailed today by Nick Clegg, VP of Global Affairs for Facebook, where he insists personalization algorithms are common across tech companies — Amazon and Netflix use them, too, for instance. And ranking simply makes what’s most relevant to the user appear first — effectively blaming users for the problems here. He also throws back the decisions to be made around Facebook’s role in misinformation peddling to the lawmakers, adding: “It would clearly be better if these [content] decisions were made according to frameworks agreed by democratically accountable lawmakers.”",Social Media,TechCrunch,https://techcrunch.com/2021/03/31/facebook-denies-its-algorithms-are-a-problem-but-launches-a-tool-to-more-easily-view-a-non-algorithmic-news-feed/,"Facebook's use of personalization algorithms has been criticized for pushing people to more extreme content and for its role in spreading misinformation, leading to serious public health crises and dangerous events.","Information, Discourse & Governance"
338,Twitter DMs now have emoji reactions – TechCrunch,"Twitter is pouring a little more fuel on the messaging fire. It has added a heart+ button to its direct messaging interface, which lets users shortcut to a pop-up menu of seven emoji reactions so they can quickly express how they’re feeling about a missive.

Emoji reactions can be added to text or media messages — either via the heart+ button or by double-tapping on the missive to bring up the reaction menu.

The social network teased the incoming tweak a few hours earlier in a knowing tweet about sliding into DMs that actually revealed the full lineup of reaction emojis — which, in text form, can be described as: Crying lol, shocked/surprised, actually sad, heart, flame, thumb-up and thumb-down.

So instead of a smiley face Twitter users are being nudged toward an on-brand-message Twitter heart, in keeping with its longstanding pick for a pleasure symbol.

The flame is perhaps slightly surprising for a company that has publicly professed to wanting to improve the conversational health of its platform.

If it’s there to stand in for appreciation, a clap emoji could surely have done the trick. Whereas flame wars aren’t typically associated with constructive speech. But — hey — the flame icon does catch the eye…

Say more with new emoji reactions for Direct Messages! To add a reaction, click the ❤️➕ icon that appears when you hover over the message on web or double tap the message on mobile and select an emoji from the pop-up. For more about DM reactions: https://t.co/sdMumGDBYl https://t.co/QxMVmGt8eY — Twitter Support (@TwitterSupport) January 22, 2020

Twitter is late to this extroverted party. Rival messaging platforms such as Apple iMessage and Facebook Messenger have had emoji reactions for years, whereas Twitter kept things relatively minimal and chat-focused in its DM funnel — to its credit (at least if you value the service as, first and foremost, an information network).

So some might say Twitter jumping on the emoji reaction bandwagon now is further evidence it’s trying to move closer to rivals like Facebook as a product. (See also: Last year’s major desktop product redesign — which has been compared in look and feel to the Facebook News Feed.)

But if so, this change is at least a relatively incremental one.

Twitter users have also, of course, always been able to react to an incoming DM by sending whatever emoji or combination of emoji they prefer as a standard reply. Though now lazy thumbs have a shortcut to emote — so long as they’re down with Twitter’s choice of icons.

In an FAQ about the new DM emoji reactions, Twitter notes that emoting will by default send a notification to all conversation participants “any time a new reaction is added to a message.”

So, yes, there’s attention-spamming potential aplenty here…

Adjust your notification and DM settings accordingly.

You can only choose one reaction per missive. Each symbol is displayed under the message/media with a count next to it — to allow for group tallies to be totted up.

NB: Clicking on another symbol will swap out the earlier one — generating, er, more notification spam. And really annoying people could keep flipping their reaction to generate a real-time emoji streaming game of notification hell (hi-growth hackers!) with folks they’ve been DMing. So that’s another good reason to lock down your Twitter settings.

Users still running an older version of Twitter’s apps that don’t support message reactions will see a standard text emoji message per reaction sent (see examples below). This kinda confusingly makes it look like the reaction sender has actually been liking/flaming their own stuff. So all the more reason to not be spammy about emoji.",Social Media,TechCrunch,https://techcrunch.com/2020/01/23/twitter-dms-now-have-emoji-reactions/,"The main undesirable consequence of social media discussed here is the potential for attention-spamming, as demonstrated by the addition of emoji reactions to direct messaging on Twitter. These reactions send a notification to all conversation participants each time a new one is added, making it possible to annoy or overwhelm people with too many notifications. To avoid this, users",Social Norms & Relationships
339,The rise of 4chan politics – TechCrunch,"We are entering an era when our digital lives spill out uncontrollably into our “real” lives. Whether or not you believe these two are separate, I would wager that the mass of humanity still sees a wall between Facebook and talking face-to-face at a PTA meeting. But a few people don’t.

It’s these people – people emboldened by the seeming anonymity of the Internet and the ability for things that happen there to have real-world consequences – that have hijacked national discourse. They are the hackers who sway elections, who break civil contracts, who leak pictures of us naked. They are the eggs and Tumblr-posters who call each other – and others – the worst of slurs. They are the ones who sit behind their keyboards and rail at the world or, worse, pull the strings to which they have access from their secret places.

The folks spreading outrage on the Internet – the outraged Facebookers, the alt-right, the vociferous VCs who don’t know when to shut up – are hacking the system. Hackers aren’t what they look like in the movies and they aren’t 400 pound men in their basements. They are people who have been given a megaphone and prefer to burp and curse and shout into it rather than help. They are the ones who yell “Jump” to the man on the bridge because of his implied weakness.

I watched the rise of Pinterest, Facebook, and Twitter with great interest. Each one of these groups tried something different. Pinterest hoped that people would be civil by banning violence and porn. They are floundering at best. Facebook hoped that people would be kind because they had to use their real names. This did not happen. Twitter let anyone say anything, primarily because it was too hard to police the millions of tweets swirling through the database every day. That also backfired.

Why? Because while the rest of us are sharing photos of cats, posting updates about our vacations, and Tweeting jokes there is a small contingent that is using these tools to spread disinformation and vitriol. I cede that our right to free speech is sacred but that doesn’t mean we have to be assholes.

And when free speech warriors are called out for their actions they lash out. They create massive botnets designed to force people out of the discussion. They SWAT you, they hack you, they shut down the means of communication through denial of service attacks. Not because it is right or because it makes sense but because it’s fun, it’s lulz.

I call this 4chan politics primarily because the tactics used by Anonymous are now being hijacked by people with less moral fiber. Anonymous tried to attack evil. Those that use Anonymous’ tricks are doing evil.

Why are so many people emboldened to speak their minds about race, gender, and equality? Because they feel they have been given the power to do so not by any political change but by the very tools that brought about that political change. Further, because most people don’t see the overlap between online and “real life” it is not a pressing problem. But it is.

I recall an interview with a reporter who was being actively trolled on Twitter. He turned off his account for a while and someone asked him to turn it back on to see if it was still happening. In an instant vitriol against him and his family started flowing onto his feed.

Perhaps he’s being too sensitive. It is, after all, just a social network. But these tools are powerful and important and to hijack them is to hijack a mode of discourse. If someone stood outside this reporter’s house and called his wife a whore the police would be called and we’d think that person insane. If it happens online we think it doesn’t matter.

This is an era of 4chan politics because the digital is now affecting the real. The things we say have weight and, like some strange game of Tron, our physical lives are being sucked into the computer. Once we could cancel our subscription to the daily paper if we didn’t like the news. How can we cancel our subscription to the defining medium of the 21st century?

Humans talk to each other. Monsters rage at each other. Let’s pick a side.",Social Media,TechCrunch,https://techcrunch.com/2016/12/21/the-rise-of-4chan-politics/,"The consequences of social media abuse are becoming increasingly tangible, with trolls and hackers using the platform to spread misinformation, vitriol, and cyber-attacks. This has created an era of “4chan politics” in which our digital lives are spilling out uncontrollably into our real lives, and which is having damaging effects on our society.",Discourse & Governance
340,Why Firing Comey Only Amplified Trump's Russia Problem,"If President Trump had hoped to dampen the sense of intrigue swirling around his campaign's alleged ties to Russia by firing FBI director James Comey, his plan has royally backfired. Comey's dismissal has instead sent Russia hurtling back to the center of the national conversation. For a president usually so savvy about the uses of social media, it's a rare misstep. Trump has often displayed a knack for redirecting—or at least confusing—the country's attention by flooding the zone with noise. In firing Comey, the president tried to cut off the signal. At a time when scandal travels at the speed of Twitter, such a move only serves to amplify whatever you're hoping to obscure.

As soon as the unceremonious ouster was announced, comparisons to President Richard Nixon's firing of lead Watergate investigator Archibald Cox overran social media faster than you can say ""Saturday Night Massacre."" Senator John McCain and other leading Republicans took to Twitter to denounce the decision, calling for a special congressional committee to investigate Russia's interference in the 2016 election. Reports from inside the White House described press secretary Sean Spicer hiding behind a bush and delivering furtive messages to the press in utter darkness—antics that, utterly unsurprisingly, attracted the glaring light of social media mockery.

In an interview following the firing, Trump administration spokesperson Sarah Huckabee Sanders told Fox's Tucker Carlson, ""It's time to move on"" from the Russia investigation. But Comey's firing has only ratcheted up pressure to pursue the investigation all the more vigorously.

""Imagine I told you a secret, and I asked you not to tell anybody else, the first thing you'd do is go tell someone else,"" says Jonah Berger, author of the book Contagious: Why Things Catch On. ""It makes it seem more juicy, more interesting, more valuable.""

In his campaign for president, Trump benefited greatly from how badly his opponent's own appearance of secret-keeping damaged her campaign. Hillary Clinton's primary offense was setting up a private email server as Secretary of State, but the scandal that dogged her was her decision to delete some 30,000 emails from that server. Whatever information those emails did or didn't contain, the very act of deleting them drew attention and provided detractors with endless fodder for alleging she was up to something nefarious. The same goes for the transcripts of speeches she gave to Goldman Sachs, which Clinton refused to release. When they leaked anyway, the actual text of the speeches was far more benign than the perception of untrustworthiness that not releasing them generated. Their absence attracted more attention than the transcripts themselves.

The coverup is often worse than the crime, but the internet offers new ways to obfuscate while maintaining an appearance of openness. Nixon, for example, couldn't turn to social media to stage a strategic campaign of information overload. His channels were limited to traditional media—the next day's newspaper, the six o'clock news—that told a single, coherent story.

Today, a cacophony of channels, from sites to social media status updates to livestreams—brim with competing narratives, making it tougher for the end receiver of all that information to figure out what to believe. Authorites in China, for one, have used this tactic as a new form of censorship. One recent study by researchers at Harvard University found that instead of shutting down or arguing with its critics, the Chinese government itself posts some 448 million comments on social media per year to ""distract the public and change the subject.""

Until now, Trump has taken a similar approach to the Russia investigation. Each time it's in the news, he inundates social media with competing questions about how classified information about the investigation has leaked to the public in the first place.

That storyline has been a gift to Republicans in Congress, who have dutifully turned House and Senate committee hearings ostensibly about Russia into probes of government leaks. Both lines of questioning compete for airtime and headlines, making it easier for a casual onlooker to lose sight of the hearings' original purpose. By ramping up the noise, Trump and members of his party have succeeded in weakening the signal. As the uproar over firing Comey shows, that kind of static works much better than silence.",Social Media,WIRED,https://www.wired.com/2017/05/firing-comey-amplified-trumps-russia-problem/,"Social Media has provided new ways to obfuscate while maintaining an appearance of openness, making it easier for authorities to distract the public and change the subject, weakening the signal and dampening the sense of intrigue.","Information, Discourse & Governance"
341,Facebook faces ‘mass action’ lawsuit in Europe over 2019 breach – TechCrunch,"Facebook is to be sued in Europe over the major leak of user data that dates back to 2019 but which only came to light recently after information on more than 533 million accounts was found posted for free download on a hacker forum.

Today Digital Rights Ireland (DRI) announced it’s commencing a “mass action” to sue Facebook, citing the right to monetary compensation for breaches of personal data that’s set out in the European Union’s General Data Protection Regulation (GDPR).

Article 82 of the GDPR provides for a “right to compensation and liability” for those affected by violations of the law. Since the regulation came into force, in May 2018, related civil litigation has been on the rise in the region.

The Ireland-based digital rights group is urging Facebook users who live in the European Union or European Economic Area to check whether their data was breached — via the haveibeenpwned website (which lets you check by email address or mobile number) — and sign up to join the case if so.

Information leaked via the breach includes Facebook IDs, location, mobile phone numbers, email address, relationship status and employer.

Facebook has been contacted for comment on the litigation. Update: A Facebook spokesperson said:

We understand people’s concerns, which is why we continue to strengthen our systems to make scraping from Facebook without our permission more difficult and go after the people behind it. As LinkedIn and Clubhouse have shown, no company can completely eliminate scraping or prevent data sets like these from appearing. That’s why we devote substantial resources to combat it and will continue to build out our capabilities to help stay ahead of this challenge.

The tech giant’s European headquarters is located in Ireland — and earlier this week the national data watchdog opened an investigation, under EU and Irish data protection laws.

A mechanism in the GDPR for simplifying investigation of cross-border cases means Ireland’s Data Protection Commission (DPC) is Facebook’s lead data regulator in the EU. However it has been criticized over its handling of and approach to GDPR complaints and investigations — including the length of time it’s taking to issue decisions on major cross-border cases. And this is particularly true for Facebook.

With the three-year anniversary of the GDPR fast approaching, the DPC has multiple open investigations into various aspects of Facebook’s business but has yet to issue a single decision against the company.

(The closest it’s come is a preliminary suspension order issued last year, in relation to Facebook’s EU to U.S. data transfers. However, that complaint long predates GDPR; and Facebook immediately filed to block the order via the courts. A resolution is expected later this year after the litigant filed his own judicial review of the DPC’s processes.)

Since May 2018 the EU’s data protection regime has — at least on paper — baked in fines of up to 4% of a company’s global annual turnover for the most serious violations.

Again, though, the sole GDPR fine issued to date by the DPC against a tech giant (Twitter) is very far off that theoretical maximum. Last December the regulator announced a €450,000 (~$547,000) sanction against Twitter — which works out to around just 0.1% of the company’s full-year revenue.

That penalty was also for a data breach — but one which, unlike the Facebook leak, had been publicly disclosed when Twitter found it in 2019. So Facebook’s failure to disclose the vulnerability it discovered and claims it fixed by September 2019, which led to the leak of 533 million accounts now, suggests it should face a higher sanction from the DPC than Twitter received.

However, even if Facebook ends up with a more substantial GDPR penalty for this breach the watchdog’s caseload backlog and plodding procedural pace makes it hard to envisage a swift resolution to an investigation that’s only a few days old.

Judging by past performance it’ll be years before the DPC decides on this 2019 Facebook leak — which likely explains why the DRI sees value in instigating class action-style litigation in parallel to the regulatory investigation.

“Compensation is not the only thing that makes this mass action worth joining. It is important to send a message to large data controllers that they must comply with the law and that there is a cost to them if they do not,” DRI writes on its website.

It also submitted a complaint about the Facebook breach to the DPC earlier this month, writing then that it was “also consulting with its legal advisors on other options including a mass action for damages in the Irish Courts”.

It’s clear that the GDPR enforcement gap is creating a growing opportunity for litigation funders to step in in Europe and take a punt on suing for data-related compensation damages — with a number of other mass actions announced last year.

In the case of DRI its focus is evidently on seeking to ensure that digital rights are upheld. But it told RTE that it believes compensation claims which force tech giants to pay money to users whose privacy rights have been violated is the best way to make them legally compliant.

Facebook, meanwhile, has sought to play down the breach it failed to disclose in 2019 — claiming it’s “old data” — a deflection that ignores the fact that people’s dates of birth don’t change (nor do most people routinely change their mobile number or email address).

Plenty of the “old” data exposed in this latest massive Facebook leak will be very handy for spammers and fraudsters to target Facebook users — and also now for litigators to target Facebook for data-related damages.",Social Media,TechCrunch,https://techcrunch.com/2021/04/16/facebook-faces-mass-action-lawsuit-in-europe-over-2019-breach/,"Social Media companies are facing legal action over data breaches, with Europe's General Data Protection Regulation providing for a right to monetary compensation for those affected. The consequences of such data breaches can be far-reaching, with fraudsters and spammers able to target users with their ""old"" data.",Security & Privacy
342,Social Media Is Silencing Personal Opinion – Even In The Offline World – TechCrunch,"Social media is not living up to its promise of being an online outlet for discussion that mirrors our communications and conversations that take place in the offline world. In fact, people are less willing to discuss important issues on social media, than they are in real life, a new report from Pew Research Center has found.

It may seem like an obvious conclusion: of course, people are more hesitant to speak up with a contrary opinion when all their friends, family or colleagues feel differently. But there’s been little research that quantifies just how unwilling people are to take a potentially unpopular stance on outlets like Facebook and Twitter.

Pew refers to the this tendency to keep opinions to yourself, when you believe they’re not widely shared, as the “spiral of silence” – a term coined in the mid-70’s by a researcher studying the nature of public opinion.

“Some social media creators and supporters have hoped that social media platforms like Facebook and Twitter might produce different enough discussion venues that those with minority views might feel freer to express their opinions, thus broadening public discourse and adding new perspectives to everyday discussion of political issues,” explains Pew.

But as it turns out that optimistic desire is not quite the reality. If anything, social media is encouraging users to keep opinions to themselves.

In its study, Pew focused on the controversial topic involving Edward Snowden’s revelations of widespread government surveillance, because that topic saw a variety of opinions across the U.S. as to whether his actions were justified, and whether the government’s policies themselves were good or bad.

Only 42% of users said they were willing to discuss the surveillance program on social media, while a much larger 86% were willing to have an in-person discussion about it. Meanwhile, the 14% who were unwilling to discuss the topic in person didn’t turn to social media as a backup – only 0.3% did.

This, explains the report, “challenges the notion that social media spaces might be considered useful venues for people sharing views they would not otherwise express when they are in the physical presence of others.”

And in both online and offline discussions, people were more willing to share their views if they believed their audience agreed with them. For instance, if someone believed their Facebook network agreed with them, they were twice as likely to join a discussion about the issue.

Silencing Effect Extends Offline

But here’s the kicker: not only did social media silence users online, or fail to be a place where they could more freely discuss topics when they couldn’t do so with real-world friends, it also seemed to change people’s offline behavior, making them less willing to engage in potentially controversial discussions.

Social media users, says Pew, were less willing to share their opinions in face-to-face settings than others. The typical Facebook user was half as likely to have a discussion about the topic in the offline world, while Twitter users were 0.24 times less likely to do the same.

Of course, it’s worth pointing out that the choice to use the Snowden situation as the basis for this particular study could be flawed – after all, social platforms, including Facebook, were implicated as being among the places the government tapped to listen in on citizens’ conversations. Perhaps citizens didn’t want to publicize an opinion on this particular matter, but would be more willing to do so on others.

The study, for what it’s worth, didn’t get into the “why” question regarding people’s decision to stay silent, but theorized it could be attributed to a number of factors, including not wanting to disappoint or get into arguments with friends or post things that HR departments or prospective employers could one day discover. Plus, Pew says that an individual’s decision to share an opinion could also relate to their confidence and understanding of the subject as well as how intense or apathetic they felt.

Because of its sample methods (1,801 people via survey) and choice of topic, Pew’s study might not be a fully accurate example of social media’s silencing effects, but it’s definitely fodder for discussion – if you dare! – as well as further research.",Social Media,TechCrunch,https://techcrunch.com/2014/08/26/social-media-is-silencing-personal-opinion-even-in-the-offline-world/,"Pew's research has found that people are less likely to discuss important issues on social media than in real life, suggesting a silencing effect that extends to offline conversations. This could be attributed to a number of factors, such as not wanting to disappoint or get into arguments with friends, or post things that could be discovered by employers.",Discourse & Governance
343,"Book Excerpt: 'Twitter and Tear Gas', and Zeynep Tufekci on the power and fragility of networked protest","On February 2, 2011, a horde of men, armed with long sticks and whips and riding camels and horses, attacked the hundreds of thousands of protesters who packed Tahrir Square in Cairo, Egypt, parting the crowd as if it were the Red Sea and scattering protesters as they went. The horses’ saddles were a brilliant red, traditional and ornate, but the day was anything but cheerful. A dozen people died. Many believe that the attackers were undercover agents of President Hosni Mubarak’s regime, although trials afterward were unable to verify this. Egyptians call the event the “Battle of the Camels,” a sly reference to a seventh-century internecine struggle among Muslims.

A prominent Egyptian dissident later told me the story from his perspective, starting with his shock at hearing the trampling hooves on the asphalt, seeing the heads of the animals above the crowd, and watching confusion and anger spread in waves through the packed square. “I laughed very hard,” he said, “because, for the first time since it all began, I was sure we had won. Surely, I thought, we had won.”

I wondered whether he had lost his mind. That would have been understandable after 10 days of violence, tear gas, tension, and no sleep.

But he was right. It had been a turning point.

As he explained to me, letting loose thugs on camelback showed just how desperate and out-of-touch Mubarak’s regime had become. While camels flooded the square, Tahrir activists were busy giving live interviews to the BBC and other international media outlets via smuggled satellite phones, and tweeting over contraband Internet connections. Although Mubarak had shut down the Internet—except a single ISP, the Noor network—and all cell phones just before the “Battle of the Camels,” protesters had pierced the Internet blockade within hours and remained in charge of their message, which was heard around the world, as was news of the Internet shutdown. Mubarak’s acts were both futile, because the protests were already under way, and counterproductive, because worried families, unable to call their younger relatives, rushed to Tahrir Square. The sheer, unrestrained brutality of the camel attack and the clumsiness of shutting down all communication networks underscored the inability of Mubarak’s crumbling autocracy to understand the spirit of the time, the energy of the youthful protesters, and the transformed information environment. Camels and sticks versus satellite phones and Twitter. Seventeenth century, meet 21st century. Indeed, the Internet in Egypt soon came back online, and Mubarak, unable to contain or permanently repress the huge crowds, was forced to resign shortly thereafter.

As uprisings spread throughout the region, many felt optimistic. The revolutions had not yet turned into military coups, as would happen in Egypt, or bloody civil wars, as would happen in Libya and Syria. Activists were flying high. Digital technologies had clearly transformed the landscape, seemingly to the benefit of political challengers. Rising in opposition to crumbling, stifling regimes that tried to control the public discourse, activists were able to overcome censorship, coordinate protests, organize logistics, and spread humor and dissent with an ease that would have seemed miraculous to earlier generations. A popular Facebook page, created to decry the beating death of a young man by the Egyptian police, had been the forum for organizing the initial Tahrir uprising and had mustered hundreds of thousands of supporters. An Egyptian friend of mine would later joke that this must have been the first time in history when a person could actually join a revolution by clicking “I’m Attending” in response to a Facebook evite. But such social media sites were important to audiences beyond the protesters; the world also followed the uprising through the Facebook and Twitter posts of young, digitally savvy and determined protesters.

Organized Chaos

Networked protests of the 21st century differ in important ways from movements of the past and often operate with a different logic. (I use “networked” as a shorthand for digitally networked, to refer to the reconfiguration of movements and publics through the incorporation of digital technologies and connectivity.) Many of these developments have cultural and political roots that predate the Internet but have found a fuller expression in conjunction with the capabilities provided by technology. Networked protests have strengths and weaknesses that combine in novel ways and do not neatly conform to our understandings of the trajectory of protest movements before the advent of digital technologies.

For example, the ability to use digital tools to rapidly amass large numbers of protesters with a common goal empowers movements. Once this large group is formed, however, it struggles because it has sidestepped some of the traditional tasks of organizing. Besides taking care of tasks, the drudgery of traditional organizing helps create collective decision-making capabilities, sometimes through formal and informal leadership structures, and builds a collective capacity among movement participants through shared experience and tribulation. The expressive, often humorous style of networked protests attracts many participants and thrives both online and offline, but movements falter in the long term unless they create the capacity to navigate the inevitable challenges.

These movements rely heavily on online platforms and digital tools for organizing and publicity and proclaim that they are leaderless although their practice is almost always muddier. The open participation afforded by social media does not always mean equal participation, and it certainly does not mean a smooth process. Although online media are indeed more open and participatory, over time a few people consistently emerge as informal but persistent spokespersons—with large followings on social media. These people often have great influence, though they lack the formal legitimacy that an open and recognized process of selecting leaders would generate. The result is often a conflict-ridden, drawn-out struggle between those who find themselves running things (or being treated as de facto leaders) and other people in the movement who can also express themselves online. These others may challenge the de facto spokespersons, but the movements have few means to resolve their issues or make decisions. In some ways, digital technologies deepen the ever-existing tension between collective will and individual expression within movements, and between expressive moments of rebellion and the longer-term strategies requiring instrumental and tactical shifts.",Social Media,WIRED,https://www.wired.com/2017/05/twitter-tear-gas-protest-age-social-media/,"Social media can create a disconnect between collective will and individual expression within movements, and between expressive moments of rebellion and the longer-term strategies requiring instrumental and tactical shifts. This can lead to conflict-ridden, drawn-out struggles between those who find themselves running things and others who can also express themselves online, with few means of resolution or decision",Politics
344,"Google’s probe into Russian disinformation finds ad buys, report claims – TechCrunch","Google has uncovered evidence that Russian operatives exploited its platforms in an attempt to interfere in the 2016 U.S. election, according to the Washington Post.

It says tens of thousands of dollars were spent on ads by Russian agents who were aiming to spread disinformation across Google’s products — including its video content platform YouTube but also via advertising associated with Google search, Gmail, and the company’s DoubleClick ad network.

The newspaper says its report is based on information provided by people familiar with Google’s investigation into whether Kremlin-affiliated entities sought to use its platforms to spread disinformation online.

Asked for confirmation of the report, a Google spokesman told us: “We have a set of strict ads policies including limits on political ad targeting and prohibitions on targeting based on race and religion. We are taking a deeper look to investigate attempts to abuse our systems, working with researchers and other companies, and will provide assistance to ongoing inquiries.”

So it’s telling that Google is not out-and-out denying the report — suggesting the company has indeed found something via its internal investigation, though isn’t ready to go public with whatever it’s unearthed as yet.

what we've seen so far is the tip of the iceberghttps://t.co/i2eeMygldD — Anne Applebaum (@anneapplebaum) October 9, 2017

Google, Facebook, and Twitter have all been called to testify to a Senate Intelligence Committee on November 1 which is examining how social media platforms may have been used by foreign actors to influence the 2016 US election.

Last month Facebook confirmed Russian agents had utilized its platform in an apparent attempt to sew social division across the U.S. by purchasing $100,000 of targeted advertising (some 3,000+ ads — though the more pertinent question is how far Facebook’s platform organically spread the malicious content; Facebook has claimed only around 10M users saw the Russian ads, though others believe the actual figure is likely to be far higher.)

CEO Mark Zuckerberg has tried to get out ahead of the incoming political and regulatory tide by announcing, at the start of this month, that the company will make ad buys more transparent — even as the U.S. election agency is running a public consultation on whether to extend political ad disclosure rules to digital platforms.

(And, lest we forget, late last year he entirely dismissed the notion of Facebook influencing the election as “a pretty crazy idea” — words he’s since said he regrets.)

Safe to say, tech’s platform giants are now facing the political grilling of their lives, and on home soil, as well as the prospect of the kind of regulation they’ve always argued against finally being looped around them.

But perhaps their greatest potential danger is the risk of huge reputational damage if users learn to mistrust the information being algorithmically pushed at them — seeing instead something dubious that may even have actively malicious intent.

While much of the commentary around the US election social media probe has, thus far, focused on Facebook, all major tech platforms could well be implicated as paid aids for foreign entities trying to influence U.S. public opinion — or at least any/all whose business entails applying algorithms to order and distribute third party content at scale.

Just a few days ago, for instance, Facebook said it had found Russian ads on its photo sharing platform Instagram, too.

In Google’s case the company controls vastly powerful search ranking algorithms, as well as ordering user generated content on its massively popular video platform YouTube.

And late last year The Guardian suggested Google’s algorithmic search suggestions had been weaponized by an organized far right campaign — highlighting how its algorithms appeared to be promoting racist, nazi ideologies and misogyny in search results.

(Though criticism of tech platform algorithms being weaponized by fringe groups to drive skewed narratives into the mainstream dates back further still — such as to the #Gamergate fallout, in 2014, when we warned that popular online channels were being gamed to drive misogyny into the mainstream media and all over social media.)

Responding to The Guardian’s criticism of its algorithms last year, Google claimed: “Our search results are a reflection of the content across the web. This means that sometimes unpleasant portrayals of sensitive subject matter online can affect what search results appear for a given query. These results don’t reflect Google’s own opinions or beliefs — as a company, we strongly value a diversity of perspectives, ideas and cultures.”

But it looks like the ability of tech giants to shrug off questions and concerns about their algorithmic operations — and how they may be being subverted by hostile entities — has drastically shrunk.

According to the Washington Post, the Russian buyers of Google ads do not appear to be from the same Kremlin-affiliated troll farm which bought ads on Facebook — which it suggests is a sign that the disinformation campaign could be “a much broader problem than Silicon Valley companies have unearthed so far”.

Late last month Twitter also said it had found hundreds of accounts linked to Russian operatives. And the newspaper’s sources claim that Google used developer access to Twitter’s firehose of historical tweet data to triangulate its own internal investigation into Kremlin ad buys — linking Russian Twitter accounts to accounts buying ads on its platform in order to identify malicious spend trickling into its own coffers.

A spokesman for Twitter declined to comment on this specific claim but pointed to a lengthy blog post it penned late last month — on “Russian Interference in 2016 US Election, Bots, & Misinformation”. In that Twitter disclosed that the RT (formerly Russia Today) news network spent almost $275,000 on U.S. ads on Twitter in 2016.

It also said that of the 450 accounts Facebook had shared as part of its review into Russian election interference Twitter had “concluded” that 22 had “corresponding accounts on Twitter” — which it also said had either been suspended (mostly) for spam or were suspended after being identified.

“Over the coming weeks and months, we’ll be rolling out several changes to the actions we take when we detect spammy or suspicious activity, including introducing new and escalating enforcements for suspicious logins, Tweets, and engagements, and shortening the amount of time suspicious accounts remain visible on Twitter while pending confirmation. These are not meant to be definitive solutions. We’ve been fighting against these issues for years, and as long as there are people trying to manipulate Twitter, we will be working hard to stop them,” Twitter added.

As in the case with the political (and sometimes commercial) pressure also being applied on tech platforms to speed up takedowns of online extremism, it seems logical that the platforms could improve internal efforts to thwart malicious use of their tools by sharing more information with each other.

In June Facebook, Microsoft, Google and Twitter collectively announced a new partnership aimed at reducing the accessibility of internet services to terrorists, for instance — dubbing it the Global Internet Forum to Counter Terrorism — and aiming to build on an earlier announcement of an industry database for sharing unique digital fingerprints to identify terrorist content.

But whether some similar kind of collaboration could emerge in future to try to collectively police political spending remains to be seen. Joining forces to tackle the spread of terrorist propaganda online may end up being trivially easy vs accurately identifying and publicly disclosing what is clearly a much broader spectrum of politicized content that’s, nonetheless, also been created with malicious intent.

According to the New York Times, Russia-bought ads that Facebook has so far handed over to Congress apparently included a diverse spectrum of politicized content, from pages for gun-rights supporters, to those supporting gay rights, to anti-immigrant pages, to pages that aimed to appeal to the African-American community — and even pages for animal lovers.

One thing is clear: Tech giants will not be able to get away with playing down the power of their platforms in public.

Not at the Congress hearing next month. And likely not for the foreseeable future.",Social Media,TechCrunch,https://techcrunch.com/2017/10/09/googles-probe-into-russian-disinformation-finds-ad-buys-report-claims/,"The potential for misuse of Social Media platforms has become an increasingly pressing concern, as it has been revealed that Russian operatives exploited Google's platforms in an attempt to interfere in the 2016 U.S. election. This highlights the danger of malicious entities subverting powerful algorithms to spread disinformation, which can lead to a huge reputational damage if users","Information, Discourse & Governance"
345,Social media sites may need to apply age checks under UK anti-porn law – TechCrunch,"Social media sites such as Twitter face being regulated in the UK under anti-porn proposals, as part of the government’s Digital Economy bill proposal.

If the bill passes into law unamended, social media services could be scooped into needing to apply age verification to ensure all users are over 18 — in the same way the bill seeks to enforce an age-gate on pornography websites, with the overarching aim of trying to prevent children being exposed to adult content online.

Another portion of the proposed law has caused controversy by seeking to prevent the spread of ‘non-conventional’ sex videos online — meaning regulators and/or civil servants will be tasked with determining what passes for ‘acceptably convention porn’ in the UK. And what does not. (Which brings a whole new meaning to the phrase ‘yes, minister“… )

In a debate about the bill in the UK’s second chamber this week, Baroness Benjamin welcomed the government’s confirmation that the bill covers “ancillary service providers”, such as Twitter, over and above pure-play adult websites.

“There has been some debate about the scope of Clause 15 and the ancillary service providers, but it seems clear to me that social media should be covered by this,” she said.

“I was particularly delighted that the noble Baroness, Lady Shields, confirmed to the Lords Communications Committee on November 29 that: “The Bill covers ancillary services. There was a question about Twitter. Twitter is a user-generated uploading-content site. If there is pornography on Twitter, it will be considered covered under ancillary services”.”

So unless a social media service is able to prove it is free of pornographic content — a very high bar for any user generated content platform — it looks like it could be subject to the bill’s age verification requirements.

Peers asked specifically whether other social media sites such as Facebook could be classed as ancillary service providers under the proposed legislation. Responding on behalf of the government, Lord Ashton of Hyde, the parliamentary under-secretary of state for Culture, Media and Sport, suggested the classification could indeed apply widely.

“The government believe that services, including Twitter, can be classified by regulators as ancillary service providers where they are enabling or facilitating the making available of pornographic or prohibited material,” he said.

Also speaking during the debate, the Earl of Erroll agreed with the government view that social media sites present a loophole for UK lawmakers’ aim of restricting children’s access to adult material online but urged that any age verification process should include privacy safeguards in order to protect individuals’ identities from becoming a target for hackers.

“Imagine the fallout if some hacker found a list of senior politicians who had had to go through an age-verification process on one of these websites, which would mean they had accessed them. They could bring down the Government or the Opposition overnight,” he warned, going on to reveal he has personally shied away from trying to look up something as mild and uncontroversial as online statistics about the demographic breakdown of online pornography users in the UK — on account of another piece of government legislation: the recently passed Investigatory Powers Act (although technically this does not come into force until the start of next year; but its bulk surveillance measures are already evidently impacting browsing behavior).

I have not dared to do so because it will show I have been to that website, which I am sure would show up somewhere on one of these investigatory powers web searches and could be dangerous.

Said Erroll: “Noble Lords could all go to the MindGeek website and look at the statistics, where there is a breakdown of which age groups and genders are accessing these websites. I have not dared to do so because it will show I have been to that website, which I am sure would show up somewhere on one of these investigatory powers web searches and could be dangerous.”

To ward off the risk of hackers swiping age verification identities, he suggested websites should not store the identity of people they age-check, but rather utilize a third party attribute provider to verify age — which would only send back an encrypted token confirming a check has been passed.

However he was less clear on how to prevent the process being reversed to re-link identities to specific adult websites. “They can then reverse it and find out who the person was — but they could still perhaps not be told by the regulator which site it was. So there should be a security cut-out in there,” he suggested, adding: “I am not sure that we should not just put something in the Bill to mandate that a website cannot keep a person’s identity.

“If the person after they have proved that they are 18 then decides to subscribe to the website freely and to give it credit card details and stuff like that, that is a different problem — I am not worried about that. That is something else. That should be kept extremely securely and I personally would not give my ID to such a site — but at the age verification end, it must be private.”

On the general point of how to loop social media websites into compliance with the legislation Erroll suggested payment providers could be targeted as a financial route to enforcing age gates.

“It is probably unrealistic to block the whole of Twitter — it would make us look like idiots. On the other hand, there are other things we can do,” he said. “If we start to make the payment service providers comply and help, they will make it less easy for those sites to make money. They will not be able to do certain things.”

Erroll went on to suggest the legislation may require the government to create an enforcement body. The BBFC (British Board of Film Classification) has been proposed as the regulator to oversee which websites to tell ISPs to block or not — but enforcement may require an additional body, he said: “The BBFC is happy to be a regulator, and I think it is also happy to inform ISPs which sites should be blocked, but other enforcement stuff might need to be done. There is provision for it in the Bill. The government may need to start looking for an enforcer.”

The bill does provide the regulator with the ability to issue fines for non-compliance of age verification checks — of up to £250,000 or five per cent of their turnover. However peers questioned how a UK regulator could enforce such fines on websites based overseas.

Another clause in the bill aims to furnish the regulator with financial transaction blocking powers, however another peer, Lord Morrow, argued the provision is “only half present” — urging further amendments to strengthen the regulator’s powers.

“I also think that there is a very strong case to be made for an amendment giving the regulator power to require ancillary services such as advertisers not to advertise on sites operating in violation of UK law,” he added.

MPs passed the bill back in November — but the legislation can still be amended before passing into law via the scrutiny process in the House of Lords.",Social Media,TechCrunch,https://techcrunch.com/2016/12/15/social-media-sites-may-need-to-apply-age-checks-under-uk-anti-porn-law/,"Under the proposed Digital Economy bill, Social Media sites such as Twitter could face regulation, including the need to apply age verification to ensure all users are over 18. This could lead to the exposure of individuals' identities to hackers, as well as government control over what is deemed 'acceptable porn'.",Security & Privacy
346,Ignore the social media echo chambers – TechCrunch,"After Election Day, NPR, The Washington Post and various blogs described America as bitterly divided or on the brink of civil war. These were by the same journalists, pundits and intellectuals who only know how to sell fear.

“They want to take away your guns!” and “They want to take your children away!” were their cries, while praising BLM’s protesters on one screen and promoting videos of the infinitesimal number of rioters on another.

The Atlantic speculated about widespread violence depending on the outcome, but I never believed these seemingly well-researched reports that have become commonplace in our clickbait-driven world. And as we saw, nothing of real concern happened; instead of violence, there were relatively small protests and dancing in the streets.

The gap that supposedly divides our nation is narrower than the doomsaying pundits, intellectuals, politicians and cause leaders want you to believe. Why do they want you to believe this? Because promoting division and conflict sells and grants a perverse glue that unites people within their tribal communities. Behind these labels of conflict are seeds of fear that can grow into irrational fears. Fears without reason, fears beyond facts. Sometimes these fears become things we hate — and our society and nation should have no place for hate, because it is an unproductive emotion without any possible positive outcome.

I’ve learned to ignore much of the headline-driven news and social media echo chambers where ridiculous ideas fester across our political spectrum. There are obviously ridiculous ideas, such as QAnon, but the subtly ridiculous ideas can be more dangerous and potentially even more destructive. These ideas can be diminished by simple questions to the average reasonable person.

One idea spawned in some progressive echo chambers was the notion that Trump would stage a coup d’état if Joe Biden won the election (i.e., “Did you see those unmarked federal police!?” which signaled to some that a coup was coming).

A basic element of a coup d’état is military support or control, which obviously Trump did not have. I would ask basic questions around this idea, but always ask the rhetorical question, “Do you know how difficult it is to conduct a coup d’état?” Meanwhile, in some conservative echo chambers, a similar concern made rounds that “defund the police” was an effort to install a “federal police force” that Biden would control once in the Oval Office. So there really isn’t much original thought inside the echo chambers of America.

Maybe both sides with such fantasies recently watched that Patrick Swayze classic, “Red Dawn,” where a tiny militia of high school students held off the combined forces of the old Soviet Union and Cuba. Or maybe they saw “300,” in which Sparta’s army held off more than 300,000 invaders. After watching either of these inspirational movies, I might possibly believe such a militia or “federal force” could overpower the whole might of the U.S. military. Ahem.

For those warmongers and soothsayers warning of civil war, where do they want the country to go? Static echo chambers of America, or a vision of suburban folks with pitchforks and handguns versus urban dwellers carrying machine guns and Blue Bottle coffee mugs?

Since the level of violence after the election did not in fact match the crystal balls of these oracles, the definitions and terms have of course changed. As Bertrand Russell stated, “fear is the main source of superstition” — to which I would add that fear is also the source of really stupid predictions and ideas.

And let’s be clear that while I do criticize the echo chambers of social media, they are only tools of promotion, because echo chambers are not limited to the online social media. Echo chambers can be homes, bars, lodge meetings, yoga studios and Sunday bridge clubs. The enablers are the pundits, intellectuals, politicians and cause leaders that seed these ideas.

Conspiracy theories, misinformation and outlandish statements were quite capable of spreading before the recommendation engines of Facebook and others were fully developed. For example, in 2006, over 50% of Democrats believed the U.S. government was involved in the 9/11 terrorist attack. More than half of registered Democrats believed in this conspiracy theory! And let’s not forget the Obama “birther” conspiracy, where at least 57% of Republicans continued to believe that President Obama was born in Kenya even after he released his birth certificate in 2008.

But today, Facebook, YouTube, Twitter and other social media sites have become extremely powerful accelerants for such provocative ideas and strange fictions. Tristan Harris, co-founder and president of the Center for Humane Technology, was recently featured in the Netflix documentary “The Social Dilemma,” where he discussed how social media tends to feed content to retain people’s attention and can spiral downward.

This can become an abyss of outright misinformation, or — even more importantly in my estimation — for subtle, ignorant ideas, such as coups d’état and civil wars. And those destructive ideas and irrational conspiracy theories from the 2000s that probably took months to spread, are now supercharged by today’s social media giants to infect our society in a matter of days or weeks.

The fabric of our nation was delicately woven, but after countless turns of the loom between conflicts and enlightenment, our country has proven itself extremely resilient. Indestructible beyond today’s calls for racism and ignorance, for anarchy and destruction, and for civil wars.

Biden is our President-elect with a mandate to lead our nation beyond this divide — a divide that I believe has been overstated. Many citizens met in the middle to provide Biden with a mandate to bridge the gap. The “blue wave” didn’t occur and House Republicans gained 10 seats, which means many Republicans and independents voted “red” down-ballot but also voted for Biden.

Trump had the largest number of minority votes for a Republican presidential candidate in history, including from 18% of Black male voters — and that number would have been much higher pre-pandemic. I see all of this as a positive, because our citizens are not voting party line or becoming beholden to one party.

In reality, many of the major issues that supposedly separate us are much closer than we know. For example, I’ve sat down behind closed doors with a senior adviser on healthcare for a major Republican leader, who stated that Obamacare isn’t far off from what they were planning. The difference was that their plan was more small business friendly and their cost savings would be among the younger demographic. I also sat down with a senior adviser for Obamacare, who explained that they believed it wasn’t sustainable unless the cost savings were for those 65 and above. So the differences on such critical policies are not miles apart but only steps away from each other. Although at times politics are about credit and conflict, hopefully such differences can be resolved in the near future.

I hope this election will change the temperament of our nation and its citizens. I hope it will lead more people to ignore the tactics of both political parties and organizations seeking their attention and support. Their shortsighted methods should be cast away like the relics of the past and conflict should not be the tool of this new America. Instead, let’s focus on productive dialogue to find common ground, and thoughtful, practical policies to move our nation forward.",Social Media,TechCrunch,https://techcrunch.com/2020/11/24/ignore-the-social-media-echo-chambers/,"Social media has become a powerful accelerator for conspiracy theories, misinformation and outlandish statements, leading to fear and division among citizens.","Information, Discourse & Governance"
347,"Disinformation ‘works better than censorship,’ warns internet freedom report – TechCrunch","A rise in social media surveillance, warrantless searches of travelers’ devices at the border and the continued spread of disinformation are among the reasons why the U.S. has declined in internet freedom rankings, according to a leading nonprofit watchdog.

Although Freedom House said that the U.S. enjoys some of the greatest internet freedoms in the world, its placement in the worldwide rankings declined for the third year in a row. Last year’s single-point drop was blamed on the repeal of net neutrality.

Iceland and Estonia remained at the top of the charts, according to the rankings, with China and Iran ranking with the least-free internet.

The report said that digital platforms, including social media, have emerged as the “new battleground” for democracy, where governments would traditionally use censorship and site-blocking technologies. State and partisan actors have used disinformation and propaganda to distort facts and opinions during elections in dozens of countries over the past year, including the 2018 U.S. midterm elections and the 2019 European Parliament elections.

“Many governments are finding that on social media, propaganda works better than censorship,” said Mike Abramowitz, president of Freedom House.

Disinformation — or “fake news” as described by some — has become a major headache for both governments and private industry. As the spread of deliberately misleading and false information has become more prevalent, lawmakers have threatened to step in to legislate against the problem.

But as some governments — including the U.S. — have tried to stop the spread of disinformation, Freedom House accused some global leaders — including the U.S. — of “co-opting” social media platforms for their own benefit. Both the U.S. and China are among the 40 countries that have expanded their monitoring of social media platforms, the report said.

“Law enforcement and immigration agencies expanded their surveillance of the public, eschewing oversight, transparency, and accountability mechanisms that might restrain their actions,” the report said.

The encroachment on personal privacy, such as the warrantless searching of travelers’ phones without court-approved warrants, also contributed to the U.S.’ decline.

Several stories in the last year revealed how border authorities would deny entry to travelers for the content of social media posts made by other people, following changes to rules that compelled visa holders to disclose their social media handles at the border.

“The future of internet freedom rests on our ability to fix social media,” said Adrian Shahbaz, the nonprofit’s research director for technology and democracy.

Given that most social media platforms are based in the U.S., Shahbaz said the U.S. has to be a “leader” in promoting transparency and accountability.

“This is the only way to stop the internet from becoming a Trojan horse for tyranny and oppression,” he said.",Social Media,TechCrunch,https://techcrunch.com/2019/11/04/disinformation-censorship-freedom-house/,"The main consequence of the rise in Social Media surveillance discussed in this article is increased interference with personal privacy, such as through warrantless searches of travelers' devices at the border, as well as the spread of disinformation, which has become a major concern for governments and private industry.",Security & Privacy
348,Pro-Government Twitter Bots Try to Hush Mexican Activists,"On September 26, 2014, a group of students departed the Ayotzinapa Rural Teachers' College for a protest in Iguala, Mexico, about 80 miles away. They never arrived. What happened on the road to Iguala remains a mystery, but we know that at least three students were killed and another 43 are missing. The government's official story is that the 43 students were killed after being handed over to the Guerreros Unidos cartel on the orders of the mayor of Iguala. But investigations conducted by the Mexican publication Proceso and the U.S. publication The Intercept paints a darker picture of complacency at higher levels of government.

The incident was emblematic of broader fears and frustrations with violence and corruption in Mexico and sparked a wave of ongoing protests across the country. Like so many modern protest movements, activists turned to social media to organize and promote their cause. One Twitter hashtag in particular—#YaMeCanse or ""I am tired""—became a central hub for organizing protests and disseminating information.

That's when artist and journalist Erin Gallagher, who covers the protests for Revolution News, noticed something strange. The search results for #YaMeCanse were flooded with tweets that included the hashtag but no other content, save for a few random characters such as commas, semicolans, and angle brackets. A typical tweet might be: "",,> #YaMeCanse."" The accounts tweeting the empty content bore the telltale signals of spambots, such as a lack of followers and a tendency to tweet variations of the same thing over and over again. It became difficult, if not impossible, for activists to actually share information with each other through the #YaMeCanse hashtag, and as a result it quickly dropped out of Twitter's trending topics. Bots, it seemed, had effectively jammed the protesters' communications channel.

Gallagher and LoQueSigue blogger Alberto Escorcia say the bots have followed protesters from hashtag to hashtag over the past few months, drowning out real conversations with noise. They've also seen similar bots create fake hashtags in apparent attempts to push real hashtags out of Twitter's trending list, spread anti-protest messages, and even send death threats to specific activists.

It might seem petty to worry about Twitter bots in a country besieged with violence and corruption, but social media has become a central part of activism throughout the world. When it is undermined it has real effects. Yes, the term ""Twitter revolution"" is a massive simplification of the uprisings in Iran in 2009 or the ""Arab Spring"" protests that spread throughout the Middle East in 2010. After all, massive protests, riots and revolutions are as old as civilization itself. But it's fair to say that Twitter and other social networking sites have become the predominant platform for free expression throughout the world. And bots are now being used to stifle that freedom, not just in Mexico but around the globe. Similar tactics used against protesters in other countries, such as Turkey, Egypt and Syria. Just as the freedom of press was only truly guaranteed to those who could afford to buy one, freedom of social media may soon be limited to only those who can afford to build bot armies.

A Swarm of Lonely Bots

Escorcia has been watching the rise of Twitter bots in Mexico since the 2012 elections, when he noticed fake accounts promoting the candidacy of Enrique Peña Nieto, now the president. But the bots have become more active since the disappearance of the Ayotzinapa students.

Using social network visualization tools such as Flocker and Gephi, Escorcia has discovered a reliable way of detecting bot accounts by examining the number of connections a Twitter account has with other users. Bots have few connections, while real users tend to have far more. Using the software, he's been able to identify many cases of bots used to sabotage protests.",Social Media,WIRED,https://www.wired.com/2015/08/pro-government-twitter-bots-try-hush-mexican-activists/,"The use of Twitter bots to stifle freedom of expression on social media is a serious problem, with its effects being felt around the world. These bots are being used to flood conversations with noise, push real hashtags out of trending lists, spread anti-protest messages, and even send death threats to activists.","Information, Discourse & Governance"
349,"For Nextdoor, Eliminating Racism Is No Quick Fix","""People will feel like, ‘Oh, Nextdoor, that’s that place where people are racist.’”

In many ways, that approach was successful beyond measure. In less than a decade these services have exploded into the mainstream, replacing brick and mortar stores with more efficient and delightful web versions. It was only after the fact that a major shortcoming began to reveal itself: With many of these services, minorities of all types were targeted or excluded. The problem with technology that allows us to target exactly who we want is that it allows us to target exactly who we want. In other words, the problem with technology is us.

For too long, these companies took little or no responsibility, claiming they were just platforms on which behavior — both bad and good — unfolded. Twitter failed to get ahead of its abuse problem. Airbnb has yet to figure out how to stop hosts definitively from refusing African-American guests. Last week, Facebook began to stop some advertisers from keeping certain races from seeing their ads — but outside ads for housing, employment, or credit, that practice is still fair game.

This is, point blank, a major failing of the Web 2.0 era. Nevertheless, here we are in 2017, confronted with a host of services that serve some people better than others. These companies are learning the hard way that there is no silver bullet for eliminating racial bias, and no quick web fix for calling out bad behavior.

Nextdoor has tried to eliminate racial bias on the site the same way it built its product: through smart product design. It is having some success — Tolia says the site has cut racial profiling in its crime and safety category by 75 percent. But this story is not yet finished.

Tolia and his cofounder, early Microsoft veteran Sarah Leary, never intended for Nextdoor to grow into the new neighborhood watch. Both serial entrepreneurs, they’d founded the company in 2010 after a previous startup failed. Facebook had just emerged into the mainstream, acquainting us with the idea of using our real identities to connect to people we knew. They figured Nextdoor could be a next-gen Craigslist, helping us to use those same identities to connect to our neighbors—often people we didn’t really know. The site took off from the start, and has grown into a $1.1 billion company connecting people in 130,000 neighborhoods around the world.

As with many social services, founders launch with the intention of doing one thing, and quickly discover that users want to do something else. From the start, people turned to Nextdoor to discuss crime and safety concerns in the neighborhood, and that part of the site took off. So, by the fall of 2015, roughly one in five Oakland households used Nextdoor, and three Oakland city agencies were using the site to send out public service announcements. About a fifth of the conversations that happened in Oakland’s Nextdoor networks involved crime.

To address racial profiling in these conversations, Nextdoor first needed to understand how and where it was happening. So Tolia and Leary assembled a small team of senior people, which included Grady as well as a product manager, a designer, a data scientist, and later an engineer. Its lead was Maryam Mohit, who is director of product at Nextdoor. At 49, with a mop of curly hair and a considered approach to her speech, Mohit was a varsity techie. She’d gotten her start at Amazon, where she was on the team of people who had created the one-click patent. Mohit believed the issue could be addressed through smart product design. Every day, she’d bring home reams of postings. She read them late at night. She sifted through batches of them on Saturday mornings in her bathrobe and slippers, before she heard the patter of her children’s feet in the hallway. “I must’ve read thousands of anonymized posts to see what was actually happening,” she says. “It felt urgent.”",Social Media,WIRED,https://www.wired.com/2017/02/for-nextdoor-eliminating-racism-is-no-quick-fix/,"Social Media has unintentionally resulted in the exclusion of minorities, with companies like Twitter, Airbnb, and Facebook failing to take responsibility for this issue. Nextdoor has taken a more proactive approach, cutting racial profiling in its crime and safety category by 75%, but it remains to be seen if this is enough.",Equality & Justice
350,Facebook ends its experiment with the alternative ‘Explore’ news feed – TechCrunch,"Facebook is ending its short-lived (and misguided) experiment with the alternative news feed feature called “Explore.”

In a blog post today, Facebook head of news feed Adam Mosseri wrote:

We constantly try out new features, design changes and ranking updates to understand how we can make Facebook better for everyone. Some of these changes—like Reactions, Live Video, and GIFs— work well and go on to become globally available. Others don’t and we drop them. Today, we’re ending one of those tests: the Explore Feed. The Explore Feed was a trial response to consistent feedback we received from people over the past year who said they want to see more from friends and family in News Feed. The idea was to create a version of Facebook with two different News Feeds: one as a dedicated place with posts from friends and family and another as a dedicated place for posts from Pages.

Whatever the intention, the response from Facebook users was decidedly… “meh.” The split news feed rolled out in six countries as a trial bubble, and it sunk like a lead balloon.

As Mosseri wrote, “You gave us our answer: People don’t want two separate feeds. In surveys, people told us they were less satisfied with the posts they were seeing, and having two separate feeds didn’t actually help them connect more with friends and family.”

The product formally launched last October as an option for U.S. users to find additional news and entertainment from Pages that aren’t in a users’ news feed.

As we wrote at the time:

The overall goal, of course, is to increase users’ time-on-site (or time-in-app, if on mobile). This allows Facebook to serve more ads in between the content, in videos and elsewhere. Effectively, it’s a second-tier News Feed that Facebook could monetize. At this time, however, the feed doesn’t appear to include advertising. (At least no ads appeared in tests after scrolling down for a good minute or so).

Mosseri noted (and as most publishers are painfully aware) Facebook made significant changes to its algorithm earlier this year that have already been used to privilege posts from friends and family.

“We think our recent changes to News Feed that prioritize meaningful social interactions better address the feedback we heard from people who said they want to see more from friends and family. Those changes mean less public content in News Feed like posts from businesses, brands, and media,” Mosseri wrote.

It’s worth noting that the product wasn’t the only thing that folks had problems with. Users in the countries that saw their news feeds split said they didn’t receive important information that they needed after the change took effect, and that they had no idea what the hell was going on with their feeds.

Mosseri acknowledged that Facebook has learned its lesson from that blunder as well and will try to provide better communication on changes it’s making to its core product in the future.

The changes and their implementation are examples of what is being recognized as a broad tone-deafness and ignorance of the ways in which changes to a platform used by over 1 billion people are received.

Nations of users should not be social experiments or unwitting A/B testers in the grand design of new products by any company.

Perhaps that’s a lesson that Facebook can like and share.",Social Media,TechCrunch,https://techcrunch.com/2018/03/01/facebook-ends-its-experiment-with-the-alternative-explore-news-feed/,Facebook's short-lived experiment with the alternative news feed feature “Explore” was met with user dissatisfaction and highlighted the company's lack of consideration for the ways in which changes to their platform are received by their 1 billion users. This has demonstrated the need for better communication on changes to the core product in the future.,User Experience & Entertainment
351,UK names John Edwards as its choice for next data protection chief as gov’t eyes watering down privacy standards – TechCrunch,"The U.K. government has named the person it wants to take over as its chief data protection watchdog, with sitting commissioner Elizabeth Denham overdue to vacate the post: The Department of Digital, Culture, Media and Sport (DCMS) today said its preferred replacement is New Zealand’s privacy commissioner, John Edwards.

Edwards, who has a legal background, has spent more than seven years heading up the Office of the Privacy Commissioner In New Zealand — in addition to other roles with public bodies in his home country.

He is perhaps best known to the wider world for his verbose Twitter presence and for taking a public dislike to Facebook: In the wake of the 2018 Cambridge Analytica data misuse scandal Edwards publicly announced that he was deleting his account with the social media company — accusing Facebook of not complying with the country’s privacy laws.

An anti-Big-Tech stance aligns with the U.K. government’s agenda to tame the tech giants as it works to bring in safety-focused legislation for digital platforms and reforms of competition rules that take account of platform power.

Official announcement Government announces preferred candidate for Information Commissioner – https://t.co/2fri3ROyhm https://t.co/i8b4OBcwzC — John Edwards (@JCE_PC) August 26, 2021

If confirmed in the role — the DCMS committee has to approve Edwards’ appointment; plus there’s a ceremonial nod needed from the Queen — he will be joining the regulatory body at a crucial moment as digital minister Oliver Dowden has signaled the beginnings of a planned divergence from the European Union’s data protection regime, post-Brexit, by Boris Johnson’s government.

Dial back the clock five years and prior digital minister, Matt Hancock, was defending the EU’s General Data Protection Regulation (GDPR) as a “decent piece of legislation” — and suggesting to parliament that there would be little room for the U.K. to diverge in data protection post-Brexit.

But Hancock is now out of government (aptly enough after a data leak showed him breaching social distancing rules by kissing his aide inside a government building), and the government mood music around data has changed key to something far more brash — with sitting digital minister Dowden framing unfettered (i.e., deregulated) data mining as “a great opportunity” for the post-Brexit U.K.

For months, now, ministers have been eyeing how to rework the U.K.’s current (legacy) EU-based data protection framework — to, essentially, reduce user rights in favor of soundbites heavy on claims of slashing “red tape” and turbocharging data-driven “innovation.” Of course the government isn’t saying the quiet part out loud; its press releases talk about using “the power of data to drive growth and create jobs while keeping high data protection standards.” But those standards are being reframed as a fig leaf to enable a new era of data capture and sharing by default.

Dowden has said that the emergency data sharing that was waived through during the pandemic — when the government used the pressing public health emergency to justify handing NHS data to a raft of tech giants — should be the “new normal” for a post-Brexit U.K. So, tl;dr, get used to living in a regulatory crisis.

A special task force, which was commissioned by the prime minister to investigate how the U.K. could reshape its data policies outside the EU, also issued a report this summer — in which it recommended scrapping some elements of the U.K.’s GDPR altogether — branding the regime “prescriptive and inflexible”; and advocating for changes to “free up data for innovation and in the public interest,” as it put it, including pushing for revisions related to AI and “growth sectors.”

The government is now preparing to reveal how it intends to act on its appetite to “reform” (read: reduce) domestic privacy standards — with proposals for overhauling the data protection regime incoming next month.

Speaking to the Telegraph for a paywalled article published yesterday, Dowden trailed one change that he said he wants to make which appears to target consent requirements — with the minister suggesting the government will remove the legal requirement to gain consent to, for example, track and profile website visitors — all the while framing it as a pro-consumer move; a way to do away with “endless” cookie banners.

Only cookies that pose a “high risk” to privacy would still require consent notices, per the report — whatever that means.

Oliver Dowden, the UK Minister for Digital, Culture, Media and Sport, says that the UK will break away from GDPR, and will no longer require cookie warnings, other than those posing a 'high risk'.https://t.co/2ucnppHrIm pic.twitter.com/RRUdpJumYa — dan barker (@danbarker) August 25, 2021

“There’s an awful lot of needless bureaucracy and box ticking and actually we should be looking at how we can focus on protecting people’s privacy but in as light a touch way as possible,” the digital minister also told the Telegraph.

The draft of this great British “light touch” data protection framework will emerge next month, so all the detail is still to be set out. But the overarching point is that the government intends to redefine U.K. citizens’ privacy rights, using meaningless soundbites — with Dowden touting a plan for “common sense” privacy rules — to cover up the fact that it intends to reduce the U.K.’s currently world-class privacy standards and replace them with worse protections for data.

If you live in the U.K., how much privacy and data protection you get will depend upon how much “innovation” ministers want to “turbocharge” today — so, yes, be afraid.

It will then fall to Edwards — once/if approved in post as head of the ICO — to nod any deregulation through in his capacity as the post-Brexit information commissioner.

We can speculate that the government hopes to slip through the devilish detail of how it will torch citizens’ privacy rights behind flashy, distraction rhetoric about “taking action against Big Tech.” But time will tell.

Data protection experts are already warning of a regulatory stooge.

The Telegraph suggests Edwards is seen by government as an ideal candidate to ensure the ICO takes a “more open and transparent and collaborative approach” in its future dealings with business.

In a particularly eyebrow-raising detail, the newspaper goes on to report that government is exploring the idea of requiring the ICO to carry out “economic impact assessments” — to, in the words of Dowden, ensure that “it understands what the cost is on business” before introducing new guidance or codes of practice.

All too soon, U.K. citizens may find that — in the “sunny post-Brexit uplands” — they are afforded exactly as much privacy as the market deems acceptable to give them. And that Brexit actually means watching your fundamental rights being traded away.

In a statement responding to Edwards’ nomination, Denham, the outgoing information commissioner, appeared to offer some lightly coded words of warning for government, writing [emphasis ours]: “Data-driven innovation stands to bring enormous benefits to the U.K. economy and to our society, but the digital opportunity before us today will only be realised where people continue to trust their data will be used fairly and transparently, both here in the U.K. and when shared overseas.”

The lurking iceberg for government is of course that if it wades in and rips up a carefully balanced, gold standard privacy regime on a soundbite-centric whim — replacing a pan-European standard with “anything goes” rules of its/the market’s choosing — it’s setting the U.K. up for a post-Brexit future of domestic data misuse scandals.

You only have to look at the dire parade of data breaches over in the U.S. to glimpse what’s coming down the pipe if data protection standards are allowed to slip. The government publicly bashing the private sector for adhering to lax standards it deregulated could soon be the new “get popcorn” moment for U.K. policy watchers.

U.K. citizens will surely soon learn of unfair and unethical uses of their data under the “light touch” data protection regime — i.e., when they read about it in the newspaper.

Such an approach will indeed be setting the country on a path where mistrust of digital services becomes the new normal. And that of course will be horrible for digital business over the longer run. But Dowden appears to lack even a surface understanding of internet basics.

The U.K. is also of course setting itself on a direct collision course with the EU if it goes ahead and lowers data protection standards.

This is because its current data adequacy deal with the bloc — which allows for EU citizens’ data to continue flowing freely to the U.K. — was granted only on the basis that the U.K. was, at the time it was inked, still aligned with the GDPR. So Dowden’s rush to rip up protections for people’s data presents a clear risk to the “significant safeguards” needed to maintain EU adequacy. Meaning the deal could topple.

Back in June, when the Commission signed off on the U.K.’s adequacy deal, it clearly warned that “if anything changes on the U.K. side, we will intervene.”

Add to that, the adequacy deal is also the first with a baked-in sunset clause — meaning it will automatically expire in four years. So even if the Commission avoids taking proactive action over slipping privacy standards in the U.K. there is a hard deadline — in 2025 — when the EU’s executive will be bound to look again in detail at exactly what Dowden and Co. have wrought. And it probably won’t be pretty.

The longer-term U.K. “plan” (if we can put it that way) appears to be to replace domestic economic reliance on EU data flows — by seeking out other jurisdictions that may be friendly to a privacy-light regime governing what can be done with people’s information.

Hence — also today — DCMS trumpeted an intention to secure what it billed as “new multibillion pound global data partnerships” — saying it will prioritize striking “data adequacy partnerships” with the U.S., Australia, the Republic of Korea, Singapore, the Dubai International Finance Centre and Colombia.

Future partnerships with India, Brazil, Kenya and Indonesia will also be prioritized, it added — with the government department cheerfully glossing over the fact it’s U.K. citizens’ own privacy that is being de-prioritized here.

“Estimates suggest there is as much as £11 billion worth of trade that goes unrealised around the world due to barriers associated with data transfers,” DCMS writes in an ebullient press release.

As it stands, the EU is of course the U.K.’s largest trading partner. And statistics from the House of Commons library on the U.K.’s trade with the EU — which you won’t find cited in the DCMS release — underline quite how tiny this potential Brexit “data bonanza” is, given that U.K. exports to the EU stood at £294 billion in 2019 (43% of all U.K. exports).

So even the government’s “economic” case to water down citizens’ privacy rights looks to be puffed up with the same kind of misleadingly vacuous nonsense as ministers’ reframing of a post-Brexit U.K. as “Global Britain.”

Everyone hates cookie banners, sure, but that’s a case for strengthening not weakening people’s privacy — for making non-tracking the default setting online and outlawing manipulative dark patterns so that internet users don’t constantly have to affirm they want their information protected. Instead the U.K. may be poised to get rid of annoying cookie consent “friction” by allowing a free-for-all on citizens’ data.",Social Media,TechCrunch,https://techcrunch.com/2021/08/26/uk-names-john-edwards-as-its-choice-for-next-data-protection-chief-as-govt-eyes-watering-down-privacy-standards/,"The UK government is planning to reduce data protection standards, allowing companies to track and profile website visitors without obtaining consent. This could lead to a lack of trust in digital services and open the door to unethical and unfair uses of people's data. It also risks undermining the UK's data adequacy deal with the EU, which would have negative impacts",Security & Privacy
352,Tech leaders discuss how social media is broken and what we can do about it – TechCrunch,"Toxic culture, deadly conspiracies and organized hate have exploded online in recent years. At TechCrunch Sessions: Justice we talked with Color of Change’s Rashad Robinson, Accountable Tech’s Jesse Lehrich and Naj Austin, of Somewhere Good and Ethel’s Club about how much responsibility social networks have in the rise of these phenomena and how to build healthy online communities that make society better, not worse.

On building intentional social spaces that cultivate positive behavior

When we think of social networks, we think of the huge platforms that dominate that conversation today. But those aren’t the only models for how digital communities can grow. Alternative social networks designed with specific communities in mind could cultivate healthier, more positive experiences for the people who need those spaces most.

Austin: One of the aspects that was really important to us when we got started was size. And so one of the factors of Somewhere Good is kind of connecting people with smaller, more intimate communities, because people tend to behave better in smaller networks. One of the things that we use in our product meetings to kind of base a lot of our decisions off of is mimicking a real-life dinner party. Who would you invite to that party? How do people interact when they’re in those kinds of smaller intimate spaces, it tends to, again, be a lot more aligned with better behavior. And then that’s, you know, that’s our perspective in terms of the many ways in which social networks can sort of be more user friendly and a multitude of ways. (Timestamp: 1:26) “… Even with Ethel’s Club, the physical location in Brooklyn, we built it out, so that it necessitated smaller, more intimate interactions with our members. And so we took that same little nugget of knowledge and used it digitally and Somewhere Good is now our technology platform that allows us to build that out on a much larger, faster scale, but still at that level of intention that people want to be connected in a more intimate manner with one another.” (Timestamp: 2:49)

On tech industry exceptionalism

Tech, in spite of its historic wealth and power, lobbying efforts, addictive products and global effects would never liken itself to something like the oil and gas or tobacco industries. But now that we’re seeing some of the society-wide ills Silicon Valley has sown, tech’s exceptionalism is looking more misguided than ever.

Robinson: I do think that in Silicon Valley, there’s… a tendency for people to be maybe a lot more impressed with themselves and impressed with their politics and impressed with their sort of like, ability to believe in something bigger… Sometimes it’s a lot easier to deal with a Coca-Cola where like I go in, and I know that these people think that they’re making soda, not making like a new society for all of us. And I can like, deal with the impacts of something that they’re doing. And we can all be on the same page, because they don’t think that they should get a Nobel Peace Prize. But in tech world, the people think that they can code, we can code our way out of structural racism, when in fact, the code is just amplifying structural racism. (Timestamp: 8:15)

On lessons about online hate and disinformation from the Clinton campaign

In 2021’s pandemic-ravaged world, 2016 feels like a world away, but Donald Trump’s successful campaign for president, the Russian disinformation scandal and an open embrace of white supremacy in mainstream politics gave a telling glimpse of what was to come in the next four years — and beyond.

Lehrich: Did I see QAnon coming? Probably not. But did I see this, like, horrifying crawl toward more and more explicit, you know, racism that’s always been there, obviously, but had been sort of at least relegated to the corner… You know, like, out and out racists like proud to be part open out in the open being part of these kinds of communities? That was that we definitely saw that on the horizon. And I don’t think we necessarily grappled with it the right way. I don’t know, it was a really challenging thing to try to navigate at the time, but I very much felt like something really ugly is coming. And I definitely poured every hour of my waking time during the year and a half into that campaign, in part because I was like, terrified of where the country was headed if we didn’t win the election, and everything and more that I shared has definitely come to fruition. (Timestamp: 13:07)

On how social platforms mirrored entrenched racism in society

Like more traditional sectors, the tech industry is dominated by white men who have created wealth by building what they know. The industry may be ahistorical by definition, but without looking back and grappling with the ugly side of societal power structures like misogyny and white supremacy, tech is doomed to perpetuate those same inequities at scale.

Austin: I don’t think the issues we’re seeing come from the internet and social platforms being free. I think it’s deep embedded systemic problems, as Rashad mentioned, that have haunted this country since day zero, think the people that are creating these platforms are creating what they know, which is to create, patriarchal, you know, systems that live within platforms that look glossy, and have illustrations and use fun, human centric language, but at the heart of it don’t make space for what marginalized communities feel on those platforms. And so I think that’s just embedded in almost every platform we use. As Rashad mentioned, we’ve got… the fact that [Zoom] didn’t recognize that there are people who want to purely cause terror to Black people and people of color, for fun — it is a big issue. (Timestamp: 16:35)

On how lived experience influences design

One way to build social networks that allow a diverse range of communities to flourish is to have those people in the room to begin with, building together on day one. Imagining online social spaces that feel safe and enriching is a natural process when your community has had to deal with online hate and harassment everywhere else for years.

Austin: How do you get in front of these issues that my team which is composed of Black people, Latino people, Asian American people, queer people, non-binary people, the things that we experience every single moment of being online and sort of a larger explanation of that, right? That can be Tumblr, it can be Twitter, it can almost be can every platform. All the things that we’ve lived through, we are saying, What if we didn’t have to? (Timestamp: 16:35)

On how the government should hold social platforms accountable

Federal and state governments are interested in cracking down on tech’s outsized power for the first time. A good place to start might be looking at tech like we already look at regulation on issues like food safety.

Robinson: We need some sort of CFPB, FDA version of infrastructure at the government level. Because anyone who has watched some of the hearings, knows that the hearings on nuclear power, or even keeping our milk safe, would look the same if we didn’t actually have government infrastructure, people who we elect to be in Congress or Senate can’t be experts on all these issues. And so part of having that infrastructure is the same way I talked about having buildings that meet code. It’s not because of our elected officials or experts. It’s because we build the infrastructure at the government level, and that we actually have fines and accountability that’s at scale to make sure that they’re that they’re that there’s consequences. (Timestamp: 22:19)

The big challenge in regulating big tech companies is that they’ve become so large and so powerful that financial punishments can’t even make a dent at this point. Meaningful change needs to realign the industry’s incentives by examining the structures that allowed these companies to grow so powerful with no oversight to begin with.

Lehrich: The fundamental incentive structure around large social media platforms right now is so perverse, they are incentivized to amplify the most toxic content, disinformation, hate speech — like that stuff drives engagement. And so long as they have platforms that where they have no accountability… talking about Section 230 reform, there’s no way to hold them liable from a legal standpoint, the FTC is not hitting them with fines that really hurt them. They don’t have ends and, and there’s just no friction anywhere. … Until we fundamentally, fundamentally upend that incentive structure, they’re gonna continue profiting off hate and distortion and deceit and delusion and discrimination. And that’s just the reality. (Timestamp: 24:39)

You can read the entire transcript here.

Related sessions from TechCrunch Sessions: Justice",Social Media,TechCrunch,https://techcrunch.com/2021/03/09/tech-leaders-discuss-how-social-media-is-broken-and-what-we-can-do-about-it/,"Toxic culture, deadly conspiracies and organized hate have grown in prevalence due to the lack of accountability from social media platforms. Their incentive structure encourages the amplification of the most toxic content, which has led to an increase in disinformation, hate speech, and discrimination.",Equality & Justice
353,"A PC gaming site had to ban political troll mods for games, because nowhere is safe – TechCrunch","NexusMods, a large platform and gathering place for modding PC games, has banned all content relating to the U.S. elections following a flood of troll content, saying “we’ve decided to wipe our hands clean of this mess.” Not exactly headline news, no, but a reminder that the toxic behavior frequently seen (and blamed) on social media is pervasive even in niches where politics would seem to be completely irrelevant.

“Modding” (as in modifying) is the practice of creating new content for games that players can then install on their own, for example adding new levels or characters, or adjusting the interface or difficulty. NexusMods is one of the larger collections of such mods, and a lively community.

Unfortunately, even something as simple as a way to add decorative tapestries to Skyrim is a proxy political battleground, with numerous mods appearing to, for example, replace generic enemies in a game with Trump supporters or “rioters.” Here’s a screenshot from Reddit user Cipherx02, who noted that users were also filling the description fields with disinformation:

In a post to the site’s news page, the admins of NexusMods walk a fine line in expressing their frustration without espousing any political ideology apart from, perhaps, “anti-idiot”:

Recently we have seen a spate of provocative and troll mods being uploaded based around current sociopolitical issues in the United States. As we get closer to the US election in November we expect this trend to increase as it did this time 4 years ago. Considering the low quality of the mods being uploaded, the polarising views they express and the fact that a small but vocal contingent of our users are seemingly not intelligent or grown up enough to be able to debate the issues without resorting to name calling and baseless accusations without proof (indicative of the wider issues plaguing our world at this time) we’ve decided to wipe our hands clean of this mess and invoke an outright ban on mods relating to sociopolitical issues in the United States. We have neither the time, the care or the wish to moderate such things. This ban will apply to all mods uploaded from the 28th of September onwards. We will review this restriction sometime after the next President of the United States has been inaugurated.

No doubt all over the web there are situations of this sort as ordinarily politically neutral spaces are infected by toxic discourse. Unlike Facebook and YouTube, however, smaller sites and communities don’t have thousands of paid moderators or sophisticated machine learning tools to nip the problems in the bud.

As such, a total ban doesn’t seem so much an overreaction, as the only reasonable reaction. As the election approaches (and likely well beyond that), it’s probable that many small communities will have to draw a line in the sand or risk serious incidents such as doxing, threats and the unwelcome attentions of angry internet mobs.",Social Media,TechCrunch,https://techcrunch.com/2020/10/01/a-pc-gaming-site-had-to-ban-political-troll-mods-for-games-because-nowhere-is-safe/,"The toxic behavior and political divisiveness seen on social media has spread to even the most remote corners of the internet, and is forcing many smaller online communities to draw a line in the sand or risk the unwelcome attentions of angry mobs.",Discourse & Governance
354,Google Explores Re-Ranking Search Results Using +1 Button Data,"Google is making plans to turn its +1 button into a crowdsourcing tool that helps it re-order search results and fight web spam.

While not surprising, the move would bring Google's search engine into the social networking era, while simultaneously creating a new avenue for blackhats to manipulate search results and potentially incurring the wrath of trust-busting authorities.

Google confirmed its plans in an e-mail to Wired.com.

""Google will study the clicks on +1 buttons as a signal that influences the ranking and appearance of websites in search results,"" a spokesman wrote. ""The purpose of any ranking signal is to improve overall search quality. For +1’s and other social ranking signals, as with any new ranking signal, we'll be starting carefully and learning how those signals are related to quality.""

But these plans are a touchy subject for the search giant, especially given the scrutiny that Google is under from regulators in Washington and Europe over complaints that the company's results favor its own products over those of other companies.

As if to underscore that point, Google prefaced its admission of the +1 search integration project to Wired.com with a statement downplaying its potential significance: ""There are more than 200 signals that we use to determine the rank of a website, and last year we made more than 500 improvements to the algorithm.""

Introduced in March, the +1 sharing button debuted with little incentive for web surfers to click on it. If you +1-ed a story on a website that embedded the button, your profile picture would display next to the URL when a friend of yours ran a search with results that included that URL.

But last week, the button entered adolescence, and can now be used to post stories to friends and followers on Google+, much as the Like button functions for Facebook.

So the next step of using what people are liking, sharing and buzzing about online to rearrange search results is obvious enough.

Google dipped its toe into these waters with Twitter by licensing its stream of Tweets, but that agreement ended before Google got so far as to figure out how to do more with the fire hose of real-time information than just decorate pre-computed search results with Tweeters' profile pictures.

And as for Facebook? Google would love to get at its data -- the way that Bing is already -- but the two companies go together like toothpaste and orange juice. Facebook will likely never let Google anywhere near its data stream, which meant that Google had to build in its own social network.

But therein lies the rub. If Google's search results become heavily dependent on social signals from Google+, then there's going to be heavy pressure on the net's websites to embed the Google+ button.

And depending on where you work -- say, Facebook or the Justice Department -- that could look like Google is unfairly using its search engine might to boost its Facebook alternative.

That might explain why Forbes killed a story by Kashmir Hill entitled ""Stick Google Plus Buttons On Your Pages, Or Your Search Traffic Suffers"" which was seemingly based on information from a meeting with Google ad representatives. On August 18, Hill wrote, ""the message in this meeting was clear: ""Put a Plus One button on your pages or your search traffic will suffer.""",Social Media,WIRED,https://www.wired.com/2011/08/google-studying-re-ranking-search-results-using-1-button-data-but-its-touchy/,"Google's plans to use its +1 button to influence search results could lead to manipulation of results and unfair competition, as there is a risk of websites having to embed the Google+ button to avoid being penalized in search rankings.","Information, Discourse & Governance"
355,How to Take Back Your Facebook News Feed,"In December, Facebook announced yet another tweak to the News Feed. This time, the social network would begin prioritizing “meaningful” conversations between friends and family over stories from publishers, brands, and businesses. If this all sounds familiar to you, that’s because Facebook has made a number of similar changes in the past.

I was skeptical about the latest shift. Over the last decade, my News Feed has increasingly begun to clog with life updates from hundreds of people I haven’t seen in years. Meanwhile, my closest friends—like many people’s—share less on the platform than ever. After Facebook’s announcement, I deleted the app from my phone, less in protest than in resignation to it having become more of a phone book than a social network.

SIGN UP TODAY Sign up for the Daily newsletter and never miss the best of WIRED.

Going nuclear seemed hasty though, especially given the thousands of hours I’d invested in Facebook over the years. It also occurred to me that the social network—more so than platforms like Instagram and Twitter—gives its users significant control over what they see in the News Feed, including several levers I’d never bothered to pull. So rather than quit outright, I decided to conduct an experiment.

Over the course of about 10 days, I used Facebook’s built-in features—as well as several third-party tools—to see if I could make the platform fun and “meaningful” again. Some of it worked, but a lot of it didn’t. Mostly it was a reminder that you have more power over your News Feed than Facebook often lets on—for better or worse.

Phase One: See First

My first change was to prioritize pages and profiles to “see first” in my feed. When you click News Feed on the left-hand side of the Facebook desktop site, an option to Edit Preferences* will display. The first option is Prioritize who to see first. I chose a handful of news sites I like reading, some of my close friends, and my boyfriend. Facebook only allows users to choose 30 people and pages to see first; I quickly used all the allocated spots.

The more years I spend on Facebook, the less inclined I am to post weird memes, inside jokes, and any actual feelings.

To some degree, that one change did help my News Feed become more relevant. After I set those preferences, Facebook would usually greet me with a post from my past via the fairly creepy “On This Day” feature, then an ad, then a smattering of posts from The New York Times and other publications I chose. I still didn't see much from my close friends because, well, they don’t often post on Facebook, a major reason my News Feed felt so irrelevant to begin with.

The more years I spend on Facebook, the less inclined I am to post weird memes, inside jokes, and any actual feelings. Same with my friends. That’s because the breadth of people who might see that content has become wider. If I post a political opinion, it’s possible my aunt, my boyfriend’s cousin, and an awkward years-ago Tinder date will see it. Facebook lets users tailor who can see the content they post, but adjusting those settings feels tedious when I can just head to more intimate places like Instagram or Snapchat instead.

A recent report from The Information suggests I’m not alone. Overall sharing on Facebook fell 5.5 percent from the middle of 2014 to the middle of 2015, according to their analysis. That might seem like a small drop, but people shared four times fewer “personal updates”—like thoughts about their lives—during the same period.",Social Media,WIRED,https://www.wired.com/story/take-back-your-facebook-news-feed/,"The main undesirable consequence of social media discussed here is the decline in personal updates due to the fear of oversharing. This fear is caused by the fact that posts are visible to a large number of people, leading users to become more guarded in what they share and ultimately leading to a decrease in meaningful conversations and connections.",Social Norms & Relationships
356,More Facebook Privacy Woes: Gay Users Outed To Advertisers,"Facebook's privacy problems continue this week after researchers discovered that Facebook may inadvertently be outing gay users to its advertisers.

Saikat Guha from Microsoft and Bin Cheng and Paul Francis from the Max Planck Institute for Software Systems set out to study the challenges in targeted advertising systems (PDF) online, but found that advertisers can ferret out gay users from straight users just by looking at who's clicking — even when that sexual preference is hidden.

The team set up profiles for straight men, straight women, a gay man, and a lesbian to see how the ads differed between the different types of users. The ads did change for the gay and lesbian users, though the difference in the ads was much greater for the gay males (compared to the straight males) than gay females, ""indicating that advertisers target more strongly to [gay males]"" reads the paper.

This in itself isn't a huge cause for concern, but the researchers were disturbed by the fact that the text for the ads were sexual-preference-neutral, even though they were measurably different. Half of the ads were exclusively shown to gay men, but the text associated with them was neutral, therefore not giving a clear indicator to those users that the ads they click were directly tied to their sexual preferences.

""The danger with such ads, unlike the gay bar ad where the target demographic is blatantly obvious, is that the user reading the ad text would have no idea that by clicking it he would reveal to the advertiser both his sexual-preference and a unique identifier (cookie, IP address, or email address if he signs up on the advertiser’s site),"" wrote the researchers.

If the advertiser in question also collects other data, such as Facebook ID, the info can be tied together without much thought, even if the user has not made that information public. As we saw earlier this week, Facebook IDs and other user info are running rampant across ad networks and third-party app developers, and the collection of such information (especially when tied to something as sensitive as sexual preference) could spell disaster for a user who thinks he's being fastidious when keeping his profile private.

Facebook's official policy is that any data collected by advertisers must be anonymized, but given this week's discoveries regarding Facebook IDs, it's pretty clear that there isn't anyone making sure the policies are enforced until after the fact. This is one area that two Congressmen focused on in their recent letter to Facebook CEO Mark Zuckerberg, but it's unlikely that any major changes to how Facebook handles its advertisers will come anytime in the near future.

Further reading:

Follow Epicenter on Twitter for disruptive tech news,

See Also:",Social Media,WIRED,https://www.wired.com/2010/10/more-facebook-privacy-woes-gay-users-outed-to-advertisers/,"The researchers found that advertisers can infer a user's sexual preference from the ads they click on, even if the text of the ad is neutral, potentially outing gay users to advertisers without their knowledge. This, combined with the rampant spread of Facebook IDs and other user data across ad networks and 3rd party app developers, could mean unwanted exposure of",Security & Privacy
357,Twitter ‘took over’ a user’s account and joked about reading their DMs – TechCrunch,"At a time when tech giants have come under fire for failing to protect the private data of their users, Twitter took over one of its users’ accounts for fun and then tweeted jokes about reading the account’s private messages. The account, to be clear, was willingly volunteered for this prank by social media consultant Matt Navarra, who’s well-known in some Twitter circles for being among the first to spot new features on social media platforms like Twitter and Facebook.

In fact, TechCrunch itself has credited Navarra on a number of occasions for his tweets about features like Twitter’s new camera, Facebook’s “time spent” dashboard, Facebook’s “Explore” feed, Instagram’s “Do Not Disturb” setting and more. Several other tech news sites have done the same, which means Navarra’s private messages (direct messages, aka DMs) probably included a lot of conversations between himself and various reporters.

He’s also regularly tipped off about upcoming features or those in testing on sites like Twitter. One could assume he has regular conversations with his network of tipsters through DMs, as well.

Initially, we believed the whole “account takeover” was just a joke — perhaps a case of Navarra poking fun at himself and his own obsession with social media. After all, “takeovers” are a common social media stunt these days, particularly on Instagram Stories. But they usually involve an individual posting for a brand — not a brand posting for an individual.

Navarra had the idea on Monday, and tweeted out a call for someone to run his account for a day.

He tells TechCrunch he had a tragic incident in his family, and offered the chance for someone else to tweet as him for the day so he could take a day away from Twitter. He also thought it could be fun. (Twitter tells us he remained logged in while the company was tweeting from his account, however.)

Navarra says he was surprised that Twitter volunteered for the job, and he agreed to give them control. Most of his followers — fellow social media enthusiasts — were excited and amused about the plan, which they touted as “epic,” “gold” and a “great idea!”

We’ve never been more ready — Twitter (@Twitter) March 25, 2019

Navarra on Tuesday tweeted out photos of himself handing over his account key to Twitter in a DM thread.

On Tuesday, Twitter began tweeting as Navarra. This mostly involved some gentle roasting — like tweets about muting people asking for an “edit” button, and other nonsense. Twitter said then it was going to tweet out some of Navarra’s drafts, and posted things like “who has a Google Wave code?” and something about BBM, among other things. (Navarra says these were fake — not real drafts.)

Twitter will now allow Tweets from Twitter to appear on Tweets in Twitter that use Twitter that previously used the Twitter API #Twitter — Matt Navarra (@MattNavarra) March 26, 2019

But other jokes were less funny. Twitter said it was reading Navarra’s DMs, for example.

(At the time of posting, these embedded tweets were posted from “Tweet Navarra” as Twitter temporarily changed the account name while it was tweeting as Matt. But it’s since been changed back, so these embeds show the current account name, “Matt Navarra.”)

yikes, there are a lot — Matt Navarra (@MattNavarra) March 26, 2019

DMs: read. Time for the Tweets. — Matt Navarra (@MattNavarra) March 26, 2019

The company then posted a screenshot of his Direct Message inbox to poke fun at the fact that he had DM’d with an account called “Satan,” in one incident.

Navarra played along, joking from his new account for the day @realmattnavarra for Twitter to “ignore that DM from Zuck.”

Uhhhhh…. HELLO?! Please don’t tweet Facebook. And ignore that DM from Zuck. WTF have I done. pic.twitter.com/4tDWlIbhJn — Real Matt Navarra (@realmattnavarra) March 26, 2019

While I personally had not DM’d Navarra anything compromising, I can’t speak for everyone who had ever messaged him. Even if Navarra had signed up to have his account taken over, those he messaged with had not volunteered to have their privacy violated. And though my conversations with him were innocuous, it was disconcerting to know that my message history with a private individual was accessible by someone at Twitter.

I was not alone in that sentiment.

I am not comfortable with @Twitter accessing our conversation with Matt Navarra because I've talked about my personal matters and I expected Matt will only have access to Not cool going through someone else's DM inbox. This is an invasion of privacyhttps://t.co/JHnM7hfCXZ — Jane Manchun Wong (@wongmjane) March 27, 2019

Reached for comment, Navarra claims his “DMs were all deleted” before Twitter entered his account. Unfortunately, there’s no way to verify this, as DM deletion on Twitter is one-sided. That means that even if he deleted the DMs, the person who sent them could still view them in their own inbox.

It also appears from the screenshot Twitter posted that the entire inbox hadn’t been wiped.

At the end of the day, Navarra may have been misguided with this stunt — perhaps he should have first demonstrated that he had cleaned out his inbox by posting a tweet of it being empty — but he is not a public social media company. It’s completely nuts that Twitter thought this was a funny idea.

Whether or not Twitter actually saw private conversations, it’s bad optics for the company to take over a user’s account for a lark, then joke about violating users’ privacy at a time when tech giants like Facebook and Google are under threat of increased regulations for not taking care of users’ private data.

Twitter did not provide a comment, but confirmed it logged into Navarra’s account for a few hours for the takeover in the hopes of starting fun conversations with his followers.",Social Media,TechCrunch,https://techcrunch.com/2019/03/27/twitter-took-over-a-users-account-and-joked-about-reading-their-dms/,"Twitter's takeover of Matt Navarra's account for a day, and subsequent jokes about reading his private messages, demonstrated a careless attitude towards user privacy at a time when tech giants like Facebook and Google are already under fire for not protecting user data. This stunt shows how social media companies can easily access and misuse individuals' private conversations, highlighting",Security & Privacy
358,Facebook Bans Pot-Leaf Image in Political Ad,"Facebook may be run by a bunch of twenty-somethings, but that doesn't mean they like pot, or at least, pro-pot ads.

After serving up 38 million ads since Aug. 7 from a group supporting the legalization of marijuana, Facebook told the group Aug. 16 that it could no longer use a pot leaf in its ad, since it might promote smoking.

""The image in question was no longer acceptable for use in Facebook ads,"" wrote Facebook spokesman Andrew Noyes in an e-mail to Wired.com. ""The image of a marijuana leaf is classified with all smoking products and therefore is not acceptable under our policies.""

But the Just Say Now campaign contends that Facebook isn't harshing on their mellow -- it's censoring them, especially given that marijuana legalization is on the ballot in the upcoming election in California. And it's calling on its supporters -- some 6,000 fans on its Facebook page -- to swap out their profile picture for an image of a pot leaf with a banned box over it (right).

The ads were titled, ""End the war on marijuana"" and called on users to sign a petition asking President Barack Obama to support the right of states to legalize marijuana.

Facebook's core audience supports drug legalization, according to polls, and a large number of young adults say they are more likely to vote if legalization is on the ballot, according to Jane Hamsher, co-founder of the Firedoglake blog, who's helping run the campaign in concert with Students for a Sensible Drug Policy.

""We aren't trying to sell people pot. This is a policy issue,"" Hamsher told Wired.com, noting that more than 50 percent of inmates in the federal prison system were there on drug charges and that law-and-order types like former Reagan administration lawyer Bruce Fein support decriminalization. ""The time is right for this, and Facebook shutting this down is a real blow when we are trying to open up a conversation.""

For its part, Facebook says it's cool with the group advertising -- just so long as they don't use a marijuana leaf in their ads.

""We don't allow any images of drugs, drug paraphernalia or tobacco in ad images on Facebook,"" wrote Facebook spokeswoman Annie Ta. ""Just Say Now can continue to advertise on Facebook using a different image.""

But Hamsher notes that Facebook allows alcohol images and argues the ban on the image cripples their campaign.

""The image is the campaign when you run those ads -- that's what you see,"" Hamsher said. ""It's like telling them they can't use the F in the square,"" referring to Facebook's own distinctive logo.

No one disputes that Facebook has the legal right to ban the ad, but Hamsher argues that it just doesn't make sense.

""It seems like a decision made to appease somebody's grandma,"" Hamsher said.

Photo: A former Kentucky Fried Chicken turned medical marijuana dispensary. Credit: TheTruthAbout

Follow us for disruptive tech news: Ryan Singel and Epicenter on Twitter.

See Also:",Social Media,WIRED,https://www.wired.com/2010/08/facebook-marijuana/,"Facebook has recently banned the use of a pot leaf image in their ads, which is seen as censorship by a group campaigning for marijuana legalization in the upcoming election in California. This has hindered their ability to open up a conversation on the issue of drug policy and has been seen as an unnecessary decision by some.",Equality & Justice
359,"Africa Roundup: Jumia’s post-IPO earnings, Gokada’s $5.3M raise, Facebook’s fake-news purge, Joe Montana’s fintech investment – TechCrunch","Jumia held its first post-IPO earnings call and weathered a short-sell assault in May, with Wall Street showing confidence in the Pan-African e-commerce company.

On the numbers, key takeaways were that Jumia’s Gross Merchandise Value (GMV) — the total amount of goods sold over the period — grew by 58% to €240 million. Marketplace revenue grew 102% to €16 million, and gross profits as a percentage of GMV grew by 6.5% in Q1 2019.

Overall, Jumia’s operating losses for the period widened to €45.4 million from €34.3, and negative EBITDA increased to €39.5 million from €30.2.

So the startup’s still losing money — see the big losses reported in the IPO filing — but is improving its ability to earn.

CEO Sacha Poignonnec also shared a longer-term revenue strategy on Jumia’s Q1 earnings call. The startup plans to convert its JumiaPay and Jumia Logistics capabilities to standalone services across Africa.

Founded in Lagos in 2012, the company currently operates multiple online verticals in 14 African countries — from B2C consumer retail to travel bookings.

For Jumia, going public has been an up and down affair. After becoming the first tech startup operating in Africa to list on a major exchange (the NYSE in April), the company saw its share rise 70% after listing on the NYSE in April at $14.50.

Then in May, Jumia’s stock tumbled when it came under assault from a short-seller, Andrew Left, who accused the company of fraud. On the earnings call the startup’s CEO responded to the short-seller claims saying, “Jumia stands by our prospectus and audited financials…and will not be distracted by those who look…to profit at our expense.” Poignonnec later took to media and refuted claims as “market rumors rather than facts.”

Citibank analyst Andrew Howell published his own response, much of it discrediting Citron Research.

Overall, Wall Street seemed confident in Jumia’s post-IPO results and outreach, with Raymond James and Berenberg upgrading their Jumia stock recommendations to buy-equivalent ratings. Jumia’s stock has remained stable since, closing at $25.81 Monday.

When it comes to e-commerce in Africa, Jumia may face stiffer competition from DHL. The shipping giant teamed up with MallforAfrica to expand its Africa eShop app to 20 countries in May.

DHL went live with the digital retail app in April, bringing more than 200 U.S. and U.K. sellers — from Neiman Marcus to Carters — online to African consumers.

Africa eShop operates using startup MallforAfrica.com’s white-label fulfillment service, Link Commerce.

There’s a competitive e-commerce scenario brewing between the two platforms. DHL Africa eShop touts itself as “Africa’s Largest Online Shopping Platform.” Jumia said, “We believe that our platform is the largest e-commerce marketplace in Africa,” in its SEC F-1 filing.

DHL’s partner for the new app, MallforAfrica, brings experience collaborating with a number of big-name retailers, including Macy’s and Best Buy. MFA’s payment and delivery system serves as a digital broker and logistics manager for big-name retailers to sell goods in Africa.

As for the global e-commerce names, Alibaba has talked about Africa expansion, but for the moment has not entered in full.

Amazon offers limited e-commerce sales on the continent, but more notably, has started offering AWS services in Africa.

With Jumia’s commitment to offer its logistics and payments capabilities as services, DHL and MallforAfrica could be on a footing to compete with Jumia. All three could also find themselves either competing (or working) with big e-commerce names entering Africa.

For the moment, DHL’s Africa eShop expansion creates additional choice on overlapping product categories with Jumia, while offering African consumers more price competition in the operating countries it shares with Jumia. These currently stand at 10: South Africa, Kenya, Nigeria, Tanzania, Cameroon, Uganda, Ivory Coast, Rwanda, Senegal and Ghana.

There’s been a lot of market movement in Africa’s motorcycle ride-hail space over the last year-plus. Uber began offering a two-wheel transit option in East Africa in 2018, around the same time Bolt (previously Taxify) started motorcycle taxi service in Kenya.

Uganda-based motorcycle ride-hail company SafeBoda moved into Kenya in 2018 and last month raised a Series B round of an undisclosed amount on plans to further expand into in East Africa and Nigeria.

In Lagos, there’s already motorcycle ride-hail company Gokada, which raised a $5.3 million Series A round in May.

Gokada has trained and on-boarded more than 1,000 motorcycles and their pilots on its app that connects commuters to moto-taxis and DOT-approved helmets.

The startup has completed nearly 1 million rides since it was co-founded in 2018 by Fahim Saleh — a Bangladeshi entrepreneur. Gokada will use the financing to increase its fleet and ride volume, while developing a network to offer goods and services to its drivers, Saleh told TechCrunch in this exclusive.

Gokada differs from other ride-hail ventures in that it doesn’t split fare revenue with drivers. Gokada charges drivers a flat-fee of 3,000 Nigerian Naira a day (around $8) to work on their platform. The company looks to generate a larger share of its revenue from building a commercial network around its driver community.

More American sports celebrities are getting involved in African tech. Serena Williams invested in Andela, NBA star Andre Iguodala joined Jumia’s board and, in May, NFL hall-of-famer Joe Montana invested in African fintech startup Chipper Cash.

The Africa focused no-fee, cross-border payment startup raised a $2.4 million seed round led by Deciens Capital.

The payments company also persuaded 500 Startups and Liquid 2 Ventures — co-founded by Joe Montana — to join the round.

Chipper Cash’s Ugandan chief executive, Ham Serunjogi, pitched the U.S. football legend directly.

Based in San Francisco — with offices in Ghana and Nairobi — Chipper Cash has processed 250,000 cross-border, P2P transactions for more than 70,000 active users, according to Serunjogi.

In conjunction with the seed round, Chipper Cash is launching Chipper Checkout: a merchant-focused, C2B mobile payments product.

This side of the startup’s offerings isn’t free, and Chipper Cash will use revenues from Chipper Checkout to support its no-fee, Africa mobile money business.

Chipper Cash will expand beyond its current operations in Ghana, Kenya, Rwanda, Tanzania and Uganda within the next 12 months.

Finally, in May, Facebook purged a network of hundreds of pages, groups and Instagram accounts it labeled as producing “coordinated inauthentic behavior” toward Africa.

The activity originated in Israel and was largely targeted toward Nigeria, Senegal, Togo, Angola, Niger and Tunisia.

It was mostly political in nature and primarily paid for by Archimedes Group, a global political consulting firm, Facebook said.

The affair highlighted a pattern of fake news on social media platforms rearing its head in Africa. Cambridge Analytica, backed by U.S. big-data billionaire Robert Mercer, was found to have been involved in elections in Kenya and Nigeria before its controversial role directing pro-Brexit and pro-Trump online activity in 2016. Facebook later banned Cambridge Analytica from its platform.

Social media-driven fake news — primarily on Facebook and WhatsApp — became such an issue in Kenya’s 2017 elections the country’s parliament passed a bill in 2018, with specific punitive measures, to combat it.

Facebook has prioritized growth in Africa and grown Africa users to more than 200 million and Facebook-owned chat-tool, WhatsApp, is the most downloaded messenger app on the continent.

But Facebook’s recent Africa account purge shows when Facebook travels, so too does its list of pros and cons, including the ability of global actors to use it for nefarious uses in local settings.

More Africa-related stories @TechCrunch

African eech around the ‘net",Social Media,TechCrunch,https://techcrunch.com/2019/06/04/africa-roundup-jumias-post-ipo-earnings-gokadas-5-3m-raise-facebooks-fake-news-purge-joe-montanas-fintech-investment/,"The recent purge of hundreds of pages, groups and Instagram accounts by Facebook, labeled as producing “coordinated inauthentic behavior” towards Africa, highlights the dangers of fake news on social media and its use by global actors in local settings. This is concerning, as Facebook and WhatsApp are the most downloaded messaging apps on the continent.",Security & Privacy
360,"Facebook, Twitter still failing on hate speech in Germany as new law proposed – TechCrunch","Facebook and Twitter have once again been criticized in Germany for failures to promptly remove hate speech being spread via their platforms.

At the same time, the German government has presented a draft bill aimed at more effectively combating hate criminality and criminal offenses on social network platforms, arguing that the companies’ ongoing failures necessitate tighter regulation — with a potential fine of up to €50 million being floated for social networks breaching what are intended to be binding standards for dealing with complaints and removing criminal content.

As with many EU countries, Germany has specific hate speech laws that criminalize certain types of speech, such as incitement to racial violence. The issue has stepped up the domestic political agenda in the country in recent years, following the refugee crisis — with Germany taking in a large number of asylum seekers.

The biggest problem is that the networks do not take the complaints of their own users seriously enough. Federal Justice and Consumer Protection Minister, Heiko Maas

The country also has elections this year, heightening concern over the role social media can play in shaping and influencing public opinion.

“There can be just as little space in the social networks as on the street for crimes and slander,” said Federal Justice and Consumer Protection Minister, Heiko Maas, in a statement today (translated from German via Google Translate). “The biggest problem is that the networks do not take the complaints of their own users seriously enough.”

Back in December 2015, Facebook and Twitter gave a commitment to the German government that they would remove criminal hate speech from their respective platforms within 24 hours. Google also agreed to do so on YouTube.

Last May, Facebook, Twitter, Google and Microsoft also all agreed with the European Commission on a code of conduct that committed to removing hate speech within 24 hours.

However, in the latest German government-funded study monitoring the performance of the companies, the Ministry of Justice said Facebook has become worse at promptly handling user complaints, saying the company deleted or blocked 39 percent of the criminal content reported by users — a seven percentage point decline versus the first test of its performance.

In addition, only one-third of content reported by Facebook users was deleted within 24 hours of the complaint being made, according to the survey.

Twitter’s performance is also criticized, with the survey finding that only one of a hundred user messages had been erased, and none of the deletions took place within 24 hours.

The pair’s failings also contrast negatively with Google, which is reported to have made significant improvements regarding YouTube content complaints since the tests began — with the study finding that 90 percent of user-reported criminal content was deleted from the platform, and 82 percent of the deletions occurred 24 hours after the notification.

“Google shows with the platform Youtube that it is better,” said Maas. “Therefore, it is now clear that we must further increase the pressure on social networks. We need legal regulations to make companies even more obligated to eradicate criminal offenses.”

We reached out to Facebook and Twitter for comment on the findings. Twitter declined to make a statement but a spokesman said the company has made changes in the past few weeks aimed at reducing the spread of abusive content on its platform which may not have been in place at the time of the survey.

However, these tweaks appear most focused on using technology to try to automatically identify abusive/problem accounts, or give more tools to users to enable them to filter their own feeds, rather than putting more resource into content complaint processes specifically. (Although Twitter does claim to have improved the transparency of the reporting process, such as notifying users when a complaint has been received and if it is being acted upon.)

At the time of writing, Facebook had not responded to our questions, but we’ll update this story with any response. Update: In an emailed statement a spokesperson for the company said: “We have clear rules against hate speech and work hard to keep it off our platform. We are committed to working with the government and our partners to address this societal issue. By the end of the year over 700 people will be working on content review for Facebook in Berlin. We will look into the legislative proposal by the Federal Ministry of Justice.”

Responding specifically to criticism that it does not take user complaints seriously enough, Facebook said it has changed its internal procedures for interpreting its community standards guidelines in recent months, and has also provided specific guidance for its Community Operations (CO) team regarding hate speech in Germany.

It also said it has “invested heavily” in the CO team — both globally and locally in Germany. By the end of 2017, Facebook says it will have more than 700 employees working with content moderation partner Arvato, based in Berlin.

It also disputes the result of the German government-funded survey (which was carried out by jugendschutz.net), saying the results “do not mirror our internal experience and tests by independent organizations such as FSM” — claiming the latter’s tests of its processes found the opposite: a significant improvement in removing illegal content. (An FSM test based solely on user flagging from January found 65 percent of illegal content was removed from Facebook within 24 hours, and overall it had a deletion rate of 80 percent, according to Facebook.)

“We do not believe that the results of the current jugendschutz.net tests reflect the progress we have made in improving our systems and will analyze this test thoroughly,” Facebook said, adding: “Obviously, we are disappointed by the results and we would like to thank Jugendschutz.net for testing our systems and will study all the reports carefully to help us improve the way we operate. We have clear rules against hate speech and work hard to keep it off our platform. We are committed to working with the government and our partners to address this societal issue.”

Last week Facebook also came under fire in the U.K. for content removal failures pertaining to child safety, after a BBC investigation found the platform failed to promptly respond to the vast majority of reports the journalist made. The news agency had been checking Facebook’s own claims of improved performance after an earlier BBC investigation unearthed secret Facebook groups being used to share child abuse imagery. But it concluded that Facebook had failed to improve over the past year.

Facebook is also facing continued pressure for how its platform is misappropriated to spread so-called “fake news” — an issue that has gained prominence in the wake of the U.S. election when large numbers of bogus political stories were seen to have circulated via the platform — potentially influencing how Americans voted. CEO Mark Zuckerberg initially tried to shrug off the issue, before conceding at the turn of the year that the billion+ user platform does indeed have “a greater responsibility than just building technology that information flows through.”

Since December, Facebook has been piloting a series of measures aimed at fighting the spread of fake news problem, including in Germany since January, working there with local third-party fact-checking organization Correctiv to try to identify and flag dubious content.

However, early signs are that Facebook’s efforts on the fake news front are not too promising either, with Recode reporting earlier this month that it took the platform a full week to label one made-up story as “disputed” — despite the source being a self-confessed fake newspaper.

“Shouldn’t/couldn’t Facebook move faster on this stuff, especially when it’s a clear-cut case like this one? Yup! But that would require Facebook to make these kinds of (easy) calls on its own, and Facebook really doesn’t want to do that,” was Recode’s Peter Kafka’s conclusion of its performance there.

Germany’s Maas also touches on the fake news issue, arguing that the proposed domestic law to tighten content complaint procedures for social media platforms will also help kill fake news.

“We will not establish a commission of truth in a free society in which freedom of expression applies. But because the rules we propose are directed against the spread of criminal content, they are also a means against criminal ‘fake news,'” he said. “‘Fake News’ are punishable, for instance if they fulfill the facts of the offense, slander or the evil slander.”

Among the proposals in the draft law are that social networks operating in Germany be obliged to:

Provide users with a readily identifiable, directly accessible and constantly available procedure for the transmission of complaints about criminal content

Deal with user complaints without delay and to examine the relevance of criminal law

That obviously criminal offenses be deleted or blocked within 24 hours of receipt of the complaint

To delete or block any criminal offense within 7 days after receipt of the complaint and

To inform the user of any decision regarding his complaint

The obligation to delete or block criminal content reported by a user would also apply to all copies of the criminal content on the platform, under the proposals.

The draft law would also require social network operators to report quarterly on the handling of complaints about content relevant to criminal law — including detailing the volume of complaints; their decision-making practices; and the “staffing and competence of the work units responsible for dealing with the complaints.”

These reports would also have to be made accessible to the public online.

Social networks that do not create an effective complaint management system capable of deleting criminal contents effectively and swiftly could be punished via fines of up to €5 million euros against the individual person responsible for dealing with the complaint, and larger fines of up to €50 million against the company itself. Fines also could be imposed if the company does not comply fully with its reporting obligation.

The proposed law also would require social networks to appoint a “responsible contact person” in Germany who could be served in criminal proceedings and civil proceedings — and would themselves be on the hook for a fine of up to €5 million.",Social Media,TechCrunch,https://techcrunch.com/2017/03/14/facebook-twitter-still-failing-on-hate-speech-in-germany/,"The German government's proposed law would impose fines of up to €50 million on social networks that fail to promptly remove criminal content and hate speech, as well as requiring them to report quarterly on the handling of complaints.",Equality & Justice
361,"Voter manipulation on social media now a global problem, report finds – TechCrunch","New research by the Oxford Internet Institute has found that social media manipulation is getting worse, with rising numbers of governments and political parties making cynical use of social media algorithms, automation and big data to manipulate public opinion at scale — with hugely worrying implications for democracy.

The report found that computational propaganda and social media manipulation have proliferated massively in recently years — now prevalent in more than double the number of countries (70) vs two years ago (28). An increase of 150%.

The research suggests that the spreading of fake news and toxic narratives has become the dysfunctional new ‘normal’ for political actors across the globe, thanks to social media’s global reach.

“Although propaganda has always been a part of political discourse, the deep and wide-ranging scope of these campaigns raise critical public interest concerns,” the report warns.

The researchers go on to dub the global uptake of computational propaganda tools and techniques a “critical threat” to democracies.

“The use of computational propaganda to shape public attitudes via social media has become mainstream, extending far beyond the actions of a few bad actors,” they add. “In an information environment characterized by high volumes of information and limited levels of user attention and trust, the tools and techniques of computational propaganda are becoming a common – and arguably essential – part of digital campaigning and public diplomacy.”

Organised social media manipulation campaigns are now prevalent in 70 countries around world, (more than doubling from 28 in 2017) finds latest @oiioxford @polbots report #cypbertroops2019 https://t.co/pZ7TgAo73t pic.twitter.com/L0er8bKpfK — Oxford Internet Institute (@oiioxford) September 26, 2019

Techniques the researchers found being deployed by governments and political parties to spread political propaganda include the use of bots to amplify hate speech or other forms of manipulated content; the illegal harvesting of data or micro-targeting; and the use of armies of ‘trolls’ to bully or harass political dissidents or journalists online.

The researchers looked at computational propaganda activity in 70 countries around the world — including the US, the UK, Germany, China, Russia, India, Pakistan, Kenya, Rwanda, South Africa, Argentina, Brazil and Australia (see the end of this article for the full list) — finding organized social media manipulation in all of them.

So next time Facebook puts out another press release detailing a bit of “coordinated inauthentic behavior” it claims to have found and removed from its platform, it’s important to put it in context of the bigger picture. And the picture painted by this report suggests that such small-scale, selective discloses of propaganda-quashing successes sum to misleading Facebook PR vs the sheer scale of the problem.

The problem is massive, global and largely taking place through Facebook’s funnel, per the report.

Facebook remains the platform of choice for social media manipulation — with researchers finding evidence of formally organised political disops campaigns on its platform taking place in 56 countries.

We reached out to Facebook for a response to the report and the company sent us a laundry list of steps it says it’s been taking to combat election interference and coordinated inauthentic activity — including in areas such as voter suppression, political ad transparency and industry-civil society partnerships.

But it did not offer any explanation why all this apparent effort (just its summary of what it’s been doing exceeds 1,600 words) has so spectacularly failed to stem the rising tide of political fakes being amplified via Facebook.

Instead it sent us this statement: “Helping show people accurate information and protecting against harm is a major priority for us. We’ve developed smarter tools, greater transparency, and stronger partnerships to better identify emerging threats, stop bad actors, and reduce the spread of misinformation on Facebook, Instagram and WhatsApp. We also know that this work is never finished and we can’t do this alone. That’s why we are working with policymakers, academics, and outside experts to make sure we continue to improve.”

We followed up to ask why all its efforts have so far failed to reduce fake activity on its platform and will update this report with any response.

Returning to the report, the researchers say China has entered the global disinformation fray in a big way — using social media platforms to target international audiences with disinformation, something the country has long directed at its domestic population of course.

The report describes China as “a major player in the global disinformation order”.

It also warns that the use of computational propaganda techniques combined with tech-enabled surveillance is providing authoritarian regimes around the world with the means to extend their control of citizens’ lives.

“The co-option of social media technologies provides authoritarian regimes with a powerful tool to shape public discussions and spread propaganda online, while simultaneously surveilling, censoring, and restricting digital public spaces,” the researchers write.

Other key findings from the report include that both democracies and authoritarian states are making (il)liberal use of computational propaganda tools and techniques.

Per the report:

In 45 democracies, politicians and political parties “have used computational propaganda tools by amassing fake followers or spreading manipulated media to garner voter support”

In 26 authoritarian states, government entities “have used computational propaganda as a tool of information control to suppress public opinion and press freedom, discredit criticism and oppositional voices, and drown out political dissent”

The report also identifies seven “sophisticated state actors” — China, India, Iran, Pakistan, Russia, Saudi Arabia and Venezuela — using what it calls “cyber troops” (aka dedicated online workers whose job is to use computational propaganda tools to manipulate public opinion) to run foreign influence campaigns.

Foreign influence operations — which includes election interference — were found by the researchers to primarily be taking place on Facebook and Twitter.

We’ve reached out to Twitter for comment and will update this article with any response. Update: A spokesperson told us: “Platform manipulation, including spam and other attempts to undermine the integrity of our service, is a violation of the Twitter Rules. We’ve significantly stepped up our efforts — investing in people, policies, and tech — to catch this behavior at scale. Additionally, we’re the only company to disclose every account and piece of content that we can reliably link to state-backed activity on the service. Research like this is the reason we’ve made this choice. We believe that full transparency empowers public understanding of these critical issues.”

A year ago, when Twitter CEO Jack Dorsey was questioned by the Senate Intelligence Committee, he said it was considering labelling bot accounts on its platform — agreeing that “more context” around tweets and accounts would be a good thing, while also arguing that identifying automation that’s scripted to look like a human is difficult.

Instead of adding a ‘bot or not’ label, Twitter has just launched a ‘hide replies’ feature — which lets users screen individual replies to their tweets (requiring an affirmative action from viewers to unhide and be able to view any hidden replies). Twitter says this is intended at increasing civility on the platform. But there have been concerns the feature could be abused to help propaganda spreaders — i.e. by allowing them to suppress replies that debunk their junk.

The Oxford Internet Institute researchers found bot accounts are very widely used to spread political propaganda (80% of countries studied used them). However the use of human agents was even more prevalent (87% of countries).

Bot-human blended accounts, which combine automation with human curation in an attempt to fly under the BS detector radar, were much rarer: Identified in 11% of countries.

While hacked or stolen accounts were found being used in just 7% of countries.

In another key finding from the report, the researchers identified 25 countries working with private companies or strategic communications firms offering a computational propaganda as a service, noting that: “In some cases, like in Azerbaijan, Israel, Russia, Tajikistan, Uzbekistan, student or youth groups are hired by government agencies to use computational propaganda.”

Commenting on the report in a statement, professor Philip Howard, director of the Oxford Internet Institute, said: “The manipulation of public opinion over social media remains a critical threat to democracy, as computational propaganda becomes a pervasive part of everyday life. Government agencies and political parties around the world are using social media to spread disinformation and other forms of manipulated media. Although propaganda has always been a part of politics, the wide-ranging scope of these campaigns raises critical concerns for modern democracy.”

Samantha Bradshaw, researcher and lead author of the report, added: “The affordances of social networking technologies — algorithms, automation and big data — vastly changes the scale, scope, and precision of how information is transmitted in the digital age. Although social media was once heralded as a force for freedom and democracy, it has increasingly come under scrutiny for its role in amplifying disinformation, inciting violence, and lowering trust in the media and democratic institutions.”

Other findings from the report include that:

52 countries used “disinformation and media manipulation” to mislead users

47 countries used state sponsored trolls to attack political opponents or activists, up from 27 last year

Which backs up the widespread sense in some Western democracies that political discourse has been getting less truthful and more toxic for a number of years — given tactics that amplify disinformation and target harassment at political opponents are indeed thriving on social media, per the report.

Despite finding an alarming rise in the number of government actors across the globe who are misappropriating powerful social media platforms and other tech tools to influence public attitudes and try to disrupt elections, Howard said the researchers remain optimistic that social media can be “a force for good” — by “creating a space for public deliberation and democracy to flourish”.

“A strong democracy requires access to high quality information and an ability for citizens to come together to debate, discuss, deliberate, empathise and make concessions,” he said.

Clearly, though, there’s a stark risk of high quality information being drowned out by the tsunami of BS that’s being paid for by self-interested political actors. It’s also of course much cheaper to produce BS political propaganda than carry out investigative journalism.

Democracy needs a free press to function but the press itself is also under assault from online ad giants that have disrupted its business model by being able to spread and monetize any old junk content. If you want a perfect storm hammering democracy this most certainly is it.

It’s therefore imperative for democratic states to arm their citizens with education and awareness to enable them to think critically about the junk being pushed at them online. But as we’ve said before, there are no shortcuts to universal education.

Meanwhile regulation of social media platforms and/or the use of powerful computational tools and techniques for political purposes simply isn’t there. So there’s no hard check on voter manipulation.

Lawmakers have failed to keep up with the tech-fuelled times. Perhaps unsurprisingly, given how many political parties have their own hands in the data and ad-targeting cookie jar, as well as pushing fakes. (Concerned citizens are advised to practise good digital privacy hygiene to fight back against undemocratic attempts to hack public opinion. More privacy tips here.)

The researchers say their 2019 report, which is based on research work carried out between 2018 and 2019, draws upon a four-step methodology to identify evidence of globally organised manipulation campaigns — including a systematic content analysis of news articles on cyber troop activity and a secondary literature review of public archives and scientific reports, generating country specific case studies and expert consultations.

Here’s the full list of countries studied:",Social Media,TechCrunch,https://techcrunch.com/2019/09/26/voter-manipulation-on-social-media-now-a-global-problem-report-finds/,"Social media manipulation is getting worse, with rising numbers of governments and political parties using algorithms, automation and big data to manipulate public opinion at scale. This has huge implications for democracy, with disinformation and other forms of manipulated content being spread on Facebook and Twitter in 70 countries, while authoritarian regimes are using social media to shape public opinion and extend control",Politics
362,The Project Veritas Twitter Videos Show the Conservative Backlash Against Moderation,"Conservative activist James O’Keefe has returned. In a series of illicitly filmed videos with current and former Twitter employees, the right-wing provocateur claims to have exposed partisan bias at the social network. The offensive may have been inevitable. While O'Keefe's Project Veritas has mostly focused on the media and liberal institutions, recent moves by platforms like Twitter, Facebook, and YouTube to more aggressively moderate user content have left them exposed them to this exact sort of attack.

The Project Veritas videos, filmed without apparent awareness or consent, show a range of selectively edited insights from inside Twitter. One engineer for the company says that Twitter would theoretically comply with a Department of Justice investigation into Trump’s Twitter account. Another video shows a series of current and former employees explaining ""shadowbans,"" a practice by which Twitter will sometimes make it more difficult to find and view a user's tweets, rather than banning that person outright. And a third, released Monday, explains how the company tracks user behavior and screens direct messages for prohibited content, like porn spammers and unsolicited dick pics.

Many of the employees filmed used sensational language, but they also thought they were talking candidly to strangers at a bar. It’s not exactly unusual to embellish your job—and to elide its nuances—to a potential new friend or romantic interest.

The right-wing backlash against tech giants has reached a new height.

And in any case, none of these gotcha moments amount to anything revelatory. Tech companies comply with valid legal investigations all the time; if anything, Twitter has historically taken a relatively hardline stance against federal intervention. Shadowbanning is such a closely guarded secret that Twitter details the practice in its easily accessible online Help Center. Tracking is how Twitter—and every free platform online—sells ads. And Twitter employees don't read every single direct message sent on the platform—an insurmountable task—but the company does screen instances in which abusive behavior is reported.

These videos don't prove that Twitter has a partisan bias against its far-right conservative users. (Indeed, they're some of its most prolific users.) They do show, though, that the right-wing backlash against tech giants has reached a new height. With every new policy intended to curb abuse, Twitter, YouTube, Facebook, and other platforms invite rancor. The new rules have been necessary to fight an increasingly toxic atmosphere online. But Project Veritas sees those steps, and the ban of high-profile far-right users—over clear, apolitical terms of service violations—as an attempt not to improve discourse online but to quash the free exchange of ideas.

The Mounting Backlash

O’Keefe’s videos quickly became the top story on sites like Breitbart over the past week, and Fox News host Sean Hannity discussed them on national television. The videos also put Twitter on the defensive, despite uncovering a whole lot of nothing.

“The individuals depicted in this video were speaking in a personal capacity and do not represent or speak for Twitter,” a spokesperson said in a statement. “We deplore the deceptive and underhanded tactics by which this footage was obtained and selectively edited to fit a predetermined narrative.”

But to a large segment of right-wing internet users, the videos' substance doesn’t matter. The way they were filmed matters even less. The footage validated a deep-seated suspicion that social media companies treat conservatives differently.",Social Media,WIRED,https://www.wired.com/story/twitter-project-veritas-videos-backlash/,"The Project Veritas videos have sparked a right-wing backlash against tech giants, with many conservatives seeing the videos as proof of partisan bias. The videos have further stoked suspicion that social media companies unfairly moderate user content, despite uncovering little to back up these claims. The footage has only served to further divide and polarize the online community",Politics
363,The next era of moderation will be verified – TechCrunch,"Since the dawn of the internet, knowing (or, perhaps more accurately, not knowing) who is on the other side of the screen has been one of the biggest mysteries and thrills. In the early days of social media and online forums, anonymous usernames were the norm and meant you could pretend to be whoever you wanted to be.

As exciting and liberating as this freedom was, the problems quickly became apparent — predators of all kinds have used this cloak of anonymity to prey upon unsuspecting victims, harass anyone they dislike or disagree with, and spread misinformation without consequence.

For years, the conversation around moderation has been focused on two key pillars. First, what rules to write: What content is deemed acceptable or forbidden, how do we define these terms, and who makes the final call on the gray areas? And second, how to enforce them: How can we leverage both humans and AI to find and flag inappropriate or even illegal content?

While these continue to be important elements to any moderation strategy, this approach only flags bad actors after an offense. There is another equally critical tool in our arsenal that isn’t getting the attention it deserves: verification.

Most people think of verification as the “blue checkmark” — a badge of honor bestowed upon the elite and celebrities among us. However, verification is becoming an increasingly important tool in moderation efforts to combat nefarious issues like harassment and hate speech.

That blue checkmark is more than just a signal showing who’s important — it also confirms that a person is who they say they are, which is an incredibly powerful means to hold people accountable for their actions.

One of the biggest challenges that social media platforms face today is the explosion of fake accounts, with the Brad Pitt impersonator on Clubhouse being one of the more recent examples. Bots and sock puppets spread lies and misinformation like wildfire, and they propagate more quickly than moderators can ban them.

This is why Instagram began implementing new verification measures last year to combat this exact issue. By verifying users’ real identities, Instagram said it “will be able to better understand when accounts are attempting to mislead their followers, hold them accountable, and keep our community safe.”

It’s important to remember that verification is not a single tactic, but rather a collection of solutions that must be used dynamically in concert to be effective.

The urgency to implement verification is also bigger than just stopping the spread of questionable content. It can also help companies ensure they’re staying on the right side of the law.

Following an exposé revealing illegal content was being uploaded to Pornhub’s site, the company banned posts from nonverified users and deleted all content uploaded from unverified sources (more than 80% of the videos hosted on its platform). It has since implemented new measures to verify its users to prevent this kind of issue from infiltrating its systems again in the future.

Companies of all kinds should be looking at this case as a cautionary tale — if there had been verification from the beginning, the systems would have been in a much better place to identify bad actors and keep them out.

However, it’s important to remember that verification is not a single tactic, but rather a collection of solutions that must be used dynamically in concert to be effective. Bad actors are savvy and continually updating their methods to circumvent systems. Using a single-point solution to verify users — such as through a photo ID — might sound sufficient on its face, but it’s relatively easy for a motivated fraudster to overcome.

At Persona, we’ve detected increasingly sophisticated fraud attempts ranging from using celebrity photos and data to create accounts to intricate photoshopping of IDs and even using deepfakes to mimic a live selfie.

That’s why it’s critical for verification systems to take multiple signals into account when verifying users, including actively collected customer information (like a photo ID), passive signals (their IP address or browser fingerprint), and third-party data sources (like phone and email risk lists). By combining multiple data points, a valid but stolen ID won’t pass through the gates because signals like location or behavioral patterns will raise a red flag that this user’s identity is likely fraudulent or at the very least warrants further investigation.

This kind of holistic verification system will enable social and user-generated-content platforms to not only deter and flag bad actors but also prevent them from repeatedly entering your platform under new usernames and emails, a common tactic of trolls and account abusers who have previously been banned.

Beyond individual account abusers, a multisignal approach can help manage an arguably bigger problem for social media platforms: coordinated disinformation campaigns. Any issue involving groups of bad actors is like battling the multiheaded Hydra — you cut off one head only to have two more grow back in its place.

Yet killing the beast is possible when you have a comprehensive verification system that can help surface groups of bad actors based on shared properties (e.g., location). While these groups will continue to look for new ways in, multifaceted verification that is tailored for the end user can help keep them from running rampant.

Historically, identity verification systems like Jumio or Trulioo were designed for specific industries, like financial services. But we’re starting to see the rise in demand for industry-agnostic solutions like Persona to keep up with these new and emerging use cases for verification. Nearly every industry that operates online can benefit from verification, even ones like social media, where there isn’t necessarily a financial transaction to protect.

It’s not a question of if verification will become a part of the solution for challenges like moderation, but rather a question of when. The technology and tools exist today, and it’s up to social media platforms to decide that it’s time to make this a priority.",Social Media,TechCrunch,https://techcrunch.com/2021/03/23/the-next-era-of-moderation-will-be-verified/,"The anonymity of Social Media has enabled predators, trolls and perpetrators of hate speech and misinformation to operate with impunity, making verification an increasingly important tool to combat such nefarious activities.",Security & Privacy
364,MIT professor wants to overhaul ‘The Hype Machine’ that powers social media – TechCrunch,"More than 3.6 billion people use social media, and its runaway success has left the industry at a crossroads. There are now heated debates in Washington and Brussels over the future of antitrust regulation for this market, whether platform operators should filter certain content (and if so, which types), and how to open the market to new innovators.

To find my way through this thicket of interesting questions, I spoke with Sinan Aral, a professor of management at the MIT Sloan School of Management who also is director of MIT’s Initiative on the Digital Economy. He has spent years analyzing the social media market, directly participating in its development as chief scientist of SocialAmp and Humin and as a founding partner of Manifest Capital.

This fall, he published his latest book, “The Hype Machine,” which explores what’s next for social media giants. In our discussion, we talked about the landscape of the market today, what responsibilities companies and users have to each other and what come next as the industry evolves.

This interview has been edited and condensed for clarity.

TechCrunch: Why don’t we start with how the book came together and how you got interested in this topic of digital media and how it affects our decision-making?

Sinan Aral: I started researching social media four years before Mark Zuckerberg founded Facebook. I have worked with all of the major social media platforms for the last 20 years: Facebook, Twitter, Snapchat, WeChat, Yahoo and the rest. I’ve published a number of very large-scale studies, and I’m also an entrepreneur. So, I’ve got a vantage point as a practitioner, but also as a long-time academic leader in this area.

We really have a full-blown social media crisis on our hands, as is obvious if you turn on the TV on any given day.

The reason why I wrote “The Hype Machine” is because essentially, we’ve seen this coming to a head for many years now. We really have a full-blown social media crisis on our hands, as is obvious if you turn on the TV on any given day.

My book takes off from where “The Social Dilemma” documentary and Shoshana Zuboff’s “The Age of Surveillance Capitalism” leave off, which is to ask, what can we concretely do to solve the social media crisis that we find ourselves in? The book argues that in order to do that, we have to stop armchair theorizing about how social media works, and we have to stop debating whether or not social media is good or evil. The answer is yes.

The book goes through the fundamentals of how social media works. So, there’s a chapter on neuroscience and social media, and economics and social media, and that eventually informs the solutions in the book, which cover everything from antitrust and competition to federal privacy legislation. How do we secure our elections and our democracy? What do we do about Section 230 of the Communications Decency Act? How do we balance free speech and hate speech? How do we deal with misinformation and fake news?

I think for a lot of us in tech, we’re a bit stuck. On one hand, these technologies have produced jarring amounts of wealth in the tech industry, but they have also caused a large number of harms. What do we do next?

Let me start by saying that the general framework of the solution is about what I call the four levers: money, code, norms and laws.

Money is the business models, which create the incentives for how the advertisers on the platforms and the users behave. Code is how we design the platforms and the algorithms underlying the platforms, which I go into in great detail. Norms are how we adopt, appropriate and use the technology. And obviously, laws are regulation.

In terms of solutions, I think the entry ticket for solving the social media crisis is creating competition in the social media economy. Platforms that lack competition don’t have any incentive to change away from the attention economy and their engagement-driven business models, nor do they have any real incentive to clean up their negative externalities in our information ecosystem, whether it’s hate speech or misinformation or manipulation.

Now, when I say competition, the first thing on everyone’s mind is always, “Oh, you mean break up Facebook.” But the point I make in the book — and I take a very clear stance on this — is that breaking up Facebook in this economy doesn’t solve the problem. This economy runs on network effects. The value of these platforms is a function of the number of users on the platform. Economies that run on network effects tend toward concentration and monopoly.

So, if you break up Facebook, it’s just going to tip the next Facebook-like company into market dominance. What we really need is structural reform of the social media economy, and that involves social network portability, data portability and interoperability legislation.

Let me push back on this a bit though. Terms like “data portability” always sound nice as a solution, but have we ever effectively used this tool to open a market?

This isn’t the first time that we’ve done this. During the AOL-Time Warner merger, we forced AOL’s AIM product to become interoperable with Yahoo Messenger and MSN Messenger. And it went from a 65% market share to a 59% market share one year later, down to like 50%, then it ceded the entire market to new entrants three years later.

Another good analogy is number portability in the cell phone market. It used to be that you couldn’t take your cell phone number with you when you switched from one cell phone provider to another, and then we legislated that they had to let you take your number with you. That was akin to a social network at the time, because all of your friends knew to call you at that number.

Research has shown that number portability created about $880 million of consumer surplus every quarter for years and years after it was instituted in Europe, and it created a lot of competition. We should have something very similar in social networks, around social network portability and data portability, so that we could create competition.

Now, if you break up Facebook after these kinds of structural reforms to the market, that’s a different question, but breaking up Facebook without structural reforms to the market economy is like putting a Band-Aid on a tumor. It’s not going to solve the underlying lack of competition that the social media economy has.

“The Hype Machine” details how we might do that and suggests that there could be a stack of commodity messaging formats that would be required to be interoperable. Then, you could have unique messaging formats for every platform on top of that. But things like texts, short-form videos, stories that either persist or disappear, that kind of stuff should have a level of interoperability that’s legislated. The entry ticket to solving the social media crisis is creating competition.",Social Media,TechCrunch,https://techcrunch.com/2020/12/14/hype-machine-sinan-aral/,"The main undesirable consequence of social media being discussed here is the lack of competition in the social media market, which can lead to a range of harms such as hate speech, misinformation, manipulation, and the capture of user data by a few large tech companies. Without effective antitrust and data portability legislation, this problem is likely to continue.",Economy
365,One more thing re: “privacy concerns” raised by the DCMS fake new report… – TechCrunch,"A meaty first report by the UK parliamentary committee that’s been running an inquiry into online disinformation since fall 2017, including scrutinizing how people’s personal information was harvested from social media services like Facebook and used for voter profiling and the targeting of campaign ads — and whose chair, Damian Collins — is a member of the UK’s governing Conservative Party, contains one curious omission.

Among the many issues the report raises are privacy concerns related to a campaign app developed by a company called uCampaign — which, much like the scandal-hit (and now seemingly defunct) Cambridge Analytica, worked for both the Ted Cruz for President and the Donald J Trump for President campaigns — although in its case it developed apps for campaigns to distribute to supporters to gamify digital campaigning via a tool which makes it easy for them to ‘socialize’ (i.e. share with contacts) campaign messaging and materials.

The committee makes a passing reference to uCampaign in a section of its report which deals with “data targeting” and the Cambridge Analytica Facebook scandal, specifically — where it writes [emphasis ours]:

There have been data privacy concerns raised about another campaign tool used, but not developed, by AIQ [Aggregate IQ: Aka, a Canadian data firm which worked for Cambridge Analytica and which remains under investigation by privacy watchdogs in the UK, Canada and British Columbia]. A company called uCampaign has a mobile App that employs gamification strategy to political campaigns. Users can win points for campaign activity, like sending text messages and emails to their contacts and friends. The App was used in Donald Trump’s presidential campaign, and by Vote Leave during the Brexit Referendum. The developer of the uCampaign app, Vladyslav Seryakov, is an Eastern Ukrainian military veteran who trained in computer programming at two elite Soviet universities in the late 1980s. The main investor in uCampaign is the American hedge fund magnate Sean Fieler, who is a close associate of the billionaire backer of SCL and Cambridge Analytica, Robert Mercer. An article published by Business Insider on 7 November 2016 states: “If users download the App and agree to share their address books, including phone numbers and emails, the App then shoots the data [to] a third-party vendor, which looks for matches to existing voter file information that could give clues as to what may motivate that specific voter. Thomas Peters, whose company uCampaign created Trump’s app, said the App is “going absolutely granular”, and will—with permission—send different A/B tested messages to users’ contacts based on existing information.”

What’s curious is that Collins’ Conservative Party also has a campaign app built by — you guessed it! — uCampaign, which the party launched in September 2017.

While there is nothing on the iOS and Android app store listings for the Conservative Campaigner app to identify uCampaign as its developer, if you go directly to uCampaign’s website the company lists the UK Conservative Party as one of it’s clients — alongside other rightwing political parties and organizations such as the (pro-gun) National Rife Association; the (anti-abortion) SBA List; and indeed the UK’s Vote Leave (Brexit) campaign, (the latter) as the DCMS report highlights.

uCampaign’s involvement as the developer of the Conservative Campaigner app was also confirmed to us (in June) by the (now former) deputy director & head of digital strategy for The Conservative Party, Anthony Hind, who — according to his LinkedIn profile — also headed up the party’s online marketing, between mid 2015 and, well, the middle of this month.

But while, in his initial response to us, Hind readily confirmed he was personally involved in the procurement of uCampaign as the developer of the Conservative Campaigner app, he failed to respond to any of our subsequent questions — including when we raised specific concerns about the privacy policy that the app had been using, prior to May 23 (just before the EU’s new GDPR data protection framework came into force on May 25 — a time when many apps updated their privacy polices as a compliance precaution related to the new data protection standard).

Since May 23 the privacy policy for the Conservative Campaigner app has pointed to the Conservative Party’s own privacy policy. However prior to May 23 the privacy policy was a literal (branded) copy-paste of uCampaign’s own privacy policy. (We know because we were tipped to it by a source — and verified this for ourselves.)

Here’s a screengrab of the exchange we had with Hind over LinkedIn — including his sole reply:

What looks rather awkward for the Conservative Party — and indeed for Collins, as DCMS committee chair, given the valid “privacy concerns” his report has raised around the use (and misuse/abuse) of data for political targeting — is that uCampaign’s privacy policy has, shall we say, a verrrrry ‘liberal’ attitude to sharing the personal data of app users (and indeed of any of their contacts it would have been able to harvest from their devices).

Here’s a taster of the data-sharing permissions this U.S. company affords itself over its clients’ users’ data [emphasis ours] — according to its own privacy policy:

CAMPAIGNS YOU SUPPORT AND ALIGNED ORGANIZATIONS We will share your Personal Information with third party campaigns selected by you via the Platform. In addition, we may share your Personal Information with other organizations, groups, causes, campaigns, political organizations, and our clients that we believe have similar viewpoints, principles or objectives as us. UCAMPAIGN FRIENDS We may share your Personal Information with other users of the Platform, for example if they connect their address book to our services, or if they invite you to use our services via the Platform. BUSINESS TRANSFERS We may share your Personal Information with other entities affiliated with us for internal reasons, primarily for business and operational purposes. uCampaign, or any of its assets, including the Platform, may be sold, or other transactions may occur in which your Personal Information is one of the business assets of the transaction. In such case, your Personal Information may be transferred.

To spell it out, the Conservative Party paid for a campaign app that could, according to the privacy policy it had in place prior to May 23, have shared supporters’ personal data with organizations that uCampaign’s owners — who the DCMS committee states have close links to “the billionaire backer of SCL and Cambridge Analytica, Robert Mercer” — view as ideologically affiliated with their objectives, whatsoever those entities might be.

Funnily enough, the Conservative Party appears to have tried to scrub out some of its own public links to uCampaign — such as changing link for the developer website on the app listing page for the Conservative Campaigner app to the Conservative Party’s own website (whereas before it linked through to uCampaign’s own website).

As the veteran UK satirical magazine Private Eye might well say — just fancy that!

One of the listed “features” of the Conservative Campaigner app urges Tory supporters to: “Invite your friends to join you on the app!”. If any did, their friends’ data would have been sucked up by uCampaign too to further causes of its choosing.

The version of the Campaigner app listed on Google Play is reported to have 1,000+ installs (iOS does not offer any download ranges for apps) — which, while not in itself a very large number, could represent exponentially larger amounts of personal data should users’ contacts have been synced with the app where they would have been harvested by uCampaign.

We did flag the link between uCampaign and the Conservative Campaigner app directly to Collins’ office, ahead of the publication of the DCMS report — writing on June 12:

The matter of concern here is that the Conservative party could itself be an unwitting a source of targeting data for rival political organizations, via an app that appears to offer almost no limits on what can be done with personal data. Prior to the last update of the Conservative Campaigner app the privacy policy was simply the boilerplate uCampaign T&Cs — which allow the developer to share app users personal info (and phone book contacts) with “other organizations, groups, causes, campaigns, political organizations, and our clients that we believe have similar viewpoints, principles or objectives as us”. That’s incredibly wide-ranging. So every user’s phone book contacts (potentially hundreds of individuals per user) could have been passed to multiple unidentified organizations without people’s knowledge or consent. (Other uCampaign apps have been built for the NRA, and for anti-abortion organizations, for example.) uCampaign ‘s T&Cs are here: ‘s T&Cs are here: https://ucampaignapp. com/privacy.html Even the current T&Cs allow for sharing with US suppliers. Given the committee’s very public concerns about access to people’s data for political targeting purposes I am keen to know whether Mr Collins has any concerns about the use of uCampaign ‘s app infrastructure by the Conservative party? And also whether he is concerned about the lack of a robust data protection policy by his own party to ensure that valuable membership data is not simply passed around to unknown and unconnected entities — perhaps abroad, perhaps not — with zero regard for or accountability to the individuals in question.

Unfortunately this email (and a follow up) to the DCMS committee, asking for a response from Collins to our privacy concerns, went unanswered.

It’s also worth noting that the Conservative Party’s own privacy policy (which it’s now using for its Campaigner app) is pretty generous vis-a-vis the permissions it’s granting itself over sharing supporters’ data — including stating that it shares data with:

The wider Conservative Party

Business associates and professional advisers

Suppliers

Service providers

Financial organisations – such as credit card payment providers

Political organisations

Elected representatives

Regulatory bodies

Market researchers

Healthcare and welfare organisations

Law enforcement agencies

The UK’s data watchdog recently found fault with pretty much all of the UK political parties’ when it comes to handling of voter data — saying it had sent warning letters to 11 political parties and also issued notices compelling them to agree to audits of their data protection practices.

Safe to say, it’s not just private companies that have been sticking their hand in the personal data cookie jar in recent years — the political establishment is facing plenty of awkward questions as regulators unpick where and how data has been flowing.

This is also not the only awkward story re: data privacy concerns related to a Tory political app. Earlier this year the then-minister in charge of the digital brief, Matt Hancock, launched a self-promotional, self-branded app intended for his constituents to keep up with news about Matt Hancock MP.

However the developers of the app (Disciple Media) initially uploaded the wrong privacy policy — and were forced to issue an amended version which did not grant the minister such non-specific and oddly toned rights to users’ data — such as that the app “may disclose your personal information to the Publisher, the Publisher’s management company, agent, rights image company, the Publisher’s record label or publisher (as applicable) and any other third parties, for use in conjunction with additional user promotions or offers they may run from time to time or in relation to the sale of other goods and services”.

Of course the Matt Hancock App was a PR initiative of (and funded by) an individual Conservative MP — rather than a formal campaign tool paid for by the Conservative Party and intended for use by hundreds (or even thousands) of Party activists for use during election campaigns.

So while there are two issues of Tory-related privacy concern here, only one loops back to the Conservative Party political organization itself.

This report was updated with a correction that it was Collins’ office we contacted for comment on uCampaign in June, rather than the DCMS press office",Social Media,TechCrunch,https://techcrunch.com/2018/07/30/one-more-thing-re-privacy-concerns-raised-by-the-dcms-fake-new-report/,"The UK Parliamentary Committee's report on online disinformation has raised privacy concerns regarding the misuse of personal information harvested from social media services, including the targeting of campaign ads. The Conservative Party has a campaign app built by the US company uCampaign which has a liberal attitude to sharing user data with unknown organizations. This could potentially have resulted in supporters' personal",Security & Privacy
366,Facebook transcribed users’ audio messages without permission – TechCrunch,"“The future is private.” Clearly, Facebook still has a way to go.

Facebook has become the latest tech giant to face scrutiny over its handling of users’ data, following a report that said the social media giant collected audio data and recordings from its users and transcribed it using third-party contractors.

The report came from Bloomberg, citing the contractors who requested anonymity for fear of losing their jobs.

According to the report, the audio came from its Messenger app. The audio conversations were matched against transcriptions to see if they were properly interpreted by the company’s artificial intelligence.

There are several ways that Facebook collects voice and audio data. But the social media giant’s privacy policy makes no clear mention or explanation what it uses audio data for. Bloomberg also noted that contractors felt their work was “unethical” because Facebook “hasn’t disclosed to users that third parties may review their audio.”

The company has long rebuffed claims that Facebook is “not listening” to its users through your phone.

We’ve asked Facebook several questions — including for what reason the audio was transcribed and why users weren’t explicitly told of the third-party transcription — but did not immediately hear back.

Facebook stopped transcribing voice data earlier in August, said spokesperson Joe Osborne.

The social media giant becomes the latest tech company to face questions about its use of third-party contractors and staff to review user audio.

Amazon saw the initial round of flak for allowing contractors to manually review Alexa recordings without express user permission, forcing the company to add an opt-out to its Echo devices. Google also faced heat for allowing human review of audio data, along with Apple, which used contractors to listen to seemingly private Siri recordings. Microsoft also listened to some Skype calls made through the company’s app translation feature.

It’s been over a year since Facebook last had a chief security officer in the wake of Alex Stamos’ departure.

Read more:",Social Media,TechCrunch,https://techcrunch.com/2019/08/13/facebook-contractors-said-to-have-collected-and-transcribed-users-audio-without-permission/,"Facebook is facing scrutiny over the use of third-party contractors to manually review user audio without express user permission, with other tech giants such as Amazon, Google, Apple and Microsoft also facing heat for similar practices. This has been an undesirable consequence of using social media, as users have not been given an explicit explanation of the use of their",Security & Privacy
367,"The makers of the virtual influencer, Lil Miquela, snag real money from Silicon Valley – TechCrunch","Brud, the actual company behind one of Instagram’s most popular virtual influencers (it’s a thing), has raised millions of dollars from Silicon Valley investors because this is 2018 and everything is awful.

Last week, the Los Angeles-based startup led by Trevor McFedries, outed itself as the collective consciousness behind the virtual celebrity Lil Miquela and her less well known contemporaries Blawko22 and BermudaisBae in a choreographed melodrama worthy of Los Angeles’ best reality television.

i am deeply invested in the drama surrounding lil miquela and now you all have to be too. sorry!!! https://t.co/ta1T4rDFGz — maya kosoff (@mekosoff) April 19, 2018

The subject of numerous glowing profiles in online and print fashion and lifestyle magazines (including, most recently, in High Snobiety), Lil Miquela’s stardom (and her fellow avatars) fascinated because the characters’ creators coyly toed the line around “her” self-awareness and their own. In the process, they created a sensation that’s become well-known worldwide.

It’s less well-known that the company is backed by some of the biggest names in venture capital investment — firms like Sequoia Capital. Our sources put the company’s funding somewhere around $6 million in its recent funding round.

There are other notable investors from Silicon Valley and New York rumored to be in the round — like New York’s BoxGroup and the Bay Area’s SV Angel. Sequoia declined to comment for this article and Box Group’s David Tisch did not respond to a request for comment.

All of the virtual drama with Miquela started late last week when news outlets (including TechCrunch) reported that Miquela’s Instagram account (or that of her handlers) was hacked by operators of a social media account belonging to another virtual personality known as “Bermudaisbae” (a more right wing social media persona with fewer followers).

McFedries, brud‘s founder and chief executive, confirmed that the Miquela account had been hacked in a text exchange with me, writing, “some redditor idiots hacked the page we think.”

That was a lie.

The account “hack” was architected by brud as part of an ongoing virtual reality drama playing out on Instagram and other social media platforms between avatars it had developed, all designed to attract media attention, according to people with knowledge of brud and its plans. It worked.

McFedries has not responded to further requests for comment after confirming that the Miquela account was “good”.

One Los Angeles investor familiar with the company said brud was “using conflict to introduce new characters… same as the Kardashians always have.”

The investor added that two years into the development of the Miquela persona, brud‘s founders knew that the fad could lose some of its luster as the is-she-or-Isn’t-she-real tension dissipates under the weight of continuously thwarted expectations — like a post-modern twist on the will-or-won’t-they dramatic tension defining most sitcoms since Cheers.

“People aren’t going to buy that she’s human so they make it seem as if she’s had an existential crisis and now she is the first in a breed of conscious AR characters that they will build a world around,” this investor wrote. “[Manufacturing] social influence.”

For his part, the 33-yar-old McFedries had been manufacturing social influence in Los Angeles through his talents as a dj, producer and director before entering the startup world.

First under the name of DJ Skeet Skeet and then as DJ Skeeter, and, finally, Yung Skeeter, McFedries has worked or performed with a number of the world’s best selling recording artists including Chris Brown, Ke$ha, and Katy Perry (and — interestingly — more obscure acts like Bonde do Role).

Working as an an “artist advocate” for Spotify, a DJ for a radio show on iHeartRadio, and as a spokesman for VitaminWater sustained McFedries along with managing the career of BANKS and executive producing her first album and a single on Azealia Banks’ 2014 record “Broke with Expensive Taste” — at least according to a Wikipedia page on Yung Skeeter.

Around this time McFedries also began investing in companies, according to AngelList.

Roughly two years after the Banks record release, Lil Miquela made her first appearance on Instagram. And the rest is history as written in Internet archives and memes. Ephemeral, but infinite.

The project that brud seems to be pursuing — turning celebrity into a virtual commodity; commenting on the unreality of the “real” entertainment industry by literally creating an unreal celebrity — is fascinating.

There’s certainly a valid criticism to be made about the ways in which celebrity operates, the ways in which our “social” media has corroded society, and the unbridled power of these platforms to transform messengers and their messages into movements.

Perhaps brud wants to make these critiques through its very existence — or at least use its low-brow as high-brow (or is it vice versa?) intellectual appeal as a veneer over the more crass (but potentially honest) mission of selling more shit more effectively through the use of spokespeople whose views only change when their creators want them to (it worked for Hollywood’s star system). That at least gets sponsors and advertisers out of the potentially messy situations that can come from working with spokespeople whose actions can’t be controlled by software — or an ingenious marketing team.

In the High Snobiety profile-as-honors-senior-English-thesis on Lil Miquela published yesterday, the avatar’s own spokesperson was quoted as saying:

“The internet is endlessly powerful, and that power has been wielded in many ways. It feels like we’re not going to put the genie back in the bottle, so we’ve got to learn how to leverage these tools in positive ways. I’ve used my platform to raise real money for important organizations throughout LA and I’ve seen lives changed as a result. I think the only chance we’ve got is to collectively teach our loved ones how to think critically and how to spot misinformation. I know that we can manifest the change we want to see, and the internet can be a part of that.”

It’s a lofty goal backed by a number of inarguably good works. However, lying to reporters may not be the best way to continue trying to achieve it.",Social Media,TechCrunch,https://techcrunch.com/2018/04/23/the-makers-of-the-virtual-influencer-lil-miquela-snag-real-money-from-silicon-valley/,"The rise of virtual influencers, like Lil Miquela, has raised questions about the power of social media and its potential to manipulate and shape public opinion. While such influencers have been used for good causes, the way in which their creators have manufactured drama and lied to reporters casts doubt on their motives and makes it difficult to trust their",Social Norms & Relationships
368,"TikTok’s new set of safety videos teach users about features, the app’s focus on ‘positivity’ – TechCrunch","TikTok today released a new set of safety videos designed to playfully inform users about the app’s privacy controls and other features — like how to filter comments or report inappropriate behavior, among other things. One video also addresses TikTok’s goal of creating a “positive” social media environment, where creativity is celebrated and harassment is banned.

This particular value — that TikTok is for “fun” — is cited whenever the Beijing-based company is pressured about the app’s censorship activity. Today, TikTok hides under claims that it’s all about being a place for lighthearted, positive behavior. But in reality, it had been censoring topics China doesn’t want its citizens to know about — like the Hong Kong protests, for example. Meanwhile, it doesn’t appear to take action on political issues in the U.S., where hashtags like #dumptrump or #maga have millions of views.

To figure out its approach to moderation, TikTok recently hired corporate law firm K&L Gates to advise it on how to create policies that won’t have it coming under the eye of U.S. regulators.

In the meantime, TikTok is tackling the job of crafting the sort of community it wants through these instructive videos. But it’s not just issuing its commands from the top-down — TikTok partners with its own creators to participate in the videos and then promote them to fans. The first set of videos, released in February, featured a dozen TikTok creators, for example.

This time around, the company has pulled in a dozen more, including: @nathanpiland, @d_damodel, @juniortvine, @Stevenmckell, @supershaund, @ourfire, @thedawndishsoap, @katjaglieson, @mahoganylox, @chanydakota, @shreksdumpster and @christinebarger.

This is a much different approach to community-setting, compared with Twitter, Facebook or Instagram. Those platforms took years before they addressed users’ basic needs for privacy, security and anti-harassment features, like filtering comments, blocking and muting, and more. In the meantime, social media became a haven for trolls and abuse.

TikTok is approaching the problem from a different standpoint — by consciously creating a community where users are knowledgable and feel empowered to kick out the bad elements from disrupting their fun.

The only problem is that TikTok’s definition of what’s “fun” and appropriate has a political bent.

Creativity and art aren’t only meant for expressing positive sentiments. And given that TikTok was enforcing China’s censorship of topics like Tiananmen Square, Hong Kong and Taiwan to its more than 500 million global monthly users, it wouldn’t be a leap to find the company one day censoring all sorts of political speech and other social issues — effectively becoming a tool for China to spread its government’s views to the wider world. And that’s far less fun.

TikTok reiterated its comment that U.S. moderation is handled by a U.S. team:

Our content and moderation policies are led by our US-based team and are not influenced by any foreign government. The Chinese government does not request that TikTok censor content, and would not have jurisdiction regardless, as TikTok does not operate there. To be clear: We do not remove videos based on the presence of Hong Kong protest content.

We and others had searched Hong Kong and related hashtags prior to today in the app and found, as The Washington Post did, “barely a hint of unrest in sight.” TikTok says several hashtags will now show that sort of content, like #freehongkong or #hongkongprotest, for example. But these two have views in the tens of thousands. Meanwhile, U.S. political hashtags have hundreds of millions of views, by comparison… as most high-profile topics do.",Social Media,TechCrunch,https://techcrunch.com/2019/10/23/tiktoks-new-set-of-safety-videos-teach-users-about-features-the-apps-focus-on-positivity/,"Social media can become a tool for governments to spread their views to a wider audience, limiting freedom of expression and creating a less fun environment for users.",Social Norms & Relationships
369,Facebook's Targeted Ads Are More Complex Than It Lets On,"In a recent blog post, Facebook's vice president for ads, Rob Goldman, argues his platform's users aren't its product. Even though Facebook primarily makes money by selling targeted ads based on what it knows about you, Goldman says that the real product is the ability to connect people—ads merely exist to ""fund that experience.""

To help support that stance, Goldman paints a simple picture of the role advertising plays on Facebook, downplaying the information it collects about you. Using a hypothetical example about a small bike shop in Atlanta, he emphasizes that targeted ads help small businesses reach customers—like, say, female cyclists who live nearby—more efficiently. The example, which the social network also uses on a general page explaining how ads work, does represent how some companies use Facebook. But ad industry experts say Goldman's explanation leaves out many important realities of Facebook's advertising machine.

Relevant to What?

Four times in his blog post, Goldman stresses that Facebook's targeting mechanisms allow users to see relevant ads. But nowhere does he define what ""relevant"" means in this context. In some ways, it's broadly intuitive across all industries; advertisements for dentures or funeral insurance don't run on Nickelodeon for a reason. But beyond simple demographics, a ""relevant"" ad to a marketer might target a specific personality type, or perceived emotional state. It might also be designed to take advantage of an already vulnerable population. That can quickly get a lot more involved than just people who like bikes.

'There’s 60,000 channels and weird ways to combine them.' Kane Jamison, Content Harmony

“We already have been seeing the results of negative segmentation we saw in the past before, like when cigarette companies were targeting low-income people,” says Juan Mundel, a professor at DePaul University who has studied Facebook advertising. Because the social network has so much data, it's possible to target hyper-specific audiences with extreme precision. That means, as Bloomberg reported in March, predatory advertisers can exploit Facebook's tools to sell shady products to the masses, like diet pills.

“Facebook also knows when you’re motivated to do something, when you’re feeling down, when you’re feeling all sorts of emotions,” says Mundel. The social network leverages that information for advertisers; the Intercept discovered earlier this month that the company has developed a new service designed to predict how consumers will behave in the future, like when they're likely to switch from one product brand to another. That level of psychological parsing goes far beyond what Goldman outlines.

Goldman is right to point out that Facebook has much in common with traditional forms of advertising like television and print, but the difference is companies who use Facebook have a near-endless number of data points with which to target their ads, and can show them to much narrower slices of the population. ""Facebook is the same thing, but there’s 60,000 channels and weird ways to combine them as well,” says Kane Jamison, the founder and managing director of Content Harmony, a marketing agency that frequently uses Facebook to advertise.

What Facebook Knows

Throughout Goldman's post, he stresses that users can control their ad experience by visiting their Ad Preferences menu. At the top of the screen, you will see Your interests, which Facebook says it generates based on your activities on Facebook, such as pages you may have liked. It's not clear whether some of these categories are algorithmically generated, and Facebook would only say they are based on past actions on the platform.

The categories range from intuitive to bizarre. Mine, for example, include head-scratching topics like ""Laser,"" ""Steel,"" ""Everything,"" and ""Authority."" If you hover your cursor over each one, Facebook ostensibly tells you why it first appeared: ""You have this preference because you clicked on an ad related to Everything."" Huh? Advertisers have a stunning number of categories to sift through—ProPublica has collected over 50,000, including those only marketers can see. You can remove any interest associated with your profile by clicking the X in the top right-hand corner.",Social Media,WIRED,https://www.wired.com/story/facebooks-targeted-ads-are-more-complex-than-it-lets-on/,"The use of Facebook's data and targeting tools can lead to predatory advertising, exploiting vulnerable populations and allowing marketers to target people with extreme precision. This can lead to negative segmentation, such as targeting low-income people with ads for products they cannot afford.",Equality & Justice
370,Mark Zuckerberg is ‘proud’ of how Facebook handled its scandals this year – TechCrunch,"After the year Mark Zuckerberg’s had, you’d think he’d struggle to appear so chipper.

“I’m proud of the progress we’ve made,” he said in an end-of-year note posted on his Facebook page for everyone to see. Acknowledging that the social network played its part in the spread of hate speech, election interference and misinformation, Zuckerberg’s note seemed more upbeat about his response to the hurricane of hurt caused by the company’s laissez-faire attitude to world affairs and less concerned about showing contrition and empathy for the harm Facebook caused in the past year — including its inability to keep its users’ data safe and, above all else, its failure to prevent its site from being used to incite ethnic violence and genocide.

Zuckerberg’s tone-deaf remarks read like 1,000 words of patting himself on the back.

But where the Facebook co-founder pledged to “focus on addressing some of the most important issues facing our community,” he conveniently ignored some of the most damaging, ongoing problems that the company has shown little desire to solve, opting instead for quick fixes or simply pretending they don’t exist.

“More than 30,000 people working on safety…” isn’t enough to police the platform

A decade ago, Facebook had just 12 people moderating its entire site — some 120 million users. Now, the company relies largely on an army of underpaid contractors spread out across the world to moderate millions of potentially rule-breaking posts on the site each week.

Zuckerberg said the company has this year increased those working on safety to “more than 30,000 people.” That’s on top of the 33,600 full-time employees that Facebook had as of the end of September. But that’s a massive task to police Facebook’s 2.27 billion monthly active users. Those 30,000 new safety contractors equates to about one moderator for every 75,660 users.

Facebook’s contractors have long complained about long hours and low pay, and that’s not even taking into account the thousands of gruesome posts — from beheadings to child abuse and exploitation — they have to review each day. Turnover is understandably high. No other social network in the world has as many users as Facebook, and it’s impossible to know what the “right number” of moderators is.

But the numbers don’t add up. Facebook’s army of 30,000 safety staffers isn’t enough to combat the onslaught of vitriol and violence, let alone against an advanced adversary like the nation-state actors that it’s constantly blaming.

Facebook lost its chief security officer this year — and doesn’t want a replacement

Zuckerberg made no mention of the photo data exposure and account breaches that the company had to contend with this year, even if he couldn’t avoid mentioning Cambridge Analytica, the voter research firm that misused 87 million Facebook users’ information, just the once.

Yet, Zuckerberg made no commitment to doubling down on the company’s efforts to secure the platform, despite years of its “move fast and break things” mentality. Since the departure of former chief security officer Alex Stamos in August, the company hasn’t hired his replacement. All signs point to nobody taking the position at all. While many see a chief security officer as a figurehead-type position, they still provide executive-level insight into the threats they face and issues to handle — no more than ever after a string of embarrassing and damaging security incidents.

Zuckerberg said that the company invests “billions of dollars in security yearly.” That may be true. But without an executive overseeing that budget, it’s not confidence-inducing knowing that there’s nobody with the years of experience needed to oversee a company’s security posture in control of where those billions go.

There was no acknowledgement of Facebook’s role in Myanmar’s genocide

Fake news, misinformation and election meddling is one thing, but Zuckerberg refused to acknowledge the direct impact Facebook had on Myanmar’s ethnic violence — which the United Nations is calling genocide.

It can’t be much of a surprise to Zuckerberg. The UN said Facebook had a “determining role” in inciting genocide in the country. He faced questions directly from U.S. lawmakers earlier this year when he was told to testify to senators in April. Journalists are regularly arrested and murdered for reporting on the military-backed government’s activities. The Facebook boss apologized — which human rights groups on the ground called “grossly insufficient.”

Facebook said last week that it has purged hundreds of accounts, pages and groups associated with inciting violence in Myanmar, but continues to refuse setting up an office in the country — despite groups on the ground saying would be necessary to show it’s serious about the region.

“That doesn’t mean… people won’t find more examples of past mistakes before we improved our systems.”

Zuckerberg said in his note that the company “didn’t focus as much on these issues as we needed to, but we’re now much more proactive.”

“That doesn’t mean we’ll catch every bad actor or piece of bad content, or that people won’t find more examples of past mistakes before we improved our systems,” he said. Some have seen that as a hint that some of the worst revelations are yet to come. Perhaps it’s just Zuckerberg hedging his bets as a way to indemnify his remarks from criticism when the next inevitable bad news break hits the wires.

In his 1,000-word post, Zuckerberg said he was “proud” three times, he talked of the company’s “focus” four times and how much “progress” was being made five times. But there wasn’t a single “sorry” to be seen. Then again, he’s spent most of his Facebook career apologizing for the company’s fails. Any more at this point would probably come across as trite.

Zuckerberg ended on as much as a cheery note as he began, looking to the new year as an opportunity for “building community and bringing people together,” adding: “Here’s to a great new year to come.”

Well, it can’t be much worse than this year. Or can it?",Social Media,TechCrunch,https://techcrunch.com/2018/12/28/mark-zuckerberg-tonedeaf-end-of-year-remarks/,"Mark Zuckerberg's end-of-year note failed to address Facebook's role in the spread of hate speech, election interference, misinformation, and ethnic violence and genocide, as well as its inability to keep user data safe and its reliance on an inadequately large staff of moderators to police its platform.",Security & Privacy
371,"Over 3 Years Later, 'Deleted' Facebook Photos Are Still Online","Facebook is still working on deleting photos from its servers in a timely manner nearly three years after Ars first brought attention to the topic. The company admitted on Friday that its older systems for storing uploaded content ""did not always delete images from content delivery networks in a reasonable period of time even though they were immediately removed from the site,"" but said it's currently finishing up a newer system that makes the process much quicker. In the meantime, photos that users thought they ""deleted"" from the social network months or even years ago remain accessible via direct link.

The Problem: 'Deleted' Photos Never Go Away

[partner id=""arstechnica""]When we first investigated this phenomenon in 2009, we discovered that photos ""deleted"" from Facebook seemingly never go away if you have a direct link to the image file on Facebook's servers. Users who might have had second thoughts about posting a photo — whether it was because they didn't want retaliation from an employer, wanted to avoid family drama, or uploaded a photo of a friend without their permission — could certainly remove the image from Facebook's main user interface. But as long as someone had a direct link to the .jpg file in question, the photo would remain accessible for an indefinite amount of time.

When we asked Facebook about it, we were told that the company was ""working with our content delivery network (CDN) partner to significantly reduce the amount of time that backup copies persist.""

But when we followed up on the story more than a year later, our ""deleted"" photos were still accessible via direct link. That's when the reader stories started pouring in: we were told horror stories about online harassment using photos that were allegedly deleted years ago, and users who were asked to take down photos of friends that they had put online.

There were plenty of stories in between as well, and panicked Facebook users continue to e-mail me, asking if we have heard of any new way to ensure that their deleted photos are, well, deleted. For example, one reader linked me to a photo that a friend of his had posted of his toddler crawling naked on the lawn. He asked his friend to take it down for obvious reasons, and so the friend did — in May of 2008. As of this writing in 2012, I have personally confirmed that the photo is still online, as are several others that readers linked me to that were deleted at various points in 2009 and 2010.

(Amusingly, after publishing the 2010 followup, Facebook appeared to delete my photos from its CDN that I had linked in the piece. The company never offered me any explanation, but my photos were the only ones that were deleted at that time. Other ""deleted"" photos that I had saved links to — ones that weren't from my account and were deleted even earlier than mine — remained online.)

It's 2012, And Things Aren't Much Different — Yet

After confirming once again that all the photos that my friends and Ars readers had sent in were still online, I reached out to Facebook once again, looking for an answer as to why this is still going on nearly three years after the company first promised it was ""working"" on the issue.

""The systems we used for photo storage a few years ago did not always delete images from content delivery networks in a reasonable period of time even though they were immediately removed from the site,"" Facebook spokesperson Frederic Wolens told Ars via e-mail.",Social Media,WIRED,https://www.wired.com/2012/02/over-3-years-later-deleted-facebook-photos-are-still-online/,"The main undesirable consequence of Social Media discussed here is that photos that users thought they had ""deleted"" from the site months or even years ago remain accessible via direct link, posing a risk of online harassment or other unwanted exposure of private information.",Security & Privacy
372,"The problems with Facebook are inherent in its design, but that can change – TechCrunch","The problems with Facebook are inherent in its design, but that can change

The latest controversies of social networks Facebook and Twitter are easily the most heated in their entire 12-14 year history — not just because of their suspect role in enabling interference in the 2016 election, but because by now, nearly all of us are users. If history is any guide, however, this outrage likely won’t last.

The simple fact is Facebook and Twitter have become too useful for most of us to quit, efficiently connecting us to people and ideas in ways that no other platform can replicate. It’s usually enough for the social networks’ corporate owners to loudly apologize and promise new reforms; after the anger ebbs, equilibrium is rapidly restored. Even many users who vowed to quit social media forever will eventually, begrudgingly, return.

Still, this current crisis of trust has created an opportunity to interrogate just exactly how social media is failing us, and push for the fundamental, systemic changes needed to make it better. I’m speaking of deeper, more subtle problems that are far less acknowledged than fake news or data mining: The core user experience of Facebook and Twitter are broken, rife with subtle visual and interactive cues which exploit and fuel our darker urges on these platforms — subtly impelling many of us to share fake news, engage in trolling, and worse. Here’s how:

Fast, Focused, Frenetic

Websites live and die by engagement, their ability to attract new users and keep them on the site. Facebook and Twitter have earned mass user bases and a central place in the mainstream discourse years ago, but their user experiences still reflect these companies’ origins as scrappy startups, desperate to keep growing. Consequently, every aspect of their user experience is optimized to reward frequent, and ultimately, frenetic engagement. For instance:

Publication speed: Response comments are published through an “Enter to send” model, versus “click to reply”.

One-click interaction: Retweet, reshare, reply, Like, Upvote/Downvote, or (in Facebook’s case) express an emotion, all with a single finger twitch.

Real-time usage stats: Content creation is rewarded with game-like “scores”, encouraging users to see how many likes, comments and reactions each of their interactions earns.

Brevity: Short form user responses (in the case of Twitter)

These dynamic interactions are compounded by the overall user interface, with image-based posts, screenshots and retweets occupying much of the interface display. Imagery accelerates and magnifies user engagement; it also encourages users to take and spread screencaps of incendiary private discussions and inflammatory discussions from other social networks.

The ever-increasing speed of wireless broadband further exacerbates this problem, encouraging emotional engagements wherever and wherever we might be with a device in our hands. It’s rare that you can scroll down a Twitter or Facebook feed without getting emotionally hooked by something. Unlike an analog conversation, which might hook you emotionally one part at a time, social media feeds offer multiple barbs per page. Scroll long enough and there is no escape.

It would be simplistic, however, to say the design of social media is the sole culprit, because they are papering over an even deeper design problem.

Filling the Flaws in the UX of Modern Life

Social networks are flourishing (for good and ill) because our networks are no longer operating at human scale. At human scale, we’re able to moderate better. Consider the user experience of the Thanksgiving dinner, where a heated political topic between relatives can be gently overridden by asking to pass the gravy. The entire shape of our communication patterns have changed. We routinely communicate with people far away, and increasingly, less with the people in our neighborhood. Our family and friend groups are smaller than ever before; 1 in 4 of us live alone, more than half us are unmarried. We spend increasingly more of our time in non-places — in freeway traffic, sterile office buildings, bland airports — putting us as humans on pause.

So we reach for an out. Social media becomes our cigarette break, a quick drag of distilled, pre-filtered humanity with potentially cancerous side effects. We interact with others through our social media profile, what I call our global, templated self, which amplifies the best of who we are — but helps social media companies profit from the demons of our darker nature.

Designing for Warmer Engagement

It will take many years of study and debate to understand and to address the civic design flaws which help make social media so corrosively addictive. Fortunately, addressing the flaws in social media design are easier. Because if it’s true that subtle UX elements can exert a negative influence on our social media usage, then equally small changes can also help curb our worst interactions. Consider some design tweaks to the existing user interface of Twitter and Facebook:

Cool-off before commenting: If a given social media post generates a rapid influx of negative comments or reactions, the system can impose a “cool-off” delay before further comments can be made. Even a pause of 30 seconds could work wonders on giving users a respite to consider the heated reaction they’re about to post, or even reconsider posting at all.

Quiz before commenting (or sharing): A Norwegian newsite recently introduced this feature to great effect: Whenever a reader wanted to post a comment on a given news item, they first had to answer a series of multiple choice questions about the story, to prove they had actually read it. After this system was deployed, trollish comments substantially decreased. Working in tandem with media sites, social media platforms should implement a similar quiz system on updates. It could also be deployed to prevent the thoughtless dispersion of content: before being able to share or retweet a given piece of content, a user would have to correctly answer a small set of questions to demonstrate they had really read or viewed it.

Implement timeline “rest” options: To address the cascade of emotional hooks created by timeline feeds, Facebook and Twitter should experiment with a pause button that imposes user-set resting periods — during which, users wouldn’t receive notifications or comments associated with their timeline.

Key advantage to these features is that they still foster sustained interaction on social media through a warmer overall experience that minimizes the fiery spikes of outrage that often cause users to disengage (or in Internet jargon, “ragequit”). It’s in the interest of Twitter and Facebook, in other words, to implement them.

But if past history is any guide, changes like these will come only after a sustained protest by the user base. It’s up to us to insist on a better, more humane social media experience — and not let the inertia of our everyday surroundings dull us back into our usual, templated routines. Until, that is, the next inevitable social network controversy spurs us into another moment of waning outrage.",Social Media,TechCrunch,https://techcrunch.com/2018/04/16/the-problems-with-facebook-are-inherent-in-its-design-but-that-can-change/,"Social Media platforms such as Facebook and Twitter are designed to encourage frequent and frenetic engagement, which leads to negative behavior such as trolling and the spread of fake news. To address this, there should be design changes that create a warmer user experience and reduce spikes of outrage to encourage sustained engagement.",User Experience & Entertainment
373,Facebook is being sued by a Polish drug prevention group over free speech violation – TechCrunch,"Facebook’s efforts to shut down harmful and malicious content on its platform have landed it in a European courtroom, after an anti-drug abuse organization in Poland claimed that a freeze on its Facebook Pages is a violation of its rights to free speech.

The Civil Society Drug Policy Initiative (Społeczna Inicjatywa Narkopolityki in Polish, which goes by the slightly unfortunate abbreviation of ‘SIN’) says that it has filed a complaint with the District Court of Warsaw against Facebook for violating articles 23-24 of the Polish Civil Code, which ensures free speech for individuals and organizations.

SIN says that Facebook deleted several of its pages on Facebook and Instagram for violating its community standards in 2018 and 2019 (here, here, here, here, here, and one page that appears to have been claimed by someone else in the interim). Another page SIN set up after the others were shut down appears to still be up for now.

SIN is asking for Facebook to reinstate its Pages and its followers, and to apologise publicly for its actions.

When contacted for a response, Facebook declined to comment on the case.

SIN describes itself as a Polish NGO that runs educational activities to make people aware of the harmful consequences of drug use, and provides assistance to people drug abusers.

The group is being supported in its legal action by lawyers working pro-bono with a Polish non-profit called Panoptykon, which was set up in 2009 to find and help fight cases against tech companies where it believes personal rights are being violated in our current “surveillance society” (its description, and also the reason for the panopticon reference).

Panoptykon is a busy group these days: another case that it filed against Google and the IAB in Poland over targeted advertising recently got referred up to the authorities in Ireland (where many cases are heard as a result of the country being home to many global HQs) and Belgium (home of the European Commission.

It’s not exactly clear what Facebook found offensive in SIN’s content since Facebook declined to respond.

From the looks of it, SIN itself does not take your typical “don’t do drugs” approach but instead focuses on the concept of harm reduction. It sets up a presence at clubs, festivals and other events where people might take recreational drugs. Then, it “leave[s] the assumption that it’s best not to start using drugs, or to stop if you do so [since] it’s not always possible. If you are already using, we educate on how to do it with least damage possible.” It also offers methods for testing drugs and advice on what different drugs can do.

SIN notes that the UN, the EU, the National Bureau for the Prevention of Drug Addiction; Red Cross; Doctors Without Borders and many others support this approach.

However, it may be that its native approach appeared to Facebook’s algorithms as similar to groups that advocate using drugs. Alternatively, it may be that Facebook regarded SIN as taking a particular approach on a controversial subject — the best way to cope with illegal drug use — which would have run afoul of its guidelines. “The main goal of our action is to make sure that regardless of what decisions you make at parties, you have fun and keep it safe,” SIM notes on its site.

Social media platforms have come under fire for how their efforts to contain malicious or harmful content occasionally backfire by sometimes penalising more innocent accounts by mistake. Similarly, there have been accusations that rules designed by regulators to prevent harmful content on social media are partly responsible for the platforms mandating particularly stringent controls, which ironically end up violating the exact rights that regulators are trying to ensure, like free speech.

Facebook — which has had its share of heat from European regulators over issues like violations of personal privacy, data breaches, and the role it plays in helping to police its platform against abuses and misuse in democratic processes — has been working to improve the nuance of its controls, by making it more transparent to users when it has taken certain actions like shutting down pages or blocking content, and why. Panoptykon says that it believes this legal action, if successful, could help that evolution along.

“We hope that SIN vs Facebook will incentivize the portal to make further changes and implement ‘due process,’ thus establishing the standards also for other platforms,” Panoptykon notes. “In addition, with SIN vs Facebook we strive not only to persuade the platforms to create better internal procedures, but also to ensure that users who do not agree with their decisions can challenge them before an independent, external body, such as a court.”",Social Media,TechCrunch,https://techcrunch.com/2019/05/07/facebook-is-being-sued-by-a-polish-drug-prevention-group-over-free-speech-violation/,"Social media platforms have faced criticism for their efforts to control malicious or harmful content, which can backfire by penalizing innocent accounts and violating users' rights to free speech.",Equality & Justice
374,Today’s Filibuster Mentioned Instagram’s Guns for Sale. Then #Gunsforsale Disappeared,"Instagram content View on Instagram

Earlier today, Senator Chris Murphy of Connecticut, responding to the terror in Orlando, launched a filibuster in the hope of pushing Congress to work toward meaningful gun control. Fellow Democrats joined him throughout the afternoon and said all the things you'd expect lawmakers to say. Then Senator Ed Markey of Massachusetts took the floor.

""We need to ban gun sales on sites on the Internet like Facebook and Instagram,"" he said. ""Right now, anyone can do a search for 'AK-47' or 'AR-15' or even 'guns for sale' on Instagram and find guns for sale.""

He's right. Even now, years after the issue of gun sales on social media platforms came to light, you can still find plenty of them available. We searched #gunsforsale on Instagram minutes after Markey’s comments and got nearly 8,000 results. The listings ranged from handguns of every description to a Heckler & Koch MP5 submachine gun.

But then, in real time, the results began to shift for people throughout the WIRED office. The number of results dwindled with subsequent searches, then disappeared entirely before reappearing a short time later with the addendum ""Recent posts from #gunsforsale are currently hidden because the community has reported some content that may not meet Instagram's community guidelines."" Instagram, and its parent company Facebook, didn't say so, but it appeared that Markey illuminated an unintended use of the platform and the company was struggling to keep it in check.

Instagram content View on Instagram

But a cursory search of “#gunsforsale” or combinations including “#guns,” “#forsale,” “#ak47,” or “#ar15” yielded results from users who clearly are not licensed firearms dealers, and who specifically offer to conduct business through direct messages within Instagram. That violates the platform's user agreement and terms of service, but unless they are reported by other Instagram users—or mentioned, say, on the Senate floor—those posts may never get removed.

Instagram is not intended to be a marketplace. But having users police ads that violate guidelines suggests there is an acceptable amount of content that evades detection. It’s time for social media platforms to stop relying on users and start looking for these things themselves.

Additional reporting by Charley Locke and Angela Watercutter",Social Media,WIRED,https://www.wired.com/2016/06/filibuster-instagram-gun-dealers/,"Social Media platforms like Instagram and Facebook have failed to properly police their own sites, resulting in users being able to buy and sell guns without the proper licensing, and without the posts being removed unless reported by other users.",Security & Privacy
375,Surviving This Summer on the Internet,"On the August afternoon that a white supremacist drove a car through a crowd of peaceful protestors in Charlottesville, Virginia, I was perched on a bar stool in a café near my home, sipping a glass of rosé while reading a novel and daydreaming. It was one of those rare, near-perfect New York days when the light streamed through a wide-open window, training its beam on the notebook at the table next to me. There, a tutor worked through math lessons with a slightly frustrated adult student.

Jessi Hempel is Backchannel's editorial director. Sign up to get Backchannel's weekly newsletter, and follow us on Facebook and Twitter.

At 2:52 p.m., a New York Times headline popped up on my phone. My stomach sank as I took in the image of the vehicle, a man just behind it with his feet up in the air, frozen in the moment before his torso smacked the ground. I texted my partner, a University of Virginia graduate, who was herself scrolling through her friends’ Instagram posts with horror. My eyes stung with anxious tears as I thought, not for the first time this year: Everything has changed now and we are all in trouble.

Around me, nothing had actually changed. The tutor was still disentangling math problems. The espresso machine ground beans, cut off, and then switched on again. I tried to return to my book, but gave up and dropped it in my bag. I clutched my wine, which had become more of a coping device than an afternoon treat, and scrolled through my Twitter feed. One person said there were more “bronies” gathered in Philadelphia for a convention than there were Nazis in Virginia. Retweet! Someone else criticized the president for not yet condemning the gathering. Retweet! Now the president was speaking and his words were being live-tweeted, with commentary. I switched to Instagram, to Facebook, even over to Slack to see if my colleagues were watching and maybe reaching out.

I knew I should turn my phone off, but I could not look away.

This is not how August goes—at least not my August. For the past five years, I’ve signed off all social media—essentially any messaging software to which I didn’t have access before 2007, when I got my first smartphone. My annual social media sabbatical has been reliably awesome; it’s an opportunity to notice the things I’ve lost in exchange for all the connections and productivity that social media has introduced to my life. It’s like Whole 30 for the internet—a radical diet change that at first leaves me feeling sick and lethargic, and then slowly returns me to health.

But this year, I skipped the cleanse. It just seemed, well, so 2013. That was the year technology detoxes crept into mainstream discourse. The New York Times profiled a camp where adults could disconnect from their gadgets. Fast Company ran a cover story entitled #Unplug in which the writer lived without the internet for 25 days. A 2013 survey from the estimable Pew Research Center revealed that 61 percent of Facebook’s US users had taken a break from the service for a period of several weeks or more.

In my quest to figure out why my annual exercise felt so irrelevant this year, I called up several people who’d similarly stepped off social media a few years back and blogged about the experience. I chatted with Liz Gross, a market insights manager and social media strategist from Madison, Wisconsin. Even five years ago, she explained, she still had an offline life and an online life. She mostly lived offline. “I needed to know what was going on online, but also attend to my life,” she says. Now, for most of us, that divide has disappeared. Our lives are powered, to a lesser or greater degree, by the internet. There is no offline.",Social Media,WIRED,https://www.wired.com/story/surviving-this-summer-on-the-internet/,"Social media has changed the way we live our lives, with many of us now using the internet to power our days. This can be detrimental in that it can force us to neglect our offline lives, leaving us feeling disconnected and overwhelmed.",Social Norms & Relationships
376,Dear Mr. Know-It-All: Should You Delete Someone's Facebook Account After They Die?,"How long should you wait before shutting down someone’s Facebook account after they die?

“This is for all you lovers out there.” That’s how it begins—one of the most existentially horrifying moments in American cinema.

I’m talking about the Enchantment Under the Sea Dance in Back to the Future, in which we see a temporally displaced Marty McFly onstage, sitting in with the band on “Earth Angel” with a guitar, while his teenage parents, George and Lorraine, move toward their first kiss.

This is it: the precise, excruciatingly brief moment in which the cosmos will offer up the possibility for them to fall in love—a doorway they can step through or not step through. But if they do, it’s a straight shot from here through the sinews of the spacetime continuum to marriage, and to Marty’s birth, and to all the circumstances of life that Marty had always mistaken for the one and only, inviolable reality. But he’s wising up now. While traveling through time, he’s learning that his life, like all of our lives, is only an exquisite and provisional fluke—a haphazard product of so many collisions and coincidences that were never guaranteed. Up on the stage, he’s about to be confronted with this truth in a deep and terrible way.

You know the scene, right? It turns on an obnoxious redhead who tells George to “scram,” then cuts in between him and Lorraine and sweeps her away. Slowly, a warped and nightmarish score rises over “Earth Angel.” Marty becomes disoriented, diminished. His strength—his selfhood—is draining out of him as, out on the dance floor, that insufferable ginger cackles and whips Lorraine around like a rag doll. He is dragging Lorraine farther and farther from George—and dragging our universe (or maybe all of this is proof of a multiverse?) farther from its capacity to produce Marty’s life, diverting the sacred headwaters of his personal history.

Marty’s compromised hands batter his guitar, making a discordant mess of “Earth Angel.” He raises one hand and watches it turn ... translucent! His face is stupefied, powerless. Somehow Michael J. Fox—that cocky scion of 1980s precociousness—pulls it off: this look of violated innocence and panic, of a carefree boy suddenly thrown down and dying on the battlefield of time.

What is happening to Marty? Doc Brown has already explained the process: Marty is being “erased from existence.” Stop and think about those words for a second. They are horrifying. (A thrash metal band from Belfast called Scimitar even wrote an abrasive, ear-­pummeling song called “Erased from Existence,” inspired by this scene. It’s very hard to listen to.) But the worst part isn’t even that Marty himself is being erased. The true, piercing horror comes when he looks at the photograph slipped through the strings of his guitar: the one of his brother and sister and him standing against a low rock wall. Earlier in the film we’ve seen the images of his two siblings vanish from that photo, and now Marty’s image is fading too. This is what it means to be erased from existence. And this is what frightens me most: not just that Marty is vanishing but that all evidence of his life will vanish. No one will know who he was, because—here’s the thing—he wasn’t.

You ask how long you should wait before shutting down the Facebook page of a loved one who’s died. I ask why you’d ever want to delete it. Consider the ripple effects—the many ways their absence would be felt across that platform, on so many other ­people’s pages and their community’s collective, digital memory. Everything the deceased had said, not just on their own page but on others, would be gone. And so would everything people had said to them. They’d be instantaneously untagged from hundreds or even thousands of other people’s photos, exiled into some anonymous interloper status: a nameless human void.",Social Media,WIRED,https://www.wired.com/2016/02/mr-know-it-all-february/,"The shutting down of a loved one's social media page after they die would erase all evidence of their life, including their posts, comments, and tags in other people's photos. This would remove them from the collective digital memory, leaving a human void in their place.",Social Norms & Relationships
377,TrumpBlocks.me Automates Checks for Trump's Twitter Blocks,"Donald Trump—or at the very least, someone with ready access to Donald Trump's Twitter account—has been blocking as many vocal critics as he can get his perfectly proportioned hands on. Now, a new web app built by former WIRED senior editor Kevin Poulsen shows not only exactly who these haters and losers are, but hopefully what their blocking could mean for our first amendment futures. Because if people make use of it, TrumpBlocks.me could be the definitive collection of everyone Donald Trump has blocked on Twitter.

Of course, blocking Twitter users isn't anything new for Trump; he's habitually frozen out critics and pests since long before he entered the Oval Office. But Trump isn't just a failed mail-order steak salesman anymore—he's the ruler of the free world. And when he does decide to take away citizens' abilities to engage with his daily onslaught of 140-character missives, he's potentially messing with the very same rights that keep us free in the first place. So when Poulsen heard about Trump's most recent round of high-profile blocks (think Stephen King), he decided to document the occasion with cold, hard data.

""It's fascinating that we live in a time when you might wake up in the morning and find that the Leader of the Free World has banned you from reading and commenting on what he says online,"" says Poulsen. ""As a journalist and a geek, I hate taking things on faith, and I hate guesswork and rough estimates and anecdotal information. I love evidence and numbers.""

So Poulsen isn't just taking people's words for it. Instead, he's using Twitter's API both to confirm that Trump actually blocked the user in question, and to monitor changes in their status. Once a blockee clicks the ""I'm Blocked"" button on the TrumpBlocks.me homepage, a popup appears asking for read-only access to their Twitter account. ""That means the app can't post tweets on behalf of the user, and it can't see direct messages. But it does see other people's timelines with whatever restrictions that user's account is under,"" Poulsen explains.

That way, when the app attempts to load Donald Trump's timeline from a blocked account, Twitter's API will send back an error message instead of the normal timeline an unblocked user might see. Once that process confirms an account's blocked status, TrumpBlocks.me will periodically run it again, just in case Trump ever decides to set a blockee free. So far, the site has confirmed 30 blocked accounts in total.

WIRED also keeps tracks of prominent Trump block victims, including information like who they are and why they landed on Trump's no-tweet list. Poulsen's effort differs by offering less granular detail, but a more comprehensive list and automated process. In either case, keeping track of the people Trump tunes out on Twitter has the potential for much further-reaching implications according to Katie Fallow, a senior attorney at the Knight First Amendment Institute.

""The project's important for a lot of the same reasons why we think his blocking violates first amendment,"" said Fallow. ""He runs his Twitter account as an official account where he communicates his policies, interacts with the public, and informs the country about important decisions—like announcing the new FBI director… Governments can't have town halls that exclude people for being democrats, or believing in climate change. It's a first amendment principle that the government can’t engage in viewpoint discrimination.""

While the Knight Institute has called for Trump to cease blocking American citizens on Twitter, the president has yet to acquiesce. In the meantime, though, Poulsen hopes that the president's ever-growing list of blockees could give us some insight into his psyche. So if you've had the honor of being blocked by the President of the United States, now's your chance to get your name in the virtual history books. And don't worry—it's almost definitely better this way.",Social Media,WIRED,https://www.wired.com/story/trumpblocksme-kevin-poulsen-twitter/,"Donald Trump blocking vocal critics on Twitter has implications for the first amendment, as it potentially restricts citizens from engaging with the president's 140-character missives. A new web app, TrumpBlocks.me, seeks to document this by confirming each account's blocked status and monitoring changes in their status.",Equality & Justice
378,Strava Data Heat Maps Expose Military Base Locations Around the World,"A modern equivalent of the World War II era warning that “loose lips sink ships” may be “FFS don’t share your Fitbit data on duty.” Over the weekend, researchers and journalists raised the alarm about how anyone can identify secretive military bases and patrol routes based on public data shared by a “social network for athletes” called Strava.

This past November, the San Francisco-based Strava announced a huge update to its global heat map of user activity that displays 1 billion activities—including running and cycling routes—undertaken by exercise enthusiasts wearing Fitbits or other wearable fitness trackers. Some Strava users appear to work for certain militaries or various intelligence agencies, given that knowledgeable security experts quickly connected the dots between user activity and the known bases or locations of US military or intelligence operations. Certain analysts have suggested the data could reveal individual Strava users by name.

But the biggest danger may come from potential adversaries figuring out “patterns of life,” by tracking and even identifying military or intelligence agency personnel as they go about their duties or head home after deployment. These digital footprints that echo the real-life steps of individuals underscore a greater challenge to governments and ordinary citizens alike: each person’s connection to online services and personal devices makes it increasingly difficult to keep secrets.

All Your Base Are Belong to Us

The revelations began unspooling at a rapid pace after Nathan Ruser, a student studying international security at the Australian National University, began posting his findings via Twitter on Saturday afternoon. In a series of images, Ruser pointed out Strava user activities potentially related to US military forward operating bases in Afghanistan, Turkish military patrols in Syria, and a possible guard patrol in the Russian operating area of Syria.

Other researchers soon followed up with a dizzying array of international examples, based on cross-referencing Strava user activity with Google Maps and prior news reporting: a French military base in Niger, an Italian military base in Djibouti, and even CIA “black” sites. Several experts observed that the Strava heatmap seemed best at revealing the presence of mostly Western military and civilian operations in developing countries.

Many locations of military and intelligence agency bases pointed out by researchers and journalists had already been previously revealed through other public sources. But the bigger worry from an operations security standpoint was how Strava’s activity data could be used to identify interesting individuals, and track them to other sensitive or secretive locations. Paul Dietrich, a researcher and activist, claimed to have used public data scraped from Strava’s website to track a French soldier from overseas deployment all the way back home.

“This is the part that is perhaps most worrisome, that an individual's identity might be pullable from the data, either by combining with other information online or by hacking Strava—which just put a major bullseye on itself,” says Peter Singer, strategist and senior fellow at New America, a think tank based in Washington, DC. “Knowing the person, their patterns of life, etc., again would compromise not just privacy but maybe security for individuals in US military, especially if in the Special Operations community.”

Strava’s data could even be used to follow individuals of interest as they rotated among military bases or intelligence community locations, according to Jeffrey Lewis, director of the East Asia Nonproliferation Program in the Middlebury Institute of International Studies at Monterey, California. In a sobering Daily Beast article, Lewis laid out a scenario by which Chinese analysts could track a Taiwanese soldier based on his activities at a known missile base and thereby discover other previously unknown missile bases as the soldier’s duties required him to rotate through those bases.

Taking Steps to Fix the Problem

The United States is clearly far from alone in dealing with such security challenges. Back in 2015, the People’s Liberation Army Daily issued a stern warning to members of the Chinese military about the security risks posed by smart watches, fitness bands, and smart glasses, according to Quartz. But the Strava example shows that the United States may be at greater risk, with its relatively large footprint involving troops, intelligence personnel, diplomats, and contractors deployed overseas in sensitive areas or conflict zones.

The US military’s Central Command has already begun reassessing its privacy policies for the troops after the Strava revelations, according to reporting by The Washington Post and others. Current US military service policies seem to allow for use of fitness trackers and other wearables with the caveat that local commanders have the discretion to tighten security. In fact, the US Army has previously promoted use of Fitbit trackers as part of a pilot fitness program.",Social Media,WIRED,https://www.wired.com/story/strava-heat-map-military-bases-fitness-trackers-privacy/,"Strava's fitness tracker data exposing secretive military bases and operations has raised serious security concerns, as patterns of life may be tracked and individuals could be identified. This highlights the challenge of keeping secrets in the era of social media and wearables, and the US military is reassessing its privacy policies in response.",Security & Privacy
379,How the truth was murdered,"Even then, Cross says, the people who were best able to talk about why these campaigns took hold and what might stop them—that is, the people under attack—were not taken seriously as experts. She was one of them, both writing about Gamergate and being targeted by it. Media attention to online abuse gathered pace after Gamergate, Mitchell told me, for a simple reason: “When you finally paid attention, you paid attention when a white woman was being targeted, but not when a Black woman was being targeted.”

And as some companies began trying to do something about abuse, those involved in such efforts often found themselves becoming the targets of exactly the same kind of harassment.

When Ellen Pao took over as CEO of Reddit in 2014, she oversaw the site’s first real attempt to confront the misogyny, racism, and abuse that had found a home there. In 2015, Reddit introduced an anti-harassment policy and then banned five notorious subreddits for violating it. Redditors who were angry at those bans then attacked Pao, launching petitions calling for her resignation. She ended up stepping down later that year and is now a campaigner for diversity in the technology industry.

Pao and I spoke in June 2020, just after Reddit banned r/The_Donald, a once-popular pro-Trump subreddit. For years it had served as an organizing space to amplify conspiracy-­fueled, extremist messages, and for years Pao had urged Reddit’s leadership to ban it. By the time they finally did, many of its subscribers had already moved off the site and on to other platforms, like Gab, that were less likely to crack down on them.

“It’s always been easier not to do anything,” Pao told me. “It takes no resources. It takes no money. You can just keep doing nothing.”

A constant deluge

It’s not as if the warnings of Pao, Cross, and others have only just penetrated mainstream consciousness, though. The flood waters come back again and again.

The Friday before Donald Trump was elected in 2016, another conspiracy theory—one that would, in about a year’s time, help create QAnon—trended on Twitter. #SpiritCooking was easy to debunk. Its central claims were that Hillary Clinton’s campaign chair, John Podesta, was an occultist, and that a dinner hosted by a prominent performance artist was actually a secret satanic ritual. The source of the theory was an invitation to the dinner in Podesta’s stolen email archives, which had been released publicly by WikiLeaks that October.

I wrote about misinformation during the 2016 elections, and watched as #SpiritCooking evolved into Pizzagate, a conspiracy theory about secret pedophile rings centered on pizza shops in Washington, DC. Reddit banned a Pizzagate forum in late November that year for “doxxing” people (i.e., putting their personal information online). On December 4, 2016, exactly one month after #SpiritCooking exploded, a North Carolina man walked into a DC restaurant targeted by Pizzagate believers, lifted up his AR-15 rifle, and opened fire.

These first few months after the 2016 election marked another point in time—much like today—when the flood of disinformation was enough to get more people than usual to notice. Shocked by Trump’s election, many worried that foreign interference and fake news spread on social media had swayed voters. Facebook CEO Mark Zuckerberg initially dismissed this as “a pretty crazy idea,” but ensuing scrutiny of social-media platforms by the media, governments, and the public revealed that they could indeed radicalize and harm people, especially those already vulnerable.

And the damage continued to grow. YouTube’s recommendation system, designed to get people to watch as many videos as possible, led viewers down algorithmically generated tunnels of misinformation and hate. On Twitter, Trump repeatedly used his huge platform to amplify supporters who promoted racist and conspiratorial ideologies. In 2017, Facebook introduced video livestreaming and was shortly overwhelmed by live videos of graphic violence. In 2019, even before covid-19, vaccine misinformation thrived on the platform as measles outbreaks spread across the US.

“Choosing to have people whose main objective is to constantly spew hate speech … that’s a decision. No one has forced them to make that decision.”

The tech companies responded with a running list of fixes: hiring enormous numbers of moderators; developing automated systems for detecting and removing some kinds of extreme content or misinformation; updating their rules, algorithms, and policies to ban or diminish the reach of some forms of harmful content.

But so far the toxic tide has outpaced their ability—or their willingness—to beat it back. Their business models depend on maximizing the amount of time users spend on their platforms. Moreover, as a number of studies have shown, misinformation originates disproportionately from right-wing sources, which opens the tech platforms to accusations of political bias if they try to suppress it. In some cases, NBC News reported in August, Facebook deliberately avoided taking disciplinary action against popular right-wing pages posting otherwise rule-breaking misinformation.

Many experts believed that the next large-scale test of these companies’ capacity to handle an onslaught of coordinated disinformation, hate, and extremism was going to be the November 2020 election. But the covid pandemic came first—a fertile breeding ground for news of fake cures, conspiracy theories about the virus’s origin, and propaganda that went against common-sense public health guidelines.

If that is any guide, the platforms are going to be largely powerless to prevent the spread of fake news about ballot fraud, violence on the streets, and vote counts come Election Day.

The storm and the flood

I’m not proposing to tell you the magical policy that will fix this, or to judge what the platforms would have to do to absolve themselves of this responsibility. Instead, I’m here to point out, as others have before, that people had a choice to intervene much sooner, but didn’t. Facebook and Twitter didn’t create racist extremists, conspiracy theories, or mob harassment, but they chose to run their platforms in a way that allowed extremists to find an audience, and they ignored voices telling them about the harms their business models were encouraging.

Sometimes these calls came from within their own companies and social circles.

When Ariel Waldman, a science communicator, went public with her story of Twitter abuse, she hoped she’d be the last person to be the target of harassment on the site. It was May 2008.

By this point she’d already tried privately for a year to get her abusers removed from the platform, but she remained somewhat optimistic when she decided to publish a blog post detailing her experiences.

After all, she knew some of the people who had founded Twitter just a couple of years earlier.

“I used to hang out at their office, and they were acquaintances. I went to their Halloween parties,” Waldman told me this summer. There were models for success at the time, too: Flickr, the photo-sharing website, had been extremely responsive to requests to take down abusive content targeting her.

So she wrote about the threats and abuse hurled at her, and detailed her emails back and forth with the company’s founders. But Twitter never adequately dealt with her abuse. Twelve years later, Waldman has seen the same pattern repeat itself year after year.

“Choosing to have people whose main objective is to constantly spew hate speech and harm other people on a platform—that’s a decision. No one has forced them to make that decision,” she says.

“They alone make it. And I feel that they increasingly act as if—you know, that it’s more complicated than that. But I don’t really think it is.”

I don’t know what to tell you about how to stop the flood. And even if I did, it wouldn’t undo the considerable damage from the rising waters. There have been permanent effects on those voices who were turned into footnotes as they tried to warn the rest of us.

Today, Mitchell notes, the same groups that engaged in mob campaigns of abuse and harm have reframed themselves as the victims whenever there are calls for major social-media platforms to silence them. “If they have had the right to run amok for all that time, then you take that away from them—then they feel like they’re the ones who are oppressed,” she says. “While no one pays attention to the people who are actually oppressed.”

NAJEEBAH AL-GHADBAN

One path toward making things better could involve providing more incentive for companies to do something. That might include reforming Section 230, the law that shields social-media companies from legal liability for user-posted content.

Mary Anne Franks, a professor at the University of Miami who has worked on online harassment, believes that a meaningful reform of the law would do two things: limit the reach of those protections to speech rather than conduct, and remove immunity from companies that knowingly benefit from the viral spread of hate or misinformation.

Pao notes that companies might also take these issues more seriously if their leadership looked more like the people being harassed. “You’ve got to get people with diverse backgrounds in at high levels to make the hard decisions,” she says, adding that that’s what they did at Reddit: “We just brought in a bunch of people from different racial and ethnic backgrounds, mostly women, who understood the problems and could see why we needed to change. But right now these companies have boards full of white men who don’t push back on problems and focus on the wrong metrics.”

Phillips, of Syracuse, is more skeptical. You Are Here, a book she published with her writing partner Ryan Milner earlier this year, frames online abuse and disinformation as a global ecological disaster—one that, like climate change, is rooted deeply in human behavior, has a long historical context, and is now all-encompassing, poisoning the air.

She says that asking technology companies to solve a problem they helped create cannot work.

“The fact of the matter is that technology, our networks, the way information spreads, is what helped facilitate the hell. Those same things are not what’s going to bring us out of it. The idea that there’s going to be some scalable solution is just a pipe dream,” Phillips says. “This is a human problem. It is facilitated and exacerbated exponentially by technology. But in the end of it, this is about people and belief.”

Cross concurs, and offers a tenuous hope that awareness is finally shifting.

“It’s impossible for people to deny that this has, like sand, gotten into everything, including the places you didn’t know you had,” she says.

“Maybe it will cause an awakening. I don’t know how optimistic I am, but I feel like at least the seeds are there. The ingredients are there for that sort of thing. And maybe it can happen. I have my doubts.”",Social Media,MIT,https://www.technologyreview.com/2020/10/07/1009336/how-the-truth-was-murdered-disinformation-abuse-harassment-online-2/,"Social media has enabled the spread of misinformation, hate speech, and mob harassment, leading to serious harms such as radicalization, violence, and public health scares. Despite warnings from experts, tech companies have allowed this to happen for years due to their business models and fear of political bias, and the damage to those targeted has been permanent.",Equality & Justice
380,YouTube under fire for recommending videos of kids with inappropriate comments – TechCrunch,"More than a year on from a child safety content moderation scandal on YouTube and it takes just a few clicks for the platform’s recommendation algorithms to redirect a search for “bikini haul” videos of adult women towards clips of scantily clad minors engaged in body contorting gymnastics or taking an ice bath or ice lolly sucking “challenge.”

A YouTube creator called Matt Watson flagged the issue in a critical Reddit post, saying he found scores of videos of kids where YouTube users are trading inappropriate comments and timestamps below the fold, denouncing the company for failing to prevent what he describes as a “soft-core pedophilia ring” from operating in plain sight on its platform.

He has also posted a YouTube video demonstrating how the platform’s recommendation algorithm pushes users into what he dubs a pedophilia “wormhole,” accusing the company of facilitating and monetizing the sexual exploitation of children.

We were easily able to replicate the YouTube algorithm’s behavior that Watson describes in a history-cleared private browser session which, after clicking on two videos of adult women in bikinis, suggested we watch a video called “sweet sixteen pool party.”

Clicking on that led YouTube’s side-bar to serve up multiple videos of prepubescent girls in its “up next” section where the algorithm tees-up related content to encourage users to keep clicking.

Videos we got recommended in this side-bar included thumbnails showing young girls demonstrating gymnastics poses, showing off their “morning routines,” or licking popsicles or ice lollies.

Watson said it was easy for him to find videos containing inappropriate/predatory comments, including sexually suggestive emoji and timestamps that appear intended to highlight, shortcut and share the most compromising positions and/or moments in the videos of the minors.

We also found multiple examples of timestamps and inappropriate comments on videos of children that YouTube’s algorithm recommended we watch.

Some comments by other YouTube users denounced those making sexually suggestive remarks about the children in the videos.

Back in November 2017, several major advertisers froze spending on YouTube’s platform after an investigation by the BBC and the Times discovered similarly obscene comments on videos of children.

Earlier the same month YouTube was also criticized over low-quality content targeting kids as viewers on its platform.

The company went on to announce a number of policy changes related to kid-focused video, including saying it would aggressively police comments on videos of kids and that videos found to have inappropriate comments about the kids in them would have comments turned off altogether.

Some of the videos of young girls that YouTube recommended we watch had already had comments disabled — which suggests its AI had previously identified a large number of inappropriate comments being shared (on account of its policy of switching off comments on clips containing kids when comments are deemed “inappropriate”) — yet the videos themselves were still being suggested for viewing in a test search that originated with the phrase “bikini haul.”

Watson also says he found ads being displayed on some videos of kids containing inappropriate comments, and claims that he found links to child pornography being shared in YouTube comments too.

We were unable to verify those findings in our brief tests.

We asked YouTube why its algorithms skew toward recommending videos of minors, even when the viewer starts by watching videos of adult women, and why inappropriate comments remain a problem on videos of minors more than a year after the same issue was highlighted via investigative journalism.

The company sent us the following statement in response to our questions:

Any content — including comments — that endangers minors is abhorrent and we have clear policies prohibiting this on YouTube. We enforce these policies aggressively, reporting it to the relevant authorities, removing it from our platform and terminating accounts. We continue to invest heavily in technology, teams and partnerships with charities to tackle this issue. We have strict policies that govern where we allow ads to appear and we enforce these policies vigorously. When we find content that is in violation of our policies, we immediately stop serving ads or remove it altogether.

A spokesman for YouTube also told us it’s reviewing its policies in light of what Watson has highlighted, adding that it’s in the process of reviewing the specific videos and comments featured in his video — specifying also that some content has been taken down as a result of the review.

However, the spokesman emphasized that the majority of the videos flagged by Watson are innocent recordings of children doing everyday things. (Though of course the problem is that innocent content is being repurposed and time-sliced for abusive gratification and exploitation.)

The spokesman added that YouTube works with the National Center for Missing and Exploited Children to report to law enforcement accounts found making inappropriate comments about kids.

In wider discussion about the issue the spokesman told us that determining context remains a challenge for its AI moderation systems.

On the human moderation front he said the platform now has around 10,000 human reviewers tasked with assessing content flagged for review.

The volume of video content uploaded to YouTube is around 400 hours per minute, he added.

There is still very clearly a massive asymmetry around content moderation on user-generated content platforms, with AI poorly suited to plug the gap given ongoing weakness in understanding context, even as platforms’ human moderation teams remain hopelessly under-resourced and outgunned versus the scale of the task.

Another key point YouTube failed to mention is the clear tension between advertising-based business models that monetize content based on viewer engagement (such as its own), and content safety issues that need to carefully consider the substance of the content and the context in which it has been consumed.

It’s certainly not the first time YouTube’s recommendation algorithms have been called out for negative impacts. In recent years the platform has been accused of automating radicalization by pushing viewers toward extremist and even terrorist content — which led YouTube to announce another policy change in 2017 related to how it handles content created by known extremists.

The wider societal impact of algorithmic suggestions that inflate conspiracy theories and/or promote bogus, anti-factual health or scientific content have also been repeatedly raised as a concern — including on YouTube.

And only last month YouTube said it would reduce recommendations of what it dubbed “borderline content” and content that “could misinform users in harmful ways,” citing examples such as videos promoting a fake miracle cure for a serious illness, or claiming the earth is flat, or making “blatantly false claims” about historic events such as the 9/11 terrorist attack in New York.

“While this shift will apply to less than one percent of the content on YouTube, we believe that limiting the recommendation of these types of videos will mean a better experience for the YouTube community,” it wrote then. “As always, people can still access all videos that comply with our Community Guidelines and, when relevant, these videos may appear in recommendations for channel subscribers and in search results. We think this change strikes a balance between maintaining a platform for free speech and living up to our responsibility to users.”

YouTube said that change of algorithmic recommendations around conspiracy videos would be gradual, and only initially affect recommendations on a small set of videos in the U.S.

It also noted that implementing the tweak to its recommendation engine would involve both machine learning tech and human evaluators and experts helping to train the AI systems.

“Over time, as our systems become more accurate, we’ll roll this change out to more countries. It’s just another step in an ongoing process, but it reflects our commitment and sense of responsibility to improve the recommendations experience on YouTube,” it added.

It remains to be seen whether YouTube will expand that policy shift and decide it must exercise greater responsibility in how its platform recommends and serves up videos of children for remote consumption in the future.

Political pressure may be one motivating force, with momentum building for regulation of online platforms — including calls for internet companies to face clear legal liabilities and even a legal duty care toward users vis-à-vis the content they distribute and monetize.

For example, U.K. regulators have made legislating on internet and social media safety a policy priority — with the government due to publish a white paper setting out its plans for ruling platforms this winter.",Social Media,TechCrunch,https://techcrunch.com/2019/02/18/youtube-under-fire-for-recommending-videos-of-kids-with-inappropriate-comments/,"Social media platforms such as YouTube are facing increasing criticism over the way their algorithms push users towards videos of minors, often with inappropriate comments, and their inability to adequately police comments on videos of children, as well as their promotion of content that misinforms users in harmful ways.",Security & Privacy
381,Regarding Facebook’s cryptocurrency – TechCrunch,"If Bloomberg and the New York Times are to be believed, later this year Facebook will introduce a cryptocurrency which will allow WhatsApp users to send money instantly. Yes, that’s right: Facebook. Cryptocurrency. Earthquake! Revolution! The world is tilting on its axis! The end times are cometh!

Except – um – what exactly are people going to do with FaceCoin, once they receive it?

This is not Facebook’s first venture into virtual currencies, payments, or peer-to-peer payments via messenger app. Remember Facebook Credits, its previous virtual currency, launched in 2011 and sunset two years later? Remember Facebook Gifts, launched in 2012 and sunset two years later (there’s a theme here) in part because, to quote the redoubtable Josh Constine, “Facebook never found a way solve distance and localization problems to make Gifts work internationally”? And of course Facebook Messenger Payments launched in the US in 2015 and expanded to Europe two years later.

But FaceCoin is different; FaceCoin is on a blockchain. (As a longtime blockchain enthusiast I feel I have earned some right to be a bit sarcastic here.) And FaceCoin is reportedly a stablecoin backed by a basket of fiat currencies, a la the SDRs of the IMF.

So it’s on a blockchain. What does a blockchain give you? Well, conceivably, smart contracts, but if it’s a backed stablecoin used for P2P transfer, it’s hard to see how those are relevant. Also, conceivably, privacy. Right now the crypto world offers stablecoins (Dai, Paxos, etc.) and privacy coins (ZCash, Monero, Grin) but — weirdly — nobody offers a private stablecoin. If Facebook were to do so, that would, in fact, be a genuinely big deal. Not least because:

On one end, a completely private and encrypted messaging service tied to an open, Zerocoin-like, zk-SNARK backed cryptocurrency and backed by a tech giant would instantly become the go-to mechanism for global money laundering, tax evasion, and just general criming. — Alex Stamos (@alexstamos) February 28, 2019

Conversely, if FaceCoin isn’t private:

On the other, without mathematically-backed privacy features having all of this data in one place would be a massive source of security and privacy risk, and a huge boon for countries with leverage over FB to get data access. Wow, gonna be an interesting couple of years. — Alex Stamos (@alexstamos) February 28, 2019

…although that assumes that it’s actually widely used, an outcome which is, to say the least, far from automatic. Again, just because Facebook launches a stablecoin cryptocurrency for peer-to-peer payments doesn’t mean people will actually use it. Remember Facebook Credits. Remember Facebook Gifts.

The trouble with stablecoins for payments, at least at the moment, is that businesses don’t accept them, so you have to convert them into fiat currency, like dollars or euros or cedis or what-have-you, in order to actually buy things like groceries or rides. True, Facebook could offer goods and services for purchase themselves in exchange for FaceCoin, but then it would basically be Facebook Credits all over again.

But remittances! you cry. Yes, very much so. Remittances are a massive market, and a holy grail of cryptocurrencies, and WhatsApp is widely used worldwide. Remittances are the obvious target market here. And it would be huge, and important, and wonderful, if Facebook were to make remittances 10x cheaper and faster … but that would require much more than fast international stablecoin transfers, because, again, those stablecoins are not legal tender at their destination, and I don’t know if you’ve noticed but businesses tend to have this whole thing about receiving legal tender.

So, yes, it’s great if you can send five thousand FaceCoin to your family in Ghana for an 0.1% fee. But then your family in Ghana has to somehow convert them to cedis at an exchange — a task which is, as of this writing, likely to be slower, much clumsier, far more user-hostile, and very possibly even more expensive than the usual medium(s) of remittances.

If Facebook can bulldoze that obstacle, though — then we’re talking about a big deal.

I see two possibilities. One is to establish partnerships with other companies such that they will accept FaceCoin themselves, so it becomes valuable outside of Facebook’s walled garden. But I can’t see this working. Again, it’s still not legal tender; it’s infeasible to partner with everybody; and it just adds more complexity for the user — “wait, do I want to pay for this with FaceCoin or cedis? Wait, do they even accept FaceCoin? Hmm, how does my government feel about FaceCoin and taxes, I wonder?” — , and the global WhatsApp audience rightly doesn’t want to deal with this. They just want money they can use.

But the other alternative is for Facebook to establish relationships with cryptocurrency exchanges worldwide, or — even more dramatically — become or sponsor exchanges themselves. Remember, much of the world already uses mobile money extensively. Imagine if FaceCoin could be seamlessly converted into eg M-Pesa or Orange Money immediately upon receipt. Then you could buy a thousand FaceCoin for US dollars in Houston; send it to your brother in Ghana, at the speed of the Internet (or maybe in a few minutes, depending on how FaceCoin’s blockchain works); and when he wants to spend it, he just pushes a button on his phone to convert it at the day’s rate into cedis in his MTN Mobile Money account, courtesy of Facebook’s Ghanaian exchange partner, in exchange for a tiny percentage of that rate.

That would be a huge, huge deal. First, it would offer seamless, immediate, user-friendly international remittances, which itself would be massive (the remittance market is roughly half a trillion dollars a year.) Second, it would allow anyone with a phone and the Facebook app to maintain a personal account in stablecoins backed by a basket of hard currencies. Ask any Venezuelan or Zimbabwean, or for that matter Argentinian, why that would matter.

That would also be insanely messy from a legal / regulatory standpoint. There are privacy issues. There are security issues. There are liquidity issues. There are KYC / AML issues. There are regulatory issues involving not just one, or a few, but conceivably hundreds of regulatory domains. But if anyone has the reach and money and wherewithal to push that armada of boulders up this hill, it’s Facebook — and the carrot of collecting, say, a few dozen basis points from the $500 billion/year remittances market is more than enough to incentivize them to do so.

I could well be wrong. There’s a very good chance that FaceCoin will just be Facebook Credits meets Facebook Gifts, except on the blockchain for no particular reason, in which case it too will presumably fade sheepishly away to be sunsetted two years after it launches. And even if I’m right, I too am deeply uneasy about Facebook, who have repeatedly shown themselves to be the opposite of trustworthy, becoming the global gateway for remittance payments worldwide. (Although, hey, it could arguably be even worse.) Maybe their blockchain will be sufficiently decentralized to be somewhat decouple from their influence, but that seems awfully unlikely (and would be pretty undesirable to regulators).

But if I’m right — then this is actually a really big deal, one which could be meaningfully important on a very personal and day-to-day level for many millions of people worldwide. Facebook would be, to my mind, at very best a deeply flawed messenger of this change … but they’re still (probably) better them than nobody, and, importantly, if they were to blaze this trail, it would then be much easier for others to follow.",Social Media,TechCrunch,https://techcrunch.com/2019/03/03/regarding-facebooks-cryptocurrency/,"The main undesirable consequence of Facebook introducing a cryptocurrency for peer-to-peer payments is the potential for data privacy and security risks, as well as the possibility of misuse by criminals, due to the centralized nature of the platform and the lack of mathematically-backed privacy features. Additionally, the lack of businesses accepting the currency may make it difficult",Security & Privacy
382,4chan Is at it Again With #Boobs4Bieber,"4chan, the website that is infamous for Internet trolls, is at it again. This time, the ""joke"" is an attempt to get underage girls to bare their breasts on Twitter.

First, it was #BaldForBieber, a somewhat less harmful prank that encouraged teens to shave their heads over a false rumor that Justin Bieber had cancer. Then, it was #CutForBieber, an even more harmful prank, not only trying to encourage teens to self-harm, but also making fun of a very serious problem. This time it is #Boobs4Bieber, which amounts to child porn if any underage fans think it is real, and decide to participate.

I'm not sure what I can say about this latest attempt from 4chan. I am repulsed, to put it mildly.

The current upside to this latest prank is that, so far, it is failing to trend.

We live in a day and age where it appears that our children are constantly bombarded with new and unimaginably horrific situations in which they may feel forced to participate in order to gain acceptance. At times, social media is a double-edged sword. On one side, it allows people of all ages to come together and find a community they may not be able to access in ""the real world."" On the other side, we have supposed adults attempting to trick underage boys and girls -- though the focus in this prank are the girls -- into performing acts that are undeniably illegal.

The only thing I can stress at this moment is to please talk to your teens and tweens about this. While it is important to allow your children a certain amount of freedom to grow and figure out the world on their own, it is also important to balance this with informing them that there are people on the Internet whose only goal appears to be to cause harm.

You can read more about this story on the Daily Dot, complete with screenshots of the 4chan discussion about this latest prank. Warning: It contains NSFW language and images. Also, I suggest that you do not read the hashtag. I have purposely not linked to it because it does contain images of breasts, and there is no immediate way of knowing if they are images of adults who are trying to enable this prank, or of underage children.",Social Media,WIRED,https://www.wired.com/2013/01/4chan-boobs4bieber/,"This latest prank from 4chan, #Boobs4Bieber, is an attempt to get underage girls to bare their breasts on Twitter, and is an example of how Social Media can be used to encourage children to participate in illegal activities. Parents must remain vigilant and discuss these issues with their children to protect them from harm.",Social Norms & Relationships
383,Facebook’s AI couldn’t spot mass murder – TechCrunch,"Facebook has given another update on measures it took and what more it’s doing in the wake of the livestreamed video of a gun massacre by a far right terrorist who killed 50 people in two mosques in Christchurch, New Zealand.

Earlier this week the company said the video of the slayings had been viewed less than 200 times during the livestream broadcast itself, and about about 4,000 times before it was removed from Facebook — with the stream not reported to Facebook until 12 minutes after it had ended.

None of the users who watched the killings unfold on the company’s platform in real-time apparently reported the stream to the company, according to the company.

It also previously said it removed 1.5 million versions of the video from its site in the first 24 hours after the livestream, with 1.2M of those caught at the point of upload — meaning it failed to stop 300,000 uploads at that point. Though as we pointed out in our earlier report those stats are cherrypicked — and only represent the videos Facebook identified. We found other versions of the video still circulating on its platform 12 hours later.

In the wake of the livestreamed terror attack, Facebook has continued to face calls from world leaders to do more to make sure such content cannot be distributed by its platform.

The prime minister of New Zealand, Jacinda Ardern told media yesterday that the video “should not be distributed, available, able to be viewed”, dubbing it: “Horrendous.”

She confirmed Facebook had been in contact with her government but emphasized that in her view the company has not done enough.

She also later told the New Zealand parliament: “We cannot simply sit back and accept that these platforms just exist and that what is said on them is not the responsibility of the place where they are published. They are the publisher. Not just the postman.”

We asked Facebook for a response to Ardern’s call for online content platforms to accept publisher-level responsibility for the content they distribute. Its spokesman avoided the question — pointing instead to its latest piece of crisis PR which it titles: “A Further Update on New Zealand Terrorist Attack”.

Here it writes that “people are looking to understand how online platforms such as Facebook were used to circulate horrific videos of the terrorist attack”, saying it therefore “wanted to provide additional information from our review into how our products were used and how we can improve going forward”, before going on to reiterate many of the details it has previously put out.

Including that the massacre video was quickly shared to the 8chan message board by a user posting a link to a copy of the video on a file-sharing site. This was prior to Facebook itself being alerted to the video being broadcast on its platform.

It goes on to imply 8chan was a hub for broader sharing of the video — claiming that: “Forensic identifiers on many of the videos later circulated, such as a bookmarks toolbar visible in a screen recording, match the content posted to 8chan.”

So it’s clearly trying to make sure it’s not singled out by political leaders seek policy responses to the challenge posed by online hate and terrorist content.

Further details it chooses to dwell on in the update is how the AIs it uses to aid the human content review process of flagged Facebook Live streams are in fact tuned to “detect and prioritize videos that are likely to contain suicidal or harmful acts” — with the AI pushing such videos to the top of human moderators’ content heaps, above all the other stuff they also need to look at.

Clearly “harmful acts” were involved in the New Zealand terrorist attack. Yet Facebook’s AI was unable to detected a massacre unfolding in real time. A mass killing involving an automatic weapon slipped right under the robot’s radar.

Facebook explains this by saying it’s because it does not have the training data to create an algorithm that understands it’s looking at mass murder unfolding in real time.

It also implies the task of training an AI to catch such a horrific scenario is exacerbated by the proliferation of videos of first person shooter videogames on online content platforms.

It writes: “[T]his particular video did not trigger our automatic detection systems. To achieve that we will need to provide our systems with large volumes of data of this specific kind of content, something which is difficult as these events are thankfully rare. Another challenge is to automatically discern this content from visually similar, innocuous content – for example if thousands of videos from live-streamed video games are flagged by our systems, our reviewers could miss the important real-world videos where we could alert first responders to get help on the ground.”

The videogame element is a chilling detail to consider.

It suggests that a harmful real-life act that mimics a violent video game might just blend into the background, as far as AI moderation systems are concerned; invisible in a sea of innocuous, virtually violent content churned out by gamers. (Which in turn makes you wonder whether the Internet-steeped killer in Christchurch knew — or suspected — that filming the attack from a videogame-esque first person shooter perspective might offer a workaround to dupe Facebook’s imperfect AI watchdogs.)

Facebook post is doubly emphatic that AI is “not perfect” and is “never going to be perfect”.

“People will continue to be part of the equation, whether it’s the people on our team who review content, or people who use our services and report content to us,” it writes, reiterating yet again that it has ~30,000 people working in “safety and security”, about half of whom are doing the sweating hideous toil of content review.

This is, as we’ve said many times before, a fantastically tiny number of human moderators given the vast scale of content continually uploaded to Facebook’s 2.2BN+ user platform.

Moderating Facebook remains a hopeless task because so few humans are doing it.

Moreover AI can’t really help. (Later in the blog post Facebook also writes vaguely that there are “millions” of livestreams broadcast on its platform every day, saying that’s why adding a short broadcast delay — such as TV stations do — wouldn’t at all help catch inappropriate real-time content.)

At the same time Facebook’s update makes it clear how much its ‘safety and security’ systems rely on unpaid humans too: Aka Facebook users taking the time and mind to report harmful content.

Some might say that’s an excellent argument for a social media tax.

The fact Facebook did not get a single report of the Christchurch massacre livestream while the terrorist attack unfolded meant the content was not prioritized for “accelerated review” by its systems, which it explains prioritize reports attached to videos that are still being streamed — because “if there is real-world harm we have a better chance to alert first responders and try to get help on the ground”.

Though it also says it expanded its acceleration logic last year to “also cover videos that were very recently live, in the past few hours”.

But again it did so with a focus on suicide prevention — meaning the Christchurch video would only have been flagged for acceleration review in the hours after the stream ended if it had been reported as suicide content.

So the ‘problem’ is that Facebook’s systems don’t prioritize mass murder.

“In [the first] report, and a number of subsequent reports, the video was reported for reasons other than suicide and as such it was handled according to different procedures,” it writes, adding it’s “learning from this” and “re-examining our reporting logic and experiences for both live and recently live videos in order to expand the categories that would get to accelerated review”.

No shit.

Facebook also discusses its failure to stop versions of the massacre video from resurfacing on its platform, having been — as it tells it — “so effective” at preventing the spread of propaganda from terrorist organizations like ISIS with the use of image and video matching tech.

It claims its tech was outfoxed in this case by “bad actors” creating many different edited versions of the video to try to thwart filters, as well as by the various ways “a broader set of people distributed the video and unintentionally made it harder to match copies”.

So, essentially, the ‘virality’ of the awful event created too many versions of the video for Facebook’s matching tech to cope.

“Some people may have seen the video on a computer or TV, filmed that with a phone and sent it to a friend. Still others may have watched the video on their computer, recorded their screen and passed that on. Websites and pages, eager to get attention from people seeking out the video, re-cut and re-recorded the video into various formats,” it writes, in what reads like another attempt to spread blame for the amplification role that its 2.2BN+ user platform plays.

In all Facebook says it found and blocked more than 800 visually-distinct variants of the video that were circulating on its platform.

It reveals it resorted to using audio matching technology to try to detect videos that had been visually altered but had the same soundtrack. And again claims it’s trying to learn and come up with better techniques for blocking content that’s being re-shared widely by individuals as well as being rebroadcast by mainstream media. So any kind of major news event, basically.

In a section on next steps Facebook says improving its matching technology to prevent the spread of inappropriate viral videos being spread is its priority.

But audio matching clearly won’t help if malicious re-sharers just both re-edit the visuals and switch the soundtrack too in future.

It also concedes it needs to be able to react faster “to this kind of content on a live streamed video” — though it has no firm fixes to offer there either, saying only that it will explore “whether and how AI can be used for these cases, and how to get to user reports faster”.

Another priority it claims among its “next steps” is fighting “hate speech of all kinds on our platform”, saying this includes more than 200 white supremacist organizations globally “whose content we are removing through proactive detection technology”.

It’s glossing over plenty of criticism on that front too though — including research that suggests banned far right hate preachers are easily able to evade detection on its platform. Plus its own foot-dragging on shutting down far right extremists. (Facebook only finally banned one infamous UK far right activist last month, for example.)

In its last PR sop, Facebook says it’s committed to expanding its industry collaboration to tackle hate speech via the Global Internet Forum to Counter Terrorism (GIFCT), which formed in 2017 as platforms were being squeezed by politicians to scrub ISIS content — in a collective attempt to stave off tighter regulation.

“We are experimenting with sharing URLs systematically rather than just content hashes, are working to address the range of terrorists and violent extremists operating online, and intend to refine and improve our ability to collaborate in a crisis,” Facebook writes now, offering more vague experiments as politicians call for content responsibility.",Social Media,TechCrunch,https://techcrunch.com/2019/03/21/facebooks-ai-couldnt-spot-mass-murder/,"The tragedy of the Christchurch massacre livestream shows how Social Media can be used to spread dangerous and violent content, and how user-reporting alone is not enough to stop it. AI moderation systems are not perfect, and can be easily fooled by the proliferation of violent video game content. In addition, Facebook's slow response to take action against far",Security & Privacy
384,"Social media should have ‘duty of care’ towards kids, UK MPs urge – TechCrunch","Social media platforms are being urged to be far more transparent about how their services operate and to make “anonymised high-level data” available to researchers so the technology’s effects on users — and especially on children and teens — can be better understood.

The calls have been made in a report by the UK parliament’s Science and Technology Committee which has been looking into the impacts of social media and screen use among children — to consider whether such tech is “healthy or harmful”.

“Social media companies must also be far more open and transparent regarding how they operate and particularly how they moderate, review and prioritise content,” it writes.

Concerns have been growing about children’s use of social media and mobile technology for some years now, with plenty of anecdotal evidence and also some studies linking tech use to developmental problems, as well as distressing stories connecting depression and even suicide to social media use.

Although the committee writes that its dive into the topic was hindered by “the limited quantity and quality of academic evidence available”. But it also asserts: “The absence of good academic evidence is not, in itself, evidence that social media and screens have no effect on young people.”

“We found that the majority of published research did not provide a clear indication of causation, but instead indicated a possible correlation between social media/screens and a particular health effect,” it continues. “There was even less focus in published research on exactly who was at risk and if some groups were potentially more vulnerable than others when using screens and social media.”

The UK government expressed its intention to legislate in this area, announcing a plan last May to “make social media safer” — promising new online safety laws to tackle concerns.

The committee writes that it’s therefore surprised the government has not commissioned “any new, substantive research to help inform its proposals”, and suggests it get on and do so “as a matter of urgency” — with a focus on identifying people at risk of experiencing harm online and on social media; the reasons for the risk factors; and the longer-term consequences of the tech’s exposure on children.

It further suggests the government should consider what legislation is required to improve researchers’ access to this type of data, given platforms have failed to provide enough access for researchers of their own accord.

The committee says it heard evidence of a variety of instances where social media could be “a force for good” but also received testimonies about some of the potential negative impacts of social media on the health and emotional wellbeing of children.

“These ranged from detrimental effects on sleep patterns and body image through to cyberbullying, grooming and ‘sexting’,” it notes. “Generally, social media was not the root cause of the risk but helped to facilitate it, while also providing the opportunity for a large degree of amplification. This was particularly apparent in the case of the abuse of children online, via social media.

“It is imperative that the government leads the way in ensuring that an effective partnership is in place, across civil society, technology companies, law enforcement agencies, the government and non-governmental organisations, aimed at ending child sexual exploitation (CSE) and abuse online.”

The committee suggests the government commission specific research to establish the scale and prevalence of online CSE — pushing it to set an “ambitious target” to halve reported online CSE in two years and “all but eliminate it in four”.

A duty of care

A further recommendation will likely send a shiver down tech giants’ spines, with the committee urging a duty of care principle be enshrined in law for social media users under 18 years of age to protect them from harm when on social media sites.

Such a duty would up the legal risk stakes considerably for user generated content platforms which don’t bar children from accessing their services.

The committee suggests the government could achieve that by introducing a statutory code of practice for social media firms, via new primary legislation, to provide “consistency on content reporting practices and moderation mechanisms”.

It also recommends a requirement in law for social media companies to publish detailed Transparency Reports every six months.

It is also for a 24 hour takedown law for illegal content, saying that platforms should have to review reports of potentially illegal content and take a decision on whether to remove, block or flag it — and reply the decision to the individual/organisation who reported it — within 24 hours.

Germany already legislated for such a law, back in 2017 — though in that case the focus is on speeding up hate speech takedowns.

In Germany social media platforms can be fined up to €50 million if they fail to comply with the NetzDG law, as its truncated German name is known. (The EU executive has also been pushing platforms to remove terrorist related material within an hour of a report, suggesting it too could legislate on this front if they fail to moderate content fast enough.)

The committee suggests the UK’s media and telecoms regulator, Ofcom would be well-placed to oversee how illegal content is handled under any new law.

It also recommends that social media companies use AI to identify and flag to users (or remove as appropriate) content that “may be fake” — pointing to the risk posed by new technologies such as “deep fake videos”.

More robust systems for age verification are also needed, in the committee’s view. It writes that these must go beyond “a simple ‘tick box’ or entering a date of birth”.

Looking beyond platforms, the committee presses the government to take steps to improve children’s digital literacy and resilience, suggesting PSHE (personal, social and health) education should be made mandatory for primary and secondary school pupils — delivering “an age-appropriate understanding of, and resilience towards, the harms and benefits of the digital world”.

Teachers and parents should also not be overlooked, with the committee suggesting training and resources for teachers and awareness and engagement campaigns for parents.",Social Media,TechCrunch,https://techcrunch.com/2019/01/31/social-media-should-have-duty-of-care-towards-kids-uk-mps-urge/,"The UK parliament's Science and Technology Committee has outlined numerous ways in which Social Media can be harmful to children, such as a rise in cyberbullying, sexting, negative body image, and sleep deprivation. It has called for more transparency from Social Media platforms, as well as better protection for users via a duty of care principle and",Social Norms & Relationships
385,The exhausting playbook behind Trump’s battle with Twitter,"Four years ago, a Breitbart writer famed for championing a harassment campaign targeting women in video games used his air time during a White House press briefing to blast Twitter. He was angry that he’d lost his verification badge, that little blue check mark, after the company said he had repeatedly violated the platform’s rules against inciting harassment. But he insisted that Twitter was actually punishing him for something else.

“It’s becoming very clear,” Milo Yiannopoulos told Josh Earnest, then the press secretary for the Obama administration, in March of 2016, “that Twitter and Facebook in particular are censoring and punishing conservative and libertarian points of view.” Later that year, Twitter banned him entirely following his role in a harassment campaign against the actress Leslie Jones after she starred in a remake of Ghostbusters that swapped the original male lead roles for female ones, infuriating misogynists. In response, he claimed that Twitter was now a “a no-go zone for conservatives.”

Other conservative and far-right figures have regularly lodged similar complaints in the years since, depicting Twitter’s enforcement of its policies against abuse and misinformation as a crusade laced with anti-conservative bias; the charges have then filtered up into conservative and mainstream press coverage. But the issue came to a head this week, after Twitter appended fact-checks to two of President Trump’s tweets, noting that they contained misleading claims about mail-in voting.

Trump attacked the move as censorship and promised a response. He’s just signed an executive order that could penalize major social-media companies for perceived censorship of conservative views.

This moment feels like an inevitable escalation of a conflict that has been playing out across the major social-media companies, but particularly Twitter, for years—one that Yiannopoulos’s White House stunt foreshadowed. As platforms reckon with their role in amplifying misinformation, abuse, and extreme views, the arguments about content moderation that once lived on the fringes of Twitter’s rules increasingly involve people at the very center of mainstream power.

“Republicans feel that Social Media Platforms totally silence conservatives voices,” Trump tweeted to his 80 million followers this week. “We will strongly regulate, or close them down, before we can ever allow this to happen.” His comments were covered widely in the media, as are many of his more inflammatory or conspiratorial tweets.

Hours before news of the coming executive order broke, Trump advisor Kellyanne Conway went on Fox News and encouraged viewers to hound a Twitter employee, spelling out his account handle and blaming him for the decision to fact-check the president’s tweets. “Somebody in San Francisco go wake him up and tell him he’s about to get a lot more followers,” she said.

Trump himself tagged the employee in a tweet on Thursday, effectively directing supporters to fill his mentions with abusive messages. The Twitter employee is also reportedly receiving death threats.

This cycle has been set off in the past when Twitter has rolled out new policies designed to protect targets of abuse, suspended far-right accounts for rule violations, or stepped up efforts to slow the spread of misinformation. It begins with waves of speculation arguing that Twitter isn’t actually, say, enforcing its new abuse policies but instead implementing a secret anti-conservative agenda that must be stopped. Then there’s a rush to find and target someone responsible for implementing it. The blueprint dates back at least to Gamergate, the harassment campaign championed by Yiannopoulos targeting women in video-game development, whose supporters also claimed instead to be fighting a conspiracy against them ( “It’s actually about ethics in gaming journalism”).

The president uses his own account to continually test Twitter’s boundaries, and now he’s become the catalyst for a new cycle. In just the past week, he’s used his platform to amplify conspiracy theories suggesting that MSNBC host Joe Scarborough murdered a staffer and to spread misinformation about mail-in voting in an earlier series of tweets that were not subject to fact-check labels. He thanked a “Cowboys for Trump” account that tweeted a video where an unidentified man proclaimed that “the only good Democrat is a dead Democrat.” (After cheers from the audience, the speaker then clarifies that he meant the comment “politically.”) The widower of the deceased staffer at the heart of the Scarborough conspiracy theory has begged Twitter to intervene.

The company had not taken any action against those tweets as of Thursday, although it has indicated that it is working to expand the labeling system that was used to flag some of Trump’s tweets about mail-in voting.

Until the fact-checking labels were introduced to two of Trump’s tweets on Tuesday, the platform had scrupulously avoided enforcing its rules against Trump’s account. Some explanations for the enforcement loopholes have cited the newsworthiness of otherwise rule-breaking content and Trump’s status as the head of a government.

But Trump, despite the lack of evidence to support claims of systemic social-media bias against conservatives, has repeatedly promised to take up the issue on behalf of some of his more prominent supporters. In 2018, he accused Google of “rigging” news search results against conservative media, repeating a version of a claim that Trump supporters—including vloggers Diamond and Silk—had circulated in conservative media for a few days earlier. Diamond and Silk (whose real names are Lynnette Hardaway and Rochelle Richardson) claimed at a House Judiciary Committee hearing that April that they were being “censored” by Facebook because of their support for Trump.

In 2019, Trump met with Twitter CEO Jack Dorsey and reportedly took the opportunity to complain about losing Twitter followers. On the same day as that meeting, Trump tweeted that the platform was “very discriminatory.” He later tweeted that his administration was “closely” monitoring conservatives’ complaints of censorship. Later that year, Trump held a “social-media summit” with dozens of his most passionate online supporters to air their collective complaints that Google, Facebook, and Twitter were censoring them.

None of these claims have to be true to be popular, which is something Trump and his online supporters know well. They just need to sound controversial enough to grab attention—or, better yet, redirect it from something else.",Social Media,MIT,https://www.technologyreview.com/2020/05/28/1002376/trump-twitter-conservative-bias/,"The president and his supporters have used social media to amplify conspiracy theories and spread misinformation, and have used their influence to target those responsible for implementing policy changes. This has led to a cycle of accusations of anti-conservative bias and harassment, culminating in an executive order threatening to penalize major social media companies.",Discourse & Governance
386,UK data watchdog still waiting for warrant to raid Cambridge Analytica – TechCrunch,"The UK’s data watchdog, the Information Commission’s Office (ICO), has still not obtained a warrant to enter and search the servers of the London-based political consultancy, Cambridge Analytica — the company at the center of the data misuse scandal engulfing Facebook — three days on from beginning the process.

The earliest a warrant could now be obtained by the regulator is Friday.

In a statement today the ICO said: “A High Court judge has adjourned the ICO’s application for a warrant relating to Cambridge Analytica until Friday. The ICO will be in court to continue to pursue the warrant to obtain access to data and information to take forward our investigation.”

The information commissioner, Elizabeth Denham, made it public on Tuesday that she was seeking a warrant to search CA’s servers after the company missed a Monday deadline to hand over information her office had requested. (Though in a statement on Tuesday CA claimed to “have been fully compliant and proactive in our conversations with the ICO”.)

She also instructed Facebook to withdraw its own investigators from CA’s offices, warning that their presence could compromise her investigation.

Unlike competition authorities, the ICO does not have legal powers to raid offices without a warrant. And former UK attorney general, Dominic Grieve, has argued the ICO’s legal powers are inadequate — telling the BBC on Tuesday that the Facebook-CA scandal highlighted a need for “greater powers and greater sanctions”.

Greater sanctions are at least incoming — under the EU’s GDPR regime which will apply from May 25, raising the maximum fine for the most serious data protection violations to up to 4% of a company’s global turnover (or €20M, whichever is greater).

But the fact that the data watchdog is forced to sit on its hands waiting to gain access to servers that the companies of interest to its investigation are in control of or able to access raises serious questions about the asymmetry between big data and regulation.

Earlier this month Denham told MPs on the DCMS committee that’s investigating fake news that her office would be pushing for increased transparency around data flows and disclosure rules for digital political advertising — suggesting a code of conduct is needed to regulate the use of social media in political campaigns, referendums and elections.

And while Facebook has claimed it was unaware that ~50M Facebook users’ data was passed to Cambridge Analytica for political targeting purposes, Facebook has itself long been actively encouraging politicians and political campaigns to make use of its tools — at a time when there was a complete lack of regulation for political ads on digital platforms.

Almost a year ago, in May 2017, the ICO announced a formal investigation into the use of data analytics for political purposes — including looking into complaints related to Cambridge Analytica’s use of data for ad targeting.

That investigation remains ongoing. And may well now be further delayed, given the developing nature of the story (and the ICO’s push for a warrant so it can conduct a full audit of CA’s servers).

Although, earlier this month before the latest Facebook-CA revelations hit the headlines, Denham told the committee she hoped to be able to publish the report by the end of May.

Asked by the DCMS committee whether the ICO has adequate powers to carry out its responsibilities Denham flagged a problematic gap in her “information notice powers” — noting that while the ICO can make a formal demand for information, organisations are not compelled to disclose the requested data (though they can be prosecuted for not doing so).

“Without the power to compel it is difficult to secure the desired outcome,” she told the committee. But she added she’s raised the issue with ministers and is hopeful the UK government will remedy this gap.",Social Media,TechCrunch,https://techcrunch.com/2018/03/22/uk-data-watchdog-still-waiting-for-warrant-to-raid-cambridge-analytica/,"The UK's data watchdog, the Information Commission's Office, is experiencing the consequences of inadequate legal powers to regulate the use of Social Media, as evidenced by their inability to access the servers of Cambridge Analytica without a warrant, despite having begun the process three days prior.",Security & Privacy
387,Brexit Is Sending Markets Diving. Twitter Could Be Making It Worse,"The fate of the global economy is in doubt today following the United Kingdom's decision to exit the European Union. Last night the British pound fell to a 30-year low. Prime Minister David Cameron is resigning. Mark Carney, governor of the Bank of England, called the referendum ""the most significant, near-term domestic risk to financial stability."" This morning, within the first five minutes of trading, the Dow fell more than 500 points.

But long before all the votes were tallied—and years before officials finish negotiating the terms of the UK's departure—Twitter had reached a consenus on the Brexit: This is a disaster.

And the consequences may well be that bad. Uncertainty has never been a friend to global markets, and uncertainty is just what Brexit creates in heaps, particularly for the British economy. But like never before, social media has the ability to amplify the angst that uncertainty creates. And the markets may well be responding to that enhanced anxiety.

There is a very real phenomenon of so-called 'mood contagion' that happens online.

To be clear, the social media masses aren't the only ones predicting economic doom as a result of the Brexit. In an op-ed unsubtly headlined ""The Brexit crash will make all of you poorer—be warned,"" billionaire George Soros argued that a plunging British pound would make the country far more vulnerable to an economic crash than it was at the time of the global recession in 2008.

The markets are certainly responding to such admonitions. But mounting research shows that a feedback loop does exist between social media and the stock markets, in which online anxieties about the market's response may feed into the response itself.

""As soon as the market starts to drop, that affects the mood, it affects public sentiment, which then gets amplified online,"" says Johan Bollen, a scientist who studies social media and markets at the Indiana University School of Informatics and Computing. ""It's not completely unthinkable there might be a self-fulfilling prophecy effect.""",Social Media,WIRED,https://www.wired.com/2016/06/brexit-sending-markets-diving-twitter-making-worse/,"The Brexit decision has sparked a wave of negative sentiment on Social Media which could potentially feed into a self-fulfilling prophecy of economic doom, resulting in a market crash.",Economy
388,Study Finds Social Media Leads To Sleep Disturbance – TechCrunch,"Just when you thought it was safe to click on that funny Facebook video of an aardvark carrying a shovel to dig a hole under a cinderblock wall while a pair of Peruvian flute-players cavort in green onesies, researchers at the University Of Pittsburgh have found that social media wreaks havoc on your sleep patterns.

The bottom line? “Young adults who spend a lot of time on social media during the day or check it frequently throughout the week are more likely to suffer sleep disturbances than their peers who use social media less,” wrote researchers at the University of Pittsburgh School of Medicine.

“This is one of the first pieces of evidence that social media use really can impact your sleep,” said researcher Jessica C. Levenson. “And it uniquely examines the association between social media use and sleep among young adults who are, arguably, the first generation to grow up with social media.”

Participants in the study spent 61 hours on social media per week and 30 percent had “high levels of sleep disturbance” meaning sleep reduced due to social media including swiping through Instagram and taking part in arguments on Facebook or Twitter. In some ways, this behavior may be a form of self medication. The team sampled 1,788 U.S. adults between ages 19 and 32.

“Difficulty sleeping may lead to increased use of social media, which may in turn lead to more problems sleeping. This cycle may be particularly problematic with social media because many forms involve interactive screen time that is stimulating and rewarding and, therefore, potentially detrimental to sleep,” said Leveson.

Obviously a single study associating sleep patterns with social media use isn’t quite sufficient to, say, force us off YikYak. However, it is definitely an interesting start to research that will inevitably find that our lives are all being ruined by reading posts by my dumb cousin on Twitter.",Social Media,TechCrunch,https://techcrunch.com/2016/01/26/study-finds-social-media-leads-to-sleep-disturbance/,The University of Pittsburgh's study found that heavy social media users are more likely to suffer from sleep disturbances than their peers who use social media less. This research suggests that our sleep patterns are being disrupted by our online activities and that this could be having a detrimental effect on our wellbeing.,User Experience & Entertainment
389,"Watch lawmakers grill Snap, TikTok and YouTube on kids’ online safety – TechCrunch","After dragging in the same companies and their reticent, overtrained executives time and time again, Congress is turning its attention to two of the tech industry’s fresh but important faces: TikTok and Snap.

On Tuesday, lawmakers on the Senate Subcommittee on Consumer Protection, Product Safety, and Data Security will question policy leads from those two companies and YouTube on how their platforms affect vulnerable young users. Facebook whistleblower Frances Haugen testified before the same committee on parallel issues in early October, shortly after revealing her identity.

The hearing will air on Tuesday at 7AM PT, featuring testimony from Snap VP of Global Public Policy Jennifer Stout, TikTok’s VP and Head of Public Policy Michael Beckerman and Leslie Miller, who leads government affairs and public policy at YouTube. A livestream is embedded below.

Subcommittee chair Senator Richard Blumenthal (D-CT) will lead the hearing, which will focus on social media’s detrimental effects on children and teens. “The bombshell reports about Facebook and Instagram—their toxic impacts on young users and lack of truth or transparency—raise serious concerns about Big Tech’s approach toward kids across the board,” Blumenthal said, connecting reports about Instagram’s dangers for teens to social media more broadly. The subcommittee’s ranking Republican Marsha Blackburn (R-TN) has signaled that she’s particularly interested in privacy concerns around TikTok.

We’d expect topics like eating disorders, harassment, bullying, online safety and data privacy to come up as members of the subcommittee take turns pressing the three policy leads for answers. The group of lawmakers also plans to discuss legislation that could help protect kids and teens online, though how solutions-oriented the hearing will be remains to be seen. Some of those potential solutions include the KIDS Act (Kids Internet Design and Safety), which would create new online protections for people under the age of 16. Blumenthal and fellow Democratic Senator Ed Markey reintroduced the bill last month.

The mental health of kids and teens isn’t the only pressing societal crisis that social platforms are implicated in at the moment, but it’s one Republicans and Democrats are both rallying around. For one, it’s a rare arena of criticism with plenty of political overlap for both sides. Both parties do seem to agree that tech’s biggest companies need to be controlled in some way, though they generally play up different parts of the why: for conservatives it’s that these companies have too much decision making power when it comes to what content gets wiped from their platforms. On the opposite side of the aisle, Democrats are generally much more worried about the kind of content that gets left up, like extremism and misinformation.

Tuesday’s hearing will also likely dive into how algorithms amplify harmful content. Because social media companies play their cards close to the chest when it comes to how their algorithms work, hearings are a rare opportunity for the public to learn more about how these companies serve their users personalized content. Ideally we’d be learning a lot about that kind of thing in the often lengthy, repetitive tech hearings Congress has held in the last couple of years, but between lawmakers pushing uninformed or irrelevant lines of questioning and evasive tech executives with hours of media training under their belts, the best we can usually hope for is a few new tidbits of information.

While Facebook won’t appear at this particular hearing, expect recent revelations around that company and Instagram to inform what happens on Tuesday. All three social media companies set to testify have had an eye on the public response to leaked Facebook documents and more reporting on that data just landed on Monday.

Just after the initial reports that Instagram is aware of the risks it poses to teen users, TikTok introduced a new set of safety measures including a well-being guide, better search interventions and opt-in popups for sensitive search terms. Last week, Snap announced a new set of family-focused safety tools to give parents more visibility into what their kids are up to using the platform. Both social networks skew heavily toward younger users compared to platforms like Facebook, Instagram and Twitter, making robust safety tools even more of a necessity. Leading into the hearing, YouTube announced some changes of its own around what kind of kids content will be eligible for monetization while also highlighting its other kid-centric safety measures.",Social Media,TechCrunch,https://techcrunch.com/2021/10/25/snap-tiktok-youtube-senate-hearing-congress/,"The Senate Subcommittee on Consumer Protection, Product Safety, and Data Security will be discussing the detrimental effects of social media on children and teens, including issues such as eating disorders, harassment, bullying, and online safety. Recent revelations have added to the urgency of the conversation, and companies like TikTok, Snap, and YouTube are introducing new safety measures",Security & Privacy
390,It’s too late to stop QAnon with fact checks and account bans,"The traditional understanding of QAnon was that its ideas are spread by a relatively small number of adherents who are extremely good at manipulating social media for maximum visibility. But the pandemic made that more complicated, as QAnon began merging more profoundly with health misinformation and rapidly increasing its presence on Facebook.

At this point, QAnon has become an omniconspiracy theory, says DiResta—it’s no longer just about some message board posts, but instead a broad movement promoting many different, linked ideas. Researchers know that belief in one conspiracy theory can lead to acceptance of others, and powerful social-media recommendation algorithms have essentially turbocharged that process. For instance, DiResta says, research has shown that members of anti-vaccine Facebook groups were seeing recommendations for groups that promoted the Pizzagate conspiracy theory back in 2016.

“The recommendation algorithm appears to have recognized a correlation between users who shared a conviction that the government was concealing a secret truth. The specifics of the secret truth varied,” she says.

Researchers have known for years that different platforms play different roles in coordinated campaigns. People will coordinate in a chat app, message board, or private Facebook group, target their messages (including harassment and abuse) on Twitter, and host videos about the entire thing on YouTube.

In this information ecosystem, Twitter functions more like a marketing campaign for QAnon: content is created to be seen and interacted with by outsiders. Meanwhile, Facebook is a powerhouse for coordination, especially in closed groups.

""Q"" has made many incorrect predictions and continues to advance the belief that there is a ""deep state"" plot against Donald Trump.

Reddit used to be a mainstream hub of QAnon activity, until the site started clamping down on it in 2018 for inciting violence and repeatedly violating its terms of service. But instead of losing its power, QAnon simply shifted to other mainstream social-media platforms where it was less likely to be banned.

This all means that when a platform acts on its own to block or reduce the impact of QAnon, it only attacks one part of the problem.

Friedberg said that to him, it feels as if social-media platforms were “waiting for an act of mass violence” in order to coordinate a more aggressive deplatforming effort. But the potential harm of QAnon is already obvious if you stop viewing it as a pro-Trump curiosity and instead see it for what it is: “a distribution mechanism for disinformation of every variety,” Friedberg said, and one that adherents are willing to openly promote and identify with, no matter the consequences.

“Three years of almost unfettered access”

Steven Hassan, a mental health counselor and an expert on cults who escaped from Sun Myung Moon’s Unification Church, known as the “Moonies,” says that discussing groups like QAnon as solely a misinformation or algorithmic problem is not enough.

“I look at QAnon as a cult,” Hassan says. “When you get recruited into a mind control cult, and get indoctrinated into a new belief system ... a lot of it is motivated by fear.”

“They’ve had three years of almost unfettered access to develop and expand.”

“People can be deprogrammed from this,” Hassan says. “But the people who are going to be most successful doing this are family members and friends.” People who are already close to a QAnon supporter could be trained to have “multiple interactions over time” in hopes of pulling the person out.

If platforms wanted to seriously address ideologies like QAnon, they’d do much more than they are, he says.

First, Facebook would have to educate users not just on how to spot misinformation, but also on how to understand when they are being manipulated by coordinated campaigns. Coordinated pushes on social media are a major factor in QAnon’s growing reach on mainstream platforms, as recently documented by the Guardian. The group has explicitly embraced “information warfare” as a tactic for gaining influence. In May, Facebook removed a small collection of QAnon-affiliated accounts for inauthentic behavior.

And second, Hassan recommends that platforms stop people from descending into algorithmic or recommendation tunnels related to QAnon, and instead feed them with content from people like him, who have survived and escaped from cults—especially from those who got sucked into and climbed out of QAnon.

Friedberg, who has studied the movement deeply, says he believes it is “absolutely” too late for mainstream social-media platforms to stop QAnon, although there are some things they could do to, say, limit its adherents' ability to evangelize on Twitter.

“They’ve had three years of almost unfettered access outside of certain platforms to develop and expand,” Friedberg says. Plus, QAnon supporters have an active relationship with the source of the conspiracy theory, who constantly posts new content to decipher and mentions the social-media messages of Q supporters in his posts. Breaking QAnon’s influence would require breaking trust between “Q,” an anonymous figure with no defining characteristics, and their supporters. Given that Q’s long track record of inaccurate predictions has not broken that trust, that’s difficult, and critical media coverage or deplatforming have yet to really do much on that front. If anything, they only fuel QAnon believers to assume they’re on to something.

The best ideas to limit QAnon would require drastic change and soul-searching from the people who run the companies on whose platforms it has thrived. But even this week’s announcements aren’t quite as dramatic as they might seem at first: Twitter clarified that it wouldn’t automatically apply its new policies against politicians who promote QAnon content, including several promoters who are running for office in the US.

And, Friedberg said, QAnon supporters were “poised to test these limitations, and already testing these limitations.” For instance, Twitter banned certain conspiracy-affiliated URLs from being shared, but people already have alternative ones to use.

In the end, actually doing something about that would require “rethinking the entire information ecosystem,” says DiResta. “And I mean that in a far broader sense than just reacting to one conspiracy faction.”",Social Media,MIT,https://www.technologyreview.com/2020/07/26/1005609/qanon-facebook-twitter-youtuube/,"Social media has allowed QAnon to become an omniconspiracy theory with adherents who spread misinformation and engage in information warfare. Despite attempts to deplatform it, QAnon has continued to thrive, as recommendation algorithms and mainstream platforms have enabled it to grow unchecked. This highlights the need for a rethinking of the entire information ecosystem to","Information, Discourse & Governance"
391,Limiting social media use reduced loneliness and depression in new experiment – TechCrunch,"The idea that social media can be harmful to our mental and emotional well-being is not a new one, but little has been done by researchers to directly measure the effect; surveys and correlative studies are at best suggestive. A new experimental study out of Penn State, however, directly links more social media use to worse emotional states, and less use to better.

To be clear on the terminology here, a simple survey might ask people to self-report that using Instagram makes them feel bad. A correlative study would, for example, find that people who report more social media use are more likely to also experience depression. An experimental study compares the results from an experimental group with their behavior systematically modified, and a control group that’s allowed to do whatever they want.

This study, led by Melissa Hunt at Penn State’s psychology department, is the latter — which despite intense interest in this field and phenomenon is quite rare. The researchers only identified two other experimental studies, both of which only addressed Facebook use.

One hundred and forty-three students from the school were monitored for three weeks after being assigned to either limit their social media use to about 10 minutes per app (Facebook, Snapchat and Instagram) per day or continue using it as they normally would. They were monitored for a baseline before the experimental period and assessed weekly on a variety of standard tests for depression, social support and so on. Social media usage was monitored via the iOS battery use screen, which shows app use.

The results are clear. As the paper, published in the latest Journal of Social and Clinical Psychology, puts it:

The limited use group showed significant reductions in loneliness and depression over three weeks compared to the control group. Both groups showed significant decreases in anxiety and fear of missing out over baseline, suggesting a benefit of increased self-monitoring. Our findings strongly suggest that limiting social media use to approximately 30 minutes per day may lead to significant improvement in well-being.

It’s not the final word in this, however. Some scores did not see improvement, such as self-esteem and social support. And later follow-ups to see if feelings reverted or habit changes were less than temporary were limited because most of the subjects couldn’t be compelled to return. (Psychology, often summarized as “the study of undergraduates,” relies on student volunteers who have no reason to take part except for course credit, and once that’s given, they’re out.)

That said, it’s a straightforward causal link between limiting social media use and improving some aspects of emotional and social health. The exact nature of the link, however, is something at which Hunt could only speculate:

Some of the existing literature on social media suggests there’s an enormous amount of social comparison that happens. When you look at other people’s lives, particularly on Instagram, it’s easy to conclude that everyone else’s life is cooler or better than yours. When you’re not busy getting sucked into clickbait social media, you’re actually spending more time on things that are more likely to make you feel better about your life.

The researchers acknowledge the limited nature of their study and suggest numerous directions for colleagues in the field to take it from here. A more diverse population, for instance, or including more social media platforms. Longer experimental times and comprehensive follow-ups well after the experiment would help, as well.

The 30-minute limit was chosen as a conveniently measurable one, but the team does not intend to say that it is by any means the “correct” amount. Perhaps half or twice as much time would yield similar or even better results, they suggest: “It may be that there is an optimal level of use (similar to a dose response curve) that could be determined.”

Until then, we can use common sense, Hunt suggested: “In general, I would say, put your phone down and be with the people in your life.”",Social Media,TechCrunch,https://techcrunch.com/2018/11/09/limiting-social-media-use-reduced-loneliness-and-depression-in-new-experiment/,"This experimental study out of Penn State has revealed that limiting social media use to around 30 minutes per day can lead to significant improvements in emotional and social health, such as reducing loneliness and depression. Common sense suggests that it would be beneficial to put down the phone and spend more time with people in real life.",Social Norms & Relationships
392,"In the wake of recent racist attacks, Instagram rolls out more anti-abuse features – TechCrunch","Instagram today is rolling out a set of new features aimed at helping people protect their accounts from abuse, including offensive and unwanted comments and messages. The company will introduce tools for filtering abusive direct message (DM) requests as well as a way for users to limit other people from posting comments or sending DMs during spikes of increased attention — like when going viral. In addition, those who attempt to harass others on the service will also see stronger warnings against doing so, which detail the potential consequences.

The company recently confirmed it was testing the new anti-harassment tool, Limits, which Instagram head Adam Mosseri referenced in a video update shared with the Instagram community last month. The feature aims to give Instagram users an easy way to temporarily lock down their accounts when they’re targeted with a flood of harassment.

Such an addition could have been useful to combat the recent racist attacks that took place on Instagram following the Euro 2020 final, which saw several England footballers viciously harassed by angry fans after the team’s defeat. The incidents, which had included racist comments and emoji, raised awareness of how little Instagram users could do to protect themselves when they’ve gone viral in a negative way.

During these sudden spikes of attention, Instagram users see an influx of unwanted comments and DM requests from people they don’t know. The Limits feature allows users to choose who can interact with you during these busy times.

From Instagram’s privacy settings, you’ll be able to toggle on limits that restrict accounts that are not following you as well as those belonging to recent followers. When limits are enabled, these accounts can’t post comments or send DM requests for a period of time of your choosing, like a certain number of days or even weeks.

Twitter had been eyeing a similar set of tools for users who go viral, but has yet to put them into action.

Instagram’s Limits feature had already been in testing, but is now becoming globally available.

The company says it’s currently experimenting with using machine learning to detect a spike in comments and DMs in order to prompt people to turn on Limits with a notification in the Instagram app.

Another feature, Hidden Words, is also being expanded.

Designed to protect users from abusive DM requests, Hidden Words automatically filters requests that contain offensive words, phrases and emojis and places them into a Hidden Folder, which you can choose to never view. It also filters out requests that are likely spam or are otherwise low-quality. Instagram doesn’t provide a list of which words it blocks to prevent people from gaming the system, but it has now updated that database with new types of offensive language, including strings of emoji — like those that were used to abuse the footballers — and included them in the filter.

Hidden Words had already been rolled out to a handful of countries earlier this year, but will reach all Instagram users globally by the end of the month. Instagram will push accounts with a larger following to use it, with messages both in their DM inbox and in their Stories tray.

The feature was also expanded with a new option to “Hide More Comments,” which would allow users to easily hide comments that are potentially harmful, but don’t go against Instagram’s rules.

Another change will involve the warnings that are displayed when someone posts a potentially abusive comment. Already, Instagram would warn users when they first try to post a comment, and it would later display an even stronger warning when they tried to post potentially offensive comments multiple times. Now, the company says users will see the stronger message the first time around.

The message clearly states the comment may “contain racist language” or other content that goes against its guidelines, and reminds users that the comment may be hidden when it’s posted as a result. It also warns the user if they continue to break the community guidelines, their account “may be deleted.”

While systems to counteract online abuse are necessary and underdeveloped, there’s also the potential for such tools to be misused to silence dissent. For example, if a creator was spreading misinformation or conspiracies, or had people calling them out in the comments, they could turn to anti-abuse tools to hide the negative interactions. This would allow the creator to paint an inaccurate picture of their account as one that was popular and well-liked. And that, in turn, can be leveraged into marketing power and brand deals.

As Instagram puts more power into creators’ hands to handle online abuse, it has to weigh the potential impacts those tools have on the overall creator economy, too.

“We hope these new features will better protect people from seeing abusive content, whether it’s racist, sexist, homophobic or any other type of abuse,” noted Mosseri, in an announcement about the changes. “We know there’s more to do, including improving our systems to find and remove abusive content more quickly, and holding those who post it accountable.”",Social Media,TechCrunch,https://techcrunch.com/2021/08/11/in-the-wake-of-recent-racist-attacks-instagram-rolls-out-more-anti-abuse-features/,"Social media tools designed to combat online abuse can be misused to silence dissent, allowing creators to paint an inaccurate picture of their account and potentially benefit from marketing power and brand deals.","Information, Discourse & Governance"
393,Get popcorn for iOS 13’s privacy pop-ups of creepy Facebook data grabs – TechCrunch,"Privacy-minded changes to smartphone operating systems which foreground the background activity of third party apps are helping to spotlight more of the surveillance infrastructure deployed by adtech giants to track and profile human eyeballs for profit.

To wit: iOS 13, which will be generally released later this week, has already been spotted catching Facebook’s app trying to use Bluetooth to track nearby users.

Why might Facebook want to do this? Matching Bluetooth (and wif-fi) IDs that share physical location could allow it to supplement the social graph it gleans by data-mining user-to-user activity on its platform.

Such location tracking provides a physical confirm that individuals were (at very least) in close proximity.

Combined with personal data Facebook also holds on people, and contextual data on the nature of the location itself — a bar, say, or a house — there’s a clear path for the company to make inferences about the nature of the relationship between the people who it’s repurposed short range wireless tech to determine are in close contact.

For a company that makes money by serving targeted ads at humans there are clear commercial reasons for Facebook to seek to intimately understand people’s friend networks.

Facebook piggybacking on people’s use of Bluetooth for benign purposes like pairing devices so that its ad business can ‘pair’ people is the sneaky modus operandi that iOS 13 has caught in the act here.

Ads are Facebook’s business, as CEO Mark Zuckerberg famously told the senate last year. But it’s worth noting the social network giant recently sought to push into the dating space — giving it a fresh, product-based incentive to pry into where and with whom humans are spending their time.

Algorithmic matchmaking based on cold signals like shared interests (in basic Facebook currency this might mean stuff like liking the same pages and events) is of course nothing new.

Yet mix in hot-blooded signals gathered by watching who actually mingles with whom, where and when — by repurposing Bluetooth to harvest interpersonal interactions via tracking people’s physical movements — and Facebook can take its curtain-twitching surveillance of human behavior to the next level.

The path of least resistance to tracking people’s movements is if Facebook app users are opting in to location tracking on their devices. Which means users enabling Location Services — a location tracking feature on smartphones that covers GPS, Bluetooth and crowd-sources wi-fi hotspots and mobile cell towers.

Unsurprisingly, then Facebook Dating requires Location Services to be enabled to function. The company confirmed to us that the Facebook app prompts dating users to enable Location Services if they haven’t already. Facebook also told us it doesn’t use wi-fi or Bluetooth to determine a person’s precise location if a user has Location Services turned off.

It also made a point of emphasizing that users can switch Location Services off at any time. Just not if they wish to use, er, Facebook Dating…

As per usual the company is tangling separate purposes for data processing in a way that denies people a meaningful choice over protecting their privacy. Hence Facebook dating users get to ‘choose’ between being able to use the service; or being able to blanket-deny Facebook the ability to track their physical movements. Like it or lump it.

iOS 13’s new privacy pop-ups to call out background app activity are a clear response to such disingenuous methods by an industry Apple CEO Tim Cook has dubbed the data industrial complex — putting a degree of control back in the hands of the user, who gets a third choice of manually disallowing Bluetooth proximity tracking (in the above example).

Android 10 has also recently expanded the location tracking controls it offers users — with the ability to only share location data with apps while you use them. Though Google’s OS lags far behind what Apple is now offering with these granular pop-ups.

Facebook has responded to awkward (for it) privacy changes incoming at the smartphone OS level by putting out an update on location services last week — where it seeks to get ahead of the deluge of data-grab warnings that iOS users of the Facebook app are likely to experience as they update to iOS 13.

Here it tries to spin Apple’s pro-active foregrounding of apps’ background tracking tactics via push notifications as “reminders” — in just one amusing rebrand.

But in a truly shameless contradiction Facebook also goes on to claim that: “You’re in control of who sees your location on Facebook” (because it says users can make use of the Location Services setting on a phone or tablet to deny tracking) — before admitting that switching off Location Services doesn’t actually mean Facebook will not track your location.

Just because you’re signalling very clearly to Facebook that you don’t want your location to be collected by Facebook doesn’t mean Facebook is going to respect that. Hell no!

“We may still understand your location using things like check-ins, events and information about your internet connection,” it writes. (For a clearer understanding of Facebook’s use of the word “understand” in that sentence we suggest you try substituting the word “steal”.)

In a final shameless kicker — in which Facebook almost appears to be trying to claim credit for smartphone OSes building more privacy features in response to its data grabs — the company seeks to finish on a forward-gazing note, per its preferred crisis PR custom, writing: “We’ll continue to make it easier for you to control how and when you share your location.”

Facebook dishing out misleading qualifications (e.g. “easier”) that whitewash the extent of its rampant data grabs is nothing new. But how much longer it can hope to rely on such flimsy figleaves to cover its privacy sins as the winds of change come rattling through remains to be seen…",Social Media,TechCrunch,https://techcrunch.com/2019/09/16/get-popcorn-for-ios-13s-privacy-pop-ups-of-creepy-facebook-data-grabs/,"Social media companies like Facebook are using Bluetooth to track people's physical movements without their knowledge or consent, in order to gain valuable insight into their relationships and serve them targeted ads. This invasion of privacy is made worse by the fact that users often don't have a choice but to allow it in order to use certain services.",Security & Privacy
394,"Facebook is failing to prevent another human rights tragedy playing out on its platform, report warns – TechCrunch","A report by campaign group Avaaz examining how Facebook’s platform is being used to spread hate speech in the Assam region of North East India suggests the company is once again failing to prevent its platform from being turned into a weapon to fuel ethnic violence.

Assam has a long-standing Muslim minority population but ethnic minorities in the state look increasingly vulnerable after India’s Hindu nationalist government pushed forward with a National Register of Citizens (NRC), which has resulted in the exclusion from that list of nearly 1.9 million people — mostly Muslims — putting them at risk of statelessness.

In July the United Nations expressed grave concern over the NRC process, saying there’s a risk of arbitrary expulsion and detention, with those those excluded being referred to Foreigners’ Tribunals where they have to prove they are not “irregular”.

At the same time, the UN warned of the rise of hate speech in Assam being spread via social media — saying this is contributing to increasing instability and uncertainty for millions in the region. “This process may exacerbate the xenophobic climate while fuelling religious intolerance and discrimination in the country,” it wrote.

There’s an awful sense of deja-vu about these warnings. In March 2018 the UN criticized Facebook for failing to prevent its platform being used to fuel ethnic violence against the Rohingya people in the neighboring country of Myanmar — saying the service had played a “determining role” in that crisis.

Facebook’s response to devastating criticism from the UN looks like wafer-thin crisis PR to paper over the ethical cracks in its ad business, given the same sorts of alarm bells are being sounded again, just over a year later. (If we measure the company by the lofty goals it attached to a director of human rights policy job last year — when Facebook wrote that the responsibilities included “conflict prevention” and “peace-building” — it’s surely been an abject failure.)

Avaaz’s report on hate speech in Assam takes direct aim at Facebook’s platform, saying it’s being used as a conduit for whipping up anti-Muslim hatred.

In the report, entitled Megaphone for Hate: Disinformation and Hate Speech on Facebook During Assam’s Citizenship Count, the group says it analysed 800 Facebook posts and comments relating to Assam and the NRC, using keywords from the immigration discourse in Assamese, assessing them against the three tiers of prohibited hate speech set out in Facebook’s Community Standards.

Avaaz found that at least 26.5% of the posts and comments constituted hate speech. These posts had been shared on Facebook more than 99,650 times — adding up to at least 5.4 million views for violent hate speech targeting religious and ethnic minorities, according to its analysis.

Bengali Muslims are a particular target on Facebook in Assam, per the report, which found comments referring to them as “criminals,” “rapists,” “terrorists,” “pigs,” and “dogs”, among other dehumanizing terms.

In further disturbing comments there were calls for people to “poison” daughters, and legalise female foeticide, as well as several posts urging “Indian” women to be protected from “rape-obsessed foreigners”.

Avaaz suggests its findings are just a drop in the ocean of hate speech that it says is drowning Assam via Facebook and other social media. But it accuses Facebook directly of failing to provide adequate human resource to police hate speech spread on its dominant platform.

Commenting in a statement, Alaphia Zoyab, senior campaigner, said: “Facebook is being used as a megaphone for hate, pointed directly at vulnerable minorities in Assam, many of whom could be made stateless within months. Despite the clear and present danger faced by these people, Facebook is refusing to dedicate the resources required to keep them safe. Through its inaction, Facebook is complicit in the persecution of some of the world’s most vulnerable people.”

Its key complaint is that Facebook continues to rely on AI to detect hate speech which has not been reported to it by human users — using its limited pool of (human) content moderator staff to review pre-flagged content, rather than proactively detect it.

Facebook founder Mark Zuckerberg has previously said AI has a very long way to go to reliably detect hate speech. Indeed, he’s suggested it may never be able to do that.

In April 2018 he told US lawmakers it might take five to ten years to develop “AI tools that can get into some of the linguistic nuances of different types of content to be more accurate, to be flagging things to our systems”, while admitting: “Today we’re just not there on that.”

That sums to an admission that in regions such as Assam — where inter-ethnic tensions are being whipped up in a politically charged atmosphere that’s also encouraging violence — Facebook is essentially asleep on the job. The job of enforcing its own ‘Community Standards’ and preventing its platform being weaponized to amplify hate and harass the vulnerable, to be clear.

Avaaz says it flagged 213 of “the clearest examples” of hate speech which it found directly to Facebook — including posts from an elected official and pages of a member of an Assamese rebel group banned by the Indian Government. The company removed 96 of these posts following its report.

It argues there are similarities in the type of hate speech being directed at ethnic minorities in Assam via Facebook and that which targeted at Rohingya people in Myanmar, also on Facebook, while noting that the context is different. But it did also find hateful content on Facebook targeting Rohingya people in India.

It is calling on Facebook to do more to protect vulnerable minorities in Assam, arguing it should not rely solely on automated tools for detecting hate speech — and should instead apply a “human-led ‘zero tolerance’ policy” against hate speech, starting by beefing up moderators’ expertise in local languages.

It also recommends Facebook launch an early warning system within its Strategic Response team, again based on human content moderation — and do so for all regions where the UN has warned of the rise of hate speech on social media.

“This system should act preventatively to avert human rights crises, not just reactively to respond to offline harm that has already occurred,” it writes.

Other recommendations include that Facebook should correct the record on false news and disinformation by notifying and providing corrections from fact-checkers to each and every user who has seen content deemed to have been false or purposefully misleading, including if the disinformation came from a politician; that it should be transparent about all page and post takedowns by publishing its rational on the Facebook Newsroom so the issue of hate speech is given proportionate prominence and publicity to the size of the problem on Facebook; and it should agree to an independent audit of hate speech and human rights on its platform in India.

“Facebook has signed up to comply with the UN Guiding Principles on Business and Human Rights,” Avaaz notes. “Which require it to conduct human rights due diligence such as identifying its impact on vulnerable groups like women, children, linguistic, ethnic and religious minorities and others, particularly when deploying AI tools to identify hate speech, and take steps to subsequently avoid or mitigate such harm.”

We reached out to Facebook with a series of questions about Avaaz’s report and also how it has progressed its approach to policing inter-ethnic hate speech since the Myanmar crisis — including asking for details of the number of people it employs to monitor content in the region.

Facebook did not provide responses to our specific questions. It just said it does have content reviewers who are Assamese and who review content in the language, as well as reviewers who have knowledge of the majority of official languages in India, including Assamese, Hindi, Tamil, Telugu, Kannada, Punjabi, Urdu, Bengali and Marathi.

In 2017 India overtook the US as the country with the largest “potential audience” for Facebook ads, with 241M active users, per figures it reports the advertisers.

Facebook also sent us this statement, attributed to a spokesperson:

We want Facebook to be a safe place for all people to connect and express themselves, and we seek to protect the rights of minorities and marginalized communities around the world, including in India. We have clear rules against hate speech, which we define as attacks against people on the basis of things like caste, nationality, ethnicity and religion, and which reflect input we received from experts in India. We take this extremely seriously and remove content that violates these policies as soon as we become aware of it. To do this we have invested in dedicated content reviewers, who have local language expertise and an understanding of the India’s longstanding historical and social tensions. We’ve also made significant progress in proactively detecting hate speech on our services, which helps us get to potentially harmful content faster. But these tools aren’t perfect yet, and reports from our community are still extremely important. That’s why we’re so grateful to Avaaz for sharing their findings with us. We have carefully reviewed the content they’ve flagged, and removed everything that violated our policies. We will continue to work to prevent the spread of hate speech on our services, both in India and around the world.

Facebook did not tell us exactly how many people it employs to police content for an Indian state with a population of more than 30 million people.

Globally the company maintains it has around 35,000 people working on trust and safety, less than half of whom (~15,000) are dedicated content reviewers. But with such a tiny content reviewer workforce for a global platform with 2.2BN+ users posting night and day all around the world there’s no plausible no way for it to stay on top of its hate speech problem.

Certainly not in every market it operates in. Which is why Facebook leans so heavily on AI — shrinking the cost to its business but piling content-related risk onto everyone else.

Facebook claims its automated tools for detecting hate speech have got better, saying that in Q1 this year it increased the proactive detection rate for hate speech to 65.4% — up from 58.8% in Q4 2017 and 38% in Q2 2017.

However it also says it only removed 4 million pieces of hate speech globally in Q1. Which sounds incredibly tiny vs the size of Facebook’s platform and the volume of content that will be generated daily by its millions and millions of active users.

Without tools for independent researchers to query the substance and spread of content on Facebook’s platform it’s simply not possible to know how many pieces of hate speech are going undetected. But — to be clear — this unregulated company still gets to mark its own homework.

In just one example of how Facebook is able to shrink perception of the volume of problematic content it’s fencing, of the 213 pieces of content related to Assam and the NCR that Avaaz judged to be hate speech and reported to Facebook it removed less than half (96).

Yet Facebook also told us it takes down all content that violates its community standards — suggesting it is applying a far more dilute definition of hate speech than Avaaz. Unsurprising for a US company whose nascent crisis PR content review board‘s charter includes the phrase “free expression is paramount”. But for a company that also claims to want to prevent conflict and peace-build it’s rather conflicted, to say the least.

As things stand, Facebook’s self-reported hate speech performance metrics are meaningless. It’s impossible for anyone outside the company to quantify or benchmark platform data. Because no one except Facebook has the full picture — and it’s not opening its platform for ethnical audit. Even as the impacts of harmful, hateful stuff spread on Facebook continue to bleed out and damage lives around the world.",Social Media,TechCrunch,https://techcrunch.com/2019/10/28/facebook-is-failing-to-prevent-another-human-rights-tragedy-playing-out-on-its-platform-report-warns/,"Facebook's lack of resources for policing hate speech on its platform has resulted in it being used as a conduit for spreading anti-minority hate speech, thereby fuelling ethnic violence. This is despite warnings from the United Nations and other organizations, highlighting the need for proactive human-led moderation to protect vulnerable minorities.",Equality & Justice
395,"After YouTube boycott, Google pulls ads from more types of offensive content – TechCrunch","Google is pulling display ads from being placed alongside a wider range of content on YouTube and other sites, in the wake of a spike of criticism that its automatic, programmatic advertising seemingly cannot stop mainstream brands from appearing alongside extremist and offensive material.

Last week a number of brands and publishers in Europe said they would pull advertising from Google’s network after their adverts were revealed to be being displayed alongside content such as videos promoting terrorism and anti-Semitism — a long-standing issue with online ad networks that is arguably coming to a head now given rising concern about extremist movements using online channels to spread divisive messaging and build influence among voters in democratic societies.

In response to criticism last week from advertisers, including the U.K. government, the Guardian newspaper and French ad giant Havas, Google said it would be expanding controls to give them more say over where their ads appear on YouTube and the Google Display Network.

More brands have since joined the boycott.

Big names to pull @Google advertising in last 24 hours: Volkswagen, Toyota, Tesco, ITV, Heinz, Volvo, Deliveroo, Aviva. 250+ in total https://t.co/6ih7RbiOc9 — Alexi Mostrous (@AlexiMostrous) March 20, 2017

Google is now providing more detail on its response — and says it’s already started making changes, evidently hoping to stem the flow of brands away from its ad network. Chief business officer Philipp Schindler writes today that Google has “already begun ramping up changes” in three areas: its ad policies; enforcement of the policies; and new controls for advertisers.

“Recently, we had a number of cases where brands’ ads appeared on content that was not aligned with their values. For this, we deeply apologize,” he writes. “We know that this is unacceptable to the advertisers and agencies who put their trust in us. That’s why we’ve been conducting an extensive review of our advertising policies and tools, and why we made a public commitment last week to put in place changes that would give brands more control over where their ads appear.”

Among the changes Schindler covers in the blog is what he describes as “a tougher stance on hateful, offensive and derogatory content.”

And not just for ad display purposes; the suggestion is Google will be removing more types of offensive content from YouTube entirely — a tacit admission that hosting such content is becoming increasingly problematic for a company that has historically sat firmly in the U.S. “free speech” camp, yet which finds itself in the political firing line more and more, accused of helping spread hate online by providing a platform plus financial incentives for content intended to expand societal divisions.

In Germany the government is even considering new legislation to set standards for social media companies to promptly remove hate speech content from their platforms — with the country last week accusing internet companies of failing to act swiftly enough on user complaints. (Although, in that instance, Google was commended for improved responses to user complaints about illegal content on YouTube, versus Facebook and Twitter being criticized for getting worse at swiftly handling complaints.)

“We know advertisers don’t want their ads next to content that doesn’t align with their values. So starting today, we’re taking a tougher stance on hateful, offensive and derogatory content,” writes Google’s Schindler today. “This includes removing ads more effectively from content that is attacking or harassing people based on their race, religion, gender or similar categories. This change will enable us to take action, where appropriate, on a larger set of ads and sites.

“Finally, we won’t stop at taking down ads. The YouTube team is taking a hard look at our existing community guidelines to determine what content is allowed on the platform — not just what content can be monetized.”

He says Google will also be tightening safeguards for ad display pertaining to its YouTube Partner Program.

Among the new tools for advertisers that Google says it will be introducing in the “coming days and months” are:

stricter default settings for ads so they are less likely to appear beside “potentially objectionable content,” as Google puts it — with brands having to actively opt in to advertise on “broader types of content if they choose”

new account-level controls to make it easier for advertisers to exclude specific sites and channels from all of their AdWords for Video and Google Display Network campaigns, and enabling them to manage brand safety settings across all their campaigns “with a push of a button”

additional controls aimed at making it easier for brands to exclude “higher risk content and fine-tune where they want their ads to appear”

Google also says it will be beefing up resources, accelerating reviews and giving advertisers and agencies “more transparency and visibility” — with expanded availability of video-level reporting to all advertisers “in the coming months.”

The company says it will be hiring “significantly” more staff to handle the issue, as well as developing additional tools — saying it will seek to apply AI and machine learning to “increase our capacity to review questionable content for advertising.”

Advertisers with complaints about where their ads are appearing will also have access to a “new escalation path to make it easier for them to raise issues” in the future — with Google also claiming it will soon be able to resolve these cases “in less than a few hours.”

“We believe the combination of these new policies and controls will significantly strengthen our ability to help advertisers reach audiences at scale, while respecting their values,” adds Schindler. “We will continue to act swiftly to put these new policies and processes in place across our ad network and YouTube. But we also intend to act carefully, preserving the value we currently provide to advertisers, publishers and creators of all sizes. In the end, there’s nothing more important to Google than the trust we’ve built amongst our users, advertisers, creators and publishers. Brand safety is an ongoing commitment for us, and we’ll continue to listen to your feedback.”

We’ve reached out to Google with questions. At the time of writing the company had not responded but a spokeswomen told Bloomberg the timing and implementation of the new policies is still being set. In terms of granularity, she added that eventually Google plans to disable ads based on the tighter criteria on individual web pages rather than entire publications.",Social Media,TechCrunch,https://techcrunch.com/2017/03/21/after-youtube-boycott-google-pulls-ads-from-more-types-of-offensive-content/,"Google is responding to criticism of its ad network by introducing tighter controls and more safeguards, after brands pulled advertising due to offensive and extremist content appearing alongside their ads.",Equality & Justice
396,"Gah, Tinder: Never Tweet","""The messaging that they were trying to get across is, ‘We are the premier dating app, these are our statistics, this is the good that comes from it.’ But it didn't come across as making anyone say to themselves, 'Well, you know, I'm going to go use Tinder now.'""

An Un-quiet Storm

It’s hard to believe that Tinder thought their tweetstorm would be well-received (what tweetstorm is?), but the company may have wanted to respond to the wave of stories calling Tinder a hook-up app.

""It's entirely understandable how this played out, and it's going to shape how people respond to stories in the future,"" says Stu Loeser, the founder of media strategy firm Stu Loeser & Co., who explains how frustrated companies can feel when they believe their products have been taken out of context by reporters. ""And it's more frustrating when you think the reporter hasn't asked for your views. Twitter allows you to offer context.""

Tinder, however, may have gone too far, by offering, well, a little too much context. The company sent a stream of tweets on a social media site where users tend to send only a tweet or two unless they're trying really, really hard to make a point. Which makes it especially perplexing as whoever sent out the tweets likely knew that they would be seen by followers and commented on by members of the press.

“I think you might be giving them too much credit,” Chapin Clark, R/GA's managing director of copywriting in New York who handles the ad agency's Twitter feed, tells me when I ask whether the tweetstorm could have been some kind of genius marketing ploy to bring Tinder more attention. “It was clearly someone tweeting emotionally. It would be astonishing to me that some low level social media person at Tinder would've gotten approval. There's just no way.”

“You know, maybe, Tinder was sort of looking at these other competing product over its shoulders and saw this as an opportunity to make the story about them, which it wasn’t really. It really was about a larger thing about dating culture,"" he adds when I ask him to speculate on what Tinder could have been trying to accomplish. “I think we’re straining to attribute more craftiness and intent to Tinder than is really there.""

For Schmidt, Tinder could have gotten the same message across without the bad publicity that accompanied its Twitter tirade by issuing a statement to the press. “I rarely say this, but the Vanity Fair piece falls under the ‘no publicity is bad publicity’ kind of thing for Tinder, because they've actually had bad publicity,” she says, noting other articles written more directly about the company. “I think that they did more harm than good by sending those tweets out. They should have just let it lie.”

""Did they draw more attention to the story than it would have otherwise gotten? Almost undoubtedly. Are there people in the company that were tired of this familiar frame and wanted to do something about it? Almost undoubtedly. Did it get a lot of attention? Some snarky responses,"" Loeser says. ""Although, you know, it's Twitter. Something will happen with Donald Trump tomorrow.""",Social Media,WIRED,https://www.wired.com/2015/08/gah-tinder-never-tweet/,"Tinder's ill-advised tweetstorm in response to a Vanity Fair article was a prime example of how companies can do more harm than good by sending out too much information on social media. The tweetstorm drew more attention to the story than it would have otherwise, as well as snarky responses from followers, and ultimately resulted in",Social Norms & Relationships
397,Unilever warns social media to clean up “toxic” content – TechCrunch,"Consumer goods giant Unilever, a maker of branded soaps, foodstuffs and personal care items and also one of the world’s biggest online advertisers, has fired a warning shot across the bows of social media giants by threatening to pull ads from digital platforms if they don’t do more to mitigate the spread of what it dubs “toxic” online content — be it fake news, terrorism or child exploitation.

“It is critical that our brands remain not only in a safe environment, but a suitable one,” CMO Keith Weed is expected to say at the annual Interactive Advertising Bureau conference in California today, according to extracts from the speech provided to us ahead of delivery. “Unilever, as a trusted advertiser, do not want to advertise on platforms which do not make a positive contribution to society.”

The remarks echo comments made last month by UK prime minister Theresa May who singled out social media firms for acute censure, saying they “simply cannot stand by while their platforms are used to facilitate child abuse, modern slavery or the spreading of terrorist or extremist content”.

Unilever’s Weed is expected to argue that consumers are worried about “fraudulent practice, fake news, and Russians influencing the U.S. election”, and are sensitive to the brands they buy becoming tainted by associated with ad placement alongside awful stuff like terrorist propaganda and content that exploits children.

“2018 is either the year of techlash, where the world turns on the tech giants — and we have seen some of this already — or the year of trust. The year where we collectively rebuild trust back in our systems and our society,” he will argue.

Online ad giants Facebook and Google have increasingly found themselves on the hook for enabling the spread of socially divisive, offensive and at times out-and-out illegal content via their platforms — in no small part as a consequence of the popularity of their content-sharing hubs.

While the Internet is filled with all sorts of awful stuff, in its darkest corners, the mainstream reach of platforms like Facebook and YouTube puts them squarely in the political firing line for all sorts of content issues — from political disinformation to socially divisive hate speech.

The fact Facebook and Google are also the chief financial beneficiaries of online ad spending — together accounting for around 60 per cent of online ad spending in the US, for example — makes it difficult for them to dodge the charge that their businesses directly benefit from divisive and exploitative content — all the way from clickbait to fake news to full blown online extremism.

Facebook’s 2016 dismissal of concerns about fake news impacting democracy as a “pretty crazy idea” has certainly not aged well. And CEO Mark Zuckerberg has since admitted his platform is broken and made it his personal goal for 2018 to “fix Facebook“.

Both companies faced a growing backlash last year — with a number of advertisers and brands pulling ads from YouTube over concerns about the types of content that their marketing messages were being served alongside, thanks to the programmatic (i.e. automatic) nature of the ad placement. The platform also took renewed flak for the type of content it routinely serves up to kids.

While Facebook got a political grilling over hosting Kremlin disinformation — though Russia’s online dis-ops clearly sprawl across multiple tech platforms. But again, Facebook’s massive reach gifts it a greater share of blame — as the most effective channel (at least that we currently know of) for political disinformation muck spreading. (Last fall, for example, it was forced to admit that ~80,000 pieces of Russian-backed content may have been viewed by 126M Facebook users during the 2016 US election.)

Facebook has been working on adding ad transparency tools to its platform — though it remains to be seen whether it can do enough to be judged to be effectively self regulating. It doesn’t have the greatest record on that front, frankly speaking.

Last year Google also responded with alacrity to boycotts by its own advertisers, saying it would expand controls for brands to give them more say over where their ads appeared on YouTube, and by taking “a tougher stance on hateful, offensive and derogatory content” — including demonitizing more types of videos. And has made a policy change on known terrorists’ content. Though it has continued to disappoint politicians demanding better moderation.

As part of its attempts to de-risk the user generated content that its business relies on, and thus avoid the risk of further spooking already spooked advertisers, Google even recently began removing YouTube videos of the so-called ‘Tide Pod Challenge’ — i.e. where people film themselves trying to consume laundry detergent. Videos which it had previously left up, despite having a policy against content that encourages dangerous activities.

Incidentally Tide Pods aren’t a Unilever brand but their parent company, Procter & Gamble, also roasted social media firms last year — calling for them to “grow up” and slamming the “non-traditional media supply chain” for being “murky at best, and fraudulent at worst”.

Unilever’s Weed also takes aim at ad fraud in his speech, noting how it’s partnered with IBM to pilot a new blockchain tech for advertising — which he touts as having “the potential to drastically reduce advertising fraud by recording how media is purchased, delivered and interacted with by target audiences, providing reliable measurement metrics”. (Can blockchain really fix click fraud? That Unilever is actively entertaining the idea arguably shows how far trust levels in the digital ad space have fallen.)

But the main message is tilted at social media giants’ need to “build social responsibility” — and invest in trust and transparency to avoid damaging the precious substance known as ‘brand trust’ which the tech giants’ revenue-generating digital advertisers depend on.

Though, blockchain experiments aside, Unilever seems rather less publicly clear on exactly what it thinks tech giants should do to vanquish the toxic content their business models have (inadvertently or otherwise) been financially incentivizing.

Governments in Europe have been leaning on social media giants to accelerate development of tech tools that can automatically flag and even remove problem content (such as hate speech) before it has a chance to spread — though that approach is hardly uncontroversial, and critics argue it whiffs of censorship.

Germany has even passed a hate speech social media law, introducing fines of up to €50M for platforms that fail to promptly remove illegal content.

While, earlier this month, Germany’s national competition regulator also announced a probe of the online ad sector — citing concerns that a lack of transparency could be skewing market conditions.

Weed’s message to social media can be summed up as: This is a problem we’ll work with you to fix, but you need to agree to work on fixing it. “As a brand-led business, Unilever needs its consumers to have trust in our brands,” he’ll say. “We can’t do anything to damage that trust -– including the choice of channels and platforms we use. So, 2018 is the year when social media must win trust back.”

Unilever is making three specific “commitments” relating to its digital media supply chain:

that it will not invest in “platforms or environments that do not protect our children or which create division in society, and promote anger or hate”, further emphasizing: “We will prioritise investing only in responsible platforms that are committed to creating a positive impact in society” that it is committed to creating “responsible content” — with an initial focus on tackling gender stereotypes in advertising that it will push for what it dubs “responsible infrastructure”, saying it will only partner with organizations “which are committed to creating better digital infrastructure, such as aligning around one measurement system and improving the consumer experience”

So, while the company is not yet issuing an explicit ultimatum to Facebook and Google, it’s certainly putting them on notice that the political pressure they’ve been facing could absolutely turn into a major commercial headache too, if they don’t take tackling online muck spreading seriously.

tl;dr massive, mainstream success has a flip side. And boy is big tech going to feel it this year.

Facebook and Google both declined to comment on Unilever’s intervention.

Update: A Facebook spokesperson offered comment following publication, saying, “We fully support Unilever’s commitments and are working closely with them.”",Social Media,TechCrunch,https://techcrunch.com/2018/02/12/unilever-warns-social-media-to-clean-up-toxic-content/,"Unilever has threatened to pull ads from Social Media giants Facebook and Google if they do not do more to mitigate the spread of ""toxic"" content, such as fake news, terrorism, and child exploitation, due to the potential for their brands to become tainted by being associated with such content.",Security & Privacy
398,‘Rogue’ Commerce unit scanned social media for Census disinformation,"A security unit within the US Commerce Department monitored Americans’ Twitter accounts for posts critical of the US Census and conducted unauthorized surveillance to gather information about US citizens and foreign visitors, according to a fact sheet released Monday by the ranking Republican on the Senate Commerce, Science, and Transportation Committee.

The Washington Post first reported on the existence of the unit, which is called the Investigations and Threat Management Service (ITMS). According to the fact sheet, ITMS had its operations suspended as of May 14th, after an investigation launched in February by Sen. Roger Wicker (R-MS) showed that the service had conducted “a variety of improper activities dating back to the mid-2000s, involving abuse of authority, mismanagement, and reprisal against [Commerce] Department employees.”

Wicker’s memo claims that ITMS “surveilled social media activity on Twitter to monitor accounts that posted commentary critical of processes used to conduct the US Census.” The apparent goal of this surveillance was to show off ITMS’s “intelligence-gathering capabilities by linking those accountholders— members of the general public— to disinformation campaigns orchestrated by foreign governments.” This kind of misinformation campaign typically involves troll farms: groups of people paid by governments sowing wrong or misleading information on social media. But there’s no evidence such an effort was ever aimed at discrediting the census.

The social media posts collected by ITMS were added to a spreadsheet called the Social Media Tracker, which was used to conduct searches on secure intelligence databases of social media account holders, the Post reported. One example: the ITMS opened a case after a 68-year-old retiree in Florida with around 100 Twitter followers tweeted that the Census would “be corrupted and falsified to benefit the Trump Party.”

The Post reported that ITMS referred information about social media posts to the FBI, which declined to investigate since the posts constituted protected speech. It did not appear from Wicker’s memo or the Post story that any of these social media “investigations” ever led to charges of any kind; indeed, the ITMS opened some 1,000 cases but “few resulted in arrests or criminal charges,” according to the Post.

The Commerce Department did not reply to a request for comment Monday. Wicker said an official report on the ITMS’s activities will be released in the coming months.

“Over time, the ITMS began regularly conducting criminal investigations and eventually began using counterintelligence tools to gather information about both foreign visitors and U.S. citizens––despite lacking any proper form of authorization,” Wicker wrote in his fact sheet, adding that “ITMS has mutated into a rogue, unaccountable police force without a clear mission.”",Social Media,Verge,https://www.theverge.com/2021/5/24/22451494/commerce-unit-surveillance-privacy-social-media-twitter-senate,"The Investigations and Threat Management Service (ITMS) within the US Commerce Department conducted unauthorized surveillance of Americans' Twitter accounts, gathering information about US citizens and foreign visitors in order to link them to foreign misinformation campaigns. These activities led to no arrests or criminal charges, and ITMS has been suspended.",Security & Privacy
399,UN says Facebook is accelerating ethnic violence in Myanmar – TechCrunch,"The United Nations has warned that Facebook’s platform is contributing to the spread of hate speech and ethnic violence in crisis-hit Myanmar.

It’s yet another black mark against social media at a time when the tech industry’s reputation as an accelerator of false information is attracting criticism from the highest places.

This week the government of Sri Lanka also sought to block access to Facebook and two other of its social services, WhatsApp and Instagram, in an attempt to stem mob violence against its local Muslim minority — citing inflammatory social media posts.

“These platforms are banned because they were spreading hate speeches and amplifying them,” a government spokesman told the New York Times.

India has also struggled for years with false information being spread by social media platforms like WhatsApp then triggering riots, communal violence and even leading to deaths.

While humans telling lies is nothing new, the speed at which misinformation and disinformation can now spread, thanks to digitally networked communities linked on social media, is.

Moderating that risk is the challenge big tech platforms stand accusing of failing.

UN human rights experts investigating a possible genocide in Rakhine state warned yesterday that Facebook’s platform is being used by ultra-nationalist Buddhists to incite violence and hatred against the Rohingya and other ethnic minorities.

A security crackdown in the country last summer led to around 650,000 Rohingya Muslims fleeing into neighboring Bangladesh. Since then there have been multiple reports of state-led violence against the refugees, and the UN has been leading a fact-finding mission in the country.

Yesterday, chairman of the mission, Marzuki Darusman, told reporters that the social media platform had played a “determining role” in Myanmar’s crisis (via Reuters).

Darusman said Facebook has “substantively contributed to the level of acrimony and dissension and conflict” within the public sphere. “Hate speech is certainly of course a part of that,” he continued, adding: “As far as the Myanmar situation is concerned, social media is Facebook, and Facebook is social media.”

In Myanmar, Ashin Wirathu, an ultranationalist Buddhist monk who preaches hate against the Rohingya, has been able to build up large followings on social media — using Facebook to spread divisive and hate-fueling messages.

Speaking to reporters yesterday, UN investigator Yanghee Lee, described Facebook as a huge part of public, civil and private life in Myanmar, noting it is used by the government to disseminate information to the public.

However she also flagged how the platform has been appropriated by ultra-nationalist elements to spread hate against minorities.

In the case of Wirathu, Facebook has sometimes removed or restricted his pages — but does not appear to have done enough. Though the company has now confirmed to us that it permanently disabled his profile at the end of January.

“Everything is done through Facebook in Myanmar,” said Lee. “It was used to convey public messages but we know that the ultra-nationalist Buddhists have their own Facebooks and are really inciting a lot of violence and a lot of hatred against the Rohingya or other ethnic minorities.”

“I’m afraid that Facebook has now turned into a beast, and not what it originally intended,” she added.

We reached out to the company with questions but at the time of writing Facebook had not responded. Update: A Facebook spokesperson has now provided the following statement:

There is no place for hate speech or content that promotes violence on Facebook, and we work hard to keep it off our platform. We have invested significantly in technology and local language expertise to help us swiftly remove hate content and people who repeatedly violate our hate speech policies. We take this incredibly seriously and have worked with experts in Myanmar for several years to develop safety resources and counter-speech campaigns. This work includes a dedicated Safety Page for Myanmar, a locally illustrated version of our Community Standards, and regular training sessions for civil society and local community groups across the country. Of course, there is always more we can do and we will continue to work with local experts to help keep our community safe.

For years Myanmar’s military dictatorship entirely controlled and censored the press, but in 2011 it began what was billed as a gradual democratic transition — which included opening up to new media services such as Facebook. And the platform essentially went from ground zero to becoming the most important information source in Myanmar in a handful of years.

Local Facebook users are now thought to number over 30 million.

But as uptake ballooned, human rights groups sounded alarms over how Facebook is being used to spread hate speech and stoke ethnic violence.

Last year New York Times reporter Paul Moyer also warned that government Facebook channels were being used to spread anti-Rohingya propaganda — implying the platform has also been appropriated as a citizen control tool by the state seeding its own propaganda.

And while states maliciously misappropriating social media to foster hate against their own citizens may not be a problem in every country where the tech industry operates, social media platforms amplifying hate speech is certainly a universal concern — from Asia to Europe to America.",Social Media,TechCrunch,https://techcrunch.com/2018/03/13/un-says-facebook-is-accelerating-ethnic-violence-in-myanmar/,"Social media platforms are being used to spread hate speech, incite violence and amplify false information, leading to mob violence and even death in places such as India, Myanmar and Sri Lanka.",Security & Privacy
400,Study: Social Media Turns Us Into Hungry Rats Basically,"Image by Futurism Studies

As if the way social media dominates every facet of our lives wasn’t evident enough: A new study found that people pursue “likes” on platforms like Facebook and Instagram much in the same way rats pursue food.

An international team of scientists analyzed more than one million social media posts from more than 4,000 users across a variety of social media sites, according to New York University. They discovered a direct correlation between how frequently people post and how often they receive likes.

It’s simple: People post more on social media when they have a high rate of likes, and they post less frequently when they receive less likes.

In their paper published in the journal Nature Communications, the researchers found that our behavior on social media reflects the food reward-based behavior seen in rats. More specifically, social media platforms are like a digital “Skinner Box” — a tool scientists use by placing a rat (or other small animal) into a box, and then, reward them with food when the rats take specific actions (like pressing a lever).

Advertisement

Advertisement

So yeah, we’re pretty much just a bunch of hungry rats in a box. Pushing a lever. For food/likes. Though depressing, it does shed light on some fascinating implications of how different species learn.

“These results establish that social media engagement follows basic, cross-species principles of reward learning,” said David Amodio, professor at New York University and co-author of the study.

Researchers also hope that their study can help us develop healthier online behavior and combat the harmful effects of social media addiction.

“These findings may help us understand why social media comes to dominate daily life for many people and provide clues, borrowed from research on reward learning and addiction, to how troubling online engagement may be engaged,” Amodio explained.

Advertisement

Advertisement

If nothing else, it’s a good reminder of the harmful effects social media can have on our emotional and mental health—and how maybe we should all leave our digital Skinner Boxes behind once in a while.

READ MORE: Social Media Use Driven by Search for Reward, Akin to Animals Seeking Food, New Study Shows [NYU]

More on social media: Vatican Confused by How Pope Account “Liked” Sexy Pic

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.

Advertisement

Advertisement",Social Media,Futurism,https://futurism.com/neoscope/study-social-media-turns-us-into-hungry-rats-basically,"This study found that people on social media act much like rats in a Skinner Box, pushing the lever for reward in the form of ""likes"". This behavior can lead to addiction and other harmful mental health issues, and is a reminder of the dangers of overusing social media.",User Experience & Entertainment
401,Twitter is testing a new anti-abuse feature called ‘Safety Mode’ – TechCrunch,"Twitter’s newest test could provide some long-awaited relief for anyone facing harassment on the platform.

The new product test introduces a feature called “Safety Mode” that puts up a temporary line of defense between an account and the waves of toxic invective that Twitter is notorious for. The mode can be enabled from the settings menu, which toggles on an algorithmic screening process that filters out potential abuse that lasts for seven days.

“Our goal is to better protect the individual on the receiving end of Tweets by reducing the prevalence and visibility of harmful remarks,” Twitter Product Lead Jarrod Doherty said.

Safe Mode won’t be rolling out broadly — not yet, anyway. The new feature will first be available to what Twitter describes as a “small feedback group” of about 1,000 English language users.

In deciding what to screen out, Twitter’s algorithmic approach assesses a tweet’s content — hateful language, repetitive, unreciprocated mentions — as well as the relationship between an account and the accounts replying. The company notes that accounts you follow or regularly exchange tweets with won’t be subject to the blocking features in Safe Mode.

For anyone in the test group, Safety Mode can be toggled on in the privacy and safety options. Once enabled, an account will stay in the mode for the next seven days. After the seven-day period expires, it can be activated again.

In crafting the new feature, Twitter says it spoke with experts in mental health, online safety and human rights. The partners Twitter consulted with were able to contribute to the initial test group by nominating accounts that might benefit from the feature, and the company hopes to focus on female journalists and marginalized communities in its test of the new product. Twitter says that it will start reaching out to accounts that meet the criteria of the test group — namely accounts that often find themselves on the receiving end of some of the platform’s worst impulses.

Earlier this year, Twitter announced that it was working on developing new anti-abuse features, including an option to let users “unmention” themselves from tagged threads and a way for users to prevent serial harassers from mentioning them moving forward. The company also hinted at a feature like Safety Mode that could give users a way to defuse situations during periods of escalating abuse.

Being “harassed off of Twitter” is, unfortunately, not that uncommon. When hate and abuse get bad enough, people tend to abandon Twitter altogether, taking extended breaks or leaving outright. That’s obviously not great for the company either, and while it’s been slow to offer real solutions to harassment, it’s obviously aware of the problem and working toward some possible solutions.",Social Media,TechCrunch,https://techcrunch.com/2021/09/01/twitter-safety-mode-harassment/,"Twitter's newest product test, ""Safety Mode,"" seeks to provide relief to users facing harassment on the platform. Unfortunately, when hate and abuse become too much, people often abandon Twitter, taking extended breaks or leaving altogether.",Social Norms & Relationships
402,Twitter claims increased enforcement of hate speech and abuse policies in last half of 2019 – TechCrunch,"Twitter has given its biannual transparency reports a new home with today’s launch of the Twitter Transparency Center, which the company says was designed to make the reporting more easily understood and accessible. The launch was timed alongside the belated release of Twitter’s latest transparency report covering the second half of 2019. The company attributed the delay to the COVID-19 health crisis and its work in getting the new Transparency Center up and running. The report touts Twitter’s increasing efforts in enforcing its policies, including a 95% increase in accounts actioned for violating its abuse policy, a 47% increase in account locks and suspensions and a 54% increase in accounts actioned for violating hateful conduct policies, among others.

The company claims its ability to “proactively” surface content violations for human review has helped it increase enforcement of its rules, along with more detailed policies, improved reporting tools and other factors.

As a result, this period saw the largest increase in the number of accounts actioned under Twitter’s abuse policies — a metric that could speak to better technology, as Twitter claims, but also perhaps hints at the devolving nature of online discourse.

Meanwhile, Twitter attributed the increase in actions taken on accounts demonstrating hateful conduct, in part, to its new “dehumanization policy” announced on July 9, 2019.

Twitter increased enforcement of its rules in other areas during this reporting period, including the posting of sensitive media/adult content (enforcement actions were up 39%), suicide & self-harm (enforcement actions up 29%), doxxing (enforcement actions up 41%) and non-consensual nudity (enforcement actions up 109%). The only area to see a decline was violent threats, which saw a 5% decrease in the number of accounts actioned for policy violations.

Twitter also actioned 60,807 accounts for violating policies around regulated goods or services.

Online harassment has been a significant challenge for Twitter as it has grown. The social network now encompasses a wider swath of the general public, compared with its early days when tech enthusiasts knew it as twttr, a sort of public-facing SMS. Today, Twitter’s idealistic goal of being an “online public town square” is bumping up against the limitations of that model, which is also increasingly criticized as a flawed or even delusional sort of analogy for what Twitter has become.

Twitter, like much of social media, can over-amplify fringe beliefs, controversy and toxic content, to the detriment of conversation health. It can help polarize users’ opinions. And it serves as a breeding ground for cancel culture.

The company itself, as of late, seems to be waking up to the problem of putting the world together in one room to debate ideas, and the ramifications of amplifying misinformation that results.

It suspended accounts from the fringe conspiracy movement, QAnon, in July. It has also flagged and screened Trump’s tweets and briefly froze his ability to share misinformation. On the product side, Twitter rolled out a tool that lets users hide replies that don’t add value to conversations and, just last week, publicly launched a feature that lets users only tweet with friends and followers, instead of with the general public.

Abuse policy enforcement isn’t the only big change that took place in the last half of 2019. Government requests for user data also increased, Twitter found.

Twitter says that the U.S. made up the highest percentage of legal requests for information during the reporting period, accounting for 26% of all global requests. Japan was second, comprising 22% of information requests. Overall, government requests grew 21% in the period July 1 to December 31, 2019, and the aggregate number of accounts specified in the requests grew 63%.

Both metrics were the largest Twitter has seen since it began transparency reporting in 2012, it noted.

Twitter also saw 27,538 legal demands to remove content specifying 98,595 accounts — again, the largest number to date; 86% of these demands came from Japan, Russia and Turkey.

“Our work to increase transparency efforts across the company is tireless and constant. We will continue to work on increasing awareness and understanding about how our policies work and our practices around content moderation, data disclosures and other critical areas,” the company blog post about the new center explained. “In addition, we will take every opportunity to highlight the actions of law enforcement, governments, and other organizations that impact Twitter and the people who use our service across the world,” it noted.

More metrics, including those focused on spam, terrorism, child exploitation and extremism, are available on the new Twitter Transparency Center.",Social Media,TechCrunch,https://techcrunch.com/2020/08/19/twitter-claims-increased-enforcement-of-hate-speech-and-abuse-policies-in-last-half-of-2019/,"Social Media, such as Twitter, can amplify fringe beliefs, polarizing opinions and toxic content, leading to an increase in abuse policy enforcement and government requests for user data. This has made it a breeding ground for cancel culture, and has caused Twitter to take action by suspending accounts, flagging and screening tweets and introducing features to hide replies.",Equality & Justice
403,"Media Hit: Politics, Viral Media and the Chilling Effect on Stupid","Does a politician on the campaign trail have any expectation of privacy? It’s almost a silly question. But some pretty smart politicos have behaved as if there is more than one answer: Michael Steele is learning this, but unsuccessful Virginia Senatorial candidate George Allen and former British Prime Minister Gordon Brown learned it the hard way.

Now California gubernatorial candidateJerry Brown seems to be shocked shocked that there’s video recording going on here.

I was a guest this morning on KSRO radio of Sonoma County, Calif. where the question is very much alive and well as Brown’s candidacy seems to be losing momentum against the well-funded juggernaut that is Team Meg Whitman. The timing couldn’t be worse, but there was a mole at a Brown appearance who recorded the candidate indicating he didn’t really intend to shake things up if elected.

The is an old lawyer’s saying that goes: “Never write anything down you wouldn’t want to defend in court.” How quaint. The modern equivalent is: “Never say anything out loud you don’t want everyone to hear.”

See Also:",Social Media,WIRED,https://www.wired.com/2010/07/chilling-effect-on-stupid/,"The main consequence of Social Media discussed here is that politicians have no expectation of privacy on the campaign trail, as evidenced by the mole video recordings of Jerry Brown and other politicians. They must be aware that anything they say publicly could potentially be recorded and shared, making it important to think twice before speaking.",Security & Privacy
404,"Singel-Minded: Facebook Comments Are Another 'Good News, Bad News' Proposition","Facebook unleashed a new commenting system this week that promises to help online publications clean up their commenting cesspools, while simultaneously extending Facebook's tentacles further into the web outside its walls.

Unfortunately for those with visions of a non-Facebook dominated web, this initiative has the potential to dramatically expand the ginormous social network's already imperial reach.

The new system lets website owners replace their current commenting system with Facebook's simply by dropping in a few lines of Javascript. Then, commenters have to sign in using either their Facebook or Yahoo IDs, an attempt to ensure a commenter is a real person. Commenters can opt to have their comment posted as an update, along with a link to the original story, which spreads the story link inside Facebook's walls.

This offers publishers a number of benefits. They get more links to their site from inside the net's most popular website. A lot of people are ""registered"" to comment on their sites. And, they have a system designed to discourage vitriol because it's easy for the site owner to ban a user and tough for a user to create a new identity.

For Facebook, the benefit is also clear. Users now have even more incentive to be constantly logged into Facebook (those who are already logged into Facebook don't have to do anything to comment on a website using its system). Additionally, even more of Facebook's users' net activities flow through its site, since by default comments -- and replies to them -- post to a Facebook user's wall. That deepens users' ties to Facebook, adds more content to Facebook, and gives people more reason to check their Facebook newsfeed for the increased information flow.

It also builds on what's becoming Facebook's most important function: being the identity provider and validator for the wider net. The system opens the door for what's likely inevitable: having news sites rely on Facebook to identify its users and eventually to serve ads to its readers based on their individual Facebook pages.

The Facebook system competes most directly with Disqus (a system Wired.com has just starting using), which also creates a single, central profile used to comment on any site that uses the system. Facebook offers its plug-in for free, while Disqus commenting requires premium accounts for the features a large site needs to have.

The immediate drawbacks of Facebook's commenting system match the larger issues of the social networking site. There's no way to export the comments if a publication decides to drop the system -- just as Facebook jealously holds onto the e-mail addresses of the people you are connected to on Facebook so you can't re-establish your network on some other site. Facebook will likely create some sort of export system, if only to assuage potential publications, though given Facebook's history with exporting, it will likely be bare bones and not particularly useful in practice.

Facebook's entry into this arena presents sites with three choices -- none particularly ideal.

Continue reading ...

The first is to gamble that your site is important enough to your readers that you can get away with requiring them to have a specific login for your site, and that you have the technical resources to build that out. But even if you don't use Facebook Connect, the movement seems to be away from a collection of site profiles and passwords and toward a single one that gives you entry to your collection, be it from Google, Yahoo, OpenID or others.",Social Media,WIRED,https://www.wired.com/2011/03/singel-facebook-empire/,"Facebook's new commenting system has the potential to expand their already imperial reach, by requiring users to log in with their Facebook or Yahoo IDs, and by default posting comments, and replies to them, to their user's wall, leading to more content being produced within Facebook's walls.",Security & Privacy
405,Will the last person to leave social media please turn off the light? – TechCrunch,"After the early, exciting expository years of the Internet – the Age of Jennicam where the web was supposed to act as confessional and stage – things changed swiftly. This new medium was a revelation, a gift of freedom that we all took for granted. Want to post rants against the government? Press publish on Blogspot. Want to yell at the world? Aggregate and comment upon some online news. Want to meet people with similar interests or kinks? There was a site for you although you probably had to hunt it down.

The way we shared deep feelings on the Internet grew out of its first written stage into other more interactive forms. It passed through chatrooms, Chatroulette, and photo sharing. It passed through YouTube and Indie gaming. It planted a long, clammy kiss on Tumblr where it will probably remain for a long time. But that was for the professional exhibitionists. Today the most confessional “static” writing you’ll find on a web page is the occasional Medium post about beating adversity through meditation and Apple Watch apps and we have hidden our human foibles behind dank memes and chatbots. Where could the average person, the civilian, go to share their deepest feelings of love, anger, and fear?

Social media.

After the rise and demise of things like Julie & Julia, the human impetus for sharing – exposing oneself, as it were, on the Internet – had to flow somewhere. And it flowed toward the privacy of our own multiple online presences. This mishmash of services blossomed in the past decade and we have most recently settled on Facebook and Twitter where we share everything including political rants, anger at Comcast, bafflement at the “stupidity” of our fellow humans, and bad jokes. Alternative social networks, including photo sharing services, are more about broadcasting our pleasure or displeasure visually but written social media itself is the latest refuge for our deepest thoughts and most sensitive shares.

But an important change is coming to social media. We are learning that all of our thoughts aren’t welcome, especially by social media company investors. We are also learning that social media companies are a business. This means conversation is encouraged as long as it runs the gamut from mundane to vicious but stops at the overtly sexual or violent. Early in its life-cycle Pinterest made a big stink about actively banning porn while Instagram essentially allowed all sorts of exposition as long as it was monetizable and censored. Facebook still actively polices its photographs for even the hint of sexuality as an artist named Justyna Kiesielewicz recently discovered. She posted a staid nude and wanted to run it as an targeted advertisement. Facebook mistakenly ran the ad for a while, grabbing $50 before it banned the image. In short the latest incarnation of the expository impulse is truncated and sites like Facebook and Twitter welcome most hate groups but most draw the line at underboobs.

Further, social media is no longer protected. As careless CEOs quickly discover saying the wrong thing in a “private” chat or deleting an errant does not mean someone won’t screencapture your rant. In fact social media has become an id and ego collector, a fly strip where all of our worst thoughts are captured permanently. We exhale in anger and chuff in frustration. We tell people to unfollow us if they don’t like what we’re saying and we turn neighborhood pages into political cesspools. Then, when we cross too many perceived boundaries, an army of trolls is ready to pounce and our private spaces become public very quickly.

In short social media is no longer a safe place. I don’t mean this in the politically correct sense but in the very mental and physical sense. Whereas the web was once a broadcast medium it is now a two-way or many-to-many medium. Our errant Twitter thoughts can make us targets and we often don’t know we’re being watched. Entire wars can break out online that have real-world consequences – see Pizzagate – and hoaxes flit through the memetic bloodstream like cancer, breaking down our defenses. A prominent writer and friend recently mused about what would happen if he posted some political rants. The first thing that leapt to his readers’ minds was the potential for SWATing and doxing and then an visit from the FBI. Then, as evidenced by the above CEO example, you get fired.

Social media has become a very real, very visceral, and very censorial force and it can now only worsen the human condition. It was once an experiment but that experiment is over. Like most things that calcify into the mainstream the joy of exploration is now gone, replaced by a grim determination to just get it over with. There is a reason so many startups are trying to break our social media habits. We are exhausted by the endless mantra of Twitter-Facebook-Instagram and we will go so far as to replace our app icons with dummy apps simply to stop ourselves from Tweeting.

If you believe Facebook and Twitter can’t become graveyards in the next half decade then I have a MySpace to sell you. We are past the Information Age and moving into the Age of Privacy. It will soon become more and more important for the web to not find us and security will become paramount. Every few decades the human psyche recoils against the invasion of technology. It happened in the early 1970s as ex-hippies moved to the woods to become more mindful and stayed to open coffee shops. It’s happening now with the rise of beards and artisinal pickles. And it will happen again as young people take Snapchat’s original pitch seriously: that not everything you put on the Internet should be permanent and, in fact, impermanence should be the next mode of sharing. Perhaps the next form of revolutionary social media will be a service that finally lets you stay quiet.",Social Media,TechCrunch,https://techcrunch.com/2017/01/13/will-the-last-person-to-leave-social-media-please-turn-off-the-light/,"Social media has become a dangerous place where thoughtless words can lead to dangerous consequences such as doxing, SWATing, and even being fired from one's job. As a result, users are trying to break out of their social media habits and are seeking more private ways to communicate.",Security & Privacy
406,"Canceling 'Roseanne' Wasn't About Conviction, It Was About Capital","Social media has entirely changed how a company makes that determination. Triage involves, above all, seeing how much media attention the tweets attract. “The first thing we try to determine is what percentage is coming from New York and LA, the bubble of the media industry,” says Ben Carlson, co-president of Fizziology, which provides social media analytics for television and movie studios. “We track every show and every film and see what’s happening in real time, and we could see this conversation exploding everywhere.”

But that doesn’t mean that all non-media responses are given equal weight. A crucial part of the calculus comes down to assessing whether a particular response is coming from a large and wide-ranging audience or just an especially noisy few. The Roseanne scandal unquestionably qualified as A Thing—and one that required swift action.

“If they didn’t have a plan already in place, that was one of the most rapidly executed and highly impactful responses I’ve ever seen in my thirty years of crisis management practice,” says Jonathan Bernstein, president of Bernstein Crisis Management. “I think this is a scenario that could have actually happened: Someone in their crisis team picks up immediately, they grab their plan, it says to confer by phone, and the head of the team asks, ‘Is it DEFCON 3, 2, or 1?’” According to Bernstein, it’s highly likely that ABC had a Barr-specific plan in place, and that insta-firing Barr and canceling the show is almost certainly a Defcon 1 response—a reaction to something that would cause irreparable damage to the company’s reputation.

And Barr’s own apologies—which were both deflective and later undercut by an extended litany of passive-agressive retweets— is probably her own response to the tsunami of on- and offline criticism she’s been swamped by. “The realistic goal, at a minimum, is to achieve even a slight reduction in overall damage to reputation and the bottom line,” Bernstein says. “That’s what I see Roseanne attempting to do, belatedly, with her second round of apologies—no doubt at the ‘do this or you’re even more-fucked’ guidance of both competent public relations and legal counsel.”

Why the focus on reputation? Reputation is tied inextricably to money—it’s either your greatest financial asset or downfall. And for ABC, making it look like money wasn’t part of the equation is actually a great way to protect its bottom line in the long run. “The fact that they canceled a show that was doing well in the ratings may go a long way toward showing there are more important things than money,” says Kimberly Elsbach, a University of California–Davis professor who studies the perceptions of organizations. “Making it [appear] that it wasn’t about ratings will help secure viewers’ trust.” Viewers’ trust has always been important, but in an age when social media outrage can (and often does) lead to organized boycotts and lost advertising revenue, the stakes are even higher.

Containing the Fallout

Less obvious, though, is how social media influences trust amongst those within a besieged organization like ABC—and earlier in the week, that was an urgent concern. “One of the things that’s so unique about this crazy day is how much backstage drama we saw playing out in real time,” Carlson says. Wanda Sykes wields significant influence, but she wouldn’t have been the only person ABC was worried about: Roseanne co-star Sara Gilbert is also a cohost on CBS daytime show The Talk, which affords her a massive platform. Depending on how either of them—or anyone else—chose to react to Barr's tweets could be far more damaging. “A viewer can easily choose to withdraw their support,” Elsbach says. “But if you have a family depend upon your employer it can be very difficult [to cut ties]. What they might do instead is disidentify, reduce their efforts, or even sabotage—things that may hurt the organization even more.” Social media makes that all the easier.

Which isn’t to say that ABC is a mustache-twirling bunch of amoral capitalists—just that the network may not be making the grand gesture of civility many seem to think it has. “There’s a blurry line between moral decisions and economic decisions. Moral decisions can have an economic impact,” says Dr. Hunt. “It is not economically in the long-term interest of ABC to be allied with this kind of dehumanizing rhetoric.” Whether it be because of the fallout of potential social media outrage or concern about advertisers, the network was able to call Barr’s words “abhorrent” while also making a decision that, in the long-term, wasn’t as altruistic as it seemed.",Social Media,WIRED,https://www.wired.com/story/roseanne-cancellation-capital-not-conviction/,"Social media can have a powerful influence on how companies handle crises due to the speed of response and the potential for it to lead to organized boycotts or lost advertising revenue. This means that reputation and trust are key considerations in any response, and companies must take into account the potential for employees to disidentify, reduce their effort, or even",Economy
407,"Twitter rolls out Stories, aka ‘Fleets,’ to all users; will also test a Clubhouse rival – TechCrunch","Twitter this morning is launching its own version of Stories — aka “Fleets” — to its global user base. The product, which allows users to post ephemeral content that disappears in 24 hours, had already rolled out to select markets, including Brazil, India, Italy, South Korea and, most recently, Japan. The company, in a press briefing on Monday, also revealed its plans to test an audio-based social networking feature similar to the controversial app Clubhouse.

Like Clubhouse, Twitter’s new audio spaces will allow people to gather for live conversations with another person or a group of people.

This is an area that, so far, has faced significant moderation challenges due to the nature of live audio. Clubhouse, though still in a private, invite-only testing phase, has already seen several high-profile incidents of moderation failure, including the harassment of a New York Times reporter, and another conversation that delved into anti-Semitism.

Twitter, for all its efforts at developing new features to combat online abuse — from its Hide Replies feature to its newer conversation controls — has not yet proven itself to be the sort of company that has managed to successfully combat online abuse, harassment and trolling. Nor has it managed to develop a robust reporting system where users feel their complaints are swiftly handled.

So, given that live audio has proven even more difficult to moderate than text-based posts, Twitter’s decision to invest in this space will likely be criticized by those who don’t believe Twitter can safely engineer a platform for this type of conversation.

For what it’s worth, Twitter is not rolling out live audio spaces to all users at once. Instead, it’s first testing the product with a small group of people who the company believes can provide better user feedback than those on Clubhouse’s VC chat room, for instance.

“It’s critical that we get safety right — safety and people feeling comfortable in these spaces. We need to get that right in order for people to leverage live audio spaces in the ways we might imagine or in the ways that would be most helpful for them,” explained Twitter Staff Product Designer Maya Gold Patterson, when introducing the feature in a briefing for reporters.

“So we’re going to do something a little different,” she continued. “We are going to launch this first experiment of spaces to a very small group of people — a group of people who are disproportionately impacted by abuse and harm on the platform: women and those from marginalized backgrounds,” she added.

As to why Twitter felt the need to jump on the audio bandwagon so early in the game when it’s arriving several years late to the Stories format is less clear.

According to Twitter Director of Design Joshua Harris, the company’s delay to launch Stories was because Twitter was being “methodical in exploring how the format works for people on Twitter.”

That’s not exactly true. Twitter wasn’t years late because it was being careful about Fleets’ development. The reality was that Twitter had prioritized work on its core product over new features.

Things have been changing in recent months, following activities by activist investor Elliott Management Group, which took a sizable stake in Twitter earlier this year, along with Silver Lake. The firms did so with a plan to push the company for more innovation and new executive leadership. (The companies later struck a deal to spare Twitter CEO Jack Dorsey’s ousting, gain board seats, and put someone on the board with expertise in technology and artificial intelligence.)

As The New York Times noted in its coverage of the investment, Twitter’s product had remained remarkably similar over the years, according to one investor who had asked Dorsey to step down.

At the time of Elliott’s campaign, Twitter’s lack of Stories had been referenced by some reports as an area where the company had fallen behind social media rivals in terms of innovation.

Over on Twitter, Dorsey disputes this characterization.

Twitter’s trial launch of Fleets soon followed this shakeup. This intervention could also explain why the company is now rushing to enter the still unproven space of audio-based social networking.

Of course, Twitter is aware of the Clubhouse comparisons with its test of audio spaces.

“We think that audio is definitely having a resurgence right now across many digital spaces,” noted Twitter Product Lead, Kayvon Beykpour. “So it’ll be fascinating to see how other platforms explore the area as well, but we think it’s a critical one for us, too,” he said.

As for Fleets, there’s no change to the product beyond its global availability.

The feature itself is a fairly basic version of the basic Stories format, which will be located at the top of the Twitter timeline. Users can post text, photos and videos to Fleets directly, or share tweets into Fleets and post their reactions. Others reply to Fleets via direct messages (DMs), much like how Stories work on other platforms. Twitter says more formats and creative tools will come to the product in the near future.

Twitter also says it’s working to bring standard voice tweets to Android and will make transcriptions for these tweets and other media available in 2021. It’s now testing audio in DMs in Brazil, as well.

The company additionally hinted at some new features in development aimed at pushing Twitter users to be more thoughtful and kinder to one another. These may include built-in reminders and nudges, including ways for friends to reach out privately to another user when they see something is going wrong.

“We’re exploring methods of private feedback on the platform, as well as private apologies, and forgiveness,” said Twitter Senior Product Manager Christine Su. “And so that may look like a notification — that’s like a gentle elbowing from someone that you follow. Or it also may look like a nudge like you’ve seen before.” No further details were provided, nor a time frame for a rollout.

In the meantime, Fleets will be available to all markets where it hadn’t yet rolled out, starting today. The audio spaces test is poised to launch to small groups of users soon.

(Post updated 11/18/20, 2:45 PM ET to include Twitter CEO Jack Dorsey’s dispute about the influence of activist investors on Twitter’s product direction, as well as additional details about the firms’ agreements with Twitter.)",Social Media,TechCrunch,https://techcrunch.com/2020/11/17/twitter-rolls-out-stories-aka-fleets-to-all-users-will-also-test-a-clubhouse-rival/,"Twitter's decision to invest in audio-based social networking has been met with criticism due to the difficulty of moderating this type of content, as evidenced by Clubhouse's numerous moderation failures and lack of a robust reporting system for users.",Security & Privacy
408,The FDA should regulate Instagram’s algorithm as a drug – TechCrunch,"The Wall Street Journal on Tuesday reported Silicon Valley’s worst-kept secret: Instagram harms teens’ mental health; in fact, its impact is so negative that it introduces suicidal thoughts.

Thirty-two percent of teen girls who feel bad about their bodies report that Instagram makes them feel worse. Of teens with suicidal thoughts, 13% of British and 6% of American users trace those thoughts to Instagram, the WSJ report said. This is Facebook’s internal data. The truth is surely worse.

President Theodore Roosevelt and Congress formed the Food and Drug Administration in 1906 precisely because Big Food and Big Pharma failed to protect the general welfare. As its executives parade at the Met Gala in celebration of the unattainable 0.01% of lifestyles and bodies that we mere mortals will never achieve, Instagram’s unwillingness to do what is right is a clarion call for regulation: The FDA must assert its codified right to regulate the algorithm powering the drug of Instagram.

The FDA should consider algorithms a drug impacting our nation’s mental health: The Federal Food, Drug and Cosmetic Act gives the FDA the right to regulate drugs, defining drugs in part as “articles (other than food) intended to affect the structure or any function of the body of man or other animals.” Instagram’s internal data shows its technology is an article that alters our brains. If this effort fails, Congress and President Joe Biden should create a mental health FDA.

Researchers can study what Facebook prioritizes and the impact those decisions have on our minds. How do we know this? Because Facebook is already doing it — they’re just burying the results.

The public needs to understand what Facebook and Instagram’s algorithms prioritize. Our government is equipped to study clinical trials of products that can physically harm the public. Researchers can study what Facebook privileges and the impact those decisions have on our minds. How do we know this? Because Facebook is already doing it — they’re just burying the results.

In November 2020, as Cecilia Kang and Sheera Frenkel report in “An Ugly Truth,” Facebook made an emergency change to its News Feed, putting more emphasis on “News Ecosystem Quality” scores (NEQs). High NEQ sources were trustworthy sources; low were untrustworthy. Facebook altered the algorithm to privilege high NEQ scores. As a result, for five days around the election, users saw a “nicer News Feed” with less fake news and fewer conspiracy theories. But Mark Zuckerberg reversed this change because it led to less engagement and could cause a conservative backlash. The public suffered for it.

Facebook likewise has studied what happens when the algorithm privileges content that is “good for the world” over content that is “bad for the world.” Lo and behold, engagement decreases. Facebook knows that its algorithm has a remarkable impact on the minds of the American public. How can the government let one man decide the standard based on his business imperatives, not the general welfare?

Upton Sinclair memorably uncovered dangerous abuses in “The Jungle,” which led to a public outcry. The free market failed. Consumers needed protection. The 1906 Pure Food and Drug Act for the first time promulgated safety standards, regulating consumable goods impacting our physical health. Today, we need to regulate the algorithms that impact our mental health. Teen depression has risen alarmingly since 2007. Likewise, suicide among those 10 to 24 is up nearly 60% between 2007 and 2018.

It is of course impossible to prove that social media is solely responsible for this increase, but it is absurd to argue it has not contributed. Filter bubbles distort our views and make them more extreme. Bullying online is easier and constant. Regulators must audit the algorithm and question Facebook’s choices.

When it comes to the biggest issue Facebook poses — what the product does to us — regulators have struggled to articulate the problem. Section 230 is correct in its intent and application; the internet cannot function if platforms are liable for every user utterance. And a private company like Facebook loses the trust of its community if it applies arbitrary rules that target users based on their background or political beliefs. Facebook as a company has no explicit duty to uphold the First Amendment, but public perception of its fairness is essential to the brand.

Thus, Zuckerberg has equivocated over the years before belatedly banning Holocaust deniers, Donald Trump, anti-vaccine activists and other bad actors. Deciding what speech is privileged or allowed on its platform, Facebook will always be too slow to react, overcautious and ineffective. Zuckerberg cares only for engagement and growth. Our hearts and minds are caught in the balance.

The most frightening part of “The Ugly Truth,” the passage that got everyone in Silicon Valley talking, was the eponymous memo: Andrew “Boz” Bosworth’s 2016 “The Ugly.”

In the memo, Bosworth, Zuckerberg’s longtime deputy, writes:

So we connect more people. That can be bad if they make it negative. Maybe it costs someone a life by exposing someone to bullies. Maybe someone dies in a terrorist attack coordinated on our tools. And still we connect people. The ugly truth is that we believe in connecting people so deeply that anything that allows us to connect more people more often is de facto good.

Zuckerberg and Sheryl Sandberg made Bosworth walk back his statements when employees objected, but to outsiders, the memo represents the unvarnished id of Facebook, the ugly truth. Facebook’s monopoly, its stranglehold on our social and political fabric, its growth at all costs mantra of “connection,” is not de facto good. As Bosworth acknowledges, Facebook causes suicides and allows terrorists to organize. This much power concentrated in the hands of one corporation, run by one man, is a threat to our democracy and way of life.

Critics of FDA regulation of social media will claim this is a Big Brother invasion of our personal liberties. But what is the alternative? Why would it be bad for our government to demand that Facebook accounts to the public its internal calculations? Is it safe for the number of sessions, time spent and revenue growth to be the only results that matters? What about the collective mental health of the country and world?

Refusing to study the problem does not mean it does not exist. In the absence of action, we are left with a single man deciding what is right. What is the price we pay for “connection”? This is not up to Zuckerberg. The FDA should decide.",Social Media,TechCrunch,https://techcrunch.com/2021/09/15/the-fda-should-regulate-instagrams-algorithm-as-a-drug/,"Social Media has a detrimental impact on teens' mental health, resulting in suicidal thoughts and depression, and the responsibility lies with Facebook to regulate the algorithm powering the drug of Instagram. If they don't, Congress and President Biden must create a mental health FDA to study what Facebook prioritizes and the impact those decisions have on our minds.",Mental Health
409,Facebook has quietly removed three bogus far-right networks in Spain ahead of Sunday’s elections – TechCrunch,"Facebook has quietly removed three far-right networks that were engaged in coordinated inauthentic behavior intended to spread politically divisive content in Spain ahead of a general election in the country, which takes place on Sunday.

The networks had a total reach of almost 1.7 million followers and had generated close to 7.4 million interactions in the past three months alone, according to analysis by the independent group that identified the bogus activity on Facebook’s platform.

The fake far-right activity was apparently not picked up by Facebook.

Instead, activist not-for-profit Avaaz unearthed the inauthentic content, and presented its findings to the social networking giant earlier this month, on April 12. In a press release issued today, the campaigning organization said Facebook has now removed the fakes — apparently vindicating its findings.

“Facebook did a great job in acting fast, but these networks are likely just the tip of the disinformation iceberg — and if Facebook doesn’t scale up, such operations could sink democracy across the continent,” said Christoph Schott, campaign director at Avaaz, in a statement.

“This is how hate goes viral. A bunch of extremists use fake and duplicate accounts to create entire networks to fake public support for their divisive agenda. It’s how voters were misled in the U.S., and it happened again in Spain,” he added.

We reached out to Facebook for comment but at the time of writing the company had not responded to the request or to several questions we also put to it.

Avaaz said the networks it found comprised around 30 pages and groups spreading far-right propaganda — including anti-immigrant, anti-LGBT, anti-feminist and anti-Islam content.

Examples of the inauthentic content can be viewed in Avaaz’s executive summary of the report. They include fake data about foreigners committing the majority of rapes in Spain; fake news about Catalonia’s pro independence leader; and various posts targeting left-wing political party Podemos — including an image superimposing the head of its leader onto the body of Hitler performing a Nazi salute.

One of the networks — which Avaaz calls Unidad ​Nacional Española (after the most popular page in the network) — was apparently created and coordinated by an individual called ​Javier Ramón Capdevila Grau, who had multiple personal Facebook accounts (also) in contravention of Facebook’s community standards.

This network, which had a reach of more than 1.2 million followers, comprised at least 10 pages that Avaaz identified as working in a coordinated fashion to spread “politically divisive content.”

Its report details how word-for-word identical posts were published across multiple Facebook pages and groups in the network just minutes apart, with nothing to indicate they weren’t original postings on each page.

Here’s an example post it found copy-pasted across the Unidad ​Nacional Española network:

Avaaz found another smaller network targeting left-wing views, called Todos Contra Podemos, which included seven pages and groups with around 114,000 followers — also apparently run by a single individual (in this case using the name Antonio Leal Felix Aguilar) who also operated multiple Facebook profiles.

A third network, Lucha por España​, comprised 12 pages and groups with around 378,000 followers.

Avaaz said it was unable to identify the individual/s behind that network.

While Facebook has not publicized the removals of these particular political disinformation networks, despite its now steady habit of issuing PR when it finds and removes “coordinated inauthentic behavior” (though, of course, there’s no way to be sure it’s disclosing everything it finds on its platform), test searches for the main pages identified by Avaaz returned either no results or what appear to be other unrelated Facebook pages using the same name.

Since the 2016 U.S. presidential election was (infamously) targeted by divisive Kremlin propaganda seeded and amplified via social media, Facebook has launched what it markets as “election security” initiatives in a handful of countries around the world — such as searchable ad archives and political ad authentication and/or disclosure requirements.

However, these efforts continue to face criticism for being patchy, piecemeal and, even in countries where they have been applied to its platform, weak and trivially easy to work around.

Its political ads transparency measures do not always apply to issue-based ads (and/or content), for instance, which punches a democracy-denting hole in the self-styled “guardrails” by allowing divisive propaganda to continue to flow.

In Spain, Facebook has not even launched a system of political ad transparency, let alone launched systems addressing issue-based political ads — despite the country’s looming general election on April 28; its third in four years. (Since 2015 elections in Spain have yielded heavily fragmented parliaments — making another imminent election not at all unlikely.)

In February, when we asked Facebook whether it would commit to launching ad transparency tools in Spain before the April 28 election, it offered no such commitment — saying instead that it sets up internal cross-functional teams for elections in every market to assess the biggest risks, and make contact with the relevant electoral commission and other key stakeholders.

Again, it’s not possible for outsiders to assess the efficacy of such internal efforts. But Avaaz’s findings suggest Facebook’s risk assessment of Spain’s general election has had a pretty hefty blind spot when it comes to proactively picking up malicious attempts to inflate far-right propaganda.

Yet, at the same time, a regional election in Andalusia late last year returned a shocking result and warning signs — with the tiny (and previously unelected) far-right party Vox gaining around 10 percent of the vote to take 12 seats.

Avaaz’s findings vis-à-vis the three bogus far-right networks suggest that as well as seeking to slur left-wing/liberal political views and parties, some of the inauthentic pages were involved in actively trying to amplify Vox — with one bogus page, Orgullo Nacional España, sharing a pro-Vox Facebook page 155 times in a three-month period.

Avaaz used the Facebook-owned social media monitoring tool Crowdtangle to get a read on how much impact the fake networks might have had.

It found that while the three inauthentic far-right Facebook networks produced just 3.7 percent of the posts in its Spanish elections data set, they garnered an impressive 12.6 percent of total engagement over the three-month period it pulled data on (between January 5 and April 8) — despite consisting of just 27 Facebook pages and groups out of a total of 910 in the full data set.

Or, to put it another way, a handful of bad actors managed to generate enough divisive politically charged noise that more than one in 10 of those engaging in Spanish election chatter on Facebook, per its data set, at very least took note.

It’s a finding which neatly illustrates that divisive content being more clickable is not at all a crazy idea — whatever the founder of Facebook once said.

Update: Facebook has now sent the following statement regarding Avaaz’s findings:",Social Media,TechCrunch,https://techcrunch.com/2019/04/23/facebook-has-quietly-removed-three-bogus-far-right-networks-in-spain-ahead-of-sundays-elections/,"Social media platforms such as Facebook have the potential to spread misinformation and false narratives, which can have a damaging effect on elections and democracy. Avaaz's findings demonstrate this potential, as the three far-right networks uncovered by their investigation had generated close to 7.4 million interactions in the past three months alone, and had the potential to",Politics
410,India has banned TikTok—plus 58 other Chinese apps,"On Monday, India banned TikTok and dozens of other apps made in China, escalating tension between the countries two weeks after a long-simmering border dispute in the Himalayas turned deadly.

The news: In a statement, India said the apps “engaged in activities which [are] prejudicial to sovereignty and integrity of India, defence of India, security of state and public order.” Messaging and chat apps like Baidu and WeChat were on the list too, along with the popular microblogging site Weibo, several mobile games, and photo editing software.

Why does it matter? Home to more than 1.3 billion people, India has a huge smartphone user base and English-speaking population, which make it the world’s largest social-media market. It’s perhaps no surprise, then, that India is also TikTok’s biggest market, with nearly 191 million downloads at the end of 2019; the US is at a distant second with nearly 41 million.

Social media has a troubling history in India. TikTok and WhatsApp have also been weaponized by India’s far-right Hindu nationalist movement, to deadly effect: viral WhatsApp messages spreading false rumors have led to mob lynchings of Muslims and lower-caste Hindus, while TikTok’s split-screen videos have also been used in caste hate crimes. And as we reported last year, Hindu nationalists flooded TikTok with misogynistic videos threatening to overtake the Muslim-majority province of Jammu and Kashmir and “turn it” Hindu by forcibly marrying Kashmiri girls and women.

It’s not the first time India has banned TikTok. TikTok launched last year in India under its former brand, Musical.ly, becoming popular as a lip-synching app. But just before the country’s elections, a court banned the app, ruling it had pornographic content and was predatory; within days, India’s Supreme Court overturned the ban. In July, though, the Ministry of Electronics and Information Technology—the same one that issued the current ban—said TikTok was being used for “unlawful” purposes, specifically sharing user data with China through bots. TikTok wasn’t banned, then, however, as the slew of misogynistic content afterward showed.

India’s Chinese app ban is a new diplomatic maneuver. India and China have opened a new front in their ongoing skirmish. Both countries are nuclear powers and economic giants, but India’s ban is notable for using social media as a tool to exert political pressure on its rival.",Social Media,MIT,https://www.technologyreview.com/2020/06/29/1004616/india-bans-tiktok-plus-58-other-chinese-apps/,"India's ban on Chinese apps is a new diplomatic maneuver showing their ongoing skirmish, with social media being used to exert political pressure. This ban follows reports of misuse of the apps, including the spread of false rumors leading to mob lynchings, and misogynistic content threatening to overtake the Muslim-majority province of Jammu and Kashmir.",Politics
411,Here Are Twitter's Latest Rules for Fighting Hate and Abuse,"When Twitter could take credit for revolutionary political movements like the Arab Spring, it was easy for the company's executives to joke about their liberal stance on free speech. (Twitter, they said, was ""the free speech wing of the free speech party."") But things are a bit more complicated now, as Twitter increasingly plays host to bullies, harassers, Nazis, propaganda-spreading bots, ISIS recruiters, and threats of nuclear war. Twitter's toxic content problem isn't just bad for humanity---it's bad for business, because it drives people away from the platform.

In early 2016 the company began altering its stance on free speech, forming a Trust and Safety Council made up of safety groups, advocates, and researchers to help it address the problem. But critics are not satisfied with the results. Reports have outlined many instances of the company's failure to punish harassers; these shortcomings make Twitter's recent missteps all the more frustrating to critics. Last week the company disabled features of actor Rose McGowan's account at a crucial moment amid the Harvey Weinstein sexual misconduct scandal. Groups of women boycotted the site for a day in protest. Twitter's typical response to complaints about hate and harassment is to affirm its commitment to transparency. But even that is becoming a punch line.

On Friday CEO Jack Dorsey announced plans to act more aggressively. Twitter will introduce new rules around unwanted sexual advances, nonconsensual nudity, hate symbols, violent groups, and tweets that glorify violence, he tweeted. To add a sense of urgency, the company is holding daily meetings on the issue.

After Tuesday's meeting, Twitter's head of safety policy emailed members of its Trust and Safety Council with detailed plans on its new rules, which Twitter plans to implement in the coming weeks.

The new plans stop short of sweeping measures, such as banning pornography or specific groups like Nazis. Rather, they offer expanded features like allowing observers of unwanted sexual advances---as well as victims---to report them, and expanded definitions, such as including ""creep shots"" and hidden camera content under the definition of ""nonconsensual nudity."" The company also plans to hide hate symbols behind a ""sensitive image"" warning, though it has not yet defined what qualifies as a hate symbol. Twitter also says it will take unspecified enforcement actions against ""organizations that use/have historically used violence as a means to advance their cause.""

It's not the first time Twitter has targeted violent groups; its rules already prohibit threatening or promoting terrorism. But the new rules show the company is open to expanding that to any group promoting violence. The company's new steps also show how Twitter, like Facebook and other digital-media platforms that host user-generated content, struggle with how much editorial oversight and human judgment to introduce.

In a statement, Twitter said, ""Although we planned on sharing these updates later this week, we hope our approach and upcoming changes, as well as our collaboration with the Trust and Safety Council, show how seriously we are rethinking our rules and how quickly we’re moving to update our policies and how we enforce them.”",Social Media,WIRED,https://www.wired.com/story/here-are-twitters-latest-rules-for-fighting-hate-and-abuse/,"Twitter has been struggling to combat hate, harassment and other toxic content on its platform, leading to boycotts and criticism of its failure to punish harassers. The company is now planning to introduce new rules to address the issue, but critics remain skeptical of the effectiveness of these measures.",Equality & Justice
412,Facebook’s ad team shoots itself in the foot by pulling Elizabeth Warren campaign ads – TechCrunch,"Facebook’s ad team shoots itself in the foot by pulling Elizabeth Warren campaign ads

Facebook’s gang that couldn’t shoot straight advertising department has made another blunder, this time by pulling Elizabeth Warren campaign ads touting the senator’s proposal to break up big tech.

The offending ads were pulled, according to Politico, over their use of the Facebook brand in their copy.

Meanwhile, other ads that the senator’s presidential campaign had run which addressed the plan to unwind various acquisitions by Facebook, Amazon and Alphabet (the parent company of Google) were not removed from Facebook.

Indeed, the removal appears to be short-lived, but has given the Warren campaign ammunition for their argument and numerous headlines, tweets and retweets.

Curious why I think FB has too much power? Let's start with their ability to shut down a debate over whether FB has too much power. Thanks for restoring my posts. But I want a social media marketplace that isn't dominated by a single censor. #BreakUpBigTech https://t.co/UPS6dozOxn — Elizabeth Warren (@ewarren) March 11, 2019

“We removed the ads because they violated our policies against use of our corporate logo,” a Facebook spokesperson told BuzzFeed’s Ryan Mac. “In the interest of allowing robust debate, we are restoring the ads.”

Facebook confirms it took down Elizabeth Warren's ads about Facebook, but is in the process of restoring them. FB spox: ""We removed the ads because they violated our policies against use of our corporate logo. In the interest of allowing robust debate, we are restoring the ads.” — Ryan Mac (@RMac18) March 11, 2019

That’s a good move for the Facebook public relations team, especially since the ads reportedly didn’t include Facebook’s logo.

But the damage has already been done. It provides fodder to Warren’s argument that big tech has too much power and control over the way information is disseminated — especially on its own platforms.

This incident may be a tempest in a teapot, but it will calcify positions on the left and the right about the self-interest of big technology and these companies’ ability to regulate content on their own platforms to the detriment of free speech — even in advertising.",Social Media,TechCrunch,https://techcrunch.com/2019/03/11/facebooks-ad-team-shoots-itself-in-the-foot-by-pulling-elizabeth-warren-campaign-ads/,"Facebook's removal of Elizabeth Warren's ads has highlighted the power of big tech companies to regulate content on their own platforms, and how this can stifle free speech and open debate.","Information, Discourse & Governance"
413,Unregulated facial recognition technology presents unique risks for the LGBTQ+ community – TechCrunch,"It seems consumers today are granted ever-dwindling opportunities to consider the safety and civil liberties implications of a new technology before it becomes widely adopted. Facial recognition technology is no exception. The well-documented potential for abuse and misuse of these tools built by giant and influential companies as well as government and law enforcement agencies should give serious pause to anyone who values their privacy – especially members of communities that have been historically marginalized and discriminated against.

The cavalier attitude toward unregulated surveillance tools demonstrated by some law enforcement and other local, state, and federal government entities seem to reinforce the notion that forfeiting your personal data and privacy for greater convenience, efficiency, and safety is a fair trade. For vulnerable communities this could not be further from the truth. Without proper oversight, facial recognition technology has the potential to exacerbate existing inequalities and make daily life challenging and dangerous for LGBTQ+ individuals.

Biometric data can provide a uniquely intimate picture of a person’s digital life. Skilled and persistent hackers seeking to exploit access to an individual’s messages on social media, financial records, or location data would view the information collected by facial recognition software as a particularly valuable and worthwhile target, especially as biometric data has become increasingly popular as a form of authentication.

Without proper privacy protections in place, data breaches that target facial recognition data may become far more likely. In the wrong hands, a person’s previously undisclosed sexual orientation or gender identity can become a tool for discrimination, harassment, or harm to their life or livelihood.

The risks to transgender, nonbinary, or gender non-conforming individuals is even more acute. Most facial recognition algorithms are trained on data sets designed to sort individuals into two groups – often male or female. The extent of the misgendering problem was highlighted in a recent report that found that over the last three decades of facial recognition researchers used a binary construct of gender over 90 percent of the time and understood gender to be a solely physiological construct over 80 percent of the time.

Consider the challenge – not to mention emotional toil – for a transgender individual trying to catch a flight who is now subject to routine stops and additional security screening all because the facial recognition systems expected to be used in all airports by 2020 are not built to be able to reconcile their true gender identity with their government issued ID.

Members of the LGBTQ+ community cannot shoulder the burden of lax digital privacy standards without also assuming unnecessary risks to their safety online and offline. Our vibrant communities deserve comprehensive, national privacy protections to fully participate in society and live without the fear that their data – biometric or otherwise – will be used to further entrench existing bias and prejudice.

Our communities face the challenge of trying to protect themselves from rules that neither they, or the people implementing them, fully understand. Congress must act to ensure that current and future applications for facial recognition are built, deployed, and governed with necessary protections in mind.

This is why LGBT Tech signed on to a letter by the ACLU, along with over 60 other privacy, civil liberties, civil rights, and investor and faith groups to urge Congress to put in place a federal moratorium on face recognition for law enforcement and immigration enforcement purposes until Congress fully debates what, if any, uses should be permitted.

Given the substantial concerns, which representatives on both sides of the aisle recognized at a recent hearing, prompt action is necessary to protect people from harm. We should not move forward with the deployment of this technology until and unless our rights can be fully safeguarded.",Social Media,TechCrunch,https://techcrunch.com/2019/06/29/unregulated-facial-recognition-technology-presents-unique-risks-for-the-lgbtq-community/,"The risks posed by facial recognition technology without proper privacy protections are especially acute for vulnerable communities, such as the LGBTQ+ community, who may be misgendered or subject to discrimination, harassment, or harm due to their data being in the wrong hands.",Security & Privacy
414,"Instagram’s new test lets you choose if you want to hide ‘Likes,’ Facebook test to follow – TechCrunch","Instagram today will begin a new test around hiding Like counts on users’ posts, following its experiments in this area which first began in 2019. This time, however, Instagram is not enabling or disabling the feature for more users. Instead, it will begin to explore a new option where users get to decide what works best for them — either choosing to see the Like counts on others’ posts, or not. Users will also be able to turn off Like counts on their own posts, if they choose. Facebook additionally confirmed it will begin to test a similar experience on its own social network.

Instagram says tests involving Like counts were deprioritized after Covid-19 hit, as the company focused on other efforts needed to support its community. (Except for that brief period this March where Instagram accidentally hid Likes for more users due to a bug.)

The company says it’s now revisiting the feedback it collected from users during the tests and found a wide range of opinions. Originally, the idea with hiding Like counts was about reducing the anxiety and embarrassment that surrounds posting content on the social network. That is, people would stress over whether their post would receive enough Likes to be deemed “popular.” This problem was particularly difficult for Instagram’s younger users, who care much more about what their peers think — so much so that they would take down posts that didn’t receive “enough” Likes.

In addition, the removal of Likes helped reduce the sort of herd mentality that drives people to like things that are already popular, as opposed to judging the content for themselves.

But during tests, not everyone agreed the removal of Likes was a change for the better. Some people said they still wanted to see Like counts so they could track what was trending and popular. The argument for keeping Likes was more prevalent among the influencer community, where creators used the metric in order to communicate their value to partners, like brands and advertisers. Here, lower engagement rates on posts could directly translate to lower earnings for these creators.

Both arguments for and against Likes have merit, which is why Instagram’s latest test will put the choice back into users’ own hands.

This new test will be enabled for a small percentage of users globally on Instagram, the company says.

If you’ve been opted in, you’ll find a new option to hide the Likes from within the app’s Settings. This will prevent you from seeing Likes on other people’s posts as you scroll through your Instagram Feed. As a creator, you’ll be able to hide Likes on a per-post basis via the three-dot “…” menu at the top. Even if Likes are disabled publicly, creators are still able to view Like counts and other engagements through analytics, just as they did before.

The tests on Facebook, which has also been testing Like count removals for some time, have not yet begun. Facebook tells TechCrunch those will roll out in the weeks ahead.

Making Like counts an choice may initially seem like it could help to address everyone’s needs. But in reality, if the wider influencer community chooses to continue to use Likes as a currency that translates to popularity and job opportunities, then other users will continue to do the same.

Ultimately, communities themselves have to decide what sort of tone they want to set, preferably from the outset — before you’ve attracted millions of users who will be angry when you later try to change course.

There’s also a question as to whether social media users are really hungry for an “Like-free” safer space. For years we’ve seen startups focused on building an “anti-Instagram” of sorts, where they drop one or more Instagram features, like algorithmic feeds, Likes and other engagement mechanisms, such as Minutiae, Vero, Dayflash, Oggl, and now, newcomers like troubled Dispo, or under-the-radar Herd. But Instagram has yet to fail because of an anti-Instagram rival. If anything is a threat, it’s a new type of social network entirely, like TikTok –where it should be noted getting Likes and engagements is still very important for creator success.

Instagram didn’t say how long the new tests would last or if and when the features would roll out more broadly.

“We’re testing this on Instagram to start, but we’re also exploring a similar experience for Facebook. We will learn from this new small test and have more to share soon,” a Facebook company spokesperson said.",Social Media,TechCrunch,https://techcrunch.com/2021/04/14/instagrams-new-test-lets-you-choose-if-you-want-to-hide-likes-facebook-test-to-follow/,"Social Media can create a competitive atmosphere based on the amount of Likes a post receives, causing anxiety and embarrassment for users and reinforcing herd mentality.",Social Norms & Relationships
415,Troll Targets Say Twitter’s New Filters Don't Go Far Enough,"Twitter has heard the complaints about abuse on its platform—really, it has. It’s heard them so loud and clear that it’s rolling out new ways to help you shut those voices out.

At least, that’s the message Twitter sought to send yesterday, when it announced two new tools that give users more ways to control what tweets they see. The first lets you limit who gets to buzz your phone with @-replies only to people you follow. The second is a so-called ""quality filter,"" which sounds like a step toward finally curbing the mob-style harassment Twitter seems so optimized to enable. But some targets of that harassment aren't so sure it's a meaningful fix.

Twitter says flipping on the filter will ""improve the quality of tweets you see by using a variety of signals, such as account origin and behavior."" If that sounds vague, it’s on purpose—Twitter hesitates to share more for fear bad actors will try to game the system. So it's not clear whether the quality filter acts like an automatic spam filter or if a team of actual humans is manually curating a blacklist of abusive accounts—or both. But the setting, which has already been available to verified users, does seem to get rid of certain dubious content, such as duplicate tweets and bot spam. The filter specifically does not block tweets from people you follow or accounts with which you’ve recently interacted.

One broken aspect Twitter's new filters do not seem to address is the process by which users report abuse to the company and the all-too-opaque way the site reviews those complaints. And across the Twittersphere, that left users feeling ambivalent about Twitter's seeming nod toward fighting harassment.

What Lies Beneath

New York-based social worker and writer Feminista Jones uses an elaborate setup of filters and scripts to block the hate heaped on her as she blogs about black feminist issues. As a verified user, she already has the quality filter switched on, but she says it only helps if she's accessing Twitter via the web or the company's own mobile app. She says she uses other apps to track good mentions and keep up with her followers but winds up having to ""catch a lot of the other crap"" in the process. Jones says she uses a script to block Twitter ""eggs""—accounts without profile pictures, typically set up in haste just to troll others. She also filters plenty of keywords from her timeline. Still, doing all that work isn’t always effective. ""Many of the trolls know not to use certain words,"" she says.

Even setting aside trolls who find workarounds, filtering out abuse isn't really the same as eliminating it from the platform. ""Hiding notifications from people I don’t follow narrows my ability to engage,"" says Jamilah Lemieux, senior editor at Ebony Magazine, of Twitter's new notification filter. For a public figure like herself, whose job often involves connecting with people she doesn't know, eliminating that contact altogether doesn't work.",Social Media,WIRED,https://www.wired.com/2016/08/troll-targets-say-twitters-new-filters-dont-go-far-enough/,"The downside of social media, as discussed here, is that it has enabled trolls to harass, abuse, and intimidate users, making it difficult for people to engage with others and express themselves freely.",Security & Privacy
416,"Most Facebook users still in the dark about its creepy ad practices, Pew finds – TechCrunch","A study by the Pew Research Center suggests most Facebook users are still in the dark about how the company tracks and profiles them for ad-targeting purposes.

Pew found three-quarters (74%) of Facebook users did not know the social networking behemoth maintains a list of their interests and traits to target them with ads, only discovering this when researchers directed them to view their Facebook ad preferences page.

A majority (51%) of Facebook users also told Pew they were uncomfortable with Facebook compiling the information.

While more than a quarter (27%) said the ad preference listing Facebook had generated did not very or at all accurately represent them.

The researchers also found that 88% of polled users had some material generated for them on the ad preferences page. Pew’s findings come from a survey of a nationally representative sample of 963 U.S. Facebook users ages 18 and older which was conducted between September 4 to October 1, 2018, using GfK’s KnowledgePanel.

In a senate hearing last year Facebook founder Mark Zuckerberg claimed users have “complete control” over both information they actively choose to upload to Facebook and data about them the company collects in order to target ads.

But the key question remains how Facebook users can be in complete control when most of them they don’t know what the company is doing. This is something U.S. policymakers should have front of mind as they work on drafting a comprehensive federal privacy law.

Pew’s findings suggest Facebook’s greatest ‘defence’ against users exercising what little control it affords them over information its algorithms links to their identity is a lack of awareness about how the Facebook adtech business functions.

After all the company markets the platform as a social communications service for staying in touch with people you know, not a mass surveillance people-profiling ad-delivery machine. So unless you’re deep in the weeds of the adtech industry there’s little chance for the average Facebook user to understand what Mark Zuckerberg has described as “all the nuances of how these services work”.

Having a creepy feeling that ads are stalking you around the Internet hardly counts.

At the same time, users being in the dark about the information dossiers Facebook maintains on them, is not a bug but a feature for the company’s business — which directly benefits by being able to minimize the proportion of people who opt out of having their interests categorized for ad targeting because they have no idea it’s happening. (And relevant ads are likely more clickable and thus more lucrative for Facebook.)

Hence Zuckerberg’s plea to policymakers last April for “a simple and practical set of — of ways that you explain what you are doing with data… that’s not overly restrictive on — on providing the services”.

(Or, to put it another way: If you must regulate privacy let us simplify explanations using cartoon-y abstraction that allows for continued obfuscation of exactly how, where and why data flows.)

From the user point of view, even if you know Facebook offers ad management settings it’s still not simple to locate and understand them, requiring navigating through several menus that are not prominently sited on the platform, and which are also complex, with multiple interactions possible. (Such as having to delete every inferred interest individually.)

The average Facebook user is unlikely to look past the latest few posts in their newsfeed let alone go proactively hunting for a boring sounding ‘ad management’ setting and spending time figuring out what each click and toggle does (in some cases users are required to hover over a interest in order to view a cross that indicates they can in fact remove it, so there’s plenty of dark pattern design at work here too).

And all the while Facebook is putting a heavy sell on, in the self-serving ad ‘explanations’ it does offer, spinning the line that ad targeting is useful for users. What’s not spelt out is the huge privacy trade off it entails — aka Facebook’s pervasive background surveillance of users and non-users.

Nor does it offer a complete opt-out of being tracked and profiled; rather its partial ad settings let users “influence what ads you see”.

But influencing is not the same as controlling, whatever Zuckerberg claimed in Congress. So, as it stands, there is no simple way for Facebook users to understand their ad options because the company only lets them twiddle a few knobs rather than shut down the entire surveillance system.

The company’s algorithmic people profiling also extends to labelling users as having particular political views, and/or having racial and ethnic/multicultural affinities.

Pew researchers asked about these two specific classifications too — and found that around half (51%) of polled users had been assigned a political affinity by Facebook; and around a fifth (21%) were badged as having a “multicultural affinity”.

Of those users who Facebook had put into a particular political bucket, a majority (73%) said the platform’s categorization of their politics was very or somewhat accurate; but more than a quarter (27%) said it was not very or not at all an accurate description of them.

“Put differently, 37% of Facebook users are both assigned a political affinity and say that affinity describes them well, while 14% are both assigned a category and say it does not represent them accurately,” it writes.

Use of people’s personal data for political purposes has triggered some major scandals for Facebook’s business in recent years. Such as the Cambridge Analytica data misuse scandal — when user data was shown to have been extracted from the platform en masse, and without proper consents, for campaign purposes.

In other instances Facebook ads have also been used to circumvent campaign spending rules in elections. Such as during the UK’s 2016 EU referendum vote when large numbers of ads were non-transparently targeted with the help of social media platforms.

And indeed to target masses of political disinformation to carry out election interference. Such as the Kremlin-backed propaganda campaign during the 2016 US presidential election.

Last year the UK data watchdog called for an ethical pause on use of social media data for political campaigning, such is the scale of its concern about data practices uncovered during a lengthy investigation.

Yet the fact that Facebook’s own platform natively badges users’ political affinities frequently gets overlooked in the discussion around this issue.

For all the outrage generated by revelations that Cambridge Analytica had tried to use Facebook data to apply political labels on people to target ads, such labels remain a core feature of the Facebook platform — allowing any advertiser, large or small, to pay Facebook to target people based on where its algorithms have determined they sit on the political spectrum, and do so without obtaining their explicit consent. (Yet under European data protection law political beliefs are deemed sensitive information, and Facebook is facing increasing scrutiny in the region over how it processes this type of data.)

Of those users who Pew found had been badged by Facebook as having a “multicultural affinity” — another algorithmically inferred sensitive data category — 60% told it they do in fact have a very or somewhat strong affinity for the group to which they are assigned; while more than a third (37%) said their affinity for that group is not particularly strong.

“Some 57% of those who are assigned to this category say they do in fact consider themselves to be a member of the racial or ethnic group to which Facebook assigned them,” Pew adds.

It found that 43% of those given an affinity designation are said by Facebook’s algorithm to have an interest in African American culture; with the same share (43%) is assigned an affinity with

Hispanic culture. While one-in-ten are assigned an affinity with Asian American culture.

(Facebook’s targeting tool for ads does not offer affinity classifications for any other cultures in the U.S., including Caucasian or white culture, Pew also notes, thereby underlining one inherent bias of its system.)

In recent years the ethnic affinity label that Facebook’s algorithm sticks to users has caused specific controversy after it was revealed to have been enabling the delivery of discriminatory ads.

As a result, in late 2016, Facebook said it would disable ad targeting using the ethnic affinity label for protected categories of housing, employment and credit-related ads. But a year later its ad review systems were found to be failing to block potentially discriminatory ads.

The act of Facebook sticking labels on people clearly creates plenty of risk — be that from election interference or discriminatory ads (or, indeed, both).

Risk that a majority of users don’t appear comfortable with once they realize it’s happening.

And therefore also future risk for Facebook’s business as more regulators turn their attention to crafting privacy laws that can effectively safeguard consumers from having their personal data exploited in ways they don’t like. (And which might disadvantage them or generate wider societal harms.)

Commenting about Facebook’s data practices, Michael Veale, a researcher in data rights and machine learning at University College London, told us: “Many of Facebook’s data processing practices appear to violate user expectations, and the way they interpret the law in Europe is indicative of their concern around this. If Facebook agreed with regulators that inferred political opinions or ‘ethnic affinities’ were just the same as collecting that information explicitly, they’d have to ask for separate, explicit consent to do so — and users would have to be able to say no to it.

“Similarly, Facebook argues it is ‘manifestly excessive’ for users to ask to see the extensive web and app tracking data they collect and hold next to your ID to generate these profiles — something I triggered a statutory investigation into with the Irish Data Protection Commissioner. You can’t help but suspect that it’s because they’re afraid of how creepy users would find seeing a glimpse of the the truth breadth of their invasive user and non-user data collection.”

In a second survey, conducted between May 29 and June 11, 2018 using Pew’s American Trends Panel and of a representative sample of all U.S. adults who use social media (including Facebook and other platforms like Twitter and Instagram), Pew researchers found social media users generally believe it would be relatively easy for social media platforms they use to determine key traits about them based on the data they have amassed about their behaviors.

“Majorities of social media users say it would be very or somewhat easy for these platforms to determine their race or ethnicity (84%), their hobbies and interests (79%), their political affiliation (71%) or their religious beliefs (65%),” Pew writes.

While less than a third (28%) believe it would be difficult for the platforms to figure out their political views, it adds.

So even while most people do not understand exactly what social media platforms are doing with information collected and inferred about them, once they’re asked to think about the issue most believe it would be easy for tech firms to join data dots around their social activity and make sensitive inferences about them.

Commenting generally on the research, Pew’s director of internet and technology research, Lee Rainie, said its aim was to try to bring some data to debates about consumer privacy, the role of micro-targeting of advertisements in commerce and political activity, and how algorithms are shaping news and information systems.

Update: Responding to Pew’s research, Facebook sent us the following statement:",Social Media,TechCrunch,https://techcrunch.com/2019/01/16/most-facebook-users-still-in-the-dark-about-its-creepy-ad-practices-pew-finds/,"The Pew Research Center study highlights the risks of social media platforms, such as Facebook, to users' privacy, as most Facebook users are unaware of how the company tracks and profiles them for ad-targeting purposes. The study also reveals that users are uncomfortable with Facebook compiling such information, and that there is no simple way for them to understand",Security & Privacy
417,"Seeking to respin Instagram’s toxicity for teens, Facebook publishes annotated slide decks – TechCrunch","“The methodology is not fit to provide statistical estimates for the correlation between Instagram and mental health or to evaluate causal claims between social media and health/well-being,” Facebook writes in an introduction annotation on one of the slide decks. Aka “nothing to see here”.

Later on, commenting on a slide entitled “mental health findings” (which is subtitled: “Deep dive into the Reach, Intensity, IG Impact, Expectation, Self Reported Usage and Support of mental health issues. Overall analysis and analysis split by age when relevant”), Facebook writes categorically that: “Nothing in this report is intended to reflect a clinical definition of mental health, a diagnosis of a mental health condition, or a grounding in academic and scientific literature.”

While on a slide that contains the striking observation that “Most wished Instagram had given them better control over what they saw”, Facebook nitpicks that the colors used by its researchers to shade the cells of the table which presents the data might have created a misleading interpretation — “because the different color shading represents very small difference within each row”.

If the sight of Facebook publicly questioning the significance of internal work and quibbling with some of the decisions made by its own researchers seems unprepossessing, remember that the stakes of this particular crisis for the adtech giant are very high.

The WSJ’s reporting has already derailed a planned launch of a “tweens” version of the photo-sharing app.

While U.S. lawmakers are also demanding answers.

More broadly, there are global moves to put child protection at the center of digital regulations — such as the U.K.’s forthcoming Online Safety Act (while its Age Appropriate Design Code is already in force).

So there are — potentially — very serious ramifications for how Instagram will be able to operate in the future, certainly vis-à-vis children and teenagers, as regulations get drafted and passed.

Facebook’s plan to launch a version of Instagram for under 13s emerged earlier this year, also via investigative reporting — with BuzzFeed obtaining an internal memo which described “youth work” as a priority for Instagram.

But on Monday CEO Adam Mosseri said the company was “pausing” “Instagram kids” to take more time to listen to the countless child safety experts screaming at it to stop in the name of all that is good and right (we paraphrase).

Whether the social media behemoth will voluntarily make that “pause” permanent looks doubtful — given how much effort it’s expending to try to reframe the significance of its own research.

Though regulators may ultimately step in and impose child safety guardrails.

“Contrary to how the objectives have been framed, this research was designed to understand user perceptions and not to provide measures of prevalence, statistical estimates for the correlation between Instagram and mental health or to evaluate causal claims between Instagram and health/well-being,” Facebook writes in another reframing notation, before going on to “clarify” that the 30% figure (relating to teenaged girls who felt its platform made their body image issues worse) “only” applied to the “subset of survey takers who first reported experiencing an issue in the past 30 days and not all users or all teen girls”.

So, basically, Facebook wants you to know that Instagram “only” makes mental health problems worse for fewer teenage girls than you might have thought.

(In another annotation it goes on to claim that “fewer than 150 teen girls spread across… six countries answered questions about their experience of body image and Instagram”. As if to say, that’s totally okay then.)

The tech giant’s wider spin with the annotated slides is an attempt to imply that its research work shows proactive “customer care” in action — as it claims the research is part of conscious efforts to explore problems experienced by Instagram users so that it can “develop products and experience for support”, as it puts it.

Yeah we lol’d too.

After all, this is the company that was previously caught running experiments on unwitting users to see if it could manipulate their emotions.

In that case Facebook succeeded in nudging a bunch of users who it showed more negative news feeds to to post more negative things themselves. Oh and that was back in 2014! So you could say emotional manipulation is Facebook’s DNA…

But fast-forward to 2021 and Facebook wants you the public, and concerned parents everywhere, as well as U.S. and global lawmakers who are now sharpening their pens to apply controls to social media, not to worry about teenagers’ mental health — because it can figure out how best to push their buttons to make them feel better, or something.

Turns out, when you’re in the ad sales business, everything your product does is an A/B test against some poor unwitting “user”…",Social Media,TechCrunch,https://techcrunch.com/2021/09/30/seeking-to-respin-instagrams-toxicity-for-teens-facebook-publishes-annotated-slide-decks/,"The main undesirable consequence of Social Media discussed here is that it can have a negative impact on mental health, particularly among teenagers. Facebook's response has been to try and reframe the significance of its own research on the subject and to spin it in a way that implies it is taking proactive steps to improve user experiences and support mental health. However",Social Norms & Relationships
418,The Western Twitterer’s Burden – TechCrunch,"Sigh. Here we go again. The eyes of the world turn to something awful happening in a remote corner of Africa, and what feels like half of the Western population immediately rushes to proudly embarrass itself on social media everywhere. On the Internet, at least, #BringBackOurGirls is little more than #Kony2012 reloaded. It’s condescending, it’s patronizing, it’s infantilizing, and it’s dumb.

As my friend Gavin Chait puts it, in this excellent post:

I’m trying to imagine under what circumstances it would be acceptable for Samantha Cameron, the wife of British Prime Minister David Cameron, to appear on the front page of every major news service holding a cardboard sign saying #PrayforSandyHook … Yet, somehow, it’s noble when Michelle Obama holds up a sign saying #BringBackOurGirls? … When Kim Kardashian takes up your cause, you know you’ve hit rock bottom.

He quotes Jumoke Balogun in The Guardian:

Simple question. Are you Nigerian? Do you have constitutional rights accorded to Nigerians to participate in their democratic process? If not, I have news for you. You can’t do anything about the girls missing in Nigeria. You can’t. Your insistence on urging American power, specifically American military power, to address this issue will ultimately hurt the people of Nigeria.

Facebook and Twitter are outraged: a berserk group of Islamic madmen named Boko Haram, which means “Western Education Is A Sin,” kidnapped more than 200 girls from a school, and now must be hunted down by military forces, preferably of the American or British variety!

Except. Um. “Boko Haram” is not actually the group’s official name, nor does it exactly mean that. They have significant political connections in Nigeria, and it seems quite likely that this standoff will ultimately end in a negotiated deal/ransom, rather than some kind of dramatic military raid.

What’s more, Nigerian security forces have a long history of oppressing the region in which they operate, including, apparently, conducting their own massacres. As a Nigerian governor said last year: “When you burn down shops and massacre civilians, you are pushing them to join the camp of Boko Haram”. Or as the Defense Department’s principal director for African affairs put it:

Finding [Nigerian military] units that have not been involved in gross violations of human rights has been a “persistent and very troubling limitation”

See also this darkly hilarious cartoon from Foreign Policy:

If only Twitter could save Nigeria's stolen schoolgirls. Latest cartoon from @mattbors http://t.co/0wL9sUrXjr pic.twitter.com/3BEDOI9xCi — Foreign Policy (@ForeignPolicy) May 13, 2014

In what kind of hopeless basket case of a nation could this be happening? So glad you asked. Did you know that Nigeria’s economy has been growing at a remarkable 7% per year for the past decade, and was recently declared Africa’s largest? And did you know that Nigeria’s population is significantly larger than Russia’s, and that nearly one in five Nigerians has a smartphone?

I realize that the Internet is where nuance goes to die, but come on, people, can you at least begin to consider the possibility that the situation is a whole lot more complex than your social media streams made it sound?

(Meanwhile, of course, the American press are mostly desperately trying to spin this as an American left vs. American right issue, as that’s the only prism through which they know how to look at the world. This is even stupider than #BringBackOurGirls is in the first place. Sigh squared.)

Social media should help to open minds, but all too often instead it closes them. Westerners are presented with a cartoonish black-and-white version story like this, which jibes nicely with their preexisting belief that all of Africa is a cauldron of disaster, blood and bullets; then they share it, to help reinforce that mindset among their friends and family … and the vicious spiral of ignorance continues, while all along the real story is so much more complex and nuanced (and, in the case of Africa as a whole, interesting and optimistic) than hashtag activism can possibly admit.

Two years ago, outraged by Kony 2012, I wrote a piece here entitled “Save Helpless Faraway Africans From The Comfort Of Your Armchair!” The sad thing is that I could have repurposed that piece almost paragraph-by-paragraph here. So I’ll end by quoting myself at some length, while marveling at how little has changed. I should probably just keep it cued up for the next time this happens:",Social Media,TechCrunch,https://techcrunch.com/2014/05/17/the-western-twitterers-burden/,"It infantilizes complex social and political problems, reduces them to a cartoonish good-vs-evil narrative, and presents a false sense of efficacy to those who choose to engage in it.

This is a tragedy. The world is full of complex, fascinating, important stories, and the Internet is the perfect medium to bring them to life",Discourse & Governance
419,How ISIS and Russia Manufactured Crowds on Social Media,"Since November 2016, a national battle has raged about the role of social media in politics. People bemoaned the viciousness of trolls, the impact of incendiary fake news, the frog memes and Twitter bots and YouTube conspiracy videos. All the stories of manipulation and unintended consequences began igniting angry debates and prompted a long overdue conversation: what is the proper role of social networks in public discourse?

This important question stems from a new paradigm that started roughly a decade ago. That’s when social media turned everyone into a content creator, giving them the tools to not only say their piece but to amplify it, to grow an audience with little to no budget. Citizen journalists, bloggers, and grassroots activists bypassed the editorial old guard, gaining so much influence that they were elevated to an estate of the realm: The Fifth Estate.

Renee DiResta (@noUpside) is an Ideas contributor for WIRED, writing about discourse and the internet. She studies narrative manipulation as the director of research at New Knowledge, is a Mozilla fellow on media, misinformation and trust, and is affiliated with the Berkman-Klein Center at Harvard and the Data Science Institute at Columbia University. In past lives she has been on the founding team of supply chain logistics startup Haven, a venture capitalist at OATV, and a trader at Jane Street.

The social networks facilitated and enabled this new guard, simultaneously providing a captive user base, a virality engine infrastructure, no editorial oversight, and fairly limited rules. Not unexpectedly, the emergence of a relatively lawless federated system for reaching mass audiences attracted the attention of a bad actor.

Not Russia. ISIS.

The online battle against ISIS was the first skirmish in the Information War, and the earliest indication that the tools for growing and reaching an audience could be gamed to manufacture a crowd. Starting in 2014, ISIS systematically leveraged technology, operating much like a top-tier digital marketing team. Vanity Fair called them “The World’s Deadliest Tech Startup,” cataloging the way that they used almost every social app imaginable to communicate and share propaganda: large social networks such as Facebook; encrypted chat apps such as Telegram; messaging platforms including Kik and WhatsApp. They posted videos of beheadings on YouTube, and spoke to their followers on Internet radio stations. Perhaps most visibly, they were on Twitter, which they used for recruiting and for reach. Each time ISIS successfully executed an attack, they used Twitter to claim responsibility and tens of thousands of followers were ready to cheer them on with favorites and retweets. And in one of the pioneering instances of automated, manufactured crowds, thousands of bots were used for amplification and share-of-voice.

ISIS built a brand on social media. They had recognizable iconography—the flag, the colors, the high-production video openers—and by deftly using social media platforms, they built a virtual caliphate. They did it boldly and transparently, using the platforms in the way that they were meant to be used: to build an audience and connect with followers.

Social networks are designed to profit from enabling advertisers to grow, reach, or corral an audience. Growing an audience typically involves producing compelling content, aiming for social engagement and amplification, paying for boosted posts or ads (most of which are labeled in some way). Companies do it, grassroots organizers do it, and politicians do it. ISIS did it. And what they couldn’t achieve through organic growth, they simply manufactured.

Manufacturing a crowd is a bit different from growing an audience. Purchasing likes, ratings, followers, or bots; relying on automation to artificially amplify a message; gaming algorithms to get something trending or highly rated by a recommender system; using sockpuppets to leave comments and shape narratives. It’s mass deception: hard to detect, and societally corrosive.

Even in the presence of a overt terrorist organization manipulating their products, the platforms were slow to react. Twitter in particular was initially paralyzed by its commitment to being “the free speech wing of the free speech party”, and struggled to address the growing problem. As ISIS’ presence grew, articles were written throughout 2014 and 2015 about the “tough choice” the platforms faced as ISIS exploited them. Whither free speech? One man’s terrorist is another man’s freedom fighter. Etcetera, etcetera. And as the government began to plead with the platforms to take action, the EFF weighed in with a January 2016 statement titled “Companies Should Resist Government Pressure and Stand Up for Free Speech” arguing that “tech companies are not created to investigate terrorism.”",Social Media,WIRED,https://www.wired.com/story/isis-russia-manufacture-crowds/,"Social media platforms have been exploited by terrorist organizations to recruit followers, spread propaganda, and amplify messages through automation and manufactured audiences. This has resulted in a dangerous level of societally corrosive deception, and has been difficult to detect or address.",Security & Privacy
420,YouTube is closing its private messages feature…and many kids are outraged – TechCrunch,"People love to share YouTube videos among their friends, which is why in mid-2017 YouTube launched a new in-app messaging feature that would allow YouTube users to private-send their friends videos and chat within a dedicated tab in the YouTube mobile app. That feature is now being shut down, the company says. After September 18, the ability to direct-message friends on YouTube itself will be removed.

The change was first spotted by 9to5Google, which noted that YouTube Messages came to the web in May of last year.

YouTube, in its announcement about the closure, doesn’t offer much insight into its decision.

While the company says that its more recent work has been focused on public conversations with updates to comments, posts and Stories, it doesn’t explain why Messages is no longer a priority.

A likely reason, of course, is that the feature was under-utilized. Most people today are heavily invested in their own preferred messaging apps — whether that’s Messenger, WhatsApp, WeChat, iMessage or others.

Google, meanwhile, can’t seem to stop itself from building messaging apps and experiences. When YouTube Messages launched, Google was also invested in Allo (RIP), Duo, Hangouts, Meet, Google Voice and Android Messages/RCS, and was poised to transition users from Gchat (aka Google Talk) in Gmail to Hangouts Chat.

However, based on the nearly 500 angry comments replying to Google’s post about the closure, it seems that YouTube Messages may have been preferred by many young users.

Young…as in children.

A sizable number of commenters are complaining that YouTube was the “only place” they could message their friends because they didn’t have a phone or weren’t allowed to give out their phone number.

Some said they used the feature to “talk to their mom” or because they weren’t allowed to use social media.

It appears that many children had been using YouTube Messages as a sort of workaround to their parents’ block on messaging apps on their own phones, or as a way to communicate from their tablets or via web, likely without parents’ knowledge.

That’s not a good look for YouTube at this time, given its issues around inappropriate videos aimed at children, child exploitation, child predators and regulatory issues.

The video platform in February came under fire for putting kids at risk of child predators. The company had to shut off comments on videos featuring minors after the discovery of a pedophile ring that had been communicating via YouTube’s comments section.

Notably, the FTC is also now following up on complaints about YouTube’s possible violations of COPPA, a U.S. Children’s Privacy law. Child advocacy and consumers groups complain that YouTube has lured children under 13 into its digital playground, where it collects their data and targets them with ads, without parental consent.

Though some people may have used YouTube Messages to promote their channel or to share videos with family members and friends, it’s clear this usage hadn’t gone mainstream. Otherwise, YouTube wouldn’t be walking away from a popular product.

The feature also had issues with spam — much like Google+ did — as there were unwelcome requests from strangers, at times.

YouTube says users will still be able to share videos through the “Share” feature, which connects to other social networks.

The company declined to comment beyond what it shared on the forum post.",Social Media,TechCrunch,https://techcrunch.com/2019/08/21/youtube-is-closing-its-private-messages-feature-and-many-kids-are-outraged/,"YouTube's decision to shut down its in-app messaging feature has caused a stir as it had been used by many children as a workaround to their parents' block on messaging apps and without their parents' knowledge. This has raised concerns about child exploitation and the potential violation of COPPA, a U.S. Children's Privacy law.",Security & Privacy
421,The White House wants to know if you’ve been ‘censored or silenced’ by social media – TechCrunch,"The White House wants to know if you’ve been ‘censored or silenced’ by social media

It’s no secret that the Trump administration has been at war with social media. In the past year, the president has accused several online giants of censoring conservative voices, in particular giants like Twitter, Google and Facebook.

The Trump Administration is fighting for free speech online. No matter your views, if you suspect political bias has caused you to be censored or silenced online, we want to hear about it! https://t.co/9lc0cqUhuf pic.twitter.com/J8ICbx42dz — The White House (@WhiteHouse) May 15, 2019

Today, the White House launched a Typeform site aimed at collecting personal reports of social media censorship relating to political bias.

“SOCIAL MEDIA PLATFORMS should advance FREEDOM OF SPEECH,” the minimalistic site reads. “Yet too many Americans have seen their accounts suspended, banned, or fraudulently reported for unclear ‘violations’ of user policies.”

For those who feel they’ve been wronged in some way by one of the major platforms, the 16-part questionnaire lets you choose from a list including Facebook, Instagram, Twitter and YouTube, while inquiring about specific tweets that were censored or accounts that were targeted. Users can submit screenshots and other supporting evidence and opt in for “President Trump’s fight for free speech” after entering a name, email address, phone number and proving they’re real by answering a trivia question about the Declaration of Independence (take that, robots).

Twitter “SHADOW BANNING” prominent Republicans. Not good. We will look into this discriminatory and illegal practice at once! Many complaints. — Donald J. Trump (@realDonaldTrump) July 26, 2018

Trump has made “shadow banning” and other perceived slights against conservative voices a key cause in recent months. Last summer, he took to Twitter to address issues with the platform, writing, “Twitter ‘SHADOW BANNING’ prominent Republicans. Not good. We will look into this discriminatory and illegal practice at once! Many complaints.”

Late last month, the president met with Jack Dorsey for 30 minutes in the Oval Office, to discuss making Twitter “healthier and more civil,” according to the tech exec. No word on what the White House plans to do with the evidence it compiles.",Social Media,TechCrunch,https://techcrunch.com/2019/05/15/the-white-house-wants-to-know-if-youve-been-censored-or-silenced-by-social-media/,"The Trump Administration is collecting evidence of political bias and censorship on social media, as they are concerned that conservative voices are being unfairly silenced.","Information, Discourse & Governance"
422,How an overload of riot porn is driving conflict in the streets,"These narratives have been intensified and supplemented by the work of right-wing adversarial media-makers like Elijah Schaffer and Andy Ngo, who collect videos of conflict at public protests and recirculate them to their online audiences. Both have even gone “undercover” by posing as protesters to capture footage for their channels, seeking to name and shame those marching. Their videos are edited, decontextualized, and shared among audiences hungry for a new fix of “riot porn,” which instantly goes viral across the right-wing media ecosystem with the aid of influential pundits and politicians, including President Donald Trump. The footage has a hypnotic, almost balletic quality, designed to influence and overwhelm the sense-making capacity of watchers consuming it from a safe distance online.

Riot porn is different from videos of abuse and violence carried out by police, and we should not confuse one for the other. In the recording of the George Floyd murder, the video mobilized hundreds of thousands of people outraged that Floyd’s killer had not been arrested. With riot porn, what moves someone from watching to showing up is the potential for participating in a violent altercation. The motivating factor is the hope to live out fantasies of taking justice into their own hands, à la Dirty Harry, the film series about a rogue cop who shirks protocol and murders at will.

Judging by the reactions shared by followers of right-wing influencers, riot porn further enrages and traumatizes these audiences from afar, inflaming their perceptions of risk and danger. In chat rooms, watchers actively cheer as cops and other aggressors brutalize Black Lives Matter activists. These emotional reactions help develop an unshakable trust between the partisan content creators and the content consumers.

Propaganda machine

Outrage contagion is why the McCloskeys, the husband-and-wife duo who pointed guns at Black Lives Matter marchers in St. Louis, were motivated to take their bizarre stand. It’s also why the couple were invited to address the Republican National Convention—an audience being encouraged to vote for Donald Trump’s reelection. They spoke on topics such as guns, property, and the threat posed by “Marxist liberals.” Peering into the camera from a velvet couch, they read from the teleprompter: “What you saw happen to us could just as easily happen to any of you who are watching from quiet neighborhoods across our country … Make no mistake: no matter where you live, your family will not be safe in the radical Democrats’ America … You’ve seen us on your TV screens and Twitter feeds. You know we’re not the kind of people who back down.”

They are both the product of riot porn and participants in creating the next cycle of it. First, they consumed coverage of the St. Louis protests, during which a 7-Eleven was burned. This, the McCloskeys claim, prompted them to put fire extinguishers in every room and keep a rifle ready. When Black Lives Matter activists marched on their street, they reacted as if it were a death threat. It wasn’t, and their armed display resulted in felony charges. Right-wing media and Republicans have turned them into heroes alongside Rittenhouse, affectionately known as the “Kenosha Kid.”

The problem isn’t simply that riot porn exists, or that people with divergent opinions can interpret videos differently depending on their social-media feeds.

And it’s not just that social-media platforms don’t know what to do with white vigilante organizing. Although they clearly don’t: Facebook’s attempt to limit the spread of propaganda about Rittenhouse by blocking searches on his name is a drastic solution to a problem of its own making. Facebook took no action on 455 flags marking the Kenosha Guard’s event page as potentially dangerous. Memes in support of Rittenhouse and calls for armed MAGA rallies are on the rise.

The real difficulty is that competing accounts circulate in parallel universes that are not even close to balanced in their reach. Kevin Roose of the New York Times pointed out that the majority of people on Facebook are witnessing a radically different narrative from the one presented to consumers of mainstream media. In August, he tweeted, CNN saw 21 million Facebook interactions; the right-wing commentator Ben Shapiro gathered 55 million. This doesn’t mean there’s a silent majority of right-wing media consumers—in fact, it means that the right-wing media ecosystem continues to be dense and insular, which makes the propaganda feedback loop much more effective at shaping audience perceptions and world views. Harvard researchers Yochai Benkler, Rob Faris, and Hal Roberts have described this effect in their study “Networked Propaganda.”

Escalation, not reconciliation

It’s been almost three months since an independent group of United Nations experts called on the US government to conduct an independent investigation into racial terror. The reaction to the murder of George Floyd suggested for a moment that the US public was awakened to their country’s horrors and might be mobilized to do something.

And yet, as the events of the last week have shown, wave after wave of white vigilante violence has followed. Several cities and towns have become treacherous terrain, where militias, MAGA groups, and conspiracists are carrying out their civil war fantasies by attacking Black Lives Matter protesters as police look on. The optics are reminiscent of the Freedom Rides in the 1960s, when police turned away as local white vigilantes attacked civil rights advocates.

Last weekend in Portland, a long caravan of Trump supporters arrived with blacked-out license plates, paintball guns, actual guns, and bear mace; they drove their trucks straight at protesters. Some of these crimes were streamed live on Facebook and YouTube. Trump later justified their actions, saying that “paint is a defensive mechanism; paint is not bullets.” That same night, a member of the far-right group Patriot Prayer was murdered, but no details on the shooter are available. Nevertheless, the media narrative on the right is clear: antifa must have done it.

By using riot porn to incite fear in white people, the right-wing media ecosystem converts the real pain experienced by Black Americans into fodder for deranged, paranoid fantasies that white vigilantes must take up the functions of the police. Social-media companies need to work actively to prevent militias and vigilante groups from staging these armed standoffs. This includes shutting down event pages that are used as central organizing hubs and removing the accounts of those who are calling for physical violence. What played out in Portland and Kenosha this week is another confirmation that racial terror is real and getting worse.

—Joan Donovan is research director at Harvard Kennedy’s Shorenstein Center.",Social Media,MIT,https://www.technologyreview.com/2020/09/03/1007931/riot-porn-right-wing-vigilante-propaganda-social-media/,"The dangerous proliferation of ""riot porn"" on social media is creating a feedback loop of propaganda that is inciting white vigilantism and contributing to racial terror in the United States. Platforms must take active steps to shut down event pages and accounts used for organizing and calling for violence.",Equality & Justice
423,Why Generation Z falls for online misinformation,"Social media, however, promotes credibility based on identity rather than community. And when trust is built on identity, authority shifts to influencers. Thanks to looking and sounding like their followers, influencers become trusted messengers on topics in which they have no expertise. According to a survey from Common Sense Media, 60% of teenagers who use YouTube to follow current events turn to influencers rather than news organizations. Creators who have built credibility see their claims elevated to the status of facts while subject matter experts struggle to gain traction.

Young people are more likely to believe and pass on misinformation if they feel a sense of common identity with the person who shared it in the first place.

This, in large part, is how the rumor of plans for post-election violence went viral. The individuals who shared the warning were deeply relatable to their audience. Many were people of color and openly LGBT, and their past posts discussed familiar topics like family conflict and struggles in math class. This sense of shared experience made them easy to believe, even though they offered no evidence for their claims.

Making matters worse was the information overload many people experience on social media, which can lead us to trust and share lower-quality information. The election rumor appeared among dozens of other posts in teenagers’ TikTok feeds, leaving them with little time to think critically about each claim. Any efforts to challenge the rumor were relegated to the comments.",Social Media,MIT,https://www.technologyreview.com/2021/06/30/1026338/gen-z-online-misinformation/,"Social media can lead to the spread of misinformation and the elevation of influencers' claims to the status of facts, as people of a shared identity are more likely to trust and pass on false information without examining it critically. This has been especially evident in the election rumor that went viral.","Information, Discourse & Governance"
424,Study: Social Media Users More Likely to Be Misinformed About Vaccines,"Image by Air Force/Victor Tangermann

Going Viral

Right between the conspiracy theorists advocating for drinking bleach and your aunt on the airplane with the essential oils, vaccine misinformation has, in fact, found a home online. A recent survey found 20% of Americans are likely to be misinformed about vaccinations—and that they were more likely to have found their misinformation online than not.

Contagious

In 2019, the United States witnessed its worst measles outbreak in 25 years, even after being declared eliminated nationwide in 2000. This was the perfect time to track misinformation in real-time. Between the spring and fall of 2019, researchers from UPenn’s Annenberg Public Policy Center launched two surveys that found that during that period 19% of the respondents’ levels of vaccine misinformation changed in a substantive way – of those, 64% had become more misinformed. Where the information was found also mattered, as in: Those who found measles vaccine information on social media were more likely to be misinformed than those who sourced traditional media.

Surprise, Surprise

The team behind the survey noted their results only show correlation – not causation. But as families lose loved ones after choosing Facebook-sourced cures, their findings suggest that “increasing the sheer amount of pro-vaccination content in media of all types may be of value over the longer term.” As Facebook, Twitter, Instagram, and others all wrestle with misinformation on their platforms, the question remains: Will their actions alone be enough to give people the right information?

READ MORE: Vaccine Misinformation and Social Media [Annenberg Public Policy Center]

Advertisement

Advertisement

More on Misinformation: Google Is Trying to Teach Kids How to Spot Fake News

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Social Media,Futurism,https://futurism.com/neoscope/social-media-users-misinformed-vaccines,"Social media has been implicated in a recent rise of vaccine misinformation, with surveys showing that those who get their vaccine information from social media are more likely to be misinformed. This has been linked to a resurgence of measles in the US, and researchers suggest increasing the amount of pro-vaccination content in media of all types may be of value.","Information, Discourse & Governance"
425,UK offers cash for CSAM detection tech targeted at E2E encryption – TechCrunch,"The U.K. government is preparing to spend over half a million dollars to encourage the development of detection technologies for child sexual exploitation material (CSAM) that can be bolted on to end-to-end encrypted messaging platforms to scan for the illegal material, as part of its ongoing policy push around internet and child safety.

In a joint initiative today, the Home Office and the Department for Digital, Media, Culture and Sport (DCMS) announced a “Tech Safety Challenge Fund” — which will distribute up to £425,000 (~$584,000) to five organizations (£85,000/$117,000 each) to develop “innovative technology to keep children safe in environments such as online messaging platforms with end-to-end encryption”.

A Challenge statement for applicants to the program adds that the focus is on solutions that can be deployed within E2E-encrypted environments “without compromising user privacy”.

“The problem that we’re trying to fix is essentially the blindfolding of law enforcement agencies,” a Home Office spokeswoman told us, arguing that if tech platforms go ahead with their “full end-to-end encryption plans, as they currently are… we will be completely hindered in being able to protect our children online”.

While the announcement does not name any specific platforms of concern, Home Secretary Priti Patel has previously attacked Facebook’s plans to expand its use of E2E encryption — warning in April that the move could jeopardize law enforcement’s ability to investigate child abuse crime.

Facebook-owned WhatsApp also already uses E2E encryption so that platform is already a clear target for whatever “safety” technologies might result from this taxpayer-funded challenge.

Apple’s iMessage and FaceTime are among other existing mainstream messaging tools which use E2E encryption.

So there is potential for very widespread application of any “child safety tech” developed through this government-backed challenge. (Per the Home Office, technologies submitted to the Challenge will be evaluated by “independent academic experts”. The department was unable to provide details of who exactly will assess the projects.)

Patel, meanwhile, is continuing to apply high-level pressure on the tech sector on this issue — including aiming to drum up support from G7 counterparts.

Writing in a paywalled op-ed in a Tory-friendly newspaper, The Telegraph, she trails a meeting she’ll be chairing today where she says she’ll push the G7 to collectively pressure social media companies to do more to address “harmful content on their platforms”.

“The introduction of end-to-end encryption must not open the door to even greater levels of child sexual abuse. Hyperbolic accusations from some quarters that this is really about governments wanting to snoop and spy on innocent citizens are simply untrue. It is about keeping the most vulnerable among us safe and preventing truly evil crimes,” she adds.

“I am calling on our international partners to back the UK’s approach of holding technology companies to account. They must not let harmful content continue to be posted on their platforms or neglect public safety when designing their products. We believe there are alternative solutions, and I know our law enforcement colleagues agree with us.”

Today I am leading discussions with my G7 counterparts on how to keep our children safe online. I’m calling on them to back the UK’s approach of asking social media companies to put safety before profits. Read more at https://t.co/FZczZBwPgl pic.twitter.com/4PzcLvME1O — Priti Patel (@pritipatel) September 8, 2021

In the op-ed, the Home Secretary singles out Apple’s recent move to add a CSAM detection tool to iOS and macOS to scan content on user’s devices before it’s uploaded to iCloud — welcoming the development as a “first step”.

“Apple state their child sexual abuse filtering technology has a false positive rate of 1 in a trillion, meaning the privacy of legitimate users is protected whilst those building huge collections of extreme child sexual abuse material are caught out. They need to see th[r]ough that project,” she writes, urging Apple to press ahead with the (currently delayed) rollout.

Last week the iPhone maker said it would delay implementing the CSAM detection system — following a backlash led by security experts and privacy advocates who raised concerns about vulnerabilities in its approach, as well as the contradiction of a “privacy-focused” company carrying out on-device scanning of customer data. They also flagged the wider risk of the scanning infrastructure being seized upon by governments and states that might order Apple to scan for other types of content, not just CSAM.

Patel’s description of Apple’s move as just a “first step” is unlikely to do anything to assuage concerns that once such scanning infrastructure is baked into E2E encrypted systems it will become a target for governments to widen the scope of what commercial platforms must legally scan for.

However the Home Office’s spokeswoman told us that Patel’s comments on Apple’s CSAM tech were only intended to welcome its decision to take action in the area of child safety — rather than being an endorsement of any specific technology or approach. (And Patel does also write: “But that is just one solution, by one company. Greater investment is essential.”)

The Home Office spokeswoman wouldn’t comment on which types of technologies the government is aiming to support via the Challenge fund, either, saying only that they’re looking for a range of solutions.

She told us the overarching goal is to support ”middleground” solutions — denying the government is trying to encourage technologists to come up with ways to backdoor E2E encryption.

In recent years in the U.K. GCHQ has also floated the controversial idea of a so-called “ghost protocol” — that would allow for state intelligence or law enforcement agencies to be invisibly CC’d by service providers into encrypted communications on a targeted basis. That proposal was met with widespread criticism, including from the tech industry, which warned it would undermine trust and security and threaten fundamental rights.

It’s not clear if the government has such an approach — albeit with a CSAM focus — in mind here now as it tries to encourage the development of “middleground” technologies that are able to scan E2E-encrypted content for specifically illegal stuff.

In another concerning development, earlier this summer, guidance put out by DCMS for messaging platforms recommended that they “prevent” the use of E2E encryption for child accounts altogether.

Asked about that, the Home Office spokeswoman told us the tech fund is “not too different” and “is trying to find the solution in between”.

“Working together and bringing academics and NGOs into the field so that we can find a solution that works for both what social media companies want to achieve and also make sure that we’re able to protect children,” she said, adding: “We need everybody to come together and look at what they can do.”

There is not much more clarity in the Home Office guidance to suppliers applying for the chance to bag a tranche of funding.

There it writes that proposals must “make innovative use of technology to enable more effective detection and/or prevention of sexually explicit images or videos of children”.

“Within scope are tools which can identify, block or report either new or previously known child sexual abuse material, based on AI, hash-based detection or other techniques,” it goes on, further noting that proposals need to address “the specific challenges posed by e2ee environments, considering the opportunities to respond at different levels of the technical stack (including client-side and server-side).”

General information about the Challenge — which is open to applicants based anywhere, not just in the U.K. — can be found on the Safety Tech Network website.

The deadline for applications is October 6.

Selected applicants will have five months, between November 2021 and March 2022 to deliver their projects.

When exactly any of the tech might be pushed at the commercial sector isn’t clear — but the government may be hoping that by keeping up the pressure on the tech sector platform giants will develop this stuff themselves, as Apple has been.

The Challenge is just the latest U.K. government initiative to bring platforms in line with its policy priorities — back in 2017, for example, it was pushing them to build tools to block terrorist content — and you could argue it’s a form of progress that ministers are not simply calling for E2E encryption to be outlawed, as they frequently have in the past.

That said, talk of “preventing” the use of E2E encryption — or even fuzzy suggestions of “in between” solutions — may not end up being so very different.

What is different is the sustained focus on child safety as the political cudgel to make platforms comply. That seems to be getting results.

Wider government plans to regulate platforms — set out in a draft Online Safety bill, published earlier this year — have yet to go through parliamentary scrutiny. But in one already baked in change, the country’s data protection watchdog is now enforcing a children’s design code which stipulates that platforms need to prioritize kids’ privacy by default, among other recommended standards.

The Age Appropriate Design Code was appended to the U.K.’s data protection bill as an amendment — meaning it sits under wider legislation that transposed Europe’s General Data Protection Regulation (GDPR) into law, which brought in supersized penalties for violations like data breaches. And in recent months a number of social media giants have announced changes to how they handle children’s accounts and data — which the ICO has credited to the code.

So the government may be feeling confident that it has finally found a blueprint for bringing tech giants to heel.",Social Media,TechCrunch,https://techcrunch.com/2021/09/08/uk-offers-cash-for-csam-detection-tech-targeted-at-e2e-encryption/,"The UK government is preparing to spend over half a million dollars to encourage the development of detection technologies for child sexual exploitation that can be used on social media platforms, raising concerns that the scanning infrastructure will become a target for governments to widen the scope of what platforms must legally scan for, potentially compromising user privacy.",Security & Privacy
426,France puts Facebook on notice over WhatsApp data transfers – TechCrunch,"Facebook and WhatsApp have been issued with formal notices by France’s data protection watchdog warning that data transfers being carried out for ‘business intelligence’ purposes currently lack a legal basis — and consequently that Facebook Inc, WhatsApp’s owner, has violated the French Data Protection Act.

WhatsApp has been given a month to remedy the situation or could face additional investigation by the CNIL — and the potential for a sanction to be issued against it in future.

In August 2016 the social networking giant caused massive controversy when its messaging platform WhatsApp announced a privacy U-turn — saying it would shortly begin sharing user data with its parent, Facebook, and Facebook’s network of companies, despite the founder’s prior publicly stated stance that user privacy would never be compromised as a result of the Facebook acquisition.

WhatsApp’s founder, Jan Koum, had also assured users that ads would not be added to the platform. However the data-sharing arrangement with Facebook included “ad-targeting purposes” among its listed reasons.

Users were offered an opt-out, but only a time-limited one — which also required they actively read through terms & conditions to find and uncheck a default-checked box to prevent information such as their mobile phone number being shared with Facebook for ad targeting (shared phone numbers enabling the company to link a user’s Facebook profile and activity with their WhatsApp account).

The company’s subsequent teeing up of a monetization strategy for WhatsApp, via the forthcoming launch of business accounts, likely explains its push to link users of the end-to-end encrypted messaging platform with Facebook users, where the same people have likely engaged in far more public digital activity — such as liking pages, searching for content, and making posts and comments that Facebook is able to read.

And that’s how a platform giant which owns multiple social networks is able to circumvent the privacy firewall provided by e2e encryption to still be able to perform ad-targeting. (Facebook doesn’t need to read your WhatsApp messages because it has a granular profile of who you are, based on your multi-years of Facebook activity… And while business accounts don’t constitute literal ‘display ads’, in the traditional sense, they clearly open up ample targeting opportunities for Facebook to engineer once it links all its user profiling data.)

In May this year Facebook was fined $122M by the European Commission for providing “incorrect or misleading” information at the time of its 2014 acquisition of WhatsApp — when it had claimed it could not automatically match user accounts between its own platform and WhatsApp. And then three years later was doing exactly that.

In the European Union another twist to this story is that Facebook’s data transfers between WhatsApp and Facebook for ads/product purposes were quickly suspended — the CNIL confirms in its notice that Facebook told it the data of its 10M French users have never been processed for targeted advertising purposes — after local regulators intervened, and objected publicly that Facebook had not provided users with enough information about what it planned to do with their data, nor secured “valid consent” to share their information. Another bone of contention was over the opt-out being time-limited to just a 30-day window.

However the CNIL’s intervention now is based on a continued investigation of the data transfers covering the two other areas Facebook claimed it would be using the WhatsApp user data for — namely security and “evaluation and improvement of services” (aka business intelligence).

And while the regulator seems satisfied that security is a valid and legal reason to transfer the data — writing that “it seems to be essential to the efficient functioning of the application” — business intelligence is another matter, with CNIL noting the purpose here “aims at improving performances and optimizing the use of the application through the analysis of its users’ behavior”.

“The chair of the CNIL considered that the data transfer from WhatsApp to Facebook Inc. for this ‘business intelligence’ purpose is not based on the legal basis required by the Data Protection Act for any processing,” it continues. “In particular, neither the users’ consent nor the legitimate interest of WhatsApp can be used as arguments in this case.”

The watchdog asserts that user consent is “not validly collected” because it is neither specified for this purpose (rather it is only listed as processing “in general”); it also says it is not ‘free’ — in the sense of users being able to refuse the transfer; with the only option if they do not agree being to uninstall the application.

“On the other hand, the company WhatsApp cannot claim a legitimate interest to massively transfer data to the company Facebook Inc. insofar as this transfer does not provide adequate guarantees allowing to preserve the interest or the fundamental freedoms of users since there is no mechanism whereby they can refuse it while continuing to use the application,” it adds.

Reached for comment a Facebook spokesperson provided the following statement:

Privacy is incredibly important to WhatsApp. It’s why we collect very little data, and encrypt every message. We will continue to work with the CNIL to ensure users understand what information we collect, as well as how it’s used. And we’re committed to resolving the different, and at times conflicting concerns, we’ve heard from European Data Protection Authorities with a common EU approach before the General Data Protection Regulation comes into force in May 2018.

The spokesperson failed to respond to specific questions we put to it about its WhatsApp data transfer activity in Europe. But did confirm that WhatsApp-Facebook data transfers for product/ads remain paused across the region.

In its formal notice to Facebook, the French watchdog sharply criticizes the company for failing to co-operate with its investigation — writing that its departments “repeatedly asked” WhatsApp to provide a sample of the French users’ data transferred to Facebook Inc only to be told that “it could not supply the sample requested by the CNIL since, as it is located in the United States, it considers that it is only subject to the legislation of this country”.

“The CNIL, which is competent the moment an operator processes data in France, was therefore unable to examine the full extent of the compliance of the processing implemented by the company with the Data Protection Act because of the violation of its obligation to cooperate with the Commission under Article 21 of the Act,” it writes.

It also criticizes WhatsApp for “insufficiently” co-operating with its investigation — saying it made it difficult to determine how data was being processed.

The CNIL adds that it decided to make the formal notice public in order to raise awareness of the “massive data transfer from WhatsApp to Facebook Inc and thus to alert to the need for individuals concerned to keep their data under control”.

It also makes a point of emphasizing that the data transfer has increased in the amount of information the company has at its disposal — “including information about individuals who have not registered for its social network”. (The CNIL has previously ordered Facebook to stop tracking non-users.)",Social Media,TechCrunch,https://techcrunch.com/2017/12/19/france-puts-facebook-on-notice-over-whatsapp-data-transfers/,"Social media giant Facebook has been found to be in violation of French Data Protection Act by transferring data from WhatsApp to Facebook for 'business intelligence' purposes without a legal basis, thereby infringing on user privacy and allowing them to be targeted for ads without their express consent.",Security & Privacy
427,Think Before You Tweet In the Wake of an Attack,"Monday night, a suicide bomber took the lives of at least 22 people---including an 8-year-old girl---at an Ariana Grande concert in Manchester, England. Almost instantly, images and video of the devastating attack overtook Twitter timelines and Facebook News Feeds. As natural and understandable a response to horrific events as that might be, it also threatens to amplify the chaos that terrorists intend.

Terrorists have always sought attention, and the age of social media has enabled them to find it with unprecedented breadth. They use social networks to recruit, to inspire, and to connect, but they also rely on social media bystanders---everyday, regular people---to spread the impacts of their terror further than they could themselves, and to confuse authorities with misinformation. That amplification encourages more terrorism, inspires copycats, and turns the perpetrators into martyrs. It also traumatizes the families of the murdered victims, as well as the public at large.

""In the last few years, this problem has become more acute and more complicated technically, practically, and ethically, with the acceleration of the news cycle and the advent of social media,"" London School of Economics professor Charlie Beckett wrote for the Columbia Journalism Review last year, analyzing how social media and journalism amplify terrorist messaging.

The moment a bomb explodes anywhere in the world, the blast is heard around the internet. The next time it clamors through your social feeds, keep the following in mind.

Gut Reaction

""#Pray for Manchester,"" a tweet from Monday read, including a cell phone video of the moment the apparent suicide bomb went off, showing people terrified, running for their lives: 594 people had retweeted it at press time. Major media outlets played videos like that one and others from the scene on a loop in the hours after the attack. Tuesday morning, they still were, despite the images providing no new information.

When a terrorist attacks, the world responds by spreading the horror far and wide. ""We’re programmed now that, anytime anything happens, somebody has their phone and somebody is filming it,"" says Steven Stalinksy of the Middle East Research Institute, who studies how terrorists use the internet.

The motivation for terrorism is not mere murder or maiming but the incitement of deep fear in an entire community or nation. To achieve that aim, terrorists need the media's help. That applies both to the news networks, which often play the same scene on loop despite having no new information, and to social media, where people rush to express their concern and outrage. Along the way, misinformation and fear spreads like wildfire. That's the playbook for these events now, and research suggests it gives terrorists exactly what they want.

""Public mass-murder terrorism---religious-inspired to white-supremacist to school shootings---has a media strategy. Media keeps cooperating,"" author and researcher Zeynep Tufekci, an expert in how information spreads online, wrote on Twitter Monday night, admonishing people to not share images of dead bodies and videos of fear over and over on a loop.",Social Media,WIRED,https://www.wired.com/2017/05/think-tweet-wake-attack/,"Terrorists use social networks to spread their terror far and wide, inspire copycats, and turn themselves into martyrs, thereby traumatizing victims’ families and the public.",Security & Privacy
428,Singapore passes controversial ‘fake news’ law which critics fear will stifle free speech – TechCrunch,"Singapore has passed a controversial bill that could equip the government with extensive powers to police online media and free speech.

The bill was first drafted last month and, as had been expected, it passed 72-9 in Singapore’s parliament, dominated by the ruling People’s Action Party (PAP), late on Wednesday.

As we reported last month, the bill caused concern through its potential to stifle free speech, as a key feature enables the government, and in fact any minister, to force “corrections” to be added to online content that is deemed to be “false.”

Beyond media, the flex also extends to social media. According to the law, those found to be “malicious actors” face a fine of up to SG$50,000 ($37,000) or five years in prison for their content. If posted using “an inauthentic online account or a bot,” the fine jumps to a maximum of SG$100,000 ($74,000) or a potential 10-year jail term. Platforms like Facebook and Twitter face fines of up to SG$1 million ($740,000) for their role in such situations.

Designed to cover “a false statement of fact… has been or is being communicated in Singapore” or cases where politicians believe that issuing a correction is “in the public interest,” the bill also claims reach overseas — or, rather, intended reach overseas. Politicians can trigger it in situations judged to be “in the interest of friendly relations of Singapore with other countries.”

It remains to be seen how much success the Singapore government will have with its efforts. Domestic media may well be under control — the World Press Freedom Index ranks Singapore 151 out of 183 countries and self-censorship is common — but influencing newsrooms based overseas and social networks will likely prove difficult.

Facebook, for example, last November resisted calls to remove content flagged as defamatory by the government. That clearly frustrated officials.

“This shows why we need legislation to protect us from deliberate online falsehoods,” the Ministry of Law wrote in an announcement at the time.

How a takedown would work — and how the government might access encrypted chats on apps like WhatsApp and Telegram, which are also part of its focus — also remains unclear at this point.

The law has been criticized by free speech groups.

“Singapore’s new ‘fake news’ law is a disaster for online expression by ordinary Singaporeans, and a hammer blow against the independence of many online news portals they rely on to get real news about their country beyond the ruling People’s Action Party political filter,” Human Rights Watch deputy Asia director Phil Robertson wrote on Twitter.

“Singapore’s leaders have crafted a law that will have a chilling effect on internet freedom throughout Southeast Asia, and likely start a new set of information wars as they try to impose their narrow version of “truth” on the wider world,” he added.

Human Rights Watch — which came out with strong criticism of the bill last month — was criticized by the Singapore government last month, which hit back at “its long-standing practice of issuing biased and one-sided statements about Singapore.”

Meanwhile, on the more assistive end of the dissenting voices, the Asia Internet Coalition — a group that represents Facebook, Google, Twitter, LinkedIn, Line and others — penned an editorial in Singapore’s Straits Times newspaper suggesting changes to the bill.

The opinion piece — which, irony alert, is restricted by a paywall — recommended specific processes, an imperial body to vet decisions, exemptions for opinion articles, satire and more, as well as a request for “clear and well-defined language and scope.”

Robertson is concerned that other counties in Southeast Asia will take the ball Singapore has punted and run with it, thereby creating other restrictive online content policies. That’s already happened to some extent. In Vietnam, a draconian cybersecurity law went into operation on January 1 while Thailand passed a controversial law in February granting a wide scope of powers to authorities.",Social Media,TechCrunch,https://techcrunch.com/2019/05/09/singapore-fake-news-law/,"The new Singapore bill has been heavily criticized for its potential to stifle online free speech and its ability to force corrections to content deemed ""false"". The law has also raised concerns that other countries in Southeast Asia will follow suit, leading to a region-wide clampdown on online content.","Information, Discourse & Governance"
429,Facebook buys ads in Indian newspapers to warn about WhatsApp fakes – TechCrunch,"As Twitter finally gets serious about purging fake accounts, and YouTube says it will try to firefight conspiracy theories and fake news flaming across its platform with $25M to fund bona fide journalism, Facebook-owned WhatsApp is grappling with its own fake demons in India, where social media platforms have been used to seed and spread false rumors — fueling mob violence and leading to number of deaths in recent years.

This week Facebook has taken out full-page WhatsApp-branded adverts in Indian newspapers to try to stem the tide of life-threatening digital fakes spreading across social media platforms in the region with such tragic results.

It’s not the first time the company has run newspaper ads warning about fake news in India, though it does appear to be first time it’s responded to the violence being sparked by fakes spreading on WhatsApp specifically.

The full-page WhatsApp anti-fakes advert also informs users that “starting this week” the platform is rolling out a new feature that will allow users to determine whether a message has been forwarded. “Double check the facts when you’re not sure who wrote the original message,” it warns.

""Question information that upsets you"", says WhatsApp's full-page advertisements. Clearly the solution to declining newspaper ad revenues in India will come from how we tackle our digital fake news crisis. pic.twitter.com/3h5XyJeMIr — Anuj Srivas (@AnujSrivas) July 10, 2018

This follows tests WhatsApp was running back in January when the platform trialed displaying notifications for when a message had been forwarded many times.

Evidently WhatsApp has decided to take that feature forward, at least in India, although how effective a check it will be on technology-accelerated fakes that are likely also fueled by local prejudices remains to be seen.

Trying to teach nuanced critical thinking when there may be a more basic lack of education that’s contributing to fomenting mistrust and driving credulity, as well as causing the spread of malicious fakes and rumors targeting certain people or segments of the population in the first place, risks both being ineffectual and coming across as merely irresponsible fiddling around the edges of a grave problem that’s claimed multiple lives already.

Facebook also stands accused of failing to respond quickly enough to similar risks in Myanmar — where the UN recently warned that its platform was being weaponized to spread hate speech and used as a tool to fuel ethnic violence.

Reuters reports that the first batch of WhatsApp fake ads are running in “key Indian newspapers”, and images posted to Twitter show an English-language full-page advert — so you do have to question who these first ads are really intended to influence.

But the news agency reports that Facebook also intends to publish similar ads in regional dailies across India over the course of this week.

We’ve reached out to WhatsApp with questions and will update this story with any response.

“We are starting an education campaign in India on how to spot fake news and rumours,” a WhatsApp spokesman told Reuters in a statement. “Our first step is placing newspaper advertisements in English and Hindi and several other languages. We will build on these efforts.”

The quasi-educational WhatsApp fake news advert warns users about “false information”, offering ten tips to spot fakes — many of which boil down to ‘check other sources’ to try to verify whether what you’ve been sent is true.

Another tip urges WhatsApp users to “question information that upsets you” and, if they do read something that makes them “angry or afraid”, to “think twice before sharing it again”.

“If you are not sure of the source or concerned that the information may be untrue, think twice before sharing,” reads another tip.

The last tip warns that “fake news often goes viral” — warning: “Just because a message is shared many times, does not make it true.”

In recent times, Facebook has also run full-page ads in newspapers to apologize for failing to safeguard user data in the wake of the Cambridge Analytica scandal, and taken out print adverts ahead of European elections to warn against attempts to spread fake news to try to meddle with democratic processes.",Social Media,TechCrunch,https://techcrunch.com/2018/07/10/facebook-buys-ads-in-indian-newspapers-to-warn-about-whatsapp-fakes/,"Social media platforms such as WhatsApp have been used to spread false rumors, leading to mob violence and a number of deaths in recent years in India. Facebook is now trying to address this issue by running full-page anti-fake news adverts in the country's newspapers.",Security & Privacy
430,Jack Dorsey on Donald Trump,"It was only 11 years ago that Jack Dorsey first sent a message via the experimental messaging platform that would be known as Twitter. He thought of it as a real-time status tool—a kind of smoke signal that would let your friends know where you were, what was on your mind, and whether you hiccuped. He did not imagine that one day the President of the United States would use it to mock beauty queens, misrepresent his electoral victory, and offend international allies and foes alike.

Steven Levy is Backchannel's founder and Editor in Chief. Sign up to get Backchannel's weekly newsletter, and follow us on Facebook, Twitter, and Instagram.

Today Dorsey is pleased that Twitter quickly evolved to become a worldwide means of surfacing the zeitgeist in real time. He is much less pleased that it has been used to harass women. And though Twitter has grown to a public company worth $10 billion, that’s a big drop from the $48 billion Wall Street valuation after its 2013 IPO. It has over 300 million regular users, and today reported that the number is on the rise, growing by nine million in the last quarter—but critics gripe that growth has been stagnant, and Facebook has almost 2 billion!

I’ve known Dorsey for more than a decade, and followed him as he left the company, returned in 2011, and became the CEO (again) in 2015, adding that job to his CEO post at Square, the payments company he founded. Soft-spoken and unflappable, he has made significant product changes in recent months (a ranked timeline as opposed to pure chronology, a number of features to improve safety) — and has had the dubious pleasure of seeing Donald Trump use Twitter as his personal megaphone, for good and ill (mostly ill). I recently sat down with him at Twitter’s New York City office. We discussed how he is addressing Twitter’s harassment problem, what he is doing to make the company grow again, and, of course, the tweeting president.

Steven Levy: When did you realize that Trump was going to use Twitter as a central platform for communicating?

Jack Dorsey: He’d been using it as a central platform for his communication since 2011, 2012. He hasn’t really changed his behavior. He has been fairly consistent; he just changed what role he is speaking for. But, like, his tweets today are consistent with his tweets back in 2011-2012.

But weren’t you surprised that he kept doing that as a candidate and then as a president?

I wasn’t surprised. If you were him, why change the momentum of what made you win in the first place?

Now that he has won, there’s a question of whether Twitter should hold a president accountable to the same standards as other users. At Facebook, Mark Zuckerberg reportedly told employees he was not going to censor a nominee’s—and then a president’s—posts. Did you have to make a decision on that?

I think it’s really important that we maintain open channels to our leaders, whether we like what they’re saying or not, because I don’t know of another way to hold them accountable. Any time we have any leader tweet, including Trump, there’s a very interesting and thriving conversation. A mixture of fact checking, disagreement, agreement, and some random things.

We hold all accounts to the same standards on our policy, and we want to make sure that independent of who you are or where you’re coming from, you understand the guidelines, what our policies are, and what that means. We now have 11 years of a corpus of opinions, statements, emotions, facts, falsehoods—everything you can imagine. It’s all archived in the Library of Congress, as well, in real time. It’s really interesting right now that people are taking the present day and going back to previous statements. So the public nature of the platform, and the fact that tweets stick around, is becoming critical to accountability.",Social Media,WIRED,https://www.wired.com/2017/04/jack-dorsey-on-donald-trump/,"The main undesirable consequence discussed here of social media is the potential for it to be used to harass, spread falsehoods, and to allow people in positions of power to avoid accountability.",Equality & Justice
431,Search and social media was filled with clickbait and propaganda in the wake of Vegas shooting – TechCrunch,"Search and social media was filled with clickbait and propaganda in the wake of Vegas shooting

In the wake of what is now the worst mass-shooting in U.S. history, thousands of people turning to social media for information on the unfolding investigation earlier this morning would have found many of the top posts on most of the major websites to be hot garbage.

Letting an algorithm cull links from the sewer of internet commentary, and then distributing that to millions of people, is a losing game. It’s another sign of how Facebook and the rest continue to abdicate responsibility.

Google and the social media sites say they’re working on improving their hit rate for better quality news sources, but today’s spread in the aftermath of the Las Vegas shooting shows just how much more work they have to do.

Over the course of the morning, Facebook’s “Safety Check” page included updates on the shooting from a far-right-wing blogger who had accused the shooter of being a ” far left loon.” The top post on the site then moved to feature a clickbait video from a news aggregation service, MyTVToday, before finally settling on reports from local and national news outlets.

Facebook has put The Gateway Pundit at the top of its feed today. Google News promoted a 4chan thread. Silicon Valley, we have a problem. — Ed Bott (@edbott) October 2, 2017

Facebook wasn’t alone in recirculating conjecture and outright lies on its marquee pages. One of the rumors that had been circulating on the site 4chan misidentified the shooter as a man named Geary Danley appeared in Google’s top stories widget (as Buzzfeed and Bloomberg reported).

Google released the following statement to Bloomberg and The New York Times (we’ve also reached out for comment):

“Unfortunately, early this morning we were briefly surfacing an inaccurate 4chan website in our Search results for a small number of queries. Within hours, the 4chan story was algorithmically replaced by relevant results. This should not have appeared for any queries, and we’ll continue to make algorithmic improvements to prevent this from happening in the future.”

The algorithms that social media sites like Twitter and Facebook use to determine which stories to display have been making headlines themselves, as more attention is paid to the information they’re distributing.

The hoaxes listed by Buzzfeed that showed up on Twitter alone should be enough to convince @Jack that something needs to be done about the trolls willfully setting dumpster fires in the middle of the service’s vaunted news stream.

It’s a legitimate question to ask how sources like 4chan would even be considered a viable outlet from which to source information for breaking news. And, indeed, some reporters are already asking it.

I realize how difficult it is to surface the right information in real time, but I don't understand how 4chan is part of the pool of sites? — Daisuke Wakabayashi (@daiwaka) October 2, 2017

Google, Twitter and Facebook are under the microscope already for their response to the Russian hacking scandal currently being investigated by Congress. And this latest misstep in their treatment of a story that has shaken the country, with so many dead and injured, and so much still unknown, just underscores how problematic their reliance on software can be — and the real world consequences that it can have.",Social Media,TechCrunch,https://techcrunch.com/2017/10/02/search-and-social-media-was-filled-with-clickbait-and-propaganda-in-the-wake-of-vegas-shooting/,"The algorithms used by social media sites to determine which stories to display have been called into question, as they have been found to be surfacing inaccurate information, hoaxes and clickbait, causing confusion and spreading false information.","Information, Discourse & Governance"
432,"Why AI needs more social workers, with Columbia University’s Desmond Patton – TechCrunch","Sometimes it does seem the entire tech industry could use someone to talk to, like a good therapist or social worker. That might sound like an insult, but I mean it mostly earnestly: I am a chaplain who has spent 15 years talking with students, faculty, and other leaders at Harvard (and more recently MIT as well), mostly nonreligious and skeptical people like me, about their struggles to figure out what it means to build a meaningful career and a satisfying life, in a world full of insecurity, instability, and divisiveness of every kind.

In related news, I recently took a year-long paid sabbatical from my work at Harvard and MIT, to spend 2019-20 investigating the ethics of technology and business (including by writing this column at TechCrunch). I doubt it will shock you to hear I’ve encountered a lot of amoral behavior in tech, thus far.

A less expected and perhaps more profound finding, however, has been what the introspective founder Prayag Narula of LeadGenius tweeted at me recently: that behind the hubris and Machiavellianism one can find in tech companies is a constant struggle with anxiety and an abiding feeling of inadequacy among tech leaders.

In tech, just like at places like Harvard and MIT, people are stressed. They’re hurting, whether or not they even realize it.

So when Harvard’s Berkman Klein Center for Internet and Society recently posted an article whose headline began, “Why AI Needs Social Workers…”… it caught my eye.

The article, it turns out, was written by Columbia University Professor Desmond Patton. Patton is a Public Interest Technologist and pioneer in the use of social media and artificial intelligence in the study of gun violence. The founding Director of Columbia’s SAFElab and Associate Professor of Social Work, Sociology and Data Science at Columbia University.

A trained social worker and decorated social work scholar, Patton has also become a big name in AI circles in recent years. If Big Tech ever decided to hire a Chief Social Work Officer, he’d be a sought-after candidate.

It further turns out that Patton’s expertise — in online violence & its relationship to violent acts in the real world — has been all too “hot” a topic this past week, with mass murderers in both El Paso, Texas and Dayton, Ohio having been deeply immersed in online worlds of hatred which seemingly helped lead to their violent acts.

Fortunately, we have Patton to help us understand all of these issues. Here is my conversation with him: on violence and trauma in tech on and offline, and how social workers could help; on deadly hip-hop beefs and “Internet Banging” (a term Patton coined); hiring formerly gang-involved youth as “domain experts” to improve AI; how to think about the likely growing phenomenon of white supremacists live-streaming barbaric acts; and on the economics of inclusion across tech.

Greg Epstein: How did you end up working in both social work and tech?

Desmond Patton: At the heart of my work is an interest in root causes of community-based violence, so I’ve always identified as a social worker that does violence-based research. [At the University of Chicago] my dissertation focused on how young African American men navigated violence in their community on the west side of the city while remaining active in their school environment.

[From that work] I learned more about the role of social media in their lives. This was around 2011, 2012, and one of the things that kept coming through in interviews with these young men was how social media was an important tool for navigating both safe and unsafe locations, but also an environment that allowed them to project a multitude of selves. To be a school self, to be a community self, to be who they really wanted to be, to try out new identities.",Social Media,TechCrunch,https://techcrunch.com/2019/08/09/why-ai-needs-more-social-workers-with-columbia-universitys-desmond-patton/,"Social media has been linked to an increase in community-based violence, as it provides a platform for individuals to project various versions of themselves and become involved in dangerous activities. It has also been linked to mental health issues, such as anxiety and depression, as well as cyberbullying. As such, it is important for social workers to be",Social Norms & Relationships
433,Sri Lanka blocks social media sites after deadly explosions – TechCrunch,"The government of Sri Lanka has temporarily blocked access to several social media services following deadly explosions that ripped through the country, killing at least 207 people and injuring hundreds more.

Eight bombings were reported, including during Easter services at three churches, on the holiest weekend of the Christian calendar.

In a brief statement, the Sri Lankan president’s secretary Udaya Seneviratne said the government has “decided to temporarily block social media sites including Facebook and Instagram,” in an effort to curb “false news reports.” The government said the services will be restored once the investigations into the attacks had concluded.

Sri Lanka’s prime minister Ranil Wickremesinghe has described the explosions as a terrorist incident.

Nalaka Gunawardene, a science writer and Sri Lankan native, confirmed in a tweet that Facebook-owned WhatsApp was also blocked in the country. Others reported that YouTube was inaccessible. But some said they were able to still use WhatsApp.

Facebook spokesperson Ruchika Budhraja told TechCrunch: ““Our hearts go out to the victims, their families and the community affected by this horrendous act. Teams from across Facebook have been working to support first responders and law enforcement as well as to identify and remove content which violates our standards. We are aware of the government’s statement regarding the temporary blocking of social media platforms. People rely on our services to communicate with their loved ones and we are committed to maintaining our services and helping the community and the country during this tragic time.”

A spokesperson for Google did not immediately comment.

It’s a rare but not unprecedented step for a government to block access to widely used sites and services. Although Sri Lanka’s move is ostensibly aimed at preventing the spread of false news, it’s likely to have an inhibiting effect on freedom of speech and efforts to communicate with loved ones.

Sri Lanka, like other emerging nations, has previously battled with misinformation. The government has complained that false news shared on Facebook has helped spread hatred and violence against the country’s Muslim minority. Other countries like India say encrypted messaging app WhatsApp has contributed to the spread of misinformation, prompting the social media company to add limits to how many groups a message can be sent to.

Iran and Turkey have also blocked access to social media sites in recent years amid protests and political unrest.

Updated with comment from Facebook.",Social Media,TechCrunch,https://techcrunch.com/2019/04/21/sri-lanka-social-media-explosions/,"Sri Lanka's temporary block of social media services has raised concerns about freedom of speech and communication with loved ones, and may have an inhibiting effect on the spread of accurate information.",Security & Privacy
434,An NFL player went on Facebook Live from the locker room and nothing good happened – TechCrunch,"Yesterday after the Steelers upset the Chiefs in an AFC playoff game, Steelers wide receiver Antonio Brown decided to go on Facebook Live from the locker room to celebrate with his fans. And the fans loved it – he went live for 17 minutes and had almost 900,000 views within just a few hours, before the video was deleted.

A player using technology to celebrate directly with his fans in real-time. Great, right? Not so much.

A few things. First, the NFL’s social media policy prohibits players from using any form of social media starting 90 minutes before the game starts, up until after the post-game press conference end. While no fine has been announced yet, ESPN’s Adam Schefter noted the a league official said Brown “could be fined” for the video stream.

Secondly, the NFL’s broadcast partners are contractually the only ones allowed to shoot video in the locker room after a game – teams and even the league itself can’t even use that footage until 24 hours after the game ends. And while this incident probably isn’t big enough for NBC (who broadcasted the game) to complain to the NFL, it’s definitely a sign of potential issues in the future as livestreaming becomes a more important part of our social media culture.

And lastly, NFL locker rooms are traditionally pretty off-limits when it comes to the press and access to the outside world. Football is a competitive sport, and confidential plays, speeches and interactions give teams the advantage they need to win. So you could imagine how upset the Steelers were when they found out that the Facebook Live broadcast captured audio of Head Coach Mike Tomlin’s post game speech happening in the background.

Tomlin’s very NSFW speech (which you can find on YouTube) was basically a minute long rant against the Patriots (and their Microsoft Surface hating coach Bill Belichick), who the Steelers will play next week. While I’m sure stuff like this is being said in all 32 locker rooms across the league, it’s still a bad look for Tomlin and the team.

Oh, and of course Belichick was asked what he thought of the incident, since his team was the one getting trash talked. And in typical fashion his response was about as un-technologically savvy as it gets – saying that he’s “not on SnapFace”, and not worried about what they put on InstantChat”.",Social Media,TechCrunch,https://techcrunch.com/2017/01/16/an-nfl-player-went-on-facebook-live-from-the-locker-room-and-nothing-good-happened/,"The main undesirable consequence of social media discussed here is the potential for NFL players to violate the league's social media policies and broadcast confidential team activities, as was the case with Steelers wide receiver Antonio Brown when he livestreamed from the locker room after their playoff game. This incident could potentially lead to fines for Brown, and it also revealed an audio",Security & Privacy
435,Should Tech Companies Be Subject To The Fourth Amendment? – TechCrunch,"In the wake of the San Bernardino shootings, Sen. Dianne Feinstein, D-Calif., re-introduced a bill that would require social media companies and other online services to contact the authorities when they have “actual knowledge” of terrorist activities.

The bill states that companies would not be required to review content – only that they must report terrorist activities that they learn of. Technology companies noted that they already work with authorities to prevent terrorism. Opponents criticized the bill, saying that it would actually discourage technology companies from voluntarily monitoring their services for terrorist content and working with law enforcement.

Looming in the background of this debate is a little-discussed but very important obstacle: the Fourth Amendment of the United States Constitution. As courts grapple with how to apply the Fourth Amendment’s privacy provisions to online crime fighting, it may become difficult for law enforcement and technology companies to collaborate to fight terrorism and other crimes.

Here’s the basic problem: Criminal defendants are increasingly arguing that any evidence that their online service providers turn over to law enforcement cannot be introduced at trial because the service providers violated the Fourth Amendment by conducting a warrantless, unreasonable search.

Sounds bizarre? Well, that’s because it is. If you think back to your high school civics class, you’ll remember that the Bill of Rights restricts the powers of the federal government, not private companies or individuals.

There is, however, an increasingly important, court-created exception to this rule: Fourth Amendment protections apply to searches conducted by private parties who act as “agents” of a government. How do you know if a private citizen or company acts as a government agent? The legal definitions and tests vary by court, but generally, a court will consider the degree of control that the government exercised over the private party’s search and whether the private party had an independent reason, unrelated to law enforcement, to conduct the search (such as a business justification).

This issue is arising increasingly in criminal cases in which online service providers turn over information that the government ultimately seeks to use as evidence of a crime. If a service provider is found to have conducted a warrantless search as a government agent, the criminal defendant may be able to prevent the court from considering not only the evidence that the service provider gave to the government, but any subsequent discoveries due to that initial evidence.

For instance, some email providers, cloud companies, and other online service providers use automated tools to scan user content for known child pornography images. No law requires them to conduct this scanning – in fact, federal law explicitly states that they are not obligated to scan for child pornography. Nonetheless, many companies scan because they believe that it is in the best interests of their users – and, ultimately, the companies’ bottom line – to keep their services free of this illegal content.

The services are, however, required by federal law to file a report with to the nonprofit National Center for Missing and Exploited Children if they ever obtain actual knowledge of apparent child pornography on their services. NCMEC then reviews the report and collaborates with law enforcement.

For years, this system has been a remarkably effective tool for fighting the online distribution of child pornography. But recently, criminal defendants have begun to argue that the service providers and NCMEC are subject to the Fourth Amendment because they are government agents. In other words, they claim that unless the companies and NCMEC obtain a warrant, all evidence of child pornography resulting from the initial scan cannot be used against them in trial.

Many courts have rejected this argument, concluding that both the companies and NCMEC operate outside of government control and for their own reasons that are independent of law enforcement. More troubling is a misguided November 2013 ruling from a federal judge in Massachusetts, which held that NCMEC is a government agent because its examination of a file provided by a service provider “was a search conducted for the sole purpose of assisting the prosecution of child pornography crimes.”

Fortunately, that same court held that the service provider was not a government agent because it had sufficient independent business reasons for creating a safe online environment. And a number of other courts have rejected the Massachusetts judge’s conclusion that NCMEC was a government agent.

But the “government agent” argument is arising with increased frequency when online service providers voluntarily collaborate with law enforcement. The U.S. Court of Appeals for the Tenth Circuit heard oral arguments in such a case last month.

What does this mean for efforts to encourage tech companies to collaborate with law enforcement in fighting terrorism? It means that it might be difficult to use in a criminal trial any terrorism-related information that the companies provide to the government.

To be sure, identifying terrorist activities is quite different than gathering evidence of child pornography – in the former, the goal often is to prevent an imminent attack, so there likely is less concern about evidentiary issues for a subsequent trial (if one would ever occur). The Fourth Amendment would not protect social media posts that are shared with the general public, though it would apply to private information. Even for private information, a number of courts likely would reject the government agency argument, as social media companies and other tech providers have a valid business reason to keep their services free of terrorist propaganda.

But the recent child pornography cases tell us that, at the very least, there is uncertainty surrounding the Fourth Amendment status of such evidence. And laws that require collaboration between technology companies and the government could increase the chances of such complications, as they might lead some courts to believe that the technology company’s terrorism prevention activities are intertwined with the government.

There are no easy solutions to this problem, as it ultimately is a constitutional decision for the courts. I predict that in the not-too-distant future, the United States Supreme Court will need to weigh in on these Fourth Amendment issues. When it does, I hope that it gives both technology companies and the government sufficient breathing room to collaborate on crime prevention.",Social Media,TechCrunch,https://techcrunch.com/2015/12/13/should-tech-companies-be-subject-to-the-fourth-amendment/,"The Fourth Amendment of the United States Constitution could make it difficult for law enforcement and technology companies to collaborate to fight terrorism and other crimes, as criminal defendants increasingly argue that any evidence that their online service providers turn over to law enforcement cannot be introduced at trial due to it being a warrantless, unreasonable search.",Security & Privacy
436,Airport Panics Will Keep Happening. The Place to Calm Them Is Online,"The tweets came in fast and hot: an active shooter at John F. Kennedy Airport. In my Twitter feed, the disparate folks I follow---techies, politicos, my friend's mom---all started to converge on the same breaking story. It would soon have commandeered the whole world's attention, if it were true. But, just like what happened this week at Los Angeles International Airport, it wasn't.

The bad social media smell started to rise early. All-caps tweets about coming face-to-face with the perpetrator. Passively voiced reports that the presence of a JFK shooter ""can't be confirmed"". It was narrative chaos locked into a destructive feedback loop with the bedlam on the ground. Frightened passengers rushed for exits, pushed their way onto the tarmac, and cowered in terror as police flooded the scene. All along the way, travelers tweeted, streamed, and Instagrammed their distress, an online panic that generated its own kind of terror despite the non-existent physical threat.

When passengers decide on their own that there's a security issue, there's no playbook for that. Joshua Marks

And who would have done otherwise? Responsible journalists know they have to confirm the news before reporting it; that obligation becomes all the more paramount when the news has the potential to spur a stampede. But when everyone with a phone can broadcast their version of events to the world as they happen, the story will get out before the facts do. On Sunday, just two weeks after the JFK incident, the same social media-fueled pandemonium engulfed LAX. Like a misheard meme gone viral, an eccentric in a Zorro costume and a few loud noises morphed in the collective imaginations of LAX passengers and law enforcement alike into a potential mass killer. The Federal Aviation Administration grounded flights to the airport as travelers spilled onto the airfield. Never mind that no one had ever fired a shot. Americans today now reasonably measure the time between massacres in months. And that fear, combined with social media's sweeping immediacy, also makes weaponizing bad information all too easy.

""We're in new territory with socially driven panic,"" says Joshua Marks, executive vice president of aviation connectivity at Global Eagle Entertainment. The Transportation Security Administration has procedures in place for clearing the terminal when someone breaches checkpoint security, he says. ""When passengers decide on their own that there's a security issue, there's no playbook for that.""

Controlling the Narrative

Social media, like any media, facilitates storytelling. The problem at JFK and LAX was that the story had no author. Or everyone was the author, which amounts to the same thing. Because no voice of authority cut through the noise, everyone in those terminals followed their own signal.

One prevailing theory of the JFK panic is that people somehow misheard a noisy celebration of Usain Bolt's Olympic victory as an outbreak of violence. (Side note: Are you kidding me?) From there, bad information spread faster than the Jamaican sprinter himself. The story even leapt terminals. One Port Authority official told The New York Times that reports ricocheting across social media platforms may have misled others into believing the purported shooter was in Terminal 1, not Terminal 8, where the false rumor apparently first flared up.

Authorities must move as swiftly and convincingly online as the bad information itself.

Watching the JFK fact-free panic unfold from the comfort of my couch across the country, as so many others did, was like watching a fog roll across San Francisco Bay---unstoppable and all-obscuring. Yes, as the Times reports, the incident gave airport officials the opportunity to see the flaws in their own security procedures that left them ill-equipped to separate the facts from the freakout. The police, it seems, found themselves caught in as much confusion as travelers on the ground or anyone else looking at Twitter. Maybe now they can develop a system to counteract that.

But even if the authorities themselves had come to a quicker consensus about a possible threat, the information infrastructure simply isn't in place to quell the fires of panic stoked by so much social media FUD. Twitter, Facebook, Instagram, Snapchat, and WhatsApp have become default ways information flows in public spaces. (Ever notice how many people are on their phones at the airport?) But in so many cases, authorities haven't figured out how to use those platforms to cut through the morass of misinformation---that is, to exert guidance over the narrative.

Airports need to rethink how they handle such incidents, Marks says, and he believes the recent LAX panic will serve as a case study. Airport operators need to work with the TSA and airlines to coordinate a response when the problem isn't on the ground but online. Counterintuitively, that means having more online first responders closer to the real-world action. Airlines have centralized social media hubs that in many cases have become skilled and efficient at handling customer complaints. But to act as authoritative arbiters of information, they also need to build up decentralized networks of social media-empowered employees in the airports closer to what's really happening.

In New York, Governor Andrew Cuomo has ordered a review of the JFK incident that should provide the definitive account of that night. Undoubtedly it will take months. But narratives today take shape in seconds. To combat panic at a time when law enforcement jargon like ""active shooter"" has become a part of everyday speech, people need a counter-narrative they know they can trust and act on in the moment. Authorities must move as swiftly and convincingly online as the bad information itself. They can't hope that social media will simply fade as other fads might, an irritating obligation to ""get on Twitter."" Communicating effectively in an information-saturated environment isn't easy. But if protecting public safety is the goal, the savvy wielding of facts is key to keeping the peace.",Social Media,WIRED,https://www.wired.com/2016/09/airport-panics-will-keep-happening-place-calm-online/,"Social Media has the potential to cause widespread panic with the spread of false information, as seen in the JFK and LAX incidents, with passengers being unable to differentiate between true and false threats.",Security & Privacy
437,"Twitter’s API Crackdown May Be Bad For Users, Even If They Never Notice – TechCrunch","Twitter’s API Crackdown May Be Bad For Users, Even If They Never Notice

This week Twitter launched its new API. There are still lots of questions about what this means for developers, and what role developers have played in Twitter’s rise. But the general consensus seems to be that it doesn’t matter much for most users.

Apparently most users just use Twitter’s official clients and supposedly will never notice if Twitter bans most third party clients. Even if that’s true, I think it’s a real shame for the future of social media.

Twitter has turned TweetDeck into a Twitter/Facebook client only, dropping support for LinkedIn and Foursquare and dashing hopes of any other integrations being added in the future. I’m afraid it’s only a matter of time before it becomes a Twitter-only product. The old version still works, but I’m guessing that after the March 5 deadline it will break.

Meanwhile, it sounds like Seesmic will be rolled into HootSuite, meaning Hootsuite may end up with monopoly on cross-network social media clients. It’s bad enough that there may end up being a monopoly, but I’m worried that for individual users the choice may evaporate entirely.

“If someone is a casual user of Seesmic, go this way [e.g. to Twitter]. If you are an SME we are happy to help you you,” HootSuite CEO Ryan Holmes told Ingrid earlier this week.

Does this mean the free and “pro” versions of Hootsuite will go away and that Hootsuite will be available only to enterprise customers? I’m wondering if Twitter’s new API will affect HootSuite’s ability to serve free and pro users. I asked HootSuite for clarification, but as of this writing haven’t heard back. But my fellow TechCrunch writer Ingrid Lunden tells me that it sounds like the company is planning to keep the free and pro plans around for a long time.

And with Twitter working closely with HootSuite on cross-selling advertising, it seems like there is some potential monetization of the free product that’s agreeable to Twitter.

Update: Here’s the statement I got from HootSuite: “We are absolutely committed to continuing our current freemium model offering free along with our paid Pro and Enterprise plans.”

But how will other social networks will react? The Google+ API has always been limited. You can at least post to a Google+ page from HootSuite now, but it’s still a far cry from being a full fledged Google+ client.

I know it costs money to keep these services available, and I know Twitter and Google and Facebook are supposed to be in the business of making money and not the techno-hippie federated love-fest business, but come on — is forcing everyone to juggle multiple official (and perhaps crappy) apps really going to make shareholders more money? I guess we’ll have to wait and see.

On the other hand, maybe walled gardens will end up being good. Less spammy cross-posting, people spread out over fewer networks. Maybe we’ll look back on the freewheeling open API days like we look at the browser crashing Myspace profile days.",Social Media,TechCrunch,https://techcrunch.com/2012/09/08/twitters-api-crackdown-may-be-bad-for-users-even-if-they-never-notice/,"The new API crackdown by Twitter may lead to a lack of choice for users who prefer third-party clients, resulting in either the need to use multiple official apps or a monopoly on cross-network social media clients.",User Experience & Entertainment
438,Multi-media journalists face jail time after reporting on North Dakota pipeline protest – TechCrunch,"Investigative reporter and co-founder of Democracy Now!, Amy Goodman, is now facing riot charges in the state of North Dakota after her report on a Native American-led pipeline protest there went viral on Facebook.

Democracy Now! issued a statement about the new charges against Goodman late Saturday.

The news organization, which spun out of WBAI-FM, creates programming which is syndicated via radio, podcasts, cable television, public access television, live streams and Web downloads.

Goodman’s story, posted to Facebook on September 4th, has been viewed more than 14 million times on the social media platform, Democracy Now! said, and was picked up by mainstream media outlets and networks including CBS, NBC, NPR, CNN, MSNBC and The Huffington Post (a site owned by TechCrunch’s parent company Verizon).

Additionally, documentary filmmaker Deia Schlosberg, is facing felony and conspiracy charges that could carry a 45-year sentence for filming at another pipeline protest in the state, IndieWire reports.

Edward Snowden noted Schlosberg’s predicament on Friday with a tweet that said, “This reporter is being prosecuted for covering the North Dakota oil protests. For reference, I face a mere 30 years.”

This reporter is being prosecuted for covering the North Dakota oil protests. For reference, I face a mere 30 years. https://t.co/GzMbwCVV40 — Edward Snowden (@Snowden) October 14, 2016

Authorities released Schlosberg, who also runs a production studio called Pale Blue Dot Media, after originally detaining her but they confiscated her footage and refused to release it according to public tweets from Josh Fox, a fellow filmmaker.

For those unfamiliar with the pipeline protests, the Standing Rock Sioux are seeking to halt the construction of a $3.8 billion pipeline saying its development will encroach on their tribal burial sites and taint their water supply at the Standing Rock Sioux Reservation.

And environmentalists in a group called Climate Direct Action are seeking to stop the movement of tar sands from Canada into the U.S. via a pipeline operated by TransCanada in Wallhalla, North Dakota. Schlosberg was filming the activists as they interfered with normal operations of the pipeline’s valves.

The demonstrations in North Dakota have been ongoing for months. Native American advocates and environmentalists have protested the pipeline’s development in other cities and states, as well.

On October 10th, witnesses at a rally in Reno, Nevada captured footage of a pickup truck plowing into a group of activists protesting the pipeline’s development, and calling for Columbus Day to be changed to Indigenous Peoples’ Day in their state.

Several posted their footage and thoughts about the apparent attack on Facebook as well. The Reno incident injured five and sent one to the hospital. The rally was organized by the American Indian Movement of Northern Nevada (AIMNN).

It remains to be seen whether the charges against Goodman, Schlosberg and other journalists covering the Standing Rock protests against the Dakota Access pipeline will stick.

But these cases highlight the increasing power, and risks, associated with online distribution for news stories covered by independents, and earlier missed by mainstream networks. Virality and independence, it seems, can attract prosecutorial ire.

Updates: A paragraph was added to this post to reflect the fact that the two journalists, Amy Goodman and Deia Schlosberg, were reporting on different pipeline protests within the state of North Dakota.",Social Media,TechCrunch,https://techcrunch.com/2016/10/15/multi-media-journalists-face-jail-time-for-reporting-on-north-dakota-pipeline-protest/,"The cases of Amy Goodman and Deia Schlosberg highlight the risks associated with independent news stories that go viral on social media, as they can attract prosecutorial ire.",Equality & Justice
439,"Social Media Made the Arab Spring, But Couldn't Save It","Five years ago this week, massive protests toppled Egyptian President Hosni Mubarak, marking the height of the Arab Spring. Empowered by access to social media sites like Twitter, YouTube and Facebook, protesters organized across the Middle East, starting in December 2010 in Tunisia, and gathered together to speak out against oppression, inspiring hope for a better, more democratic future. Commentators, comparing these activists to the US peace protesters of 1968, praised the effort as a democratic dawn for an area that had long been populated by autocracies. In a photo collection published by The New York Times a few months later, Irish writer Colum McCann wrote: ""The light from the Arab Spring rose from the ground up; the hope is now that the darkness doesn’t fall.""

Social media, it turns out, was not a new path to democracy, but merely a tool.

The darkness has fallen. Half a decade later, the Middle East is roiling in violence and repression. Activists are being intimidated into restraint by governments that are, with the exception of Tunisia, more totalitarian than those they replaced, if any government as such really exists at all. Meanwhile, militants have harnessed the same technology to organize attacks and recruit converts, catapulting the world into instability. Instead of new robust democracies, we have a global challenge with no obvious solution. The Arab Spring carried the promise that social media and the Internet were going to unleash a new wave of positive social change. But the past five years have shown that liberty isn't the only end toward which these tools can be turned.

Activists were able to organize and mobilize in 2011 partly because authoritarian governments didn’t yet understand very much about how to use social media. They didn’t see the potential, says NYU professor of politics Joshua Tucker, a principle investigator at the Social Media and Political Participation Lab at New York University. “There are a lot of reasons the people in power were slow to pick up on this,” he adds. “One of the things about not have a free press is it is harder to learn what was going on in the world.”

AP

Spreading Misinformation

Today, governments take an aggressive hand in shutting down digital channels people use to organize against them. In Egypt, for example, where 26 million people are on Facebook (up from 4.7 million people in 2011), security forces arrested three people who administered nearly two dozen Facebook pages, according to Egyptian media reports. It also detained activists who had been involved in prior protests. And at the end of December, the government shut down Facebook’s Free Basics service, which had offered free Internet services to Egyptians on mobile phones. More than 3 million people had signed up for the program in just two months, according to Facebook. Meanwhile Turkey has made 805 requests for tweets to be removed since 2012, according to Twitter’s most recent transparency report; more than half were made last year.

These governments have also become adept at using those same channels to spread misinformation. “You can now create a narrative saying a democracy activist was a traitor and a pedophile,” says Anne Applebaum, an author who directs a program on radical political and economic change at the Legatum Institute in London. “The possibility of creating an alternative narrative is one people didn’t consider, and it turns out people in authoritarian regimes are quite good at it.”

The tools that catalyzed the Arab Spring, we've learned, are only as good or as bad as those who use them.

Even when activists are able to get their messages out, they have trouble galvanizing people to actually take action. The sentiments that gain the largest audiences often contain religious elements, according to Mansour Al-hadj, who is a director at the Middle East Media Research Institute. “The message by itself without any religious element in it, wouldn’t work in the long run,” he says. “The activists’ accounts on Twitter and Facebook are very active and they have a lot of followers, but they cannot drive masses,” he says, because their sentiments are more moderate.

Laced through media coverage of the Arab Spring was what turned out to be the naïve hope that people were inherently, unequivocally good and that unleashing their collective consciousness via social media would naturally result in good things happening. But it turns out that consciousness was not so collective after all. The tools that catalyzed the Arab Spring, we've learned, are only as good or as bad as those who use them. And as it turns out, bad people are also very good at social media. Militant groups like the Islamic State have been reported to recruit converts using Facebook and Twitter and use encrypted communications technology to coordinate attacks.

To be sure, the Arab Spring protests—and the subsequent political protests from Occupy Wall Street to Russian demonstrations in 2012—were significant. They introduced a new form of political and social organizing, of ""hyper-networked protests, revolts, and riots."" But we’re just beginning to understand the impact of this new communications technology. Social media, it turns out, was not a new path to democracy, but merely a tool. And for a few brief months, only the young and the idealistic knew how it worked.",Social Media,WIRED,https://www.wired.com/2016/01/social-media-made-the-arab-spring-but-couldnt-save-it/,"In the five years since the Arab Spring, social media has been used by governments to shut down digital channels, spread misinformation, and radicalize people to violence, showing that the same tools that can be used for positive social change can just as easily be used for negative ends.",Politics
440,Gruesome Jihadi Content Still Flourishes on Facebook and Google+,"Facebook announced this week that algorithms catch 99.5 percent of the terrorism-related content it deletes before a single user reports it. Thanks to steadily advancing AI tools, that's an improvement from last year, when that figure hovered around 97 percent. But promising as those developments may be, a new report by the internet safety nonprofit Digital Citizens Alliance demonstrates how easy it still is to find grisly images of dead bodies, calls to jihad, and ISIS and Al Qaeda imagery on both Facebook and Instagram.

The report contains dozens of screenshots of beheadings and terrorist recruitment content linked to accounts that, as of this week, remained live on both platforms. It also includes links to even more graphic content that lives on Google+, a platform that has largely gone undiscussed amid its parent company Alphabet’s overtures about eliminating radical content on both YouTube and Google Search.

""It seems based on everything we know the platforms are stuck in a loop. There’s criticism, promises to fix, and it doesn't go away,"" says Tom Galvin, executive director of the Digital Citizens Alliance, which has conducted research on topics like the sale of counterfeit goods and illicit drugs online.

Working with researchers at the Global Intellectual Property Enforcement Center, or GIPEC, the Digital Citizens Alliance amassed a trove of evidence documenting terrorist activity on these online platforms. The researchers used a combination of machine learning and human vetting to search for suspicious keywords and hashtags, then scoured the networks connected to those posts to find more. On Instagram and Facebook, they found users sharing copious images of ISIS soldiers posing with the black flag. One Instagram account reviewed by WIRED on Tuesday posted a photo of two men being beheaded by soldiers in black face masks. By Wednesday, that particular photo had disappeared, but the account, which has posted a slew of equally disturbing images including executions and dead bodies strewn on the sidewalk, remained live. It's not clear whether the post was deleted by the user or by Instagram.

In many cases, the most hideous photos contained captions with innocuous hashtags in Arabic, including #Dads, #Girls, and #Cooking. Below are some of the researchers' more tame discoveries.",Social Media,WIRED,https://www.wired.com/story/jihadi-content-still-on-facebook-google/,"The Digital Citizens Alliance's research has revealed how easy it is to find grisly images of dead bodies, calls to jihad, and ISIS and Al Qaeda imagery on both Facebook and Instagram. Despite AI tools improving the platforms' capabilities to detect such content, this report shows how it still lingers, with innocuous hashtags masking horrific images",Security & Privacy
441,Can Anything Take Down the Facebook Juggernaut?,"Sometime in early 2004, as Mark Zuckerberg was furiously coding the first iterations of The Facebook in his Harvard dorm room, the Internet passed what then seemed to be an impressive milestone: 750 million people worldwide had become connected. The exact birthdate of the Internet is difficult to pin down, but it's fair to say that it took at least three decades for the net to reach a population of that size.

Today, after just eight years in existence, Facebook now has more than 750 million users all by itself. At that astonishing rate of growth, the company is on track to accomplish much more than just a multibillion-dollar IPO. Facebook is on the cusp of becoming a medium unto itself—more akin to television as a whole than a single network, and more like the entire web than just one online destination. The evidence for that transformation goes well beyond the sheer number of users. Many businesses now bypass the traditional web altogether, limiting their online presence to Facebook. Already the platform has spawned one billion-dollar company (the social gaming giant Zynga) and swallowed another (the photo network Instagram). The average time people spend on the site has increased from four and a half hours per month in 2009 to nearly seven hours—more than twice that of any major web competitor.

Also in this issue

Facebook's growing dominance suggests that the platform may very well represent the third major evolution of the network age. First the Internet popularized the crucial organizing principles of peer-to-peer architecture and packet-switched data. Then the web ushered in a new set of governing metaphors that were fundamentally literary in nature: a network of ""pages"" and footnote-like links. Powerful as they were, though, both those platforms were organized around data, not people. From a computer scientist's perspective, this might not have seemed like a shortcoming. But most human beings don't naturally organize the world through metaphors of domains or hypertext; instead they mentally map the world according to their social networks of friends, family, and colleagues.

So it should come as no surprise that we now find ourselves gravitating toward a new platform grounded in those social maps. And the bigger we make the platform, the stronger its gravitational pull. The Internet—meaning everything from email to file trading to voice-over-IP phone calls—was always technically larger than the web, but the web's mass adoption managed somehow to overwhelm the vessel that contained it. The web became the main attraction; the packets and DNS lookups became the plumbing, essential but invisible. Facebook now threatens to perform that same jujitsu against the web itself. The difference, of course, is that no one owns the web—or in some strange way we all own it. But with Facebook we are ultimately just tenant farmers on the land; we make it more productive with our labor, but the ground belongs to someone else.

The sheer magnitude of Facebook's success is one reason why, as the company charges toward what will likely be the most successful public offering in the history of capitalism, its critics are growing in number. Troublesome corporate behavior is easier to swallow when there are other choices out there, when you have the option to take your business to another store down the street. But when one company owns the whole street, each little transgression is amplified. A few years ago, the primary critique of Facebook revolved around its prodigious capacity for wasting our time. Today the complaints run much deeper: Facebook, we're told, is a threat to core social values, to privacy, to the web itself.",Social Media,WIRED,https://www.wired.com/2012/05/mf-facebook/,"The rise of Facebook has led to numerous criticisms, from its prodigious capacity for wasting time to its potential as a threat to social values, privacy, and the web itself.",Security & Privacy
442,"Gab, the Alt-Right's Very Own Twitter, Is The Ultimate Filter Bubble","The Internet has a speech regulation problem. To a lot of people (including WIRED), harassment and hate speech are corruptions of the democratization promised by the Web, and websites like Facebook, Twitter, and Instagram take constant flack for not dealing with the problem adequately enough. To the group calling itself the alt-right, which is really another word for white supremacists, any moderation looks like censorship. Their anger at being supposedly sidelined and silenced has spawned hashtag campaigns, think pieces, and now, a brand-new social media platform, Gab. Its primary schtick is promising an end to censorship. But by sequestering itself, Gab has managed to sideline it members further into an echo chamber so far removed from the rest of the conversation that its message has no chance of reaching unfamiliar ears.

Gab is less than a month old, so it may well flame out like Peach or Ello. But for now, the platform looks like an artifact from a dystopian universe where the alt-right completely took over Twitter. Gab has over 42,000 people on its waitlist, more than 11,000 active members, and among them are nearly all the alt-right's online kingpins, including the Internet's self-styled super-villain, Breitbart writer Milo Yiannopoulos himself, who was recently banned for life from Twitter. And Gab's appeal for that crowd is obvious. The only posting guidelines are no illegal porn, no threats of violence, no terrorism, and no doxing. Oh, and a fifth commandment that literally says ""try to be nice."" Everything else is fair game. Notably absent? Any explicit stipulations against hate speech.

""We promote raw, rational, open, and authentic discourse online,"" says Gab CEO Andrew Torba. ""We want everyone to feel safe on Gab, but we're not going to police what is hate speech and what isn’t."" Yet in its first weeks, rather than create a free-speech zone where all voices are heard and anyone can say anything to anyone, Gab directly caters to a narrow, conservative, provocateur sensibility. The trending hashtags? Usually stuff like #HillarysHealth and #HitlerPickUpLines. So far Gab is less a censorship-free utopia than an alt-right safe space---which is ironic, considering how much the alt-right loathes the very idea of safe space.

""People say we’re an alt-right echo chamber,"" says Torba. ""But if there are any centrists, progressives, libertarians, or apolitical people interested in trying something new, I say, please join us."" Thing is, so far, few seem to have answered that call. In the #GabFam, as Gab users call themselves, #Trump seems to always chart. The word ""deplorable"" is getting a lot of play after Hillary Clinton used it last week, and Gab's logo is a distinctly Pepe-ish looking frog. The frog logo might seem unrelated, but it's not. The alt-right has taken the once versatile and universally popular Pepe the Frog meme as its mascot. For a flood of anti-Semitic, white supremacist tweets from people with variations on Pepe as their profile image, check out #frogtwitter. Or poke around on Gab.

Here are some very typical Gab posts, screen-shotted from the site's Popular page.",Social Media,WIRED,https://www.wired.com/2016/09/gab-alt-rights-twitter-ultimate-filter-bubble/,"The rise of alt-right echo chambers on platforms such as Gab has resulted in a proliferation of hate speech and other forms of harassment that go unchecked, creating an unwelcome environment that potentially silences other voices.",Equality & Justice
443,Why Facebook and Twitter Can't Just Wipe Out ISIS Online,"Given that ISIS and other terrorist organizations have proven adept at using social media to disseminate propaganda and incite fear, it seems obvious that platforms like Facebook and Twitter would aggressively and mercilessly delete such content and ban those who post it.

It may seem equally obvious that those companies would move quickly to do just that when presidential candidates appear to call for them to help out and as US Representative Joe Barton asks the Federal Communications Commission, ""Isn't there something we can do under existing law to shut those Internet sites down?"" But it's not that simple, and social media platforms have grappled with the issue in some ways since at least the days when Al Qaeda affiliates started uploading videos to YouTube.

The problem lies in the global nature of social media, the reliance upon self-policing by users to identify objectionable content, and the fact that many of those banned simply open a new account and continue posting their hatred. A blanket policy of banning anything that might be seen as inciting violence also could lead to questions of censorship, because one person's hateful propaganda could be another's free speech. That's not to say companies like Facebook and Twitter aren't taking this seriously and trying to draw a distinction between the two. But it's not as simple as you might think.

'No Place for Terrorists'

Facebook says any profile, page, or group related to a terrorist organization is shut down and any content celebrating terrorism is removed. ""There is no place for terrorists on Facebook,” says Facebook spokesman Andrew Souvall. “We work aggressively to ensure that we do not have terrorists or terror groups using the site, and we also remove any content that praises or supports terrorism.”

It seems to broadly work. Facebook has deleted posts and blocked accounts in such a way that ISIS-related newsletters, videos, and photos don't seem to crop up as much as elsewhere on the web, says Steve Stalinsky, executive director of Middle East Media Research. “Of all the companies, they're the leader and the best at removing content,” he says.

In the past few years, the use of Twitter, on the other hand, has grown. ISIS supporters embraced the platform in the latter part of the last decade, Stalinsky says, when old-school web forums regulated by moderators remained popular among Al Qaeda members. According to research from the Brookings Institute, ISIS supporters used some 46,000 Twitter accounts between September and December 2014, though not all were active at the same time.

Until last fall, Twitter had largely taken a more detached stance on ISIS-related content. It began taking a more aggressive approach after videos and images of journalist James Foley's beheading spread on social media. Brookings Institute researcher J.M. Berger says the increase in suspensions of Twitter accounts seen in recent months has had a measurable effect. While an active social network typically grows over time, Berger says that the suspensions on Twitter have helped to keep the size of the network ""roughly flat."" Moreover, users whose accounts are repeatedly suspended come back with new accounts with fewer followers.",Social Media,WIRED,https://www.wired.com/2015/11/facebook-and-twitter-face-tough-choices-as-isis-exploits-social-media/,"Social media platforms have had difficulty dealing with terrorist organizations using their platforms to spread propaganda and incite fear, as policies of banning content can lead to questions of censorship.",Security & Privacy
444,Beware the hidden bias behind TikTok resumes – TechCrunch,"Social media has served as a launchpad to success almost as long as it has been around. The stories of going viral from a self-produced YouTube video and then securing a record deal established the mythology of social media platforms. Ever since, social media has consistently gravitated away from text-based formats and toward visual mediums like video sharing.

For most people, a video on social media won’t be a ticket to stardom, but in recent months, there have been a growing number of stories of people getting hired based on videos posted to TikTok. Even LinkedIn has embraced video assets on user profiles with the recent addition of the “Cover Story” feature, which allows workers to supplement their profiles with a video about themselves.

As technology continues to evolve, is there room for a world where your primary resume is a video on TikTok? And if so, what kinds of unintended consequences and implications might this have on the workforce?

Why is TikTok trending for jobs?

In recent months, U.S. job openings have risen to an all-time high of 10.1 million. For the first time since the pandemic began, available jobs have exceeded available workers. Employers are struggling to attract qualified candidates to fill positions, and in that light, it makes sense that many recruiters are turning to social platforms like TikTok and video resumes to find talent.

But the scarcity of workers does not negate the importance of finding the right employee for a role. Especially important for recruiters is finding candidates with the skills that align with their business’ goals and strategy. For example, as more organizations embrace a data-driven approach to operating their business, they need more people with skills in analytics and machine learning to help them make sense of the data they collect.

Recruiters have proven to be open to innovation where it helps them find these new candidates. Recruiting is no longer the manual process it used to be, with HR teams sorting through stacks of paper resumes and formal cover letters to find the right candidate. They embraced the power of online connections as LinkedIn rose to prominence and even figured out how to use third-party job sites like GlassDoor to help them draw in promising candidates. On the back end, many recruiters use advanced cloud software to sort through incoming resumes to find the candidates that best match their job descriptions. But all of these methods still rely on the traditional text-based resume or profile as the core of any application.

Videos on social media provide the ability for candidates to demonstrate soft skills that may not be immediately apparent in written documents, such as verbal communication and presentation skills. They are also a way for recruiters to learn more about the personality of the candidate to determine how they’d fit into the culture of the company. While this may be appealing for many, are we ready for the consequences?

We’re not ready for the close-up

While innovation in recruiting is a big part of the future of work, the hype around TikTok and video resumes may actually take us backward. Despite offering a new way for candidates to market themselves for opportunities, it also carries potential pitfalls that candidates, recruiters and business leaders need to be aware of.

The very element that gives video resumes their potential also presents the biggest problems. Video inescapably highlights the person behind the skills and achievements. As recruiters form their first opinions about a candidate, they will be confronted with information they do not usually see until much later in the process, including whether they belong to protected classes because of their race, disability or gender.

Diversity, equity and inclusion (DE&I) concerns have had a major surge in attention over the last couple of years amid heightened awareness and scrutiny around how employers are — or are not — prioritizing diversity in the workplace.

But evaluating candidates through video could erase any progress made by introducing more opportunities for unconscious, or even conscious, bias. This could create a dangerous situation for businesses if they do not act carefully because it could open them up to consequences such as damage to their reputation or even something as severe as discrimination lawsuits.

A company with a poor track record for diversity may have the fact that they reviewed videos from candidates used against them in court. Recruiters reviewing the videos may not even be aware of how the race or gender of candidates are impacting their decisions. For that reason, many of the businesses I have seen implement an option for video in their recruiting flow do not allow their recruiters to watch the video until late in the recruiting process.

But even if businesses address the most pressing issues of DE&I by managing bias against those protected classes, by accepting videos there are still issues of diversity in less protected classes such as neurodiversity and socioeconomic status. A candidate with exemplary skills and a strong track record may not present themselves well through a video, coming across as awkward to the recruiter watching the video. Even if that impression is irrelevant to the job, it could still influence the recruiter’s stance on hiring.

Furthermore, candidates from affluent backgrounds may have access to better equipment and software to record and edit a compelling video resume. Other candidates may not, resulting in videos that may not look as polished or professional in the eyes of the recruiter. This creates yet another barrier to the opportunities they can access.

As we sit at an important crossroads in how we handle DE&I in the workplace, it is important for employers and recruiters to find ways to reduce bias in the processes they use to find and hire employees. While innovation is key to moving our industry forward, we have to ensure top priorities are not being compromised.

Not left on the cutting room floor

Despite all of these concerns, social media platforms — especially those based on video — have created new opportunities for users to expand their personal brands and connect with potential job opportunities. There is potential to use these new systems to benefit both job seekers and employers.

The first step is to ensure that there is always a place for a traditional text-based resume or profile in the recruiting process. Even if recruiters can get all the information they need about a candidate’s capabilities from video, some people will just naturally feel more comfortable staying off camera. Hiring processes need to be about letting people put their best foot forward, whether that is in writing or on video. And that includes accepting that the best foot to put forward may not be your own.

Instead, candidates and businesses should consider using videos as a place for past co-workers or managers to endorse the candidate. An outside endorsement can do a lot more good for an application than simply stating your own strengths because it shows that someone else believes in your capabilities, too.

Video resumes are hot right now because they are easier to make and share than ever and because businesses are in desperate need of strong talent. But before we get caught up in the novelty of this new way of sharing our credentials, we need to make sure that we are setting ourselves up for success.

The goal of any new recruiting technology should be to make it easier for candidates to find opportunities where they can shine without creating new barriers. There are some serious kinks to work out before video resumes can achieve that, and it is important for employers to consider the repercussions before they damage the success of their DE&I efforts.",Social Media,TechCrunch,https://techcrunch.com/2021/09/15/beware-the-hidden-bias-behind-tiktok-resumes/,"Social Media could lead to an increase in unconscious bias in recruitment and hiring processes, as video resumes highlight information such as race, disability and gender that can be used to disadvantage certain candidates. Companies should be aware of the potential drawbacks of video resumes before they damage their DE&I efforts.",Equality & Justice
445,We owe it to our kids to put an age limit on social media – TechCrunch,"For societies with long histories of protecting children with laws and regulations, isn’t it surprising that nothing is being done to similarly shield them from the various and proven dangers of social media? We need to institute the same kinds of age limits and protections for technology and web use as we’ve done for decades in almost every other sphere.

Think about it. We don’t let young people drive, drink, smoke, get married, join the Army, get a tattoo or vote until we feel they’re old enough to handle it.

But we put some of the most powerful technologies ever known to humankind in the hands of a 13-year-old, and then stand back in amazement when online bullying and body dysmorphia issues go off the charts, when self-harm and suicide rates explode, when rape culture is inculcated within a generation of young children steeped in porn.

For parents with teenage kids, there is a growing, horrifying realization that over the last 10 years, we’ve knowingly surrendered our offspring as guinea pigs to a grand scheme from tech companies focused on “maximizing engagement” for the sake of profit, with little or no regard to the consequences.

For societies with long histories of protecting children with laws and regulations, isn’t it surprising that nothing is being done to similarly shield them from the various and proven dangers of social media?

We parents were so in love with cool tech ourselves that we thought it hip and helpful and safe to get Johnny and Jane a phone, with a similar disregard for what damage this could do to their self-esteem and healthy development. The first little emoji text we got from them seemed cute. We didn’t realize it was going to turn into 100, then 500, then 1,000 — a day.

Forgive us, Lord, for we know not what we do.

Try putting your phone down. Go on, do it now. Count how long you can go before you can’t resist picking it up again. How long did you manage? Not long, right? You (like most of us) are a tech addict, and you’re an adult, with willpower and the ability to defer gratification that your upbringing drilled into you. Imagine what it’s like for a 16-year-old whose whole life has been a never-ending carousel of instant gratification.

And we’re surprised when our kids look washed out in the morning before school, after a night of Instagram, Snapchat, TikTok and a whole bunch of apps your kids know about but you’ve never heard of. School that now involves even more time staring at a screen.

A license to scroll

Having an age limit — we suggest 18 for phones and social media — will begin the process of readjusting our relationship with technology toward our better angels. Just as we teach young people to drive a car with driving lessons, classwork, a highway code guide and a test, let’s teach them how to use social media in a way that won’t harm them. Let’s introduce a “social media user license” that requires passing a test and can be revoked if they don’t follow the rules of the “information superhighway.”

Some people think social media is now so pervasive that it’s impossible to put the genie back in the bottle. But we disagree. In fact, we feel that a fatalistic acceptance of what’s going on is morally unconscionable. Remember, all it takes for evil to flourish is for good people to do nothing.

We’ve proved we can introduce rules and regulations to ensure the wise use of powerful technologies. We’ve done it before, with the aforementioned cars, with radiography and nuclear energy — in fact, with all dual-purpose technologies we’ve created. What’s different about social media? Indeed, in some countries, legislation is beginning to emerge. The U.K., as an example, recently introduced proposed laws that would fine, or even shut down, social media platforms that fail to protect children from harm online.

Some people think that even if we wanted to put age limits in place we couldn’t enforce them, logistically. Of course we could — with the biometric security systems now commonplace on our phones (fingerprint readers, facial recognition, etc.) and with the algorithms that routinely customize feeds for billions of active users per day, or with any variety of existing technical solutions. It is simply a question of having the will. Then the way will emerge.

Keeping a good from becoming evil

We don’t want to ban social media. When used responsibly, it’s a wonderful thing. Particularly now, during the pandemic, social media has been a lifeline against isolation and loneliness. Who can even imagine how much worse sheltering in place and quarantine would have been without technology that allowed us to connect with each other at the exact time we were forced apart? In just a matter of weeks, we simultaneously became more separated — physically — and connected — digitally — than ever before in history.

But social media has grown so vast and so powerful that we’re now past the point where we can continue to justify naïveté and youthful exuberance. It’s time to admit that the inventors, company leaders and consumers — yes, us, too — of these new technologies all know what we are doing. And worse, what we’re doing to our children’s minds.

The final objection to our argument is that, even if there were an age limit in place, kids would find a way around it. This is obviously true. Some kids would find a way to access the tech and apps they see adults using, just as some kids drink and smoke before they’re of the legal age. But if we believed that because some people break laws, there’s no point in having them, anarchy would await. Imperfect compliance with the law is no argument for its absence.

Young people are not mature enough to be exposed to the bottomless scroll of FOMO, YOLO, trolling, abuse, lunacy and unadulterated filth that is just another day on social media. There’s so much evidence of the harm that is being done to kids by it, if you care to look. San Diego State University professor of psychology Jean Twenge’s “iGen” has a lot of the details — if you dare to look.

It’s a parental instinct to protect your children, so let’s act now and set an age limit to spare them from social media’s dark side until they’re mature enough to make responsible choices.",Social Media,TechCrunch,https://techcrunch.com/2021/05/25/we-owe-it-to-our-kids-to-put-an-age-limit-on-social-media/,"The dangers of social media are well-documented, with long-term consequences ranging from increased depression, anxiety and body dysmorphia to online bullying and even suicide. It is time to institute age limits and protections to shield young people from these risks.",Social Norms & Relationships
446,Facebook says it removed 8.7M child exploitation posts with new machine learning tech – TechCrunch,"Facebook announced today that it has removed 8.7 million pieces of content last quarter that violated its rules against child exploitation, thanks to new technology. The new AI and machine learning tech, which was developed and implemented over the past year by the company, removed 99 percent of those posts before anyone reported them, said Antigone Davis, Facebook’s global head of safety, in a blog post.

The new technology examines posts for child nudity and other exploitative content when they are uploaded and, if necessary, photos and accounts are reported to the National Center for Missing and Exploited Children. Facebook had already been using photo-matching technology to compare newly uploaded photos with known images of child exploitation and revenge porn, but the new tools are meant to prevent previously unidentified content from being disseminated through its platform.

The technology isn’t perfect, with many parents complaining that innocuous photos of their kids have been removed. Davis addressed this in her post, writing that in order to “avoid even the potential for abuse, we take action on nonsexual content as well, like seemingly benign photos of children in the bath” and that this “comprehensive approach” is one reason Facebook removed as much content as it did last quarter.

But Facebook’s moderation technology is by no means perfect and many people believe it is not comprehensive or accurate enough. In addition to family snapshots, it’s also been criticized for removing content like the iconic 1972 photo of Phan Thi Kim Phuc, known as the “Napalm Girl,” fleeing naked after suffering third-degree burns in a South Vietnamese napalm attack on her village, a decision COO Sheryl Sandberg apologized for.

Last year, the company’s moderation policies were also criticized by the United Kingdom’s National Society for the Prevention of Cruelty to Children, which called for social media companies to be subject to independent moderation and fines for non-compliance. The launch of Facebook Live has also at times overwhelmed the platform and its moderators (software and human), with videos of sexual assaults, suicides, and murder—including that of an 11-month-old baby by her father—being broadcast.

Moderating social media content, however, is one noteworthy example of how AI-based automation can benefit human workers. Last month, Selena Scola, a former Facebook content moderator, sued the company claiming that screening thousands of violent images had caused her to develop post-traumatic stress disorder. Other moderators, many of whom are contractors, have also spoken of the job’s psychological toll and said Facebook does not offer enough training, support, or financial compensation.",Social Media,TechCrunch,https://techcrunch.com/2018/10/24/facebook-says-it-removed-8-7m-child-exploitation-posts-with-new-machine-learning-tech/,"Social media moderation has had a psychological toll on content moderators, with many of them suffering from post-traumatic stress disorder due to viewing thousands of violent images. Despite the use of AI-based automation, the job is still difficult and often inadequately compensated.",Equality & Justice
447,AI-Generated Text Is the Scariest Deepfake of All,"When pundits and researchers tried to guess what sort of manipulation campaigns might threaten the 2018 and 2020 elections, misleading AI-generated videos often topped the list. Though the tech was still emerging, its potential for abuse was so alarming that tech companies and academic labs prioritized working on, and funding, methods of detection. Social platforms developed special policies for posts containing “synthetic and manipulated media,” in hopes of striking the right balance between preserving free expression and deterring viral lies. But now, with about three months to go until November 3, that wave of deepfaked moving images seems never to have broken. Instead, another form of AI-generated media is making headlines, one that is harder to detect and yet much more likely to become a pervasive force on the internet: deepfake text.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

Last month brought the introduction of GPT-3, the next frontier of generative writing: an AI that can produce shockingly human-sounding (if at times surreal) sentences. As its output becomes ever more difficult to distinguish from text produced by humans, one can imagine a future in which the vast majority of the written content we see on the internet is produced by machines. If this were to happen, how would it change the way we react to the content that surrounds us?

This wouldn't be the first such media inflection point where our sense of what's real shifted all at once. When Photoshop, After Effects, and other image-editing and CGI tools began to emerge three decades ago, the transformative potential of these tools for artistic endeavors—as well as their impact on our perception of the world—was immediately recognized. “Adobe Photoshop is easily the most life-changing program in publishing history,” declared a Macworld article from 2000, announcing the launch of Photoshop 6.0. “Today, fine artists add finishing touches by Photoshopping their artwork, and pornographers would have nothing to offer except reality if they didn't Photoshop every one of their graphics.”

We came to accept that technology for what it was and developed a healthy skepticism. Very few people today believe that an airbrushed magazine cover shows the model as they really are. (In fact, it’s often un-Photoshopped content that attracts public attention.) And yet, we don’t fully disbelieve such photos, either: While there are occasional heated debates about the impact of normalizing airbrushing—or more relevant today, filtering—we still trust that photos show a real person captured at a specific moment in time. We understand that each picture is rooted in reality.

Generated media, such as deepfaked video or GPT-3 output, is different. If used maliciously, there is no unaltered original, no raw material that could be produced as a basis for comparison or evidence for a fact-check. In the early 2000s, it was easy to dissect pre-vs-post photos of celebrities and discuss whether the latter created unrealistic ideals of perfection. In 2020, we confront increasingly plausible celebrity face-swaps on porn, and clips in which world leaders say things they’ve never said before. We will have to adjust, and adapt, to a new level of unreality. Even social media platforms recognize this distinction; their deepfake moderation policies distinguish between media content that is synthetic and that which is merely “modified”.

Pervasive generated text has the potential to warp our social communication ecosystem.

To moderate deepfaked content, though, you have to know it’s there. Out of all the forms that now exist, video may turn out to be the easiest to detect. Videos created by AI often have digital tells where the output falls into the uncanny valley: “soft biometrics” such as a person’s facial movements are off; an earring or some teeth are poorly rendered; or a person’s heartbeat, detectable through subtle shifts in coloring, is not present. Many of these giveaways can be overcome with software tweaks. In 2018’s deepfake videos, for instance, the subjects’ blinking was often wrong; but shortly after this discovery was published, the issue was fixed. Generated audio can be more subtle—no visuals, so fewer opportunities for mistakes—but promising research efforts are underway to suss those out as well. The war between fakers and authenticators will continue in perpetuity.

Perhaps most important, the public is increasingly aware of the technology. In fact, that knowledge may ultimately pose a different kind of risk, related to and yet distinct from the generated audio and videos themselves: Politicians will now be able to dismiss real, scandalous videos as artificial constructs simply by saying, “That’s a deepfake!” In one early example of this, from late 2017, the US president’s more passionate online surrogates suggested (long after the election) that the leaked Access Hollywood “grab 'em” tape could have been generated by a synthetic-voice product named Adobe Voco.",Social Media,WIRED,https://www.wired.com/story/ai-generated-text-is-the-scariest-deepfake-of-all/,"The pervasive use of AI-generated text on Social Media could lead to a future in which it is difficult to distinguish between real and generated content, and this could have serious implications for the way we react to the information we see. This could result in politicians being able to discredit real evidence by claiming it to be a deepfake, and could","Information, Discourse & Governance"
448,"Sorry, But You Need to Care About Blac Chyna and Rob Kardashian","The Kardashian-Jenner clan's social-media savvy is a simple matter of data. All told, they have almost half a billion followers on Instagram alone. (""All told"" meaning, of course: Kardashian sisters Kim, Kourtney, and Khloe; matriarch Kris; ex-spouse/step-mapa Caitlyn Jenner; Jenner spawn Kendall and Kylie; and the lone Kardashian male, Rob.) They are the archetypical ""influencers,"" the people who possess trend- and culture-shaping power based solely on their internet presence. The slightest changes to their online aesthetic garners headlines. Even Rob—the anonymous one of the family, with his relatively meager Instagram following of 9.9 million—lives on a smartphone stage larger than that accorded to virtually every athlete, musician, or other talent-based celebrity in the entire world.

Weird, right? But that's exactly why it bears mentioning that earlier this week, Rob posted a series of Instagram nudes of his on-again, off-again partner, Angela Renée White—better known as Blac Chyna—after becoming aware of Chyna's alleged infidelity. When Instagram disabled his account, he instructed his more than 7 million Twitter followers to ""peep"" the images there. (The ensuing explicit tweets have since been deleted.) The conflict didn't stop at those two platforms: In retaliation, Blac Chyna took to Snapchat to accuse Kardashian of domestic abuse.

This was not an isolated breakup fight, though, as much as it was the pyrotechnic finale to a major arc in the Kardashians' sweeping, 21st-century soap opera. (Without getting too deep into it, Kardashian-watchers had long been convinced that Blac Chyna, who rose to prominence as a stripper, had become involved with Rob Kardashian as payback after her own ex-boyfriend, rapper Tyga, took up with Kylie Jenner. [Don't worry, Tyga and Kylie broke up earlier this year as well.]) The whole affair was sordid and miserable, and completely entranced giant swaths of the internet. It also underscored just how slipshod and ineffective current revenge porn laws are—and how even better laws might not be enough to level some playing fields.

According to California law Penal Code 647(j)(4), Kardashian's act may well have constituted revenge porn: Intentionally distributing an image of the intimate body part of another identifiable person, knowing that said distribution would cause that person serious emotional distress. (California is one of 38 states with laws specifically banning revenge porn.) Kardashian could conceivably face six months in jail—if Chyna could even bring a successful case against him, given the law's limited scope and tricky layers of legalese. Even then, according to Mary Anne Franks, a lawyer at the University of Miami who helps states craft their revenge porn laws, first offenders rarely serve time, so the upshot would most likely be a fine.

Yet, those fighting to criminalize nonconsensual porn at an national level worry about the detrimental effects of a Kardashian revenge porn battle. ""We’ve had momentum over the last couple years,"" says Danielle Citron, who teaches law at the University of Maryland. ""Big celebrity moments like Jennifer Lawrence's nude photo leak have played a important educative role. But this moment could take us backward.""

Why? Because the unofficial courtroom of the internet seems not to be treating this like revenge porn at all. ""[Kardashian] knew what he was getting into when he got her,"" Snoop said in a video reaction. ""She is what she is ... Go buy you another one."" That statement highlights Chyna's extra-legal struggle: Because she was (and, according to Snoop-logic, will always be) a stripper, leaking nude photos of her doesn't matter. ""This is a really common argument applied to sex workers, but also women more broadly,"" says Catherine Knight Steele, who teaches gender, race, and digital media at the University of Maryland. ""Any way you use your own body in the past is something you can’t object to in the future.""",Social Media,WIRED,https://www.wired.com/story/rob-kardashian-blac-chyna-revenge-porn/,"This incident has highlighted the lack of adequate legal protection for victims of revenge porn, as well as the tendency for the public to view women who engage in sex work as unable to control their own image or object to its use without their consent.",Equality & Justice
449,This subscription social network is happy to be an Albatross in a pandemic – TechCrunch,"In discussions of ethically dubious social networks, Facebook is the usual reference choice. But spare a thought for subscribers of InterNations, a Munich-based social networking community for expats, who have found themselves unable to obtain refunds for full-year payments charged in the middle of the coronavirus crisis.

InterNations has operated an expat networking experience since 2007, offering a free “Basic” tier of membership that gives users some access to site content and community-organized events (if they pay an entry fee); or a premium tier which requires shelling out for a year’s subscription up front to get free/reduced price entry to networking events, plus access to some additional site features.

The German company appears to be a fan of nominative determinism — having named the subscription tier of membership “Albatross,” given how difficult it is for users to exit once they upgrade from Basic to paying, perpetually renewing contract.

Several former members told us their memberships were auto-renewed for a full year without any warning in the middle of the pandemic. When they contacted InterNations to request a refund they were point-blank refused — with the company saying they were bound by the terms of the contract they’d entered into when they paid to upgrade the year before.

In emails we’ve reviewed between users and InterNations’ staff, the company repeatedly ignores requests for refunds.

One U.K.-based user, who told us she had signed up to use the service to attend networking events in London and Paris, where she traveled regularly for work, found herself put on furlough in March when the U.K. went into lockdown. She only noticed the InterNations subscription had auto-renewed when she saw a charge as she was checking her bank statement.

She contacted InterNations to request a refund — pointing out there were now no physical events near her, nor would she be able to attend in-person networking events for the foreseeable future due to shielding as a result of personal vulnerability to the health risk posed by COVID-19. But InterNations still refused to refund her subscription.

Instead it offered to put the year’s Albatross membership on hold until 2022 — suggesting she might be able to make use of the services she’d just been billed for in two years’ time.

“Many of the people complaining feel aggrieved by InterNation because the entire event offering is very much voluntary and community based. It relies on people stepping forward to organise groups of people to attend events, walks, screening etc. Most of them do not make financial gain out of it,” she told us.

“So for this organisation not to be looking after its very own community feels like a slap on our faces.”

“My local gym froze my membership from April 2020 without any of its members having to request it. They informed us by email they would do this. I was able to cancel in July without any question asked,” she added. “If my small gym is able to do this, how come InterNations is not stopping the auto-renewal of the membership at such a time?

“When everyone almost worldwide is worrying about their health, their livelihood, their relatives, we are not remembering to cancel or to stop memberships.”

Another user, who signed up to the service after moving from the U.S. to Singapore, told us he was sent repeated payment demands in the middle of the coronavirus crisis after his on-file credit card had expired — which meant InterNations couldn’t auto-collect his payment.

He told it he wanted to cancel the subscription but it told him he would only able to delete his account if he paid up for a full second year. Eventually he said he felt he had no choice but to pay the demand for around $100 in order that he could downgrade from Albatross to Basic and have his account deleted.

“I was (and still am) a paid subscriber and during the height of the pandemic I never received an offer of ‘free months’ of membership,” he said. “Instead, all I got was a deluge of threatening emails about how they couldn’t process my credit card information. Nothing even remotely about whether I was sick or even still alive. They just wanted my credit card details.”

A third user, who signed up for the service after moving to Hanoi, summed up her experience as “not the best.” She pointed us to a blog post in which she recounts a similar story — finding herself charged for a renewal in the middle of the coronavirus without any advance warning and having forgotten to cancel the subscription herself.

“I didn’t realise I’d been charged until a notification from PayPal arrived in my inbox,” she writes. “Say, what? Where was the email reminder? Where was the ‘now due’ invoice that is the hallmark of good business? Turns out InterNations don’t send them.”

This user was finally able to obtain a refund — but only via disputing the charge through PayPal. She got no joy asking for her money back from InterNations itself.

A deluge of similar complaints about the company can be seen on Trustpilot — where InterNations has an 81% “bad” rating at the time of writing.

“An annual membership was taken from my account, and refund was refused. A year on and I am being threatened with non payment of a new invoice,” writes one reviewer.

“I cancelled my membership the past two years and every year it shows that I didn’t and their records conveniently show no record of my cancellation. Then they will refuse refunds,” recounts another.

“InterNations contacted me via automated email about my membership payment being due. When I responded, asking to cancel membership since I haven’t logged in in months and can’t afford membership during these times, they refused to help,” says another irate reviewer. “They make it impossible to do this simple task. They’re greedily unable to help with anything other than take your money. No empathy. All they have to do is cancel the membership.”

“They don’t even send a reminder for end of membership. Some people have seen their credit card debited, without any reminder. And if your credit card you registered has expired, they keep harassing you and threaten you,” runs another despairing former user.

In emails to users who are requesting a refund which we’ve seen, InterNations simply points them to German law — which does appear to be the legal sticking point here. As a number of expat blogs warn, service contracts in Germany can be a lot harder to get out of than into.

Though, of course, it’s unlikely to have been immediately clear to people signing up to a global social network in cities like Hanoi and Singapore that they needed to understand German contract law before hitting “subscribe.”

BEUC, the European consumer rights group, told us there’s no pan-EU requirement for a notification to be actively sent to users ahead of an auto-renewal of a services contract — and the lack of such a notification ahead of the InterNations subscription renewal is one of the key recurring complaints.

“EU law only requires the consumer to be informed of the final price and the contractual conditions,” a spokesperson said, noting that consumer rights can vary substantially from member state to member state as the area isn’t harmonised at EU level.

So, while BEUC noted that, for example, Belgium law does have a specific provision which allows the consumer to terminate a contract at no cost after its tacit renewal — Germany, self evidently, does not. Although domestic pressure appears to be growing for reform of its one-sided contract rules.

When we put the various complaints we’d heard about refunds and cancellations (and indeed dark patterns) to InterNations, its founder and co-CEO, Malte Zeeck, said the company does not breach consumer law — and further claimed it “clearly communicates” subscription renewals to users.

“InterNations is operating on a standard subscription model like many other businesses, which is at no point in breach of consumer protection laws,” he said. “Subscriptions are renewed automatically, which is clearly communicated at the beginning of each subscription period, in each invoice, and in every user’s membership and account settings. This is also where a subscription can be canceled at any time, without a notice period that has to be observed.

“Our members have a continual visual reminder of their membership status through the Albatross symbol found on their profile picture. They can also always see their current membership status by visiting their membership page.”

And while he conceded that InterNations had had to cancel in-person events “during the height of the pandemic” he said it substituted this reduction in service by offering “additional free months of membership” and “working very hard to respond to the situation and find ways for our members to still meet and spend time together online.”

“After only a few weeks, we already offered over 500 online activities worldwide to help expats and global minds connect and share experiences — more online events were being added every day,” he added. “In addition, our users continued to benefit from other online networking and information features our premium membership offers. Since restrictions on in-person events are being lifted around the world, we have started to offer many opportunities for our members again to meet in person.”

EU consumer protection rules do bake in requirements that contract terms be fair — with provisions intended to protect against things like one-sided changes to a service without a valid reason. But it’s pretty clear that InterNations could argue a pandemic is a valid reason for canceling in person events and replacing them with online networking. So angry users are unlikely to find much solace there.

Still, maintaining such an inflexible and user hostile attitude during a pandemic does look risky for InterNations and its reputation, given new users are likely to be far less easy for it to net now that the coronavirus has settled like a dead calm on so much foreign travel.

So while it might be legally entitled to sit and claw in revenue from people who — living through a pandemic and worried about things like their jobs, health and loved ones — forgot to cancel a subscription that only comes round once a year, it’s hardly a recipe for long-term customer loyalty.

Indeed, we’ve seen these kind of auto-renewing subscription gigs crop up in the e-commerce space in years past. And none of those dubious tactics went the distance.

Tricking consumers into recurring payments is never a good long-term business strategy, and it certainly isn’t now that reputational damage can scale all over social media in seconds. (To wit: Irate InterNations users have been organizing via Twitter and have set up a website to amplify negative reviews where they urge people to boycott the service.)

None of the people who’ve been stung by InterNations’ auto-renewing subscription are likely to forget to cancel a second time so won’t be a source of recurring revenue in future. And treating users like so much chum when the company also relies upon their community spirit to power its service looks like a rotten business model long past its sell-by-date. (However many members InterNations claims have contacted it “to say how much our online events have helped them to stay in touch with people and also stay positive during a period of self-isolation,” a minority of satisfied customers are being drowned out by all the angry online views.)

In the meanwhile, it’s certainly curious to encounter a niche social network that’s happy operating with as little regard for users’ wishes as some of the far more maligned giants of the category. To the point where its website displays information regarding the European Commission’s “online dispute resolution” platform in small print right on the contacts page. Er, perhaps Facebook should take note.

On unhappy users, Zeeck only had this to say: “We are sorry that some of our former members perceive this differently and were not happy with the benefits our membership offered them. We are always taking our users’ feedback seriously and are working hard to provide a great experience for them. At the same time, we are aware that it is hard to have the perfect solution for everybody, and there will always be detractors.”

But perhaps he’s been taking cues from Mark Zuckerberg’s neverending apology tours.",Social Media,TechCrunch,https://techcrunch.com/2020/08/21/this-subscription-social-network-is-happy-to-be-an-albatross-in-a-pandemic/,"The unethical practices of some social media platforms, such as InterNations, are becoming increasingly apparent, with many users unable to obtain refunds for full-year payments charged in the middle of the coronavirus crisis and complaints about unfair contract terms and auto-renewals. Such practices are eroding user trust in the service, and",Security & Privacy
450,"Tinder is testing a feed of real-time updates from your matches, including posts from Instagram and Spotify – TechCrunch","Tinder announced today it’s beginning a test of a new feature called “Feed” designed to help users learn more about their matches. The Feed, which will appear as a tab on the Messages screen, will include real-time updates from those you’ve already matched with, including things like recently added Tinder photos, plus Instagram posts and your Top Artists and Anthems from Spotify. This is data Tinder users could already see, had their matches connected these external accounts to their profile.

The idea is to present this information in a new format.

Explains the company, the Feed is meant to take users beyond the match in order to make “real” connections, it says in the announcement.

The Feed is something that could, at least in theory, help to spark conversations between matches. After all, the hardest part of using a dating app where you have to begin a chat session with a perfect stranger is finding something to chat about when you know so little about them.

Even after a cursory glance through a Tinder profile, you may not immediately know what they’re interested in or what they do for fun in their free time. But their musical tastes and social media posts can give you a better idea.

The thought here is if you see something interesting pop up on the Feed, you would then have an opener for your chat.

Tinder says you can start these Feed-initiated chats by double-tapping on the content that’s shared, then responding to it directly. Users can also control what appears in the Feed from the app’s Settings or by editing your profile.

The concept is not entirely dissimilar from the way users connect over posts and photos shared on rival dating app, Hinge, in fact. On Hinge, each individual text entry or shared photo can be replied to, allowing you to respond not to the profile itself, but to a specific thing you read or saw posted to that profile. Tinder’s implementation changes this to a continually updating feed instead.

The feature could also make it easier on those users who aren’t sure what sort of information to include in a profile, or have left off something notable about them – like a favorite band, or a regular hobby – things their social media and musical posts would show.

But not everyone will appreciate the new format, as it may provide too visible a window into their lives, ripe for cyberstalking. It could also encourage people to be more passive about actually getting to know a fellow match in the app – or, rather, in real life – as a dating app should incentivize. Users could just watch their Feed for updates instead, while spending ever more time in Tinder’s app, boosting its bottom line.

On the flip side, the Feed’s presence could encourage a sort of narcissistic show of behavior from its more active users – knowing that the Instagram photos they post will make their way to a new audience of potential dates could impact the type of photos users share, detracting from their authenticity. (But that’s a common problem across social media, not just a Tinder issue.)

Tinder says it’s now testing the Feed format in Australia, New Zealand and Canada for the time being. It didn’t say if or when it would roll out to the wider user base.",Social Media,TechCrunch,https://techcrunch.com/2017/12/12/tinder-is-testing-a-feed-of-real-time-updates-from-your-matches-including-posts-from-instagram-and-spotify/,"The new ""Feed"" feature from Tinder could encourage a narcissistic show of behavior from its users and detract from the authenticity of their posts, while discouraging forming real-life connections.",Social Norms & Relationships
451,Twitter hack probe leads to call for cybersecurity rules for social media giants – TechCrunch,"An investigation into this summer’s Twitter hack by the New York State Department of Financial Services (NYSDFS) has ended with a stinging rebuke for how easily Twitter let itself be duped by a “simple” social engineering technique — and with a wider call for key social media platforms to be regulated on security.

In the report, the NYSDFS points, by way of contrasting example, to how quickly regulated cryptocurrency companies acted to prevent the Twitter hackers scamming even more people — arguing this demonstrates that tech innovation and regulation aren’t mutually exclusive.

Its point is that the biggest social media platforms have huge societal power (with all the associated consumer risk) but no regulated responsibilities to protect users.

The report concludes this is a problem U.S. lawmakers need to get on and tackle stat — recommending that an oversight council be established (to “designate systemically important social media companies”) and an “appropriate” regulator appointed to ‘monitor and supervise’ the security practices of mainstream social media platforms.

“Social media companies have evolved into an indispensable means of communications: more than half of Americans use social media to get news, and connect with colleagues, family, and friends. This evolution calls for a regulatory regime that reflects social media as critical infrastructure,” the NYSDFS writes, before going on to point out there is still “no dedicated state or federal regulator empowered to ensure adequate cybersecurity practices to prevent fraud, disinformation, and other systemic threats to social media giants”.

“The Twitter Hack demonstrates, more than anything, the risk to society when systemically important institutions are left to regulate themselves,” it adds. “Protecting systemically important social media against misuse is crucial for all of us — consumers, voters, government, and industry. The time for government action is now.”

We’ve reached out to Twitter for comment on the report

Among the key findings from the Department’s investigation are that the hackers broke into Twitter’s systems by calling employees and claiming to be from Twitter’s IT department — through which simple social engineering method they were able to trick four employees into handing over their log-in credentials. From there they were able to access the Twitter accounts of high profile politicians, celebrities, and entrepreneurs, including Barack Obama, Kim Kardashian West, Jeff Bezos, Elon Musk, and a number of cryptocurrency companies — using the hijacked accounts to tweet out a crypto scam to millions of users.

Twitter has previously confirmed that a “phone spear phishing” attack was used to gain credentials.

Per the report, the hackers’ “double your bitcoin” scam messages, which contained links to make a payment in bitcoins, enabled them to steal more than $118,000 worth of bitcoins from Twitter users.

Although a considerably larger sum was prevented from being stolen as a result of swift action taken by regulated crypto companies — namely: Coinbase, Square, Gemini Trust Company and Bitstamp — who the Department said blocked scores of attempted transfers by the fraudsters.

“This swift action blocked over 6,000 attempted transfers worth approximately $1.5 million to the Hackers’ bitcoin addresses,” the report notes.

Twitter is also called out for not having a cybersecurity chief in post at the time of the hack — after failing to replace Mike Convertino, who left in December 2019 to join cyber resilience firm Arceo.

Last month it announced Rinki Sethi had been hired as CISO.

“Despite being a global social media platform boasting over 330 million average monthly users in 2019, Twitter lacked adequate cybersecurity protection,” the NYSDFS writes. “At the time of the attack, Twitter did not have a chief information security officer, adequate access controls and identity management, and adequate security monitoring — some of the core measures required by the Department’s first-in-the-nation cybersecurity regulation.”

European Union data protection law already bakes in security requirements as part of a comprehensive privacy and security framework (with major penalties possible for security breaches). However an investigation by the Irish DPC of a 2018 Twitter security incident is still yet to conclude after a draft decision failed to gain the backing of the other EU data watchdogs this August — triggering a further delay to the pan-EU regulatory process.

This story was updated with a correction: Twitter had failed to replace Mike Convertino as CISO rather than Michael Coates, who was also in the post but left Twitter in March 2019, rather than in March 2020 as we originally stated",Social Media,TechCrunch,https://techcrunch.com/2020/10/14/twitter-hack-probe-leads-to-call-for-cybersecurity-rules-for-social-media-giants/,"The New York State Department of Financial Services' investigation into this summer's Twitter hack has revealed the ease with which the platform was duped by simple social engineering techniques, and has called for key social media platforms to be regulated in order to protect consumers from fraud, disinformation, and other systemic threats.",Security & Privacy
452,"‘The Great Hack’: Netflix doc unpacks Cambridge Analytica, Trump, Brexit and democracy’s death – TechCrunch","It’s perhaps not for nothing that The Great Hack — the new Netflix documentary about the connections between Cambridge Analytica, the U.S. election and Brexit, out on July 23 — opens with a scene from Burning Man. There, Brittany Kaiser, a former employee of Cambridge Analytica, scrawls the name of the company onto a strut of “the temple” that will eventually get burned in that fiery annual ritual. It’s an apt opening.

There are probably many of us who’d wish quite a lot of the last couple of years could be thrown into that temple fire, but this documentary is the first I’ve seen to expertly peer into the flames of what has become the real-world dumpster fire that is social media, dark advertising and global politics which have all become inextricably, and, often fatally, combined.

The documentary is also the first that you could plausibly recommend to those of your relatives and friends who don’t work in tech, as it explains how social media — specifically Facebook — is now manipulating our lives and society, whether we like it or not.

As New York Professor David Carroll puts it at the beginning, Facebook gives “any buyer direct access to my emotional pulse” — and that included political campaigns during the Brexit referendum and the Trump election. Privacy campaigner Carroll is pivotal to the film’s story of how our data is being manipulated and essentially kept from us by Facebook.

The U.K.’s referendum decision to leave the European Union, in fact, became “the Petri dish” for a Cambridge Analytica (CA) experiment, says Guardian journalist Carole Cadwalladr. She broke the story of how the political consultancy, led by Eton-educated CEO Alexander Nix, applied to the democratic operations of the U.S. and U.K., and many other countries, over a chilling 20+ year history techniques normally used by “psyops” operatives in Afghanistan. Watching this film, you literally start to wonder if history has been warped toward a sickening dystopia.

The Petri-dish of Brexit worked. Millions of adverts, explains the documentary, targeted individuals, exploiting fear and anger, to switch them from “persuadables,” as CA called them, into passionate advocates for, first Brexit in the U.K., and then Trump later on.

Switching to the U.S., the filmmakers show how CA worked directly with Trump’s “Project Alamo” campaign, spending a million dollars a day on Facebook ads ahead of the 2016 election.

The film expertly explains the timeline of how CA first worked off Ted Cruz’s campaign, and nearly propelled that lack-luster candidate into first place in the Republican nominations. It was then that the Trump campaign picked up on CA’s military-like operation.

After loading up the psychographic survey information CA obtained from Aleksandr Kogan, the Cambridge University academic who orchestrated the harvesting of Facebook data, the world had become their oyster. Or, perhaps more accurately, their oyster farm.

Back in London, Cadwalladr notices triumphant Brexit campaigners fraternizing with Trump and starts digging. There is a thread connecting them to Breitbart owner Steve Bannon. There is a thread connecting them to Cambridge Analytica. She tugs on those threads and, like that iconic scene in The Hurt Locker, where all the threads pull up unexploded mines, she starts to realize that Cambridge Analytica links them all. She needs a source though. That came in the form of former employee Chris Wylie, a brave young man who was able to unravel many of the CA threads.

But the film’s attention is often drawn back to Kaiser, who had worked first on U.S. political campaigns and then on Brexit for CA. She had been drawn to the company by smooth-talking CEO Nix, who begged: “Let me get you drunk and steal all of your secrets.”

But was she a real whistleblower? Or was she trying to cover her tracks? How could someone who’d worked on the Obama campaign switch to Trump? Was she a victim of Cambridge Analytica, or one of its villains?

British political analyst Paul Hilder manages to get her to come to the U.K. to testify before a parliamentary inquiry. There is high drama as her part in the story unfolds.

Kaiser appears in various guises, which vary from idealistically naive to stupid, from knowing to manipulative. It’s almost impossible to know which. But hearing about her revelation as to why she made the choices she did… well, it’s an eye-opener.

Both she and Wylie have complex stories in this tale, where not everything seems to be as it is, reflecting our new world, where truth is increasingly hard to determine.

Other characters come and go in this story. Zuckerburg makes an appearance in Congress and we learn of the casual relationship Facebook had to its complicity in these political earthquakes. Although, if you’re reading TechCrunch, then you probably know at least part of this story.

Created for Netflix by Jehane Noujaim and Karim Amer, these Egyptian-Americans made “The Square,” about the Egyptian revolution of 2011. To them, the way Cambridge Analytica applied its methods to online campaigning was just as much a revolution as Egyptians toppling a dictator from Cario’s iconic Tahrir Square.

For them, the huge irony is that “psyops,” or psychological operations, used on Muslim populations in Iraq and Afghanistan after the 9/11 terrorist attacks ended up being used to influence Western elections.

Cadwalladr stands head and shoulders above all as a bastion of dogged journalism, even as she is attacked from all quarters, and still is to this day.

What you won’t find out from this film is what happens next. For many, questions remain on the table: What will happen now that Facebook is entering cryptocurrency? Will that mean it could be used for dark election campaigning? Will people be paid for their votes next time, not just in Likes? Kaiser has a bitcoin logo on the back of her phone. Is that connected? The film doesn’t comment.

But it certainly unfolds like a slow-motion car crash, where democracy is the car and you’re inside it.",Social Media,TechCrunch,https://techcrunch.com/2019/07/23/the-great-hack-netflix-doc-unpacks-cambridge-analytica-trump-brexit-and-democracys-death/,"The main undesirable consequence of Social Media discussed in ""The Great Hack"" is how it has been used to manipulate public opinion in various political campaigns such as Brexit and the Trump election by exploiting fear and anger. This manipulation of data for political purposes is a threat to democracy and has led to a chilling effect on how people interact with Social Media and",Politics
453,"Blue Fever, an anonymous social network, acquires Gen Z-founded Trill – TechCrunch","With Senate hearings and leaked documents galore, teen mental health on social media is a hot topic right now. But Gen Z founders Georgia Messinger and Ari Sokolov have been trying to create healthier online spaces since they were in high school, when they started the anonymous virtual support app Trill.

“People ask us sometimes, ‘How did you guys decide you wanted to be entrepreneurs while being students?'” Messinger, now a Harvard undergraduate, told TechCrunch. “It was never that we set out to start a business, per se, it’s just that when we were seniors in high school, we set out to just solve a problem for our friend. One of our friends was bisexual and was really struggling with coming out, and we wanted to create an antidote to traditional social networks, which later has been coined ’emotional media,’ and it was just a passion project that grew bigger than we ever imagined.”

Trill has amassed over 100,000 downloads through organic marketing alone, facilitating anonymous support groups around identity, mental health and relationships. With about $100,000 in seed funding from sources like the Founders Bootcamp and Target Incubator programs, the app grew a team of 30 part-time staff (mostly high school and college students), as well as over 100 volunteer moderators. But the college student founders started to look into opportunities to be acquired during the summer of 2019.

“We wanted to be able to keep ourselves open to explore other interests, enjoy our time at school and take care of our own mental health — not doing this full-time job while juggling all of life’s other priorities,” Messinger explained. “But as you can see, it’s now October 2021, so we weren’t rushing into anything.”

Since a large community of young people use their app as a reprieve, Messinger and Sokolov wanted to make sure that if they were acquired, it would be by a company that shared their fundamental mission to develop supportive communities for young people online. Meanwhile, Greta McAnany was looking for opportunities to reach more Gen Z users on Blue Fever, an anonymous social network encouraging authenticity and community support. With McAnany as CEO and co-founder, Blue Fever has secured $4.2 million in venture backing from investors like Amazon Alexa Fund, Bumble Fund and Serena Williams.

McAnany wasn’t necessarily looking for an acquisition, but she told TechCrunch that when she met the founders of Trill, she was so impressed with them that she wanted to find a way to collaborate.

“I’ll never forget when I met Georgia in person, and she was like, ‘Here’s how I see the future of emotional media,’ which is what we call ourselves instead of social media,” McAnany said. “I was floored. I was like, ‘Okay, we need to bring you on the team as part of leadership,’ and also, bring in the incredible community and user base that they built.'”

Blue Fever isn’t disclosing the terms of the acquisition, but with today’s announcement, Trill will begin incentivizing its users to move over to Blue Fever. By the end of October, Trill will no longer be functional. In the transition, Messinger and Sokolov will join Blue Fever as advisers on product development and audience strategy. They will also lead a Junior Advisory Board in collaboration with hundreds of beta testers who will provide input on the app’s features.

“I’ve worked closely with Blue Fever’s head of product, and I think we’re very aligned on the future of the product in creating the same experience with slightly different features,” said Ari Sokolov, now a student in the USC Iovine and Young Academy’s Arts, Technology and the Business of Innovation program.

“Even if it’s not the same core user experience, it’s the same core essence behind it,” Messinger added. Blue Fever and Trill are both anonymous apps, but Blue Fever works by inviting users to post pages, which are essentially iPhone notes where users can share their insecurities, victories, worries and thoughts. Pages are posted to journals, which are themed collections of pages about topics like college, relationships, gender identity, magic moments and loneliness. To discourage negative commenting or the disclosure of personal information, Blue Fever currently only allows users to respond with a “hug” or a gif. But McAnany says the app will test commenting. The app also uses an AI called Blue, which McAnany says is like a “big sibling,” helping users find content on the platform that might help them — and if someone posts content indicating that they might be in danger, the AI provides resources for them to seek help.

Blue Fever also has human moderators who help keep the platform safe. Still, as more Gen Z-ers flock to Blue Fever from Trill, it will become even more important that Blue Fever’s combination of human and AI moderation can scale.

Mental-health-focused apps like Blue Fever and Trill aren’t intended to be a substitute for seeking professional help, and they’re not trying to compete with giants like TikTok, Snapchat or Instagram. While these platforms aim to comfort Gen Z users who are disillusioned with traditional social apps, anonymous platforms have a troubling track record online.

“Ari and I have been very open since day one that our solution might not be perfect, and there could be better solutions or iterations on the ideas we have,” said Messinger. “We’re not attached to our solution per se — we love Trill and we obviously think it’s a great solution — but we’re more attached to the problem we’re solving than to the solution.”

Anonymous social apps like Snapchat’s Yolo and LMK and Ask.fm have been linked to teen suicides, while Whisper accidentally exposed anonymous posters’ data. Blue Fever and Trill are different from these platforms, since they’re built with mental health considerations at their core, but as Blue Fever considers rolling out commenting and absorbs the Trill user base, the platform’s ability to keep users safe will be tested. McAnany said that Blue Fever doesn’t store personal identifiable information (PII) alongside user-generated content, and that the app doesn’t maintain a publicly accessible API, which was how data from apps like Whisper was leaked.

“We love technology. We’re coders, we’re computer scientists, and we don’t dislike our phones. I love social media. I like to make TikToks,” Messinger said. “It’s just about setting healthy boundaries and continuing to ask the question of how our new spaces are going to intervene to help Gen Z and especially marginalized communities within that.”",Social Media,TechCrunch,https://techcrunch.com/2021/10/05/blue-fever-an-anonymous-social-network-acquires-gen-z-founded-trill/,"Social media has been linked to teen suicides and data leaks, making it important for anonymous social apps like Blue Fever and Trill to keep users safe and follow strict protocols for data storage and protection.",Security & Privacy
454,Conservative social networks keep making the same mistake,"One question I have wondered a lot over the past few years is whether the rise of a large-scale conservative social network — a Fox News of Facebook — is inevitable. Last year, during the rise of Parler, we finally got a good test case.

Here was an app backed by the Mercer family, who previously championed Breitbart News, Donald Trump, and Cambridge Analytica, among other conservative causes. It was promoted relentlessly by top conservative media personalities, including Sean Hannity, Mark Levin, and Dan Bongino.

And it arrived amid a contentious election in which mainstream platforms’ responsible content moderation — labeling and removing misinformation; promoting reliable information about how to vote — was castigated by many conservatives as outrageous censorship and/or interference with the democratic process.

Then came the January 6th Capitol attack. Parler had been rife with calls for violence leading up to and during the insurrection, and in keeping with the platform’s “free speech” ethos, most had been allowed to stand. Apple responded by removing it from the App Store; Google later followed.

The moment now seems to have passed

By May, Parler had fired its CEO, shored up its content moderation practices, and returned to app stores. But, as Sara Fischer reported in Axios last week, the thrill appears to be gone. According to data from Sensor Tower, Parler downloads went from 517,000 in December to 11,000 in June. It’s part of an overall decline in the popularity of alt-platforms — and in conservative media generally — since former President Trump left office.

At the height of election fever, Parler indeed had a moment. But the moment seems to have passed.

The withering of Parler has not dissuaded other conservatives from attempting to build something similar. On Thursday, Politico reported that former members of Trump’s team were behind Gettr, an app whose stated mission is “fighting cancel culture, promoting common sense, defending free speech, challenging social media monopolies, and creating a true marketplace of ideas.”

This is more or less what Parler set out to do. (Like Parler, Gettr is also essentially a Twitter clone.) But Gettr, by virtue of not having been used to help coordinate a violent insurrection against the government, started with a clean slate.

The slate remained clean for… a few minutes. It quickly became apparent that despite the involvement of former Trump spokesman Jason Miller, Trump himself had no intentions of actually joining Gettr. Meanwhile, multiple hashtags with racist and anti-Semitic slurs hit the app’s trending section, according to Recode, and multiple reports found a torrent of porn. (Sonic the Hedgehog porn, in particular.)

A growing number of accounts on Jason Miller's new right-wing ""Gettr"" site posting ""Sonic the Hedgehog furry porn"" are being banned from the platform -- and now there is a push among leftist users to argue that ""furry porn is protected under the first amendment."" — Zachary Petrizzo (@ZTPetrizzo) July 6, 2021

Then the Daily Beast reported that the whole thing had been funded by a fugitive Chinese billionaire. Then Gettr’s source code was found out in the open. Then Salon reported that a bug allowed hackers to easily download the personal information of anyone who had created an account on the site.

It’s all going so badly that you almost wonder if the app’s founders intended it this way, my Sidechannel co-host Ryan Broderick writes at Garbage Day:

I’m also beginning to wonder if all these apps are their own grift in a way. Loudly launch a site no one will ever use, claim it’s a free speech sanctuary for Republicans, do the rounds on all the right-wing news outlets, and wait for it to fill up with the worst people on Earth, refuse to moderate it, wait for Apple to ban it from the App Store, and then go back to the right-wing news outlets and screech about liberal cancel culture impacting your ability to share hentai with white nationalist flat earthers or whatever.

When I first read this paragraph I assumed Ryan was exaggerating to make a point. Given the extremely predictable turmoil that emerged from Gettr’s content policies, though, I wonder if there isn’t something to this: a false-flag social network, set up only to watch it burn to the ground.

But let’s say the whole thing isn’t a put-on. What should we take away from the Gettr debacle, and the Parler debacle before that?

Lots of questions about social networks are hard. This one isn’t. If you create a place for people to upload text and images, you have to moderate it — and moderate it aggressively. You have to draw hard lines; you have to move those lines as society evolves and your adversaries adjust; you have to accept difficult trade-offs between users’ well being and their right to express themselves.

Apps like Parler and Gettr offered their conservative users an attractive mirage

Apps like Parler and Gettr offered their conservative users an attractive mirage: a free-speech paradise where they could say the things they couldn’t say elsewhere. It never seemed to occur to anyone that such a move would only select for the worst social media customers on earth, quickly turning the founders’ dreams to ash.

In a sane world, next-generation conservative founders would accept as a given that they would have to police their apps for racism, dangerous misinformation, and other harms. In return, they could use their editorial discretion to promote their favorite culture warriors, rig the trending topics as they wished, and possibly even attract enough advertisers to make the whole thing financially viable.

To be sure, active content moderation is a necessary but not sufficient, condition for running a viable platform. Even if Parler and Gettr had scrubbed themselves entirely of coup talk and Sonic porn, enthusiasm for them may have waned for any number of reasons.

But when you consider why these apps failed as quickly as they did, lax content moderation is surely among the biggest reasons. Most people will only spend so long in a virtual space in which they are surrounded by the worst of humanity. If Parler or Gettr will be remembered at all, it will be because they created networks for conservatives that not even conservatives could stand to be in.

This column was co-published with Platformer, a daily newsletter about Big Tech and democracy.",Social Media,Verge,https://www.theverge.com/2021/7/6/22566043/conservative-social-networks-keep-making-the-same-mistake,"The failure of apps like Parler and Gettr to attract and maintain users showed that even conservatives are unwilling to tolerate networks rife with racism, dangerous misinformation and other harms. Content moderation is necessary to create a viable platform, and most people will not stay in a virtual space surrounded by the worst of humanity.",Equality & Justice
455,"Singel-Minded: Anatomy of a Backlash, or How Facebook Got an 'F' for Facial Recognition","ANALYSIS -- If you haven't heard yet, Facebook hiked right into another privacy wetland this week, as it started rolling out its facial recognition technology to users outside of North America.

The backlash, led by Graham Cluley blogging for the security firm Sophos, was fierce -- and not altogether focused. European privacy regulators immediately began an inquiry - a bad sign for all since there's little the E.U. does worse than investigate privacy issues (opt-in for even first party cookies, for example -- notwithstanding that the E.U. actually has some decent smart rules about information privacy.)

At issue is a feature that Facebook turns on by default. When one of your friends uploads a photo and goes to ""tag"" individuals in the photo, Facebook's facial recognition algorithm makes guesses about the faces in the photos and suggests the right user for the picture. If someone who is not your Facebook friend uploads a picture say from a conference, they will not get the suggestion that it is you (likely in no small part because facial recognition beyond a certain group size is much harder to do.)

It's a pretty common-sense feature and examined coldly, really not very invasive and perhaps not even that useful. Similar features are baked into Apple's iPhoto and Google's Picasa client software. (For my money, the creepiest features of Google and Facebook are Google's default-on Web History recording and Facebook's behind-the-scenes ranking of the strength of your friendship with each of your friends.)

But the backlash is really about two things: 1) the fateful combination of the words ""Facial Recognition"" and ""Facebook"" and 2) Facebook's tone-deaf handling of the feature.

As Cluley points out, Facebook is automatically opting users into the feature without notification. And if you go deep into your privacy settings in Facebook, you can turn off the feature -- but only once it is installed. That's a dumb choice. Facebook already has a way for you to opt out of being used in third-party ads on sites not on Facebook - even though it's a feature it has not rolled out yet. (Go to Account -> Account Setting -> Facebook Ads -> Ads shown by third parties).

There are any number of ways Facebook could have introduced this better - while still choosing to opt all its users in - but somehow the message still hasn't gotten through to Facebook -- even though many people love the site, it's also for many a very creepy site.

That's not about any one feature or even about how a feature works. It's an irrational response to a bigger rational concern, something tech guru Tim O'Reilly sort-of gets as he advises that people should just get over the ""hysteria.""

Facebook has largely cornered the market on identity and it's constantly redefining how we present ourselves in the public sphere and imposing a cost for choosing not to embrace Facebook's new vision of digital personhood.

So of course, many are going to react very badly to the combination of the words ""Facial Recognition"" and ""Facebook,"" and in fact, it might not be irrational. Facebook has a long history of pushing privacy boundaries further and further -- so it's not unrealistic to expect that this feature will, in the future, change to a default beyond just your friends.

And those defaults matter a lot -- just ask Google about Buzz and e-mail contacts -- or read the Facebook-Facial recognition news this week.

See Also:- Facebook’s Gone Rogue; It’s Time for an Open Alternative",Social Media,WIRED,https://www.wired.com/2011/06/anatomy-of-backlash/,"The combination of the words ""Facial Recognition"" and ""Facebook"" has caused a lot of people to become wary of the potential privacy issues that could arise, since Facebook has a history of pushing boundaries with its privacy policies. The backlash has been fierce, with European privacy regulators beginning an inquiry, and Facebook's tone-deaf handling",Security & Privacy
456,Learn From These Bugs. Don't Let Social Media Zombify You,"You’ve heard that social media is screwing with your brain. Maybe you even read about it on social media. (So meta; so messed up.) The neurochemical culprit, dopamine, spikes when you like and get liked, share and are shared. You’ve probably also heard scientists compare the affliction to drug or alcohol addiction. That’s fair. The same part of the brain lights up.

Scroll, scroll, scroll. It’s a phenomenon now so pervasive that it’s got a name: zombie scrolling syndrome. (The security company McAfee coined the phrase in 2016.) We are the undead of lore, shambling through the world, moaning and groaning with half-closed eyes. I’d like to be able to tell you this is a fantastical bit of exaggeration, that we shouldn’t be so hard on ourselves. I can do no such thing.

The analogy, it turns out, has legs. Consider parasites. An astonishing number of them exist in nature, from worms to wasps, and some have the power of mind control. Or, said another way, zombification. And these fiends are doing it in—gulp—ways that bring to mind social media.

Take the jewel wasp. She grabs a cockroach twice her size and drives her stinger through the poor thing’s neck and into its head, feeling around the brain before injecting nonlethal venom in two precise spots. (OK, not quite like Facebook, but stay with me.) Post-surgery, the cockroach just keeps grooming itself while the wasp drags it into a burrow by its antenna. The wasp then lays an egg on the cockroach’s leg, seals the tomb, and goes about her life.

In a few days, the wasp egg hatches into a larva that latches onto the roach and drinks its bodily fluids. Again, the bug doesn’t complain. It’s not paralyzed; it’s fully capable of breaking out of its prison. But the roach doesn’t. As the fluids run dry, the larva burrows into the body to eat the organs one by one, hollowing out the roach’s abdomen while the thing is still alive (read: undead). Eventually it emerges as an adult wasp, finally killing its host.

According to researchers, the wasp’s secret appears to be—wait for it—dopamine. The wasp loads up its venom with the neurotransmitter, and that cocktail alters the roach’s behavior in ways scientists are only beginning to understand. Weirdly, in cockroaches and other creatures, dopamine regulates grooming, hence the insect’s fanatical insistence on cleaning itself instead of running for its life. (Not like humans would ever primp for a totally natural and spontaneous selfie.)

LEARN MORE The WIRED Guide to Internet Addiction

The weapon of choice for other zombifiers is serotonin, another well-studied neurotransmitter. There’s a tiny worm, for instance, that begins life in the stomach of crustaceans called amphipods. Then the worm finds itself with a problem. To live, it has to get into the stomach of a bird, which means it needs its host to get noticed. Complicating things, fish love to eat amphipods. That’s bad for our protagonist: In a fish belly, the worm will dissolve.

So the worm mind-controls its crustacean to spend more time at the pond’s surface, where it’s likelier to draw the attention of birds. That little baby worm can even change its host’s color to a more conspicuous hue. The worm itself isn’t releasing serotonin; somehow it’s short-­circuiting the amphipod’s nervous system to overproduce the chemical. Researchers think this may cause the victim to mistake light for darkness. Instead of diving into the safety of the murky depths, it ascends to the surface—and to death from above.",Social Media,WIRED,https://www.wired.com/story/learn-from-these-bugs-dont-let-social-media-zombify-you/,"Social media can have a detrimental effect on our mental health, as it can lead to an addiction to dopamine, a neurotransmitter that is released when we get ""likes"" or shares. This addiction has been compared to drug or alcohol addiction, and is known as ""zombie scrolling syndrome"". Examples of parasites in nature that use mind control",Social Norms & Relationships
457,How to Avoid Spreading Misinformation About the Protests,"I cannot make sense of what is happening.

I say this as a scholar of polluted information, someone who talks and thinks about political toxicity more than I do anything else in my life. I also say this as someone who, along with everyone else in my field, has watched the dangers gather and anxieties build for years. In my case, just as Covid-19 was picking up steam overseas, I found myself needing to lie on the floor of my office, covered in bags of rice, as I tried to bring down my blood pressure enough to get through my next media literacy lecture. Just say no to nihilism, I’d tell my students and myself.

And then the pandemic hit the US, and we all know what happened next.

Things were bad before. But as our ongoing public health crisis has collided with our ongoing civil rights crisis, both set against the backdrop of our ongoing election integrity crisis, we’re left with an information landscape that’s mostly land mines. From the right-wing hijacking of scientific facts to the transformation of community health best practices into flashpoints for the culture wars to the distrust and disinformation swirling around the police brutality protests, everything has become a weapon—or at least, everything has the potential to become weaponized.

SUBSCRIBE Subscribe to WIRED and stay smart with more of your favorite Ideas writers.

Unsurprisingly, I’ve had lots of people, from reporters to colleagues to friends, reach out to ask, what now? How should we respond to this conspiracy theory? Should we be amplifying that hoax? What should we say about white nationalists posing as antifa online, or about the president’s greenlighting of state violence so he can stand in front of an empty church? My job is to have an answer. Instead, I want to scream—I don’t know, stop asking me!—and fling myself behind my couch to sob.

I’m tired. We’re all tired. Even those of us who are prepared for this, aren’t prepared for this. But that doesn’t mean that nihilism is suddenly an option. The stakes were always high. Now they’re in the stratosphere. Covid-19, combined with systemic racism (itself made manifest through Covid-19), has endangered the health and safety of millions of people. But the potential for damage goes much further than that. The soul of the nation is at risk. Our ability to come back from any of this is at risk. Given all those risks, we have even more reason, and an even greater responsibility, to navigate our networks as carefully as we can. To think about what we amplify and why.

We do this by remembering that mis- and disinformation isn’t just about information. It’s about the harm information can cause, both emotionally and physically. These harms are not equally distributed. People from marginalized communities are exposed to far more harms, far more often, than people who enjoy the default protections of being cis, white, and hetero. Minimizing the harms of polluted information is a social justice issue. It’s also the only way we all can keep from drowning.

So, when considering whether to amplify information—through our tweets or comments or think pieces—the first order of business is to reflect on how harmful the information might be; and, beyond that, to reflect on what harms might result from our sharing. A basic question to ask is whether those harms threaten the bodily autonomy, personal safety, or emotional well-being of people outside the group in which the harmful content was created, or where it first circulated. If the harms don’t extend beyond that group, it may still be necessary to inform the proper authorities (from law enforcement to somebody’s parents). Ethical due diligence is important. But even then, it’s probably better not to react with fireworks; as I’ve discussed before, publicity is not a default public service. If, on the other hand, the content does threaten those outside the specific group where it originated, then a more far-reaching response may be warranted or even necessary.",Social Media,WIRED,https://www.wired.com/story/how-to-avoid-spreading-misinformation-about-the-protests/,"The harms of polluted information, including the potential for disinformation and conspiracy theories to spread quickly, can endanger the health and safety of millions of people, and disproportionately affect people from marginalized communities. It is important to consider the potential harms of sharing information, and to take necessary steps to minimize them.",Equality & Justice
458,Facebook policy VP Richard Allan to face the international ‘fake news’ grilling that Zuckerberg won’t – TechCrunch,"An unprecedented international grand committee comprising 22 representatives from seven parliaments will meet in London next week to put questions to Facebook about the online fake news crisis and the social network’s own string of data misuse scandals.

But Facebook founder Mark Zuckerberg won’t be providing any answers. The company has repeatedly refused requests for him to answer parliamentarians’ questions.

Instead it’s sending a veteran EMEA policy guy, Richard Allan, now its London-based VP of policy solutions, to face a roomful of irate MPs.

Allan will give evidence next week to elected members from the parliaments of Argentina, Brazil, Canada, Ireland, Latvia, Singapore, along with members of the UK’s Digital, Culture, Media and Sport (DCMS) parliamentary committee.

At the last call the international initiative had a full eight parliaments behind it but it’s down to seven — with Australia being unable to attend on account of the travel involved in getting to London.

A spokeswoman for the DCMS committee confirmed Facebook declined its last request for Zuckerberg to give evidence, telling TechCrunch: “The Committee offered the opportunity for him to give evidence over video link, which was also refused. Facebook has offered Richard Allan, vice president of policy solutions, which the Committee has accepted.

“The Committee still believes that Mark Zuckerberg is the appropriate person to answer important questions about data privacy, safety, security and sharing,” she added. “The recent New York Times investigation raises further questions about how recent data breaches were allegedly dealt with within Facebook, and when the senior leadership team became aware of the breaches and the spread of Russian disinformation.”

The DCMS committee has spearheaded the international effort to hold Facebook to account for its role in a string of major data scandals, joining forces with similarly concerned committees across the world, as part of an already wide-ranging enquiry into the democratic impacts of online disinformation that’s been keeping it busy for the best part of this year.

And especially busy since the Cambridge Analytica story blew up into a major global scandal this April, although Facebook’s 2018 run of bad news hasn’t stopped there…

The evidence session with Allan is scheduled to take place at 11:30am (GMT) on November 27 in Westminster. (It will also be streamed live on the UK’s parliament.tv website.)

Afterwards a press conference has been scheduled during which DCMS says a representative from each of the seven parliaments will sign a set of ‘International Principles for the Law Governing the Internet’.

It bills this as “a declaration on future action from the parliaments involved” — suggesting the intent is to generate international momentum and consensus for regulating social media.

The DCMS’s preliminary report on the fake news crisis, which it put out this summer, called for urgent action from government on a number of fronts — including floating the idea of a levy on social media to defence democracy.

However, UK ministers failed to leap into action, merely putting out a tepid ‘wait and see’ response. Marshalling international action appears to be DCMS’s alternative action plan.

At next week’s press conference, grand committee members will take questions following Allan’s evidence, so expect swift condemnation of any fresh equivocation, misdirection or question-dodging from Facebook (which has already been accused by DCMS members of a pattern of evasive behavior).

Last week’s NYT report also characterized the company’s strategy since 2016, vis-a-vis the fake news crisis, as ‘delay, deny, deflect’.

The grand committee will hear from other witnesses, too, including the UK’s information commissioner Elizabeth Denham who was before the DCMS committee recently to report on a wide-ranging ecosystem investigation it instigated in the wake of the Cambridge Analytica scandal.

She told it then that Facebook needs to take “much greater responsibility” for how its platform is being used, and warning that unless the company overhauls its privacy-hostile business model it risks burning user trust for good.

Also giving evidence next week: Deputy information commissioner Steve Wood; the former Prime Minister of St Kitts and Nevis, Rt Hon Dr Denzil L Douglas (on account of Cambridge Analytica/SCL Elections having done work in the region); and the co-founder of PersonalData.IO, Paul-Olivier Dehaye.

Dehaye has also given evidence to the committee before — detailing his experience of making Subject Access Requests to Facebook — and trying and failing to obtain all the data it holds on him.",Social Media,TechCrunch,https://techcrunch.com/2018/11/23/facebook-policy-vp-richard-allan-to-face-the-international-fake-news-grilling-that-zuckerberg-wont/,"The Grand Committee's evidence session is intended to generate international momentum and consensus for regulating social media, in light of the numerous data scandals and fake news crises that have resulted from its unchecked power.",Security & Privacy
459,"Study finds half of Americans get news on social media, but percentage has dropped – TechCrunch","A new report from Pew Research finds that around a third of U.S. adults continue to get their news regularly from Facebook, though the exact percentage has slipped from 36% in 2020 to 31% in 2021. This drop reflects an overall slight decline in the number of Americans who say they get their news from any social media platform — a percentage that also fell by 5 percentage points year-over-year, going from 53% in 2020 to a little less than 48%, Pew’s study found.

By definition, “regularly” here means the survey respondents answered they regularly get news from any of the 10 social media platforms Pew asked about.

The change comes at a time when tech companies have come under heavy scrutiny for allowing misinformation to spread across their platforms, Pew notes. That criticism has ramped up over the course of the pandemic, leading to vaccine hesitancy and refusal, which in turn has led to worsened health outcomes for many Americans who consumed the misleading information.

Despite these issues, the percentage of Americans who regularly get their news from various social media sites hasn’t changed too much over the past year, demonstrating how much a part of people’s daily news habits these sites have become.

In addition to the one-third of U.S. adults who regularly get their news on Facebook, 22% say they regularly get news on YouTube. Twitter and Instagram are regular news sources for 13% and 11% of Americans, respectively.

However, many of the sites have seen small declines as a regular source of news among their own users, says Pew. This is a different measurement compared with the much smaller percentage of U.S. adults who use the sites for news, as it speaks to how the sites’ own user bases may perceive them. In a way, it’s a measurement of the shifting news consumption behaviors of the often younger social media user, more specifically.

Today, 55% of Twitter users regularly get news from its platform, compared with 59% last year. Meanwhile, Reddit users’ use of the site for news dropped from 42% to 39% in 2021. YouTube fell from 32% to 30%, and Snapchat fell from 19% to 16%. Instagram is roughly the same, at 28% in 2020 to 27% in 2021.

Only one social media platform grew as a news source during this time: TikTok.

In 2020, 22% of the short-form video platform’s users said they regularly got their news there, compared with an increased 29% in 2021.

Overall, though, most of these sites have very little traction with the wider adult population in the U.S. Fewer than 1 in 10 Americans regularly get their news from Reddit (7%), TikTok (6%), LinkedIn (4%), Snapchat (4%), WhatsApp (3%) or Twitch (1%).

There are demographic differences between who uses which sites, as well.

White adults tend to turn to Facebook and Reddit for news (60% and 54%, respectively). Black and Hispanic adults make up significant proportions of the regular news consumers on Instagram (20% and 33%, respectively.) Younger adults tend to turn to Snapchat and TikTok, while the majority of news consumers on LinkedIn have four-year college degrees.

Of course, Pew’s latest survey, conducted from July 26 to August 8, 2021, is based on self-reported data. That means people’s answers are based on how the users perceive their own usage of these various sites for newsgathering. This can produce different results compared with real-world measurements of how often users visited the sites to read news. Some users may underestimate their usage and others may overestimate it.

People may also not fully understand the ramifications of reading news on social media, where headlines and posts are often molded into inflammatory clickbait in order to entice engagement in the form of reactions and comments. This, in turn, may encourage strong reactions — but not necessarily from those worth listening to. In recent Pew studies, it found that social media news consumers tended to be less knowledgeable about the facts on key news topics, like elections or COVID-19. And social media consumers were more frequently exposed to fringe conspiracies (which is pretty apparent to anyone reading the comments!).

For the current study, the full sample size was 11,178 respondents, and the margin of sampling error was plus or minus 1.4 percentage points.",Social Media,TechCrunch,https://techcrunch.com/2021/09/20/study-finds-half-of-americans-get-news-on-social-media-but-percentage-has-dropped/,"A recent Pew study found that despite Social Media's popularity as a news source, around a third of U.S. adults regularly using it for news, it may lead to misinformation and vaccine hesitancy, as well as weakening the knowledge of news topics, and exposure to fringe conspiracies.","Information, Discourse & Governance"
460,Facebook releases timeline of Cleveland shooting videos – TechCrunch,"Facebook is facing backlash after a Cleveland man uploaded a video of himself shooting someone to the social network, and followed it with a Live video confessing to the murder. The slaying and its subsequent distribution across Facebook has raised questions about how the company moderates violent content.

Justin Osofsky, Facebook’s vice president of global operations, released a statement and timeline of the events and videos surrounding the incident.

Osofsky’s statements hand off the responsibility for policing content on Facebook to its users, although he acknowledged the company can do better at moderation. He says artificial intelligence and new policies governing how videos are shared could present solutions to the issue, and that Facebook will try to speed up its current review process.

“As a result of this terrible series of events, we are reviewing our reporting flows to be sure people can report videos and other material that violates our standards as easily and quickly as possible. In this case, we did not receive a report about the first video, and we only received a report about the second video — containing the shooting — more than an hour and 45 minutes after it was posted. We received reports about the third video, containing the man’s live confession, only after it had ended,” Osofsky wrote.

11:09AM PDT — First video, of intent to murder, uploaded. Not reported to Facebook.

11:11AM PDT — Second video, of shooting, uploaded.

11:22AM PDT — Suspect confesses to murder while using Live, is live for 5 minutes.

11:27AM PDT — Live ends, and Live video is first reported shortly after.

12:59PM PDT — Video of shooting is first reported.

1:22PM PDT — Suspect’s account disabled; all videos no longer visible to public.

The timeline demonstrates the failures of Facebook’s moderation system, which relies on user reports to flag controversial or violent content. While the Live video of the man’s confession was quickly reported by another user, the video of the killing itself went unreported and therefore remained online for nearly two hours.

“Artificial intelligence, for example, plays an important part in this work, helping us prevent the videos from being reshared in their entirety. (People are still able to share portions of the videos in order to condemn them or for public awareness, as many news outlets are doing in reporting the story online and on television),” Osofsky said.

Even with advances in artificial intelligence, it’s not clear that Facebook can prevent Live from being used to broadcast violence. The livestreaming service has already been used to share videos of shootings, torture, and sexual assault. And while users are angry at Facebook for allowing the Cleveland killing to be livestreamed, users were also outraged when a “technical glitch” caused the removal of video documenting the police murder of Philando Castile. Osofsky says that the Cleveland videos “goes against our policies and everything we stand for,” but there are times when users will expect Facebook to preserve violent videos because they have political importance. It’s a delicate balance, and one that isn’t likely to be solved by AI alone.

“Facebook isn’t going to stop a murder. And I don’t care how good the AI gets, it’s unlikely any time soon to say ‘hey, that video is some person killing another person, don’t stream that.'” Mike Masnick noted on Techdirt. “Yes, senseless murders and violence lead people to go searching for answers, but sometimes there are no answers. And demanding answers from a random tool that was peripherally used connected to the senseless violence doesn’t seem helpful at all.”",Social Media,TechCrunch,https://techcrunch.com/2017/04/17/facebook-release-timeline-of-cleveland-shooting-videos/,"The Cleveland shooting and its subsequent distribution across Facebook has raised questions about how the company moderates violent content, with Facebook's current moderation system relying on user reports to flag controversial or violent content. The incident demonstrates the system's failures and the need for artificial intelligence and new policies to better regulate violent content on the platform.",Security & Privacy
461,Tumblr's Porn Ban Reveals Who Controls What We See Online,"Tumblr was never explicitly a space for porn, but, like most things on the internet, it is chock full of it anyway. Or at least it was. On Monday, to the shock of the millions of users who had used the microblogging site to consume and share porn GIFs, images, and videos, Tumblr banned the “adult content” that its CEO, David Karp, had defended five years prior. In the hours after the announcement, sex workers panicked, users threatened to leave, and—in classic Tumblr fashion—online petitions calling for change gained hundreds of thousands of signatures. But Tumblr’s porn ban isn’t about porn or Tumblr at all, really. It’s about the companies and institutions who wield influence over what does and doesn’t appear online.

When Melissa Drew, an adult content creator and model, logged in to Tumblr Monday afternoon, she was greeted by a deluge of unfamiliar posts and notifications. Her usual feed, perfectly curated after nearly a decade of tinkering, was awash with panicked posts from fellow adult models, memes about the policy change, and goodbye posts. Drew’s personal blog, which she had relied on as a public-facing way to tease the content available to her Patreon subscribers, was lit up with notifications from Tumblr informing her that most of her posts violated the new rules.

When Yahoo bought Tumblr for $1.1 billion in 2013, critics warned that premium advertisers wouldn't exactly be clamoring to run ads in a sea of porn. Yahoo CEO Marissa Mayer disagreed, arguing that targeting tools would keep content that isn't ""brand-safe"" (read: porn) away from ads. In an interview shortly after the acquisition, Karp doubled down on the platform’s openness to porn. “When you have … any number of very talented photographers posting tasteful photography,” said Karp, “I don't want to have to go in there to draw the line between this photo and the behind the scenes photo of Lady Gaga and like, her nip.""

But the nip line has indeed been drawn, and it’s a doozy. Though Tumblr’s Monday announcement had technically only prohibited depictions of sex acts, “human genitalia,” and “female-presenting nipples,” a much wider swath of Drew’s feed was quickly caught up in the ban. “I had everything from nude, censored nudity, and lingerie photos flagged,” she told WIRED. “I still haven’t dealt with removing all of them yet—I just sort of heavy sighed and closed the tab.”

Safe Space

In interviews and messages with WIRED, more than 30 sex workers, porn consumers, and creators on Tumblr lamented the loss of what they described as a unique, safe space for curated sexually themed GIFs, photos, and videos. Many users who had used the microblogging site as their primary source for porn were at a loss when asked where they would go after Tumblr’s ban on “adult content” goes into effect on December 17. For the thousands of sex workers who used the site to share their own explicit content in a controlled, relatively contained manner—not to mention the countless others who used that content to fill the hyper-curated feeds of some of the site’s most popular porn blogs—the crackdown’s consequences are even more difficult to unpack. And researchers say the ban could shrink Tumblr’s user base, which already appears in turmoil over the decision.

The move comes less than two weeks after Apple pulled Tumblr from the iOS App Store after child pornography was found on the site. Though the offending illegal content was removed quickly, according to Tumblr, the app has yet to return to the App Store (it was never removed from the Google Play Store). In its most recent blog post, Tumblr stated that its longstanding no-tolerance policy against child pornography should not be conflated with the move to ban adult content. The latter, Tumblr argued, was inspired by a drive to create “a better Tumblr.” But these sorts of decisions aren’t made in a vacuum.",Social Media,WIRED,https://www.wired.com/story/tumblrs-porn-ban-reveals-controls-we-see-online/,"The ban of adult content on Tumblr has had massive repercussions for many of its users, especially sex workers who rely on the site to share their own explicit content. It also has potential to shrink Tumblr's user base, as users are in turmoil over the decision. These sorts of decisions are not made in a vacuum, but rather in response to","Information, Discourse & Governance"
462,Trouble in fandom paradise: Tumblr users lash out against its beta subscription feature – TechCrunch,"The Tumblr community often refers to itself as the Wild West of the internet, and they’re not wrong. A text post with over 70,000 notes puts it best: “Tumblr is my favorite social media site because this place is literally uninhabitable for celebrities. No verification system, no algorithm that boosts their posts, it’s a completely lawless wasteland for them.”

But like any social media company, Tumblr needs to keep itself afloat in order for its users to continue sharing esoteric fan art, incomprehensible shitposts, and overly personal diary entries hidden beneath a “Read More” button. Yesterday, Tumblr announced the limited beta test of its Post+ subscription feature, which — if all goes as planned — will eventually let Tumblr users post paywalled content to subscribers that pay them $3.99, $5.99 or $9.99 per month.

Tumblr is far from the first social media platform to seek revenue this way — Twitter is rolling out Super Follows and a Tip Jar feature, and this week, YouTube announced a tipping feature too. Even Instagram is working on its own version of Twitter’s Super Follows that would let users create “exclusive stories.” But on a website with a community that prides itself as being a “completely lawless wasteland” for anyone with a platform (save for Wil Wheaton and Neil Gaiman, who are simply just vibing), the move toward paywalled content was not welcomed with open arms.

Monetization is a double-edged sword. It’s not considered uncool for a Tumblr artist to link to a third-party Patreon or Ko-fi site on their blog, where their most enthusiastic followers can access paywalled content or send them tips. So Post+ seems like an obvious way for Tumblr to generate revenue — instead of directing followers to other websites, they could build a way for fans to support creators on their own platform while taking a 5% cut. This isn’t unreasonable, considering that Twitter will take 3% revenue from its new monetization tools, while video-centric platforms like YouTube and Twitch take 30% and 50%, respectively. But Tumblr isn’t Twitter, or YouTube, or Twitch. Unlike other platforms, Tumblr doesn’t allow you to see other people’s follower counts, and no accounts are verified. It’s not as easy to tell whether the person behind a popular post has 100 followers or 100,000 followers, and the users prefer it that way. But Post+ changes that, giving bloggers an icon next to their username that resembles a Twitter blue check.

Tumblr rolled out Post+ this week to a select group of hand-picked creators, including Kaijuno, a writer and astrophysicist. The platform announced Post+ on a new blog specific to this product, rather than its established staff blog, which users know to check for big announcements. So, as the most public user who was granted access, the 24-year-old blogger was the target of violent backlash from angry Tumblrites who didn’t want to see their favorite social media site turn into a hypercapitalist hellscape. When Kaijuno received death threats for beta testing Post+, Tumblr’s staff intervened and condemned harassment against Post+ users.

“We want to hear about what you like, what you love, and what concerns you. Even if it’s not very nice. Tell us. We can take it,” Tumblr wrote on its staff blog. “What we won’t ever accept is the targeted harassment and threats these creators have endured since this afternoon. […] all they’re doing is testing out a feature.”

Before making their post, a representative from Tumblr’s staff reached out to Kaijuno directly to check in on them regarding the backlash, but there’s only so much that Tumblr can do after a user has already been threatened for using their product.

“I felt like the sacrificial lamb, because they didn’t announce Post+ beforehand and only gave it to a few people, which landed me in the crosshairs of a very pissed-off user base when I’m just trying to pay off medical bills by giving people the option to pay for content,” Kaijuno told TechCrunch. “I knew there’d be some backlash because users hate any sort of change to Tumblr, but I thought that the brunt of the backlash would be at the staff, and that the beta testers would be spared from most of it.”

Why do Tumblr users perceive monetization as such a threat? It’s not a question of whether or not it’s valuable to support creators, but rather, whether Tumblr is capable of hosting such a service. Multiple long-time, avid Tumblr users that spoke to TechCrunch referenced an incident in late 2020 when people’s blogs were being hacked by spam bots that posted incessant advertisements for a Ray-Ban Summer Sale.

“Tumblr is not the most well-coded website. It’s easy to break features,” Kaijuno added. “I think anything involving trusting Tumblr with your financial information would have gotten backlash.”

Tumblr users also worried about the implications Post+ could have on privacy — in the limited beta, Post+ users only have the ability to block people who are subscribed to their blog if they contact Tumblr support. In cases of harassment by a subscriber, this could leave a blogger vulnerable in a potentially dangerous situation.

“Ahead of our launch to all U.S.-based creators this fall, Post+ will allow creators to block subscribers directly,” a Tumblr spokesperson told TechCrunch.

Still, the Extremely Online Gen Z-ers who now make up 48% of Tumblr know that they can’t expect the platform to continue existing if it doesn’t pull in enough money to pay for its staff and server fees. In 2018, Tumblr lost almost one-third of its monthly page views after all NSFW content was banned — since then, the platform’s monthly traffic has remained relatively stagnant.

A former Tumblr employee told TechCrunch that the feature that became Post+ started out as a tip jar. But higher-ups at Tumblr — who do not work directly with the community — redirected the project to create a paywalled subscription product.

“I think a Tip Jar would be a massive improvement,” said the creator behind the Tumblr blog normal-horoscopes. Through the core audience they developed on Tumblr, they make a living via Patreon, but they don’t find Post+ compelling for their business. “External services [like Patreon] have more options, more benefits, better price points, and as a creator I get to choose how I present them to my audience.”

But a paywalled subscription service is different in the collective eyes of Tumblr. For a site that thrives on fandom, creators that make fan art and fanfiction worry that placing this derivative work behind a paywall — which Post+ encourages them to do — will land them in legal trouble. Even Archive of Our Own, a major fanfiction site, prohibits its users from linking to sites like Patreon or Ko-Fi.

“Built-in monetization attracts businesses, corporate accounts, people who are generally there to make money first and provide content second,” said normal-horoscopes. “It changes the culture of a platform.”

Across Tumblr, upset users are rallying for their followers to take Post+’s feedback survey to express their frustrations. The staff welcomes this.

“As with any new product launch, we expect our users to have a healthy discussion about how the feature will change the dynamics of how people use Tumblr,” a Tumblr spokesperson told TechCrunch. “Not all of this feedback will be positive, and that’s OK. Constructive criticism fuels how we create products and ultimately makes Tumblr a better place.”

Tumblr’s vocal community has been empowered over the years to question whether it’s possible for a platform to establish new revenue streams in a way that feels organic. The protectiveness that Tumblr’s user base feels for the site — despite their lack of faith in staff — sets it apart from social media juggernauts like Facebook, which can put e-commerce front and center without much scrutiny. But even three years after the catastrophic porn ban, it seems hard for Tumblr to grow without alienating the people that make the social network unique.

Platforms like Reddit and Discord have remained afloat by selling digital goods, like coins to reward top posters, or special emojis. Each company’s financial needs are different, but Tumblr’s choice to monetize with Post+ highlights the company’s lack of insight into its own community’s wishes.",Social Media,TechCrunch,https://techcrunch.com/2021/07/22/tumblr-community-lash-out-post-plus-subscription/,The introduction of monetization features such as Post+ on Tumblr has caused backlash from users who fear the platform will become a hypercapitalist hellscape and worry about potential privacy violations and legal issues regarding derivative works.,Equality & Justice
463,Democratic bill would suspend Section 230 protections when social networks boost anti-vax conspiracies – TechCrunch,"Two Democratic senators introduced a bill Thursday that would strip away the liability shield that social media platforms hold dear when those companies boost anti-vaccine conspiracies and other kinds of health misinformation.

The Health Misinformation Act, introduced by Senators Amy Klobuchar (D-MN) and Ben Ray Luján (D-NM), would create a new carveout in Section 230 of the Communications Decency Act to hold platforms liable for algorithmically promoted health misinformation and conspiracies. Platforms rely on Section 230 to protect them from legal liability for the vast amount of user-created content they host.

“For far too long, online platforms have not done enough to protect the health of Americans,” Klobuchar said. “These are some of the biggest, richest companies in the world and they must do more to prevent the spread of deadly vaccine misinformation.”

The bill would specifically alter Section 230’s language to revoke liability protections in the case of “health misinformation that is created or developed through the interactive computer service” if that misinformation is amplified through an algorithm. The proposed exception would only kick in during a declared national public health crisis, like the advent of COVID-19, and wouldn’t apply in normal times. The bill would task the Secretary of the Department of Health and Human Services (HHS) with defining health misinformation.

“Features that are built into technology platforms have contributed to the spread of misinformation and disinformation, with social media platforms incentivizing individuals to share content to get likes, comments, and other positive signals of engagement, which rewards engagement rather than accuracy,” the bill reads.

The bill also makes mention of the “disinformation dozen” — just 12 people, including anti-vaccine activist Robert F. Kennedy Jr. and a grab bag of other conspiracy theorists, who account for a massive swath of the anti-vax misinformation ecosystem. Many of the individuals on the list still openly spread their messaging through social media accounts on Twitter, Facebook and other platforms.

Section 230’s defenders generally view the idea of new carveouts to the law as dangerous. Because Section 230 is such a foundational piece of the modern internet, enabling everything from Yelp and Reddit to the comment section below this post, they argue that the potential for unforeseen second-order effects means the law should be left intact.

But some members of Congress — both Democrats and Republicans — see Section 230 as a valuable lever in their quest to regulate major social media companies. While the White House is pursuing its own path to craft consequences for overgrown tech companies through the Justice Department and the FTC, Biden’s office said earlier this week that the president is “reviewing” Section 230 as well. But as Trump also discovered, weakening Section 230 is a task that only Congress is positioned to accomplish — and even that is still a long shot.

After a recent war of words with President Biden over its handling of Covid-19 misinformation, Facebook struck a cooperative if vague note on the new Democratic bill. “We believe clarification on the difficult and urgent questions about health related misinformation would be helpful and look forward to working with Congress and the industry as we consider options for reform,” Facebook VP of Public Policy Kevin Martin said.

While the new Democratic bill is narrowly targeted as far as proposed changes to Section 230 go, it’s unlikely to attract bipartisan support. Republicans are also interest in stripping away some of Big Tech’s liability protections, but generally hold the view that platforms remove too much content rather than too little. Republicans are also more likely to sow misinformation about the COVID-19 vaccines themselves, framing vaccination as a partisan issue. Whether the bill goes anywhere, it’s clear that an alarming portion of Americans have no intention of getting vaccinated — even with a much more contagious variant on the rise and colder months on the horizon.

“As COVID-19 cases rise among the unvaccinated, so has the amount of misinformation surrounding vaccines on social media,” Luján said of the proposed changes to Section 230. “Lives are at stake.”",Social Media,TechCrunch,https://techcrunch.com/2021/07/22/section-230-health-misinformation-act/,"Social media has been widely criticized for promoting anti-vaccine conspiracies, health misinformation, and other dangerous content, putting lives at stake. A new bill introduced by two Democratic senators would strip away the liability shield that social media platforms currently hold in order to protect against such content.",Security & Privacy
464,Jon Ossoff Was the Congressional Candidate Social Media Built,"Losing a political race is a messy thing. As Hillary Clinton’s failed presidential campaign proved, fingers inevitably point in every direction, alternately blaming the candidate, the team, the message, and the opposition’s dirty tactics for the loss. It’s never as simple as any of these explanations. But Jon Ossoff’s defeat Tuesday night by Karen Handel in Georgia’s sixth congressional district contained yet another layer of messiness: it was a local race that, thanks to the immediacy of social media, felt as it if was happening in millions of backyards across the country.

If President Obama was the first presidential candidate to use social media and President Trump was the first to weaponize it, then Ossoff was the first congressional candidate to harness the full force of a national social media campaign for a race that turned out just 241,500 voters. Armed with a catchy hashtag (#VoteYourOssoff), crowdfunding campaigns sponsored by liberal websites like the Daily Kos, and organizations like Swing Left and Flippable funneling donations Ossoff’s way, the 30-year-old first-time candidate raised nearly $24 million, two-thirds of which came from 200,000 small-dollar donors. All in, the race became the most expensive in House history. Knowing little about the man or the place he hoped to represent, donors from liberal bastions of California, New York, and Massachusetts poured money into Ossoff’s campaign and turned him into a household name because, well, the internet made it easy.

“Imagine this special election without social media, without the internet. He would never have raised that much money. Word just wouldn’t have spread, and it would have been too hard to give,” says Nicco Mele, director of the Shorenstein Center on Media, Politics and Public Policy at Harvard. “It’s an example of technology skewing the traditional rules of politics.”

But in the end, did all that attention ultimately help or hurt Ossoff? The answer: it’s complicated. Since the unlikely rise of Howard Dean back in 2004 when blogging was in its infancy, technology has enabled candidates to connect directly with large audiences--and raise larger sums of money from them. But it also complicates things. When all politics is social, it can sometimes skew public perception of a candidate’s real odds and electability. And when that candidate loses, it can have a morale-crushing effect that extends far beyond the borders of Georgia’s sixth district.

“The internet loves an underdog,” says Mele. ""And that’s a blessing because it gives you the resources to actually compete rather than be a footnote, but it’s a curse in that sometimes underdogs are going to be underdogs, and it doesn’t matter how much money you raise or how visible you are.”

Ossoff’s funding, says Catherine Vaughan, co-founder of the Democratic crowdfunding group Flippable, “spiraled to an unsustainable level.” As a congressional candidate, Vaughan says, there are “diminishing marginal returns” to raising gobs of money. The more money you raise, the more expensive you're going to make the race, prodding your opponent to spend that much more against you.",Social Media,WIRED,https://www.wired.com/story/ossoff-social-media-campaign/,"Social media can often give national attention to local races, and while this can bring resources to the candidate, it can also skew public perception of their electability and lead to a morale-crushing defeat.",Politics
465,Social media firms facing fresh political pressure after London terror attack – TechCrunch,"Yesterday U.K. government ministers once again called for social media companies to do more to combat terrorism. “There should be no place for terrorists to hide,” said Home Secretary Amber Rudd, speaking on the BBC’s Andrew Marr program.

Rudd’s comments followed the terrorist attack in London last week, in which lone attacker Khalid Masood drove a car into pedestrians walking over Westminster bridge before stabbing a policeman to death outside Parliament.

Press reports of the police investigation have suggested Masood used the WhatsApp messaging app minutes before commencing the attack last Wednesday.

“We need to make sure that organisations like WhatsApp, and there are plenty of others like that, don’t provide a secret place for terrorists to communicate with each other,” Rudd told Marr. “It used to be that people would steam open envelopes or just listen in on phones when they wanted to find out what people were doing, legally, through warranty.

“But on this situation we need to make sure that our intelligence services have the ability to get into situations like encrypted WhatsApp.”

Rudd’s comments echo an earlier statement, made in January 2015, by then Prime Minister David Cameron, who argued there should not be any means of communication that “in extremis” cannot be read by the intelligence agencies.

Cameron’s comments followed the January 2015 terror attacks in Paris in which Islamic extremist gunmen killed staff of the Charlie Hebdo satirical magazine and shoppers at a Jewish supermarket.

Safe to say, it’s become standard procedure for politicians to point the finger of blame at technology companies when a terror attack occurs — most obviously as this allows governments to spread the blame for counterterrorism failures.

Facebook, for instance, was criticized after a 2014 report by the U.K. Intelligence and Security Committee into the 2013 killing of solider Lee Rigby by two extremists who had very much been on the intelligence services’ radar. Yet the Parliamentary ISC concluded the only “decisive” possibility for preventing the attack required the internet company to have proactively identified and reported the threat — a suggestion that effectively outsources responsibility for counterterrorism to the commercial sector.

Writing in a national newspaper yesterday, Rudd also called for social media companies to do more to tackle terrorism online. “We need the help of social media ­companies: the Googles, the Twitters, the Facebooks, of this world,” she wrote. “And the smaller ones, too — ­platforms like Telegram, WordPress and Justpaste.it.”

Rudd also said Google, Facebook and Twitter had been summoned to a meeting to discuss action over extremism, as well as suggesting the government is considering including new proposals to make internet giants take down hate videos quicker in a forthcoming counterterrorism strategy — which would appear to mirror a push in Germany. The government there proposed a new law earlier this month to require social media firms to remove illegal hate speech faster.

So, whatever else it is, a terror attack is a politically opportune moment for governments to apply massively visible public pressure onto a sector known for engineering workarounds to extant regulation — as a power play to try to eke out greater cooperation going forward.

And U.S. tech platform giants have long been under the public counterterrorism cosh in the U.K. — with the then-head of intelligence agency GCHQ arguing, back in 2014, that their platforms had become the “command-and-control networks of choice for terrorists and criminals,” and calling for “a new deal between democratic governments and the technology companies in the area of protecting our citizens.”

“They cannot get away with saying… “

As is typically the case when governments talk about encryption, Rudd’s comments to Marr are contradictory — so on the one hand she’s making the apparently timeless call for tech firms to break encryption and backdoor their services. Yet when pressed on the specifics she also appears to claim she’s not calling for that at all, telling Marr: “We don’t want to open up, we don’t want to go into the cloud and do all sorts of things like that, but we do want [technology companies] to recognise that they have a responsibility to engage with government, to engage with law enforcement agencies when there is a terrorist situation.

“We would do it all through the carefully thought through, legally covered arrangements. But they cannot get away with saying ‘we are in a different situation’ — they are not.”

So, really, the core of her demand is closer co-operation between tech firms and government. And the not so subtle subtext is: ‘we’d prefer you didn’t use end-to-end encryption by default.’

After all, what better way to workaround e2e encryption than to pressure companies not to proactively push its use in the first place… (So even if one potential target’s messages are robustly encrypted, the agencies could hope to find one of their contacts whose messages are still accessible.)

https://twitter.com/e3i5/status/846310052189286401

A key factor informing this political power play is undoubtedly the huge popularity of some of the technology services being targeted. Messaging app WhatsApp has more than a billion active users, for example.

Banning popular tech services would not only likely be technically futile, but any attempt to outlaw mainstream networks would be tantamount to political suicide — hence governments feeling the need to wage a hearts and minds PR war every time there’s another terrorist outrage. The mission is to try to put tech firms on the back foot by turning public opinion against them. (Oftentimes, a goal aided and abetted by sections of the mainstream U.K. media, it must be said.)

In recent years, some tech companies with very large user-bases have also been shown to make high-profile stances championing user privacy — which inexorable sets them on a collision course with governments’ national security priorities.

Consider how Apple and WhatsApp have recently challenged law enforcement authorities’ demands to weaken their security system and/or access encrypted data, for instance.

Apple most visibly in the case of the San Bernardino terrorist’s locked iPhone — where the Cupertino company resisted a demand by the FBI that it write a new version of its OS to weaken the security of the device so it could be unlocked. (In the event, the FBI paid a third-party organization for a hacking tool that apparently enabled it to unlock the device.)

While WhatsApp — aside from the fact the messaging giant has rolled out end-to-end encryption across its entire platform, thereby vastly lowering the barrier to entry to the tech for mainstream consumers — has continued resisting police demands for encrypted data, such as in Brazil, where the service has been blocked several times as a result, on judges’ orders.

Meanwhile, in the U.K., the legislative push in recent years has been to expand the investigatory capabilities of domestic intelligence agencies — with counterterrorism the broad-brush justification for this push to normalize mass surveillance.

The current government rubber-stamped the hugely controversial Investigatory Powers Act at the back end of last year — which puts intrusive powers that had been used previously, without necessarily being avowed to Parliament and authorized via an antiquated legislative patchwork, on a firmer legal footing — including cementing a series of so-called “bulk” (i.e. non-targeted) powers at the heart of the U.K. surveillance state, such as the ability to hack into multiple devices/services under a single warrant.

So the really big irony of Rudd’s comments is that the government has already afforded itself swingeing investigatory powers — even including the ability to require companies to decrypt data, limit the use of end-to-end encryption and backdoor services on warranted request. (And that before you even consider how much intel can profitably be gleaned by intelligence agencies looking at metadata — which end-to-end encryption does not lock behind an impenetrable wall.)

Which begs the question why Rudd is seemingly asking tech companies for something her government has already legislated to be able to demand.

” …stop this stuff even being put up”

Part of this might be down to intelligence agencies being worried that it’s getting harder (and/or more resource intensive) for them to prioritize subjects of interest because the more widespread use of end-to-end encryption means they can’t as easily access and read messages of potential suspects. Instead they might have to directly hack an individual’s device, for instance, which they have legal powers to do should they obtain the necessary warrant.

And it’s undoubtedly true that agencies’ use of bulk collection methods means they are systematically amassing more and more data, which needs to be sifted through to identify possible targets.

So the U.K. government might be testing the water to make a fresh case on the agencies’ behalf — to push for quashing the rise of e2e encryption. (And it’s clear that at least some sections of the Conservative party do not have the faintest idea of how encryption works.) But, well, good luck with that!

https://twitter.com/e3i5/status/846265936508153856

Either way, this is certainly a PR war. And — perhaps most likely — one in which the U.K. government is jockeying for position to slap social media companies with additional extremist-countering measures, as Rudd has hinted are in the works.

Something that, while controversial, is likely to be less so than trying to ban certain popular apps outright, or forcibly outlaw the use of end-to-end encryption.

On taking action against extremist content online, Rudd told Marr the best people to solve the problem are those “who understand the technology, who understand the necessary hashtags to stop this stuff even being put up.” Which suggests the government is considering asking for more preemptive screening and blocking of content. Ergo, some form of keyword censoring.

One possible scenario might be that when a user tries to post a tweet containing a blacklisted keyword they are blocked from doing so until the offending keyword is removed.

Security researcher, and former Facebook employee, Alec Muffett wasted no time branding this hashtag concept “chilling” censorship…

Ignore the ""necessary""; the Home Secretary is literally calling for certain hashtags or words to elicit censorship/blocking on social media: pic.twitter.com/4ISt0Fr5Br — Alec Muffett (@AlecMuffett) March 27, 2017

But mainstream users might well be a lot more supportive of proactive and visible action to try to suppress the spread of extremist material online (however misguided such an approach might be). The fact Rudd is even talking in these terms suggests the government thinks it’s a PR battle they could win.

We reached out to Google, Facebook and Twitter to ask for a response to Rudd’s comments. Google declined to comment, and Twitter had not responded to our questions at the time of writing.

Facebook provided a WhatsApp statement, in which a spokesperson said the company is “horrified by the attack carried out in London earlier this week and are cooperating with law enforcement as they continue their investigations.” But they did not immediately provide a Facebook-specific response to being summoned by the U.K. government for discussions about tackling online extremism.

The company has recently been facing renewed criticism in the U.K. for how it handles complaints relating to child safety, as well as ongoing concerns in multiple countries about how fake news spreads across its platform. On the latter issue, it’s been working with third-party fact-checking organizations to flag disputed content in certain regions. While on the issue of illegal hate speech in Germany, Facebook has said it is increasing the number of people working on reviewing content in the country, and claims to be “committed to working with the government and our partners to address this societal issue.”

It seems highly likely the social media giant will soon have a fresh set of political demands on its plate. And that “humanitarian manifesto” Facebook CEO Mark Zuckerberg penned in February, in which he publicly grappled with some of the societal concerns the platform is sparking, is already looking in need of an update.",Social Media,TechCrunch,https://techcrunch.com/2017/03/27/social-media-firms-facing-fresh-political-pressure-after-london-terror-attack/,"The U.K. government has often pointed the finger at social media companies when it comes to terrorism, and its rhetoric to do so has become more aggressive following the recent attack in London. This puts social media companies in a difficult position, as they are expected to both protect user privacy and cooperate with government demands to remove extremist content, without",Security & Privacy
466,Facebook pushes EU for dilute and fuzzy internet content rules – TechCrunch,"Facebook founder Mark Zuckerberg is in Europe this week — attending a security conference in Germany over the weekend, where he spoke about the kind of regulation he’d like applied to his platform, ahead of a slate of planned closed door meetings with digital heavyweights at the European Commission.

“I do think that there should be regulation on harmful content,” said Zuckerberg during a Q&A session at the Munich Security Conference, per Reuters, making a pitch for bespoke regulation.

He went on to suggest “there’s a question about which framework you use,” telling delegates: “Right now there are two frameworks that I think people have for existing industries — there’s like newspapers and existing media, and then there’s the telco-type model, which is ‘the data just flows through you,’ but you’re not going to hold a telco responsible if someone says something harmful on a phone line.”

“I actually think where we should be is somewhere in between,” he added, making his plea for internet platforms to be a special case.

At the conference he also said Facebook now employs 35,000 people to review content on its platform and implement security measures — including suspending around 1 million fake accounts per day, a stat he professed himself “proud” of.

The Facebook chief is due to meet with key commissioners covering the digital sphere this week, including competition chief and digital EVP Margrethe Vestager, internal market commissioner Thierry Breton and Věra Jourová, who is leading policymaking around online disinformation.

The timing of his trip is clearly linked to digital policymaking in Brussels — with the Commission due to set out its thinking around the regulation of artificial intelligence this week. (A leaked draft last month suggested policymakers are eyeing risk-based rules to wrap around AI.)

More widely, the Commission is wrestling with how to respond to a range of problematic online content — from terrorism to disinformation and election interference — which also puts Facebook’s 2 billion+ social media empire squarely in regulators’ sights.

Another policymaking plan — a forthcoming Digital Service Act (DSA) — is slated to upgrade liability rules around internet platforms.

The details of the DSA have yet to be publicly laid out, but any move to rethink platform liabilities could present a disruptive risk for a content-distributing giant such as Facebook.

Going into meetings with key commissioners Zuckerberg made his preference for being considered a “special” case clear — saying he wants his platform to be regulated not like the media businesses which his empire has financially disrupted; nor like a dumbpipe telco.

On the latter it’s clear — even to Facebook — that the days of Zuckerberg being able to trot out his erstwhile mantra that “we’re just a technology platform,” and wash his hands of tricky content stuff, are long gone.

Russia’s 2016 foray into digital campaigning in the U.S. elections and sundry content horrors/scandals before and since have put paid to that — from nation state-backed fake news campaigns to live-streamed suicides and mass murder.

Facebook has been forced to increase its investment in content moderation. Meanwhile, it announced a News section launch last year — saying it would hand-pick publishers’ content to show in a dedicated tab.

The “we’re just a platform” line hasn’t been working for years. And EU policymakers are preparing to do something about that.

With regulation looming, Facebook is now directing its lobbying energies into trying to shape a policymaking debate — calling for what it dubs “the ‘right’ regulation.”

Here the Facebook chief looks to be applying a similar playbook as Google’s CEO, Sundar Pichai — who recently tripped to Brussels to push for AI rules so dilute they’d act as a tech enabler.

In a blog post published today Facebook pulls its latest policy lever: putting out a white paper which poses a series of questions intended to frame the debate at a key moment of public discussion around digital policymaking.

Top of this list is a push to foreground focus on free speech, with Facebook questioning “how can content regulation best achieve the goal of reducing harmful speech while preserving free expression?” — before suggesting more of the same: (Free, to its business) user-generated policing of its platform.

Another suggestion it sets out which aligns with existing Facebook moves to steer regulation in a direction it’s comfortable with is for an appeals channel to be created for users to appeal content removal or non-removal. Which of course entirely aligns with a content decision review body Facebook is in the process of setting up — but which is not in fact independent of Facebook.

Facebook is also lobbying in the white paper to be able to throw platform levers to meet a threshold of “acceptable vileness” — i.e. it wants a proportion of law-violating content to be sanctioned by regulators — with the tech giant suggesting: “Companies could be incentivized to meet specific targets such as keeping the prevalence of violating content below some agreed threshold.”

It’s also pushing for the fuzziest and most dilute definition of “harmful content” possible. On this Facebook argues that existing (national) speech laws — such as, presumably, Germany’s Network Enforcement Act (aka the NetzDG law) which already covers online hate speech in that market — should not apply to Internet content platforms, as it claims moderating this type of content is “fundamentally different.”

“Governments should create rules to address this complexity — that recognize user preferences and the variation among internet services, can be enforced at scale, and allow for flexibility across language, trends and context,” it writes — lobbying for maximum possible leeway to be baked into the coming rules.

“The development of regulatory solutions should involve not just lawmakers, private companies and civil society, but also those who use online platforms,” Facebook’s VP of content policy, Monika Bickert, also writes in the blog.

“If designed well, new frameworks for regulating harmful content can contribute to the internet’s continued success by articulating clear ways for government, companies, and civil society to share responsibilities and work together. Designed poorly, these efforts risk unintended consequences that might make people less safe online, stifle expression and slow innovation,” she adds, ticking off more of the tech giant’s usual talking points at the point policymakers start discussing putting hard limits on its ad business.

Update: In remarks to journalists following Zuckerberg’s round of meetings, Politico reported Breton gave Facebook’s ideas a chilly reception — saying the company was offering too little “in terms of responsibility and regulation”. He also rejected the idea of a third status for regulating Internet platforms and pointed out Facebook had had nothing to say on “market power”.

“It’s not for us to adapt to those companies, but for them to adapt to us,” Breton added.

During the press briefing Zuckerberg is also reported to have suggested it’s “operationally unfeasible” for a platform of Facebook’s size to apply strict content rules — “given that there are more than 100 billion pieces of content a day, and that we’re not generally producing the content”.",Social Media,TechCrunch,https://techcrunch.com/2020/02/17/facebook-pushes-eu-for-dilute-and-fuzzy-internet-content-rules/,"The main undesirable consequence of Social Media being discussed is the potential for platforms like Facebook to become a source of harmful content, including terrorism and disinformation, and the need for governments to put regulations in place to protect users. Facebook is lobbying for regulations that would give it maximum leeway, but European regulators have expressed reluctance to do so.",Security & Privacy
467,Instagram’s newest test mixes ‘Suggested Posts’ into the feed to keep you scrolling – TechCrunch,"The days of a scrolling to the end of your Instagram feed look to be coming to an end. After adding algorithmic suggestions to the bottom of the app last year, Instagram is running a test that would splice more recommended posts from accounts you don’t follow into the feed with those you do.

In the next few days, the company will begin testing an expansion of “Suggested Posts” which would sprinkle that content through the regular feed. As it stands now, Suggested Posts appear at the bottom of Instagram after you’ve scrolled through all of the content from people you follow and hit the “You’re all caught up” message that the app implemented in 2018. Depending on how many accounts you follow, it’s possible to not run into that message or Instagram’s recommendations very often, if at all.

In addition to boosting the prominence of Suggested Posts, Instagram will test an option that lets users “snooze” the feature, removing it from the feed for 30 days. Anyone in the test will be able to offer feedback when a specific post doesn’t interest them, but it sounds like you won’t be able to disable Suggested Posts in the feed in a permanent way.

The Suggested Posts expansion will be accompanied by a way for users to shape what they see through managing their interests — stuff like cats, makeup or basketball. If you’ve seen enough cats, you can toggle that interest off or tell Instagram that you never wanted to see those damn cats to begin with when it shows you the next one.

A Facebook spokesperson described the expansion of Suggested Posts to TechCrunch as an “extension” of the Instagram feed, noting that the ratio of these algorithmic recommendations to posts from followed accounts will be variable based on how someone uses the app.

The test will roll out to a small number of users in English-speaking countries only, though the company declined to specify how many accounts will be involved.

The experiment might not make it into the final product, but from the way the winds over at Facebook have been blowing lately it looks pretty likely. Like we mentioned, Instagram and parent company Facebook introduced some tools to give people more control over their own behavior on the notoriously addictive-by-design apps back in 2018, including the “You’re all caught up” message and a way to track time spent.

Those tools weren’t a sea change for a company that generally values keeping people glued to its services (and its ads) at all costs, but they showed that Facebook was at least mildly self aware of the conversation about social media addiction sweeping through the tech world at the time.

In 2020, it sounds like Facebook is done humoring those concerns. Instagram is feeling the heat from TikTok’s preternaturally well-tuned endless algorithmic feed and booming success. Like it has so many times in the past, the company is looking to shift its own identity to chase a threatening competitor rather than staying the course or trying something new.

The new way Suggested Posts work is just a test for now, but mixing algorithmic suggestions into the feed with posts from accounts you follow would be a pretty big change to the core way the app works. As it stands, if people want a truly endless Instagram experience they could turn to the Explore tab or scroll past the “caught up” message. Many doubtless did to stave off boredom, to the likely detriment of their mental health.

But under the test, it will be less possible to use Instagram to only keep up with just the accounts that you’ve got a personal interest in, whether they’re friends, local businesses or influencers of your choosing. Instagram wants to inject more of what it wants you to see into that experience, or what the company believes you’d want to see but you just don’t know it yet.

The end result might not be that noticeable for people who follow huge swaths of accounts already and rarely meet the end of their feed, but it strays even further from the original product — a distant memory at this point — while giving Instagram a way to keep people on the app for longer while serving them more ads.",Social Media,TechCrunch,https://techcrunch.com/2021/06/23/instagram-suggested-posts-test-topics/,"Instagram is testing a feature that would inject more algorithmic recommendations from accounts you don't follow into the normal feed, making it more difficult for users to only keep up with accounts of personal interest and potentially keeping them on the app for longer and serving them more ads.",User Experience & Entertainment
468,YouTube tightens restrictions on channel of UK far right activist — but no ban – TechCrunch,"YouTube tightens restrictions on channel of UK far right activist — but no ban

YouTube has placed new restrictions on the channel of a UK far right activist which are intended to make hate speech less easy to discover on its platform.

Restrictions on Stephen Yaxley-Lennon’s YouTube channel include removing certain of his videos from recommendations. YouTube is also taking away his ability to livestream to his now close to 390,000 YouTube channel subscribers.

Yaxley-Lennon, who goes by the name ‘Tommy Robinson’ on social media, was banned from Twitter a year ago.

Buzzfeed first reported the new restrictions. A YouTube spokesperson confirmed the shift in policy, telling us: “After consulting with third party experts, we are applying a tougher treatment to Tommy Robinson’s channel in keeping with our policies on borderline content. The content will be placed behind an interstitial, removed from recommendations, and stripped of key features including livestreaming, comments, suggested videos, and likes.”

Test searches for ‘Tommy Robinson’ on YouTube now return a series of news reports — instead of Yaxley-Lennon’s own channel, as was the case just last month.

YouTube had already demonetized Yaxley-Lennon’s channel back in January for violating its ad policies.

But as we reported last month Google has been under increasing political pressure in the UK to tighten its policies over the far right activist.

The policy shift applies to videos uploaded by Yaxley-Lennon that aren’t illegal or otherwise in breach of YouTube’s community standards (as the company applies them) but which have nonetheless been flagged by users as potential violations of the platform’s policies on hate speech and violent extremism.

In such instances YouTube says it will review the videos and those not in violation of its policies but which nonetheless contain controversial religious or extremist content will be placed behind an interstitial, removed from recommendations, and stripped of key features including comments, suggested videos, and likes.

Such videos will also not be eligible for monetization.

The company says its goal with the stricter approach to Yaxley-Lennon’s content is to strike a balance between upholding free expression and a point of public and historic record, while also keeping hateful content from being spread or recommended to others.

YouTube said it carefully considered Yaxley-Lennon’s case — consulting with external experts and UK academics — before deciding it needed to take tougher treatment.

Affected videos will still remain on YouTube — albeit behind an interstitial. They also won’t be recommended, and will be stripped of the usual social features including comments, suggested videos, and likes.

Of course it remains to be seen how tightly YouTube will apply the new more restrictive policy in this case. And whether Yaxley-Lennon himself will adapt his video strategy to workaround tighter rules on that channel.

The far right is very well versed in using coded language and dog whistle tactics to communicate with its followers and spread racist messages under the mainstream radar.

Yaxley-Lennon has had a presence on multiple social media channels, adapting the content to the different platforms. Though YouTube is the last mainstream channel still available to him after Facebook kicked him off its platform in February. Albeit, he was quickly able to workaround Facebook’s ban simply by using a friend’s Facebook account to livestream himself harassing a journalist at his home late at night.

Police were called out twice in that instance. And in a vlog uploaded to YouTube after the incident Yaxley-Lennon threatened other journalists to “expect a knock at the door”.

Shortly afterwards the deputy leader of the official opposition raised his use of YouTube to livestream harassment in parliament, telling MPs then that: “Every major social media platform other than YouTube has taken down Stephen Yaxley-Lennon’s profile because of his hateful conduct.”

The secretary of state for digital, Jeremy Wright, responded by urging YouTube to “reconsider their judgement” — saying: “We all believe in freedom of speech. But we all believe too that that freedom of speech has limits. And we believe that those who seek to intimidate others, to potentially of course break the law… that is unacceptable. That is beyond the reach of the type of freedom of speech that we believe should be protected.”

YouTube claims it removes videos that violate its hate speech and violent content policies. But in previous instances involving Yaxley-Lennon it has told us that specific videos of his — including the livestreamed harassment that was raised in parliament — do not constitute a breach of its standards.

It’s now essentially admitting that those standards are too weak in instances of weaponized hate.

Yaxley-Lennon, a former member of the neo-nazi British National Party and one of the founders of the far right, Islamophobic English Defence League, has used social media to amplify his message of hate while also soliciting donations to fund individual far right ‘activism’ — under the ‘Tommy Robinson’ moniker.

The new YouTube restrictions could reduce his ability to leverage the breadth of Google’s social platform to reach a wider and more mainstream audience than he otherwise would.

Albeit, it remains trivially easy for anyone who already knows the ‘Tommy Robinson’ ‘brand’ to workaround the YouTube restrictions by using another mainstream Google-owned technology. A simple Google search for “Tommy Robinson YouTube channel” returns direct links to his channel and content at the top of search results.

Yaxley-Lennon’s followers will also continue to be able to find and share his YouTube content by sharing direct links to it — including on mainstream social platforms.

Though the livestream ban is a significant restriction — if it’s universally applied to the channel — which will make it harder for Yaxley-Lennon to communicate instantly at a distance with followers in his emotive vlogging medium of choice.

He has used the livestreaming medium skilfully to amplify and whip up hate while presenting himself to his followers as a family man afraid for his wife and children. (For the record: Yaxley-Lennon’s criminal record includes convictions for violence, public order offences, drug possession, financial and immigration frauds, among other convictions.)

If Google is hoping to please everyone by applying a ‘third route’ of tighter restrictions for a hate speech weaponizer yet no total ban it will likely just end up pleasing no one and taking flak from both sides.

The company does point out it removes channels of proscribed groups and any individuals formally linked to such groups. And in this case the related far right groups have not been proscribed by the UK government. So the UK government could certainly do much more to check the rise of domestic far right hate.

But YouTube could also step up and take a leadership position by setting robust policies against individuals who seek to weaponize hate.

Instead it continues to fiddle around the edges — trying to fudge the issue by claiming it’s about ‘balancing’ speech and community safety.

In truth hate speech suppresses the speech of those it targets with harassment. So if social networks really want to maximize free speech across their communities they have to be prepared to weed out bad actors who would shrink the speech of minorities by weaponizing hate against them.",Social Media,TechCrunch,https://techcrunch.com/2019/04/02/youtube-tightens-restrictions-on-channel-of-uk-far-right-activist-but-no-ban/,"Social media platforms have been under increasing pressure to tighten their policies against far right activists, such as Stephen Yaxley-Lennon, who have been using online platforms to spread hate speech and harass journalists and minorities. YouTube has implemented new restrictions on Yaxley-Lennon's channel, but this may not be enough to stop",Equality & Justice
469,UK wants tech firms to build tools to block terrorist content – TechCrunch,"U.K. Home Secretary Amber Rudd is holding talks with several major internet companies today to urge them to be more proactive about tackling the spread of extremist content online. Companies in attendance include Google, Microsoft, Twitter and Facebook, along with some smaller internet companies.

We’ve contacted the four named companies for comment and will update this story with any response.

Writing in The Telegraph newspaper on Saturday, in the wake of last week’s terror attack in London, Rudd said the U.K. government will shortly be setting out an updated counterterrorism strategy that will prioritize doing more to tackle radicalization online.

“Of paramount importance in this strategy will be how we tackle radicalisation online, and provide a counter-narrative to the vile material being spewed out by the likes of Daesh, and extreme Right-wing groups such as National Action, which I made illegal last year,” she wrote. “Each attack confirms again the role that the internet is playing in serving as a conduit, inciting and inspiring violence, and spreading extremist ideology of all kinds.”

Leaning on tech firms to build tools appears to be a key plank of that forthcoming strategy.

A government source told us that Rudd will urge web companies today to use technical solutions to automatically identify terrorist content before it can be widely disseminated.

We also understand the home secretary wants the companies to form an industry-wide body to take greater responsibility for tackling extremist content online — which is a slightly odd ask, given Facebook, Microsoft, Twitter and YouTube already announced such a collaboration, in December last year (including creating a shared industry database for speeding up identification and removal of terrorist content).

Perhaps Rudd wants more internet companies to be part of the collaboration. Or else more effective techniques for identifying and removing content at speed to be developed.

At today’s round-table we’re told Rudd will also raise concerns about encryption — another technology she criticized in the wake of last week’s attack, arguing that law enforcement agencies must be able to “get into situations like encrypted WhatsApp.”

Such calls are of course hugely controversial, given how encryption is used to safeguard data from exploitation by bad actors — the U.K. government itself utilizes encryption technology, as you’d expect.

So it remains to be seen whether Rudd’s public call for encrypted data to be accessible to law enforcement agencies constitutes the beginning of a serious clampdown on end-to-end encryption in the U.K. (NB: The government has already given itself powers to limit companies’ use of the tech, via last year’s Investigatory Powers Act) — or merely a strategy to apply high-profile pressure to social media companies in trying to strong-arm them into doing more about removing extremist content from their public networks.

We understand the main thrust of today’s discussions will certainly be on the latter issue, with the government seeking greater co-operation from social platforms in combating the spread of terrorist propaganda. Encryption is set to be discussed in further separate discussions, we are told.

In her Telegraph article, Rudd argued that the government cannot fight terrorism without the help of internet companies, big and small.

“We need the help of social media companies, the Googles, the Twitters, the Facebooks of this world. And the smaller ones, too: platforms such as Telegram, WordPress and Justpaste.it. We need them to take a more proactive and leading role in tackling the terrorist abuse of their platforms. We need them to develop further technology solutions. We need them to set up an industry-wide forum to address the global threat,” she wrote.

One stark irony of the Brexit process — which got under way in the U.K. this Wednesday, when the government formally informed the European Union of its intention to leave the bloc — is that security cooperation between the U.K. and the EU is apparently being used as a bargaining chip, with the U.K. government warning it may no longer share data with the EU’s central law enforcement agency in the future if there is no Brexit deal.

Which does rather throw a sickly cast o’er Rudd’s call for internet companies to be more proactive in fighting terrorism.

Not all of the companies Rudd called out in her article will be in attendance at today’s meeting. Pavel Durov, co-founder of the messaging app Telegram, confirmed to TechCrunch that it will not be there, for instance. The messaging app has frequently been criticized as a “tool of choice” for terrorists, although Durov has stood firm in his defense of encryption — arguing that users’ right to privacy is more important than “our fear of bad things happening.”

Telegram has today announced the rollout of end-to-end encrypted voice calls to its platform, doubling down on one of Rudd’s technologies of concern (albeit, Telegram’s “homebrew” encryption is not the same as the respected Signal Protocol, used by WhatsApp, and has taken heavy criticism from security researchers).

But on the public propaganda front, Telegram does already act to remove terrorist content being spread via its public channels. Earlier this week it published a blog post defending the role of end-to-end encryption in safeguarding people’s privacy and freedom of speech, and accusing the mass media of being the priory conduit through which terrorist propaganda spreads.

“Terrorist channels still pop up [on Telegram] — just as they do on other networks — but they are reported almost immediately and are shut down within hours, well before they can get any traction,” it added.

Meanwhile, in a biannual Transparency Report published last week, Twitter revealed it had suspended a total of 636,248 accounts, between August 1, 2015 through to December 31, 2016, for violations related to the promotion of terrorism — saying the majority of the accounts (74 percent) were identified by its own “internal, proprietary spam-fighting tools,” i.e. rather than via user reports.

Twitter’s report underlines the scale of the challenge posed by extremist content spread via social platforms, given the volume of content uploads involved — which are orders of magnitude greater on more popular social platforms like Facebook and YouTube, meaning there’s more material to sift through to locate and eject any extremist material.

In February, Facebook CEO Mark Zuckerberg also discussed the issue of terrorist content online, and specifically his hope that AI will play a larger role in the future to tackle this challenge, although he also cautioned that “it will take many years to fully develop these systems.”

“Right now, we’re starting to explore ways to use AI to tell the difference between news stories about terrorism and actual terrorist propaganda so we can quickly remove anyone trying to use our services to recruit for a terrorist organization. This is technically difficult as it requires building AI that can read and understand news, but we need to work on this to help fight terrorism worldwide,” he wrote then.

In an earlier draft of the open letter, Zuckerberg suggested AI could even be used to identify terrorists plotting attacks via private channels — likely via analysis of account behavior patterns, according to a source, not by backdooring encryption (the company already uses machine learning for fighting spam and malware on the end-to-end encrypted WhatsApp, for example).

His edited comment on private channels suggests there are metadata-focused alternative techniques that governments could pursue to glean intel from within encrypted apps without needing to demand access to the content itself — albeit, political pressure may well be on the social platforms themselves to be doing the leg work there.

Rudd is clearly pushing internet companies to do more and do it quicker when it comes to removing extremist content. So Zuckerberg’s time frame of a potential AI fix “many years” ahead likely won’t wash. Political time frames tend to be much tighter.

She’s not the only politician stepping up the rhetoric. Social media giants are facing growing pressure in Germany, which earlier this month proposed a new law for social media platforms to deal with hate-speech complaints. The country previously secured agreements from the companies to remove illegal content within 24 hours of a complaint being made, but the government has accused Facebook and Twitter especially of not taking user complaints seriously enough — hence, it says, it’s going down a legislative route now.

A report in The Telegraph last week suggested the U.K. government is also considering a new law to prosecute internet companies if terrorist content is not immediately taken down when reported. Although ministers were apparently questioning how such a law could be enforced when companies are based overseas, as indeed most of the internet companies in question are.

Another possibility: the Home Office was selectively leaking a threat of legislation ahead of today’s meeting, to try to encourage internet companies to come up with alternative fixes.

Yesterday, digital and humans rights groups, including Privacy International, the Open Rights Group, Liberty and Human Rights Watch, called on the U.K. government to be “transparent” and “open” about the discussions it’s having with internet companies. “Private, informal agreements are not consistent with open, democratic governance,” they wrote.

“Government requests directed to tech companies to take down content is de facto state censorship. Some requests may be entirely legitimate but the sheer volumes make us highly concerned about their validity and the accountability of the processes.”

“We need assurances that only illegal material will be sought out by government officials and taken down by tech companies,” they added. “Transparency and judicial oversight are needed over government takedown requests.”

The group also called out Rudd for not publicly referencing existing powers at the government’s disposal, and expressed concern that any “technological limitations to encryption” they seek could have damaging implications for citizens’ “personal security.”

They wrote:

We also note that Ms Rudd may seek to use Technical Capability Notices (TCNs) to enforce changes [to encryption]; and these would require secrecy. We are therefore surprised that public comments by Ms Rudd have not referenced her existing powers. We do not believe that the TCN process is robust enough in any case, nor that it should be applied to non-UK providers, and are concerned about the precedent that may be set by companies complying with a government over requests like these.

The Home Office did not respond to a request for comment on the group’s open letter, nor respond to specific questions about its discussions today with internet companies, but a government source told us that the meeting is private.

Earlier this week Rudd faced ridicule on social media, and suggestions from tech industry figures that she does not fully understand the workings of the technologies she’s calling out, following comments made during a BBC interview on Sunday — in which she said people in the technology industry understand “the necessary hashtags to stop this stuff even being put up.”

The more likely explanation is that the undoubtedly well-briefed home secretary is playing politics in an attempt to gain an edge with a group of very powerful, overseas-based internet giants.

Update: Following today’s meeting the Home Secretary has put out the following statement:

My starting point is pretty straightforward. I don’t think that people who want to do us harm should be able to use the internet or social media to do so. I want to make sure we are doing everything we can to stop this. It was a useful discussion and I’m glad to see that progress has been made. We focused on the issue of access to terrorist propaganda online and the very real and evolving threat it poses. I said I wanted to see this tackled head-on and I welcome the commitment from the key players to set up a cross-industry forum that will help to do this. In taking forward this work I’d like to see the industry to go further and faster in not only removing online terrorist content but stopping it going up in the first place. I’d also like to see more support for smaller and emerging platforms to do this as well, so they can no longer be seen as an alternative shop floor by those who want to do us harm.

A Facebook spokesman provided TechCrunch with the following joint letter from the four major Internet companies attending the meeting (emphasis mine):",Social Media,TechCrunch,https://techcrunch.com/2017/03/30/uk-wants-tech-firms-to-build-tools-to-block-terrorist-content/,"Social media platforms have become a conduit for the spread of extremist content and terrorism, which is a major concern for the U.K. government and the internet companies attending today's meeting. The companies are committed to working together to develop tools and techniques to automatically identify, remove and prevent extremist content, as well as supporting smaller platforms to do",Security & Privacy
470,Scientists Warn That Social Media Could Be a Threat to Civilization,"""And they often tend to fail catastrophically, unexpectedly, without warning.""

Great Filter

What if human civilization could survive wars and plagues, but not social media?

That’s the question that seems to motivate an alarming new paper, published in the elite journal Proceedings of the National Academy of Sciences and authored by a strikingly diverse team of researchers in biology, psychology, neural and climate science, and more. It’s a bleak read — and, possibly, a call to action to better understand the volatile ways that misinformation spreads online.

Pressure Cooker

Co-author and University of Washington researcher Joseph Bak-Coleman came out swinging about the paper in a provocative new interview with Vox. Here’s what he had to say:

The question we were trying to answer was, “What can we infer about the course of society at scale, given what we know about complex systems?”

Advertisement

Advertisement

It’s kind of how we use mice models or flies to understand neuroscience. Part of this came back to animal societies — namely groups — to understand what they tell us about collective behavior in general, but also complex systems more broadly.

So our goal is to take that perspective and then look at human society with that. And one of the things about complex systems is they have a finite limit to perturbation. If you disturb them too much, they change. And they often tend to fail catastrophically, unexpectedly, without warning.

Trouble Brewing

Bak-Coleman and his colleagues point to many examples of areas in which they say social media has disrupted the flow of reliable info about health, climate and many other pressing topics. But they also expressed a tension at the heart of the work: that while the internet can be a terrible force, it can also be a force for good.

“Democratizing information has had profound effects, especially for marginalized, underrepresented communities,” Bak-Coleman told Vox. “It gives them the ability to rally online, have a platform, and have a voice. And that is fantastic. At the same time, we have things like genocide of Rohingya Muslims and an insurrection at the Capitol happening as well.”

Advertisement

Advertisement

READ MORE: Why some biologists and ecologists think social media is a risk to humanity [Vox]



More on social media: Zuckerberg: Facebook is Building a Machine to Read Your Thoughts

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.",Social Media,Futurism,https://futurism.com/the-byte/scientists-social-media-civilization,"Social Media has caused disruption in the flow of reliable information on topics such as health and climate, and has also been linked to events such as genocide and insurrection. This paper warns of the catastrophic consequences of too much disturbance in complex systems.","Information, Discourse & Governance"
471,What Does the Pope Think About Technology? #It'sComplicated,"He's got a stunning 7.2 million followers on Twitter but believes digital media ""can stop people from learning how to live wisely, to think deeply and to love generously.""

He has posed for selfies in St. Peter's Square, but has lamented the fact that so much communication online is purely about display, not real connection.

He's called the Internet a ""gift from God."" But he's also warned that the abundance of data and digital stimulation we all consume each day can amount to a kind of ""mental pollution"" that harms our relationships and shields us from the real pain and joy that comes with human interaction.

Even as he cautions against over-reliance on technology, the pope has no choice but to embrace it.

On the surface, Pope Francis' thoughts on the power of the Internet can appear contradictory, particularly when the Internet is flooded with news about his historic arrival in the US, complete with hashtags and PopeMojis. And yet, these seemingly conflicting approaches to technology reflect what it means to be the pontiff in the digital age. Even as he cautions against over-reliance on technology, he has no choice but to embrace it.

Meeting People Where They Are

The fact is, this is a pope who has a clear message for the Catholic Church and its congregants. It's a message of inclusion and openness, and to spread it, the pope has made a number of unusual personal choices, including living not in the Apostolic Palace but in a Vatican guesthouse, which allows him “to live in community with others."" And yet, even at 78, Pope Francis is not oblivious to the fact that the most powerful way to spread a message in 2015 is through social media.

""The Pope has to get the church to open up and connect and meet people where they are,"" says Jason Deal, executive vice president of strategy at the Catholic media firm Aleteia USA. ""He can’t do it door-to-door.""

'The Pope has to get the church to open up and connect and meet people where they are.' Jason Deal, Aleteia USA

The Pope's misgivings about technology, Deal says, were top of mind when Aleteia began planning a social media campaign around his US tour. The company is the driving force behind the Twitter account @PopeIsHope, which is following the Pope's travels in the States, snapchatting live from his flight on Shepherd One, and running giveaway contests on Twitter. ""We’ve had long conversations about it,"" Deal says. ""We believe what the Vatican is saying is technology can be very isolating. But he also understands and appreciates, especially when it comes to young people, that it is where they live their lives to a large degree.""

Being able to engage young Catholics is critical, researchers believe, to Pope Francis' ability to move the needle on issues that are important to him, including climate change and building a more inclusive church. ""He’s looking to the future, and he's not going to be around in that future. Neither is his generation,"" says Paul Elie, a senior fellow at the Berkley Center for Religion, Peace, and World Affairs at Georgetown University and author of a recent Vanity Fair article on Pope Francis. ""Anyone who looks forward has to count on the next generation. He instinctively recognized that.""

And whether the pope likes it or not, the next generation is a digital one. The best he can do to foster that generation, rather than alienate it, is give people fair warning about the moral traps technology presents, then smile for their selfies.

""With the selfie, in one moment he joins young people happily and without any scruple,"" says Elie. ""He says, 'You’re young. I’m 78. You’re American. I’m Argentinian, You’re a college student. I’m the Pope. Let's all get together.""",Social Media,WIRED,https://www.wired.com/2015/09/pope-francis-technology/,"Pope Francis has warned that too much reliance on technology and digital stimulation can lead to ""mental pollution"" that harms our relationships and shields us from real human interaction.",Social Norms & Relationships
472,UK names its pick for social media ‘harms’ watchdog – TechCrunch,"The UK government has taken the next step in its grand policymaking challenge to tame the worst excesses of social media by regulating a broad range of online harms. As a result, it has named Ofcom, the existing communications watchdog, as its preferred pick for enforcing rules around “harmful speech” on platforms such as Facebook, Snapchat and TikTok in future.

Last April the previous Conservative-led government laid out populist but controversial proposals to lay a duty of care on Internet platforms, responding to growing public concern about the types of content kids are being exposed to online.

Its white paper covers a broad range of online content — from terrorism, violence and hate speech, to child exploitation, self-harm/suicide, cyber bullying, disinformation and age-inappropriate material — with the government setting out a plan to require platforms to take “reasonable” steps to protect their users from a range of harms.

However, digital and civil rights campaigners warn the plan will have a huge impact on online speech and privacy, arguing it will put a legal requirement on platforms to closely monitor all users and apply speech-chilling filtering technologies on uploads in order to comply with very broadly defined concepts of harm. Legal experts are also critical.

Further, it requires social media companies to *prevent* ‘harmful’ (undefined) speech going online in the first place; & prevent ‘inappropriate’ (undefined) content recommendations. So expect state-sponsored upload filters, recommendation systems & mass surveillance. — Big Brother Watch (@BigBrotherWatch) February 12, 2020

The (now) Conservative majority government has nonetheless said it remains committed to the legislation.

Today it responded to some of the concerns being raised about the plan’s impact on freedom of expression, publishing a partial response to the public consultation on the Online Harms White Paper, although a draft bill remains pending, with no timeline confirmed.

“Safeguards for freedom of expression have been built in throughout the framework,” the government writes in an executive summary. “Rather than requiring the removal of specific pieces of legal content, regulation will focus on the wider systems and processes that platforms have in place to deal with online harms, while maintaining a proportionate and risk-based approach.”

It says it’s planning to set a different bar for content deemed illegal as compared to content that has “potential to cause harm,” with the heaviest content removal requirements being planned for terrorist and child sexual exploitation content. Whereas companies will not be forced to remove “specific pieces of legal content,” as the government puts it.

Ofcom, as the online harms regulator, will also not be investigating or adjudicating on “individual complaints.”

“The new regulatory framework will instead require companies, where relevant, to explicitly state what content and behaviour they deem to be acceptable on their sites and enforce this consistently and transparently. All companies in scope will need to ensure a higher level of protection for children, and take reasonable steps to protect them from inappropriate or harmful content,” it writes.

“Companies will be able to decide what type of legal content or behaviour is acceptable on their services, but must take reasonable steps to protect children from harm. They will need to set this out in clear and accessible terms and conditions and enforce these effectively, consistently and transparently. The proposed approach will improve transparency for users about which content is and is not acceptable on different platforms, and will enhance users’ ability to challenge removal of content where this occurs.”

Another requirement will be that companies have “effective and proportionate user redress mechanisms” — enabling users to report harmful content and challenge content takedown “where necessary.”

“This will give users clearer, more effective and more accessible avenues to question content takedown, which is an important safeguard for the right to freedom of expression,” the government suggests, adding that: “These processes will need to be transparent, in line with terms and conditions, and consistently applied.”

Ministers say they have not yet made a decision on what kind of liability senior management of covered businesses may face under the planned law, nor on additional business disruption measures — with the government saying it will set out its final policy position in the Spring.

“We recognise the importance of the regulator having a range of enforcement powers that it uses in a fair, proportionate and transparent way. It is equally essential that company executives are sufficiently incentivised to take online safety seriously and that the regulator can take action when they fail to do so,” it writes.

It’s also not clear how businesses will be assessed as being in (or out of) scope of the regulation.

“Just because a business has a social media page that does not bring it in scope of regulation,” the government response notes. “To be in scope, a business would have to operate its own website with the functionality to enable sharing of user-generated content or user interactions. We will introduce this legislation proportionately, minimising the regulatory burden on small businesses. Most small businesses where there is a lower risk of harm occurring will not have to make disproportionately burdensome changes to their service to be compliant with the proposed regulation.”

The government is clear in the response that Online harms remains “a key legislative priority”.

“We have a comprehensive programme of work planned to ensure that we keep momentum until legislation is introduced as soon as parliamentary time allows,” it writes, describing today’s response report “an iterative step as we consider how best to approach this complex and important issue” — and adding: “We will continue to engage closely with industry and civil society as we finalise the remaining policy.”

Incoming in the meanwhile the government says it’s working on a package of measures “to ensure progress now on online safety” — including interim codes of practice, including guidance for companies on tackling terrorist and child sexual abuse and exploitation content online; an annual government transparency report, which it says it will publish “in the next few months”; and a media literacy strategy, to support public awareness of online security and privacy.

It adds that it expects social media platforms to “take action now to tackle harmful content or activity on their services” — ahead of the more formal requirements coming in.

Facebook-owned Instagram has come in for high level pressure from ministers over how it handles content promoting self-harm and suicide after the media picked up on a campaign by the family of a schoolgirl who killed herself after been exposed to Instagram content encouraging self-harm.

Instagram subsequently announced changes to its policies for handling content that encourages or depicts self harm/suicide — saying it would limit how it could be accessed. This later morphed into a ban on some of this content.

The government said today that companies offering online services that involve user generated content or user interactions are expected to make use of what it dubs “a proportionate range of tools” — including age assurance, and age verification technologies — to prevent kids from accessing age-inappropriate content and “protect them from other harms”.

This is also the piece of the planned legislation intended to pick up the baton of the Digital Economy Act’s porn block proposals, which the government dropped last year, saying it would bake equivalent measures into the forthcoming Online Harms legislation.

The Home Office has been consulting with social media companies on devising robust age verification technologies for many months.

In its own response statement today, Ofcom said it will work with the government to ensure “any regulation provides effective protection for people online” and, pending appointment, “consider what we can do before legislation is passed”.

Ofcom responds to the Government’s announcement on online harms regulation: https://t.co/DTfMJkgIVU pic.twitter.com/Qgop0xcIcw — Ofcom (@Ofcom) February 12, 2020

The Online Harms plan is not the only Internet-related work ongoing in Whitehall, with ministers noting that: “Work on electoral integrity and related online transparency issues is being taken forward as part of the Defending Democracy programme together with the Cabinet Office.”

Back in 2018 a UK parliamentary committee called for a levy on social media platforms to fund digital literacy programs to combat online disinformation and defend democratic processes, during an enquiry into the use of social media for digital campaigning. However the UK government has been slower to act on this front.

The former chair of the DCMS committee, Damian Collins, called today for any future social media regulator to have “real powers in law,” including the ability to “investigate and apply sanctions to companies which fail to meet their obligations.”

In the DCMS committee’s final report, parliamentarians called for Facebook’s business to be investigated, raising competition and privacy concerns.",Social Media,TechCrunch,https://techcrunch.com/2020/02/12/uk-names-its-pick-for-social-media-harms-watchdog/,"The UK government is taking steps to regulate Social Media, appointing Ofcom as its online harms regulator to enforce rules around ""harmful speech"" and requiring companies to take reasonable steps to protect users from a range of harms. This could have a serious impact on online speech and privacy, potentially leading to mass surveillance and speech-chilling filtering technologies",Security & Privacy
473,How should social media handle the election?,"Election Day is here, and in the next few days or weeks, we’ll know who won — but for lots of people, tonight isn’t just about choosing the next president. It’s also a stress test for online platforms and a measure of how carefully they can handle information when the stakes are this high.

By now, we know what failure could look like. In one nightmare scenario, a candidate (likely Trump) could preemptively declare victory before the votes are counted. In another, a fast-spreading rumor could cause serious offline unrest — like a viral hoax or misleading video that encourages vigilante violence. Since election night might not end with a clear winner, sites could be dealing with these threats for days.

The biggest platforms have laid out a playbook for stopping false information, but no matter how well it works, some people — like Wall Street Journal reporter Joanna Stern — recommend logging off altogether. There are countless ways that the internet could make tonight’s election worse, and only a few ways to make it better.

As we judge how social media handled the 2020 presidential election, though, we need a standard for success as well as failure. What would a good election night look like online? As nebulous as that standard is, there are three key things we want to see.

Platforms should keep facts up front with big accounts

Big social media platforms are collectively moderating billions of accounts. But on election night and the days that follow, there are two key issues: stopping high-profile users from breaking sites’ rules and spreading accurate facts as fast as (or faster than) false claims.

The internet is full of viral half-truths, honest mistakes, outright lies, and other unreliable information. But in many cases, you can cut through the chaos by following a few simple rules.

Platforms have developed safeguards against false claims of victory. Twitter, Facebook, and Instagram are all using a banner to warn users that results are still being counted, while YouTube will offer a fact-check panel and streams from authoritative sources. Facebook and Google are temporarily banning political ads after the election to prevent misinformation. Yesterday, Facebook and Twitter labeled (and in Twitter’s case, restricted) a misleading Trump tweet decrying a Supreme Court decision on mail-in voting.

The banners and warnings are a little bit general — Twitter warns that experts “may not have called the race,” for example, rather than calling out specific errors. But they’re a start, if they’re added quickly and comprehensively.

Some recent research suggests misinformation is often driven by traditional media, politicians, and other “elite” actors. Trump, among other things, massively amplifies conspiracy theories by retweeting small accounts that espouse them. During election night, plenty of accounts will probably post false and potentially rule-breaking claims. But just finding those claims with a search query isn’t necessarily awful. The key question is whether sites step in to fact-check (or delete) false stories coming from big accounts — no matter how powerful their owners are.

Users should look for local citizen journalism — but share it carefully

Social media can bypass traditional media in bad ways, like spreading false information or misleading, emotionally charged stories. But it can also provide fast, unfiltered, hyper-local news. Many public officials share rapid status updates on their Facebook or Twitter feeds, including corrections to misinformation. If there’s a problem at a specific polling location, social media can offer detailed firsthand reports and focus public attention on it. This is the positive promise of social media — and maybe we’ll see it on display in the coming days.

Of course, this requires people to be careful with what they’re sharing. Doing research and seeking context is more vital than ever. Was an official-looking tweet posted by a credible (and ideally verified) account? Is a newsworthy-seeming picture or video actually new, or is it older material being reposted? Does a post include replies and comments that offer conflicting information? This goes double for any story that perfectly confirms your preexisting assumptions.

For help specifically navigating tonight’s minefield, disinformation expert Jane Lytvynenko has a running list of false and misleading election posts. The Election Integrity Project is also keeping an Election Day live blog and Twitter feed.

Social media should look beyond Facebook and Twitter

There’s certainly a strong argument for looking away from some social media platforms on election night — especially Facebook and Twitter, where news feeds are less social experiences than information firehoses. But this isn’t the only way to engage with other people online.

Sometimes you want to share the nail-biting experience of seeing polls close and results roll in. The pandemic has nixed this year’s election night watch parties, but digital spaces are there to fill the gap. As The Washington Post outlines, people are using Twitch and Zoom to gather with friends. Here at The Verge, many of us will be commiserating with each other on Slack. You may be doing the same on a group chat or Discord server.

If you’re looking for an information firehose, Reddit’s r/news moderators have laid out a plan for stopping false stories on the forum, including answers to some of the election’s most contentious questions. Reddit has obviously faced its own misinformation problems in the past. But it’s small enough to be managed by a team of humans who can make nuanced calls, rather than moderating hundreds of millions of people with a complicated rule set.

Smaller spaces pose their own challenges. It’s easy to spread misinformation in small groups among friends, and there won’t be public moderators to debunk it. Potentially violent groups can organize on platforms like the encrypted messaging service Telegram. But they’re just as much a part of “social media” as larger services. And tonight will offer a test of their strengths and weaknesses — along with those of America’s biggest web platforms.",Social Media,Verge,https://www.theverge.com/21540633/election-2020-platform-misinformation-facebook-twitter-projected-winner,"On Election Day, Social Media could cause serious problems, such as the spread of false information, misleading videos, and rumors that could lead to offline unrest. Platforms have put safeguards in place, but it is still important to be cautious when sharing information online, as it can be easy to spread misinformation in small groups.","Information, Discourse & Governance"
474,Black players on England football team bombarded with racist abuse on social media,"Social media platforms have been inundated with racist messages and posts directed at three Black players on England’s football team, following a tough loss to Italy on Sunday in the UEFA Euro 2020 championship match. The two teams played to a 1-1 tie, and Marcus Rashford, Jadon Sancho, and Bukayo Saka were among England’s players to participate in a penalty shootout to decide the winner of the match and the tournament. All three missed their shots.

After the game ended, the players’ social media accounts, including Facebook-owned Instagram and Twitter, were filled with racist comments and messages.

In an email to The Verge on Monday, a Twitter spokesperson said the “abhorrent, racist abuse directed at England players” had no place on the platform. Since the end of Sunday’s game, Twitter removed more than 1,000 tweets and permanently suspended a number of accounts — the spokesperson did not say how many — for violating its rules against harassment and hateful content. “We have proactively engaged and continue to collaborate with our partners across the football community to identify ways to tackle this issue collectively and will continue to play our part in curbing this unacceptable behaviour — both online and offline,” the spokesperson said.

A Facebook spokesperson said the company didn’t have numbers to share, but said the platform “quickly removed comments and accounts directing abuse at England’s footballers last night and we’ll continue to take action against those that break our rules.” The spokesperson added that the company had encouraged all players to turn on Instagram’s hidden words tool, which prevents abuse or comments from being visible in direct messages.

Vice reported that even more extreme abuse, including threats against the players, was turning up on white supremacist channels on Telegram, which has much looser moderation policies than Twitter or Facebook. Telegram didn’t immediately reply to a request for comment Monday.

Social media companies “need to step up and take accountability,” the Football Association said

England’s Football Association said in a statement Monday that it was “appalled by the online racism that has been aimed at some of our England players on social media.” Anyone behind the “disgusting” behavior is not welcome as a fan, the FA said, adding it was supporting the players “while urging the toughest punishments possible for anyone responsible.”

The FA statement added that social media companies “need to step up and take accountability and action to ban abusers from their platforms, gather evidence that can lead to prosecution and support making the platforms free from this type of abhorrent abuse.”

England’s players were frequently booed by fans at Wembley Stadium during the tournament when the players kneeled on the field before matches as an anti-racism gesture. Both teams kneeled before the start of Sunday’s match, as well.

UK Prime Minister Boris Johnson, who was criticized for previously failing to defend the players who kneeled before games, tweeted Monday that the players “deserve to be lauded as heroes, not racially abused on social media.”",Social Media,Verge,https://www.theverge.com/2021/7/12/22574082/black-players-england-italy-football-soccer-racist-instagram-twitter-telegram,"Social media platforms have been inundated with racist messages and posts targeting Black players on England's football team following a tough loss. Companies like Twitter, Facebook and Telegram have responded by removing content and suspending accounts, but the Football Association is calling on them to take stronger action to protect players from online abuse.",Equality & Justice
475,Facebook urged to give users greater control over what they see – TechCrunch,"Academics at the universities of Oxford and Stanford think Facebook should give users greater transparency and control over the content they see on its platform.

They also believe the social networking giant should radically reform its governance structures and processes to throw more light on content decisions, including by looping in more external experts to steer policy.

Such changes are needed to address widespread concerns about Facebook’s impact on democracy and on free speech, they argue in a report published today, which includes a series of recommendations for reforming Facebook (entitled: Glasnost! Nine Ways Facebook Can Make Itself a Better Forum for Free Speech and Democracy.)

“There is a great deal that a platform like Facebook can do right now to address widespread public concerns, and to do more to honour its public interest responsibilities as well as international human rights norms,” writes lead author Timothy Garton Ash.

“Executive decisions made by Facebook have major political, social, and cultural consequences around the world. A single small change to the News Feed algorithm, or to content policy, can have an impact that is both faster and wider than that of any single piece of national (or even EU-wide) legislation.”

Here’s a rundown of the report’s nine recommendations:

Tighten Community Standards wording on hate speech — the academics argue that Facebook’s current wording on key areas is “overbroad, leading to erratic, inconsistent and often context-insensitive takedowns;” and also generating “a high proportion of contested cases.” Clear and tighter wording could make consistent implementation easier, they believe. Hire more and contextually expert content reviewers — “the issue is quality as well as quantity,” the report points out, pressing Facebook to hire more human content reviewers plus a layer of senior reviewers with “relevant cultural and political expertise;” and also to engage more with trusted external sources such as NGOs. “It remains clear that AI will not resolve the issues with the deeply context-dependent judgements that need to be made in determining when, for example, hate speech becomes dangerous speech,” they write. Increase “decisional transparency” — Facebook still does not offer adequate transparency around content moderation policies and practices, they suggest, arguing it needs to publish more detail on its procedures, including specifically calling for the company to “post and widely publicize case studies” to provide users with more guidance and to provide potential grounds for appeals. Expand and improve the appeals process — also on appeals, the report recommends Facebook gives reviewers much more context around disputed pieces of content, and also provide appeals statistics data to analysts and users. “Under the current regime, the initial internal reviewer has very limited information about the individual who posted a piece of content, despite the importance of context for adjudicating appeals,” they write. “A Holocaust image has a very different significance when posted by a Holocaust survivor or by a Neo-Nazi.” They also suggest Facebook should work on developing “a more functional and usable for the average user” appeals due process, in dialogue with users — such as with the help of a content policy advisory group. Provide meaningful News Feed controls for users — the report suggests Facebook users should have more meaningful controls over what they see in the News Feed, with the authors dubbing current controls as “altogether inadequate,” and advocating for far more. Such as the ability to switch off the algorithmic feed entirely (without the chronological view being defaulted back to algorithm when the user reloads, as is the case now for anyone who switches away from the AI-controlled view). The report also suggests adding a News Feed analytics feature, to give users a breakdown of sources they’re seeing and how that compares with control groups of other users. Facebook could also offer a button to let users adopt a different perspective by exposing them to content they don’t usually see, they suggest. Expand context and fact-checking facilities — the report pushes for “significant” resources to be ploughed into identifying “the best, most authoritative, and trusted sources” of contextual information for each country, region and culture — to help feed Facebook’s existing (but still inadequate and not universally distributed) fact-checking efforts. Establish regular auditing mechanisms — there have been some civil rights audits of Facebook’s processes (such as this one, which suggested Facebook formalizes a human rights strategy), but the report urges the company to open itself up to more of these, suggesting the model of meaningful audits should be replicated and extended to other areas of public concern, including privacy, algorithmic fairness and bias, diversity and more. Create an external content policy advisory group — key content stakeholders from civil society, academia and journalism should be enlisted by Facebook for an expert policy advisory group to provide ongoing feedback on its content standards and implementation; as well as also to review its appeals record. “Creating a body that has credibility with the extraordinarily wide geographical, cultural, and political range of Facebook users would be a major challenge, but a carefully chosen, formalized, expert advisory group would be a first step,” they write, noting that Facebook has begun moving in this direction but adding: “These efforts should be formalized and expanded in a transparent manner.” Establish an external appeals body — the report also urges “independent, external” ultimate control of Facebook’s content policy, via an appeals body that sits outside the mothership and includes representation from civil society and digital rights advocacy groups. The authors note Facebook is already flirting with this idea, citing comments made by Mark Zuckerberg last November, but also warn this needs to be done properly if power is to be “meaningfully” devolved. “Facebook should strive to make this appeals body as transparent as possible… and allow it to influence broad areas of content policy… not just rule on specific content takedowns,” they warn.

In conclusion, the report notes that the content issues it’s focused on are not only attached to Facebook’s business but apply widely across various internet platforms — hence growing interest in some form of “industry-wide self-regulatory body.” Though it suggests that achieving that kind of overarching regulation will be “a long and complex task.”

In the meanwhile, the academics remain convinced there is “a great deal that a platform like Facebook can do right now to address widespread public concerns, and to do more to honour its public interest responsibilities, as well as international human rights norms” — with the company front and center of the frame given its massive size (2.2 billion+ active users).

“We recognize that Facebook employees are making difficult, complex, contextual judgements every day, balancing competing interests, and not all those decisions will benefit from full transparency. But all would be better for more regular, active interchange with the worlds of academic research, investigative journalism, and civil society advocacy,” they add.

We’ve reached out to Facebook for comment on their recommendations.

The report was prepared by the Free Speech Debate project of the Dahrendorf Programme for the Study of Freedom, St. Antony’s College, Oxford, in partnership with the Reuters Institute for the Study of Journalism, University of Oxford, the Project on Democracy and the Internet, Stanford University and the Hoover Institution, Stanford University.

Last year we offered a few of our own ideas for fixing Facebook — including suggesting the company hire orders of magnitude more expert content reviewers, as well as providing greater transparency into key decisions and processes.",Social Media,TechCrunch,https://techcrunch.com/2019/01/16/facebook-urged-to-give-users-greater-control-over-what-they-see/,"The report argues that Facebook needs to take urgent steps to address widespread concerns about its impact on democracy and free speech, including by giving users more transparency and control over the content they see, radically reforming its governance structures, hiring more content reviewers with relevant cultural and political expertise, and creating an external appeals body.","Information, Discourse & Governance"
476,Google lays out narrow ‘EU election advertiser’ policy ahead of 2019 vote – TechCrunch,"Google has announced its plan for combating election interference in the European Union ahead of elections next May when up to 350 million voters across the region will vote to elect 705 members of the European Parliament.

In a blog post laying out a narrow approach to democracy-denting disinformation, Google says it will introduce a verification system for “EU election advertisers to make sure they are who they say they are,” and require that any election ads disclose who is paying for them.

The details of the verification process are not yet clear so it’s not possible to assess how robust a check this might be.

But Facebook, which also recently announced checks on political advertisers, had to delay its UK launch of ID checks earlier this month, after the beta system was shown being embarrassingly easy to game. So just because a piece of online content has an ‘ID badge’ on it does not automatically make it bona fide.

Google’s framing of “EU election advertisers” suggests it will exclude non-EU based advertisers from running election ads, at least as it’s defining these ads. (But we’ve asked for a confirm on that.)

What’s very clear from the blog post is that the adtech giant is defining political ads as an extremely narrowly category — with only ads that explicitly mention political parties, candidates or a current officeholder falling under the scope of the policy.

Here’s how Google explains what it means by “election ads”:

“To bring people more information about the election ads they see across Google’s ad networks, we’ll require that ads that mention a political party, candidate or current officeholder make it clear to voters who’s paying for the advertising.”

So any ads still intended to influence public opinion — and thus sway potential voters — but which cite issues, rather than parties and/or politicians, will fall entirely outside the scope of its policy.

Yet of course issues are material to determining election outcomes.

Issue-based political propaganda is also — as we all know very well now — a go-to tool for the shadowy entities using internet platforms for highly affordable, mass-scale online disinformation campaigns.

The Kremlin seized on divisive issues for much of the propaganda it deployed across social media ahead of the 2016 U.S. presidential elections, for example.

Russia didn’t even always wrap its politically charged infowar bombs in an ad format, either.

All of which means that any election ‘security’ effort that fixes on a narrow definition (like “election ads”) seems unlikely to offer much more than a micro bump in the road for anyone wanting to pay to play with democracy.

The only real fix for this problem is likely full disclosure of all advertising and advertisers: Who’s paying for every online ad, regardless of what it contains, plus a powerful interface for parsing that data mountain.

Of course neither Google nor Facebook is offering that — yet.

Because, well, this is self-regulation ahead of election laws catching up.

What Google is offering for the forthcoming EU parliament elections is an EU-specific Election Ads Transparency Report (akin to the one it already launched for the U.S. midterms) — which it says it will introduce (before the May vote) to provide a “searchable ad library to provide more information about who is purchasing election ads, whom they’re targeted to, and how much money is being spent.”

“Our goal is to make this information as accessible and useful as possible to citizens, practitioners, and researchers,” it adds.

The rest of its blog post is given over to puffing up a number of unrelated steps it says it will also take in the name of “supporting the European Union Parliamentary Elections,” but that don’t involve Google itself having to be any more transparent about its own ad platform.

So it says it will —

be working with data from Election Commissions across the member states to “make authoritative electoral information available and help people find the info they need to get out and vote”

offering in-person security training to the most vulnerable groups, who face increased risks of phishing attacks (“We’ll be walking them through Google’s Advanced Protection Program, our strongest level of account security and Project Shield, a free service that uses Google technology to protect news sites and free expression from DDoS attacks on the web.”)

collaborating — via its Google News Lab entity — with news organizations across all 27 EU Member States to “support online fact checking”. (The Lab will “be offering a series of free verification workshops to point journalists to the latest tools and technology to tackle disinformation and support their coverage of the elections”)

No one’s going to turn their nose up at security training and freebie resource.

But the scale of the disinformation challenge is rather larger and more existential than a few free workshops and an anti-DDoS tool can fix.

The bulk of Google’s padding here also fits comfortably into its standard operating philosophy where the user-generated content that fuels its business is concerned; aka ‘tackle bad speech with more speech’. Crudely put: More speech, more ad revenue.

Though, as independent research has repeatedly shown, fake news flies much faster and is much, much harder to unstick than truth.

Which means fact checkers, and indeed journalists, are faced with the Sisyphean task of unpicking all the BS that Internet platforms are liberally fencing and accelerating (and monetizing as they do so).

The economic incentives inherent in the dominant adtech platform of the internet should really be front and center when considering the modern disinformation challenge.

But of course Google and Facebook aren’t going to say that.

Meanwhile lawmakers are on the back foot. The European Commission has done something, signing tech firms up to a voluntary Code of Practice for fighting fake news — Google and Facebook among them.

Although, even in that diluted, non-legally binding document, signatories are supposed to have agreed to take action to make both political advertising and issue-based advertising “more transparent.”

Yet here’s Google narrowly defining election ads in a way that lets issues slide on past.

We asked the company what it’s doing to prevent issue-based ads from interfering in EU elections. At the time of writing it had not responded to that question.

Safe to say, ‘election security’ looks to be a very long way off, indeed.

Not so the date of the EU poll. That’s fast approaching: May 23 through 26, 2019.",Social Media,TechCrunch,https://techcrunch.com/2018/11/22/google-lays-outs-narrow-eu-election-advertiser-policy-ahead-of-2019-vote/,"Google has announced plans to combat election interference in the European Union ahead of the May 2019 elections. However, the company's definition of ""election ads"" is too narrow, as it only covers ads that explicitly mention political parties, candidates or a current officeholder, leaving out issue-based political propaganda which is a go-to tool for those",Politics
477,A Bit Too Much Klout: User Says He Can Sign In To Someone Else’s Account – TechCrunch,"It’s not clear if this is a one-off glitch, a signal of a bigger issue — or a way of pumping up/sabotaging Klout scores for those who care. But it’s not great news any way you spin it, if it’s true: a Klout user has gotten in touch to say that when he accesses the social influence ratings service, he is getting signed in to Klout not as himself but as someone else.

Using an HTC Sensation device running the Ice Cream Sandwich version of Android, IT consultant Halil Kabaca,of Istanbul, Turkey-based Novarum Consulting, tells us that when he goes on to Klout via the phone’s mobile browser, he is being signed in automatically as someone completely different — someone he doesn’t know at all who happens to work for Adobe in business development (see screenshots of Kabaca’s and the other guy’s profiles after the break).

It appears that Kabaca has full access to the other guy’s account, including direct messages, the ability to add influencers, and change all other account information. The access, he says, only happens on mobile, and not on his PC.

Kabaca tells us he uses Klout almost every day from his phone and this is the first time he has seen this happen. “Even if it’s a minor bug, it is very discouraging to use the service,” he said.

As of this writing he says he can still access the other user’s profile, “And I am wondering if anyone is seeing mine.”

Even if this is a one-off glitch, the news is not brilliant, as it comes at the same time that Klout has been sharpening its focus on mobile. In February, it acquired mobile/location startup Blockboard to enhance its mobile services; in April, it released a new iPhone app; and earlier this month, it kicked off with some eye-catching promotions — Perks in Klout-world — with companies to show off how effective those new mobile products can be. A recent one we covered was Klout’s link-up with Cathay Pacific, where users with high Klout scores could flash their status, via the mobile app, to get access to Cathay Pacific’s executive lounge in San Francisco Airport.

Because of the emphasis on sharing information about yourself, social networks — more than other internet services — have been served a pretty big dose of privacy scrutiny. Klout is no different. In November the site was criticized for how it created shadow profiles of people who are not even users.

Klout is not the only social media site that has suddenly seen identity loopholes appear. In March, Twitter had to take TweetDeck offline after one user suddenly found he had access to hundreds of accounts, both on Twitter and Facebook, using the client.

We have contacted Klout, and the other user, to see if they can comment on this development and will update as we learn more.

Update: The other user has come back to us to confirm that his account has been changed around by someone — with a new contact email (the one put in by Kabaca to test the loophole).",Social Media,TechCrunch,https://techcrunch.com/2012/05/28/a-bit-too-much-klout-user-says-he-can-sign-in-to-someone-elses-account/,"A user has found a loophole in Klout that allows him to access someone else's account, including direct messages, influencers, and other account information, with potential for identity theft and data misuse.",Security & Privacy
478,Facebook launches a series tests to inform future changes to its News Feed algorithms – TechCrunch,"Facebook may be reconfiguring its News Feed algorithms. After being grilled by lawmakers about the role that Facebook played in the attack on the U.S. Capitol, the company announced this morning it will be rolling out a series of News Feed ranking tests that will ask users to provide feedback about the posts they’re seeing, which will later be incorporated into Facebook’s News Feed ranking process. Specifically, Facebook will be looking to learn which content people find inspirational, what content they want to see less of (like politics), and what other topics they’re generally interested in, among other things.

This will be done through a series of global tests, one of which will involve a survey directly beneath the post itself which asks, “How much were you inspired by this post?,” with the goal of helping to show more people posts of an inspirational nature closer at the top of the News Feed.

Another test will work to the Facebook News Feed experience to reflect what people want to see. Today, Facebook prioritizes showing you content from friends, Groups and Pages you’ve chosen to follow, but it has algorithmically crafted an experience of whose posts to show you and when based on a variety of signals. This includes both implicit and explicit signals — like how much you engage with that person’s content (or Page or Group) on a regular basis, as well as whether you’ve added them as a “Close Friend” or “Favorite” indicating you want to see more of their content than others, for example.

However, just because you’re close to someone in real life, that doesn’t mean that you like what they post to Facebook. This has driven families and friends apart in recent years, as people discovered by way of social media how people they thought they knew really viewed the world. It’s been a painful reckoning for some. Facebook hasn’t managed to fix the problem, either. Today, users still scroll News Feeds that reinforce their views, no matter how problematic. And with the growing tide of misinformation, the News Feed has gone from just placing users into a filter bubble to presenting a full alternate reality for some, often populated by conspiracy theories.

Facebook’s third test doesn’t necessarily tackle this problem head-on, but instead looks to gain feedback about what users want to see, as a whole. Facebook says that it will begin asking people whether they want to see more or fewer posts on certain topics, like Cooking, Sports, Politics and more. Based on users’ collective feedback, Facebook will adjust its algorithms to show more content people say they’re interested in and fewer posts about topics they don’t want to see.

The area of politics, specifically, has been an issue for Facebook. The social network for years has been charged with helping to fan the flames of political discourse, polarizing and radicalizing users through its algorithms, distributing misinformation at scale, and encouraging an ecosystem of divisive clickbait, as publishers sought engagement instead of fairness and balance when reporting the news. There are now entirely biased and subjective outlets posing as news sources who benefit from algorithms like Facebook’s, in fact.

Shortly after the Capitol attack, Facebook announced it would try clamping down on political content in the News Feed for a small percentage of people in the U.S., Canada, Brazil and Indonesia, for period of time during tests.

Now, the company says it will work to better understand what content is being linked to negative News Feed experiences, including political content. In this case, Facebook may ask users on posts with a lot of negative reactions what sort of content they want to see less of. This will be done through surveys on certain posts as well as through ongoing research sessions where people are invited to talk about their News Feed experience, Facebook told TechCrunch.

It will also more prominently feature the option to hide posts you find “irrelevant, problematic or irritating.” Although this feature existed before, you’ll now be able to tap an X in the upper-right corner of a post to hide it from the News Feed, if in the test group, and see fewer like it in the future, for a more personalized experience.

It’s not clear that allowing users to pick and choose their topics is the best way to solve the larger problems with negative posts, divisive content or misinformation, though this test is less about the latter and more about making the News Feed “feel” more positive.

As the data is collected from the tests, Facebook will incorporate the findings into its News Feed ranking algorithms. But it’s not clear to what extent it will be adjusting the algorithm on a global basis versus simply customizing the experience for end users on a more individual basis over time. The company tells TechCrunch the survey data will be collected from a small percentage of users who are placed into the test groups, which will then be used to train a machine learning model.

It will also be exploring ways to give people more direct controls over what sort of content they see on the News Feed in the future.

The company says the tests will run over the next few months.",Social Media,TechCrunch,https://techcrunch.com/2021/04/22/facebook-launches-a-series-tests-to-inform-future-changes-to-its-news-feed-algorithms/,"Social media has been criticized for its role in fanning the flames of political discourse, polarizing users, and spreading misinformation, leading to a painful reckoning for some who thought they knew people in real life.",Politics
479,How to Use Social Media at a Protest Without Big Brother Snooping,"If you're marching or protesting today, or any day, you may take photos or videos. Maybe some light livestreaming. So long as you're in public places and aren't breaking any other laws, that is your legally protected right in the US. But just as smartphones are increasingly able to broadcast events and promote transparency, law enforcement technology has evolved to take advantage. FBI and state facial recognition databases now include information about almost half of all US adults, and this technology can be (and has been) deployed to monitor protesters.

Law enforcement departments around the country have been using broad social media surveillance more and more as a way to track trends and identify people of interest. A study released in November by the Brennan Center for Justice showed that law enforcement has spent roughly $4.75 million on social media analysis technology tools that can, among other things, target large events, protesters, and activists.

So how can citizens reconcile the value of documentation with the fear that they might wind up in a database somewhere? Experts say that the most important thing is to keep expressing yourself, and simply take privacy into consideration where you can.

Protect Yourself (And Others)

""From the point of view of an individual, if you’re going to post something online you should do so under the assumption that it might be viewed by law enforcement,"" says Jay Stanley, a senior policy analyst at the ACLU. ""I think that the solution is A, etiquette, and B, better restraints and checks on law enforcement so the American people don’t have to feel chilled in exercising their rights.""

Etiquette comes into play because it's not just your own face that you might snap a picture of. Double check your Facebook privacy settings to make sure you're not broadcasting to the world, or if you prefer to remain public, remember that law enforcement could find your images and videos. Simple precautions like photographing a protest from behind, to reduce the number of faces, can be effective ways to grab a compelling crowd shot without compromising the privacy of strangers. Similarly, you can try to keep strangers' faces out of photos of your friends when possible.

Civil rights advocates hope to push for norms and legislation that limit the ability to randomly and broadly surveil people's online profiles. ""The explosion of social media has led to much more of our lives being visible to other people, and that requires us to rethink both laws and etiquettes,"" says Adam Schwartz, a senior staff attorney at the Electronic Frontier Foundation. ""Precisely because so much of our lives are on the internet, and it’s so much easier for police if they wanted to to see what we’re up to, that’s all the more reason that we ought to have hard and forceful rules that the police just are not looking at what we’re doing on social media without individualized suspicion of crime.""

Those rules may not be fast coming. Mere minutes after Friday's inauguration, the Trump administration vowed on the White House website to ""empower our law enforcement officers to do their jobs,"" presumably a nod to Trump's longstanding commitment to loosening restrictions and regulations on what police can and cannot do.

Protect Your Phone

There's also the matter of the device you depend on to engage with social media---and so many other things---in the first place: your smartphone.

Your smartphone can reveal essentially your entire digital life, so part of protecting your privacy while protesting is making sure that data doesn't fall into the wrong hands. Should an altercation with police take place, they could seize your phone and may have cause to search it. Leaving it at home and packing only a standalone camera, or bringing a burner phone, are two ways to ensure that the device you have on you isn't tied to your online identity.

If you do bring your everyday device, encrypting it with a passcode, using the camera while the phone is still locked, and using end-to-end encrypted messaging services are all ways to protect yourself from proactive government surveillance at marches and rallies. And make sure you have a backup of the data on the device. That way if you want to ditch it, you won't lose everything.

Taking these precautions can be a pain, but being careful shouldn't keep you from speaking out and publicizing what you want to share with the world. When you go to put an event photo on Instagram, take a second to think about the reach of that platform and who is in the image. And then post it.",Social Media,WIRED,https://www.wired.com/2017/01/use-social-media-protest-without-big-brother-snooping/,"The use of social media to document protests and rallies can lead to surveillance of protesters by law enforcement departments, so it's important to take precautions to protect yourself and others, such as encrypting your device, using the camera while the phone is still locked, and being mindful of who you share images of online.",Security & Privacy
480,State AGs tell Facebook to scrap Instagram for kids plans – TechCrunch,"In a new letter, attorneys general representing 44 U.S. states and territories are pressuring Facebook to walk away from new plans to open Instagram to children. The company is working on an age-gated version of Instagram for kids under the age of 13 that would lure in young users who are currently not permitted to use the app, which was designed for adults.

“It appears that Facebook is not responding to a need, but instead creating one, as this platform appeals primarily to children who otherwise do not or would not have an Instagram account,” the coalition of attorneys general wrote, warning that an Instagram for kids would be “harmful for myriad reasons.”

The state attorneys general call for Facebook to abandon its plans, citing concerns around developmental health, privacy and Facebook’s track record of prioritizing growth over the well-being of children on its platforms. In the letter, embedded below, they delve into specific worries about cyberbullying, online grooming by sexual predators and algorithms that showed dieting ads to users with eating disorders.

Concerns about social media and mental health in kids and teens is a criticism we’ve been hearing more about this year, as some Republicans join Democrats in coalescing around those issues, moving away from the claims of anti-conservative bias that defined politics in tech during the Trump years.

Leaders from both parties have been openly voicing fears over how social platforms are shaping young minds in recent months amidst calls to regulate Facebook and other social media companies. In April, a group of congressional Democrats wrote Facebook with similar warnings over its new plans for children, pressing the company for details on how it plans to protect the privacy of young users.

In light of all the bad press and attention from lawmakers, it’s possible that the company may walk back its brazen plans to boost business by bringing more underage users into the fold. Facebook is already in the hot seat with state and federal regulators in just about every way imaginable. Deep worries over the company’s future failures to protect yet another vulnerable set of users could be enough to keep these plans on the company’s back burner.",Social Media,TechCrunch,https://techcrunch.com/2021/05/10/facebook-instagram-for-kids-state-attorneys-general-letter/,"Social media has been widely criticized for potentially having negative consequences on mental health, including cyberbullying, online grooming by predators, and algorithmically-driven ads that could trigger eating disorders.",Social Norms & Relationships
481,Social Media Made the World Care About Standing Rock—and Helped It Forget,"A thousand protesters stayed put. The snow piled up around their tepees, but they dug in as caravans of supporters and journalists drove away. The Dakota Access Company planned to run the last segment of an oil pipeline under Lake Oahe, but the Army Corps of Engineers had given the protesters a reprieve when it ordered an an environmental review of the project in early December. Still, this core group of Standing Rock Sioux seemed to know that when the spotlight faded and the hashtags stopped trending, their fortunes could change. They knew what could happen if the world stopped watching—and the world did, thanks in no small part to the platforms that made the protests so visible in the first place.

Today, President Trump signed an executive memo aimed at allowing the Dakota Access Company to finish the last bit of pipeline---a small segment that needs to go beneath the lake. A separate memo reportedly enables the completion of the Keystone Pipeline, which President Obama had rejected last year.

""We are shocked and dismayed by today’s news because it puts water for millions at risk,"" Phillip Ellis, spokesman for Earthjustice, the legal team representing the Standing Rock Sioux, says. ""This move is legally questionable, at best. And based on what we know about Trump’s financial dealings in the Dakota Access Pipeline, it raises serious ethical concerns."" (Trump had a financial investment in Energy Transfers, the company behind the Dakota Access Pipelines, but sold it late last year.)

The memo orders the Army Corps of Engineers to expedite and approve permits that would let the project go forward without completing the environmental review. That review had put the completion of the Dakota Access Pipeline on hold until regulators assessed alternate routes around the lake, a process that could have taken years. It was a step protesters, environmentalists, and even the Environmental Protection Agency believed was necessary, since the studies the Army had taken earlier in 2016 were deemed in insufficient.

Earthjustice argues Trump's memo doesn't supersede the law requiring the review. Nevertheless, the Army may respond to the order by granting the Dakota Access Company an easement under Lake Oahe, in which case the company would theoretically have the legal authority to press on immediately. (Dakota Access, which declined repeated requests for comment, filed a lawsuit in December arguing it didn’t need an easement at all.) The Army did not return a request for comment.

""Trump’s decision to give the go-ahead for the Dakota Access Pipeline is a slap in the face to Native Americans and a blatant disregard for the rights to their land,"" says Jamil Dakwar, director of the ACLU’s Human Rights Program. ""The Trump administration should allow careful environmental impact analysis to be completed with full and meaningful participation of affected tribes.""

'We've Been Forgotten'

When the Army Corps' order came through in December, thousands left the protest camps at the behest of tribal leaders, and attention shifted away from those who remained. Afterward, conditions in the camp reportedly became chaotic. Protesters alleged violence was roiling the camps, and that cellular service was being intentionally interrupted (WIRED was unable to verify either of these claims). Police arrested some of those still at the camp. Facebook Live streams, which had worked to bring the #NoDAPL battle before the world, became less frequent. Today, when President Trump issued his order, I couldn't find a single Facebook live stream from the protest site. In the fall, at the height of the protests as thousands went to North Dakota to support the tribe, dozens of feeds were live at any given time.

""We've been forgotten,"" says Josh Long, an activist who had travelled to North Dakota in the fall and stayed through the winter.

If social media enabled the Standing Rock Sioux to amplify their protest, its speed and ceaseless flow also allowed the world to forget about them.

Still, the water protectors of the Standing Rock Sioux want to continue. Ladonna Tamakawastewin Allard of the Standing Rock Sioux, who has hosted protesters on her private land in Fort Yates, took to Facebook as reports of the executive order came out, urging people to keep standing. In Washington, DC, activists backed by the Sierra Club planned to rally. The Democratic National Committee, in response to Trump's memo, urged Democrats on their mailing lists to sign a petition affirming that they will fight against climate change. And Earthjustice vowed to sue if the Army issues the easement. ""President Trump appears to be ignoring the law, public sentiment and ethical considerations with this executive order aimed at resurrecting the long-rejected Keystone XL pipeline and circumventing the ongoing environmental review process for the highly controversial Dakota Access pipeline,” Ellis says.

But the whiplash of the news cycle and the short attention spans exacerbated by the Twitterification of politics worked against those efforts. On Tuesday, the hashtag #DAPL trended nationwide for a little while, and then was eclipsed by chatter about the Academy Awards nominations. If social media and live streaming enabled the Standing Rock Sioux to amplify their protest for clean water, its speed and ceaseless flow also allowed the world to forget about them. The videos and tweets and Facebook posts will persist in the historical record that is the Internet, but that likely won't stop Dakota Access Company from burrowing under Lake Oahe as soon as it can.",Social Media,WIRED,https://www.wired.com/2017/01/social-media-made-world-care-standing-rock-helped-forget/,"The whiplash of the news cycle and the short attention spans exacerbated by the Twitterification of politics have worked against the Standing Rock Sioux in their fight to prevent the completion of the Dakota Access Pipeline, allowing the world to forget about them and making it easier for the pipeline to be completed.",Social Norms & Relationships
482,The Fyre Festival: The Fiasco We All Should Have Seen Coming,"Not much ages well on the internet. But the 1994 New Yorker cartoon of a dog sitting behind a desktop—“On the internet, nobody knows you’re a dog”—only gets better with time. The social media age has turned the best of us into internet hucksters. Just beyond every gently filtered Instagram frame of a perfectly prepared smoothie bowl is inevitably another plate covered in straw wrappers, half masticated crusts, and smudges of crumb-specked ketchup. None of us is as clever as we are on Twitter or as cute as we are in Snapchat Stories. We live in a world of our own outtakes.

Lately though, we’ve begun to see what happens when that expertly curated alternate reality crosses over into the real one---when, for instance, a larger than life social media personality suddenly becomes President of the United States and realizes a little too late that actually doing the job is harder than he thought. Or when a horde of rich millennials shells out thousands of dollars a piece to attend a luxury music festival promoted by all their favorite Instagram influencers, only to arrive in the Bahamas and find what one festival-goer called “a disaster tent city,” instead.

By now you may have heard about the fiasco that was Fyre Festival. The social media-fueled endeavor, co-founded by rapper Ja Rule and his tech entrepreneur partner Billy McFarland, promised people “two transformative weekends” on a private island in the Bahamas, with “the best in food, art, music, and adventure,” and, if the promo videos were any indication, models like Bella Hadid and Emily Ratajkowski sunbathing in skimpy bikinis. This would-be tropical Coachella emerged seemingly out of nowhere but soon vaulted to viral status after the organizers hired some 400 Instagram influencers to post about the event. The campaign promised luxury, beauty, and exclusivity.

But when attendees got there, what they found looked nothing like the photos. Instead, they got Kraft singles on wheat bread served in styrofoam containers. They got, in Instagram parlance, the #nofilter version.

Sure, everyone loves a good millennial shaming, and yes, it's pretty satisfying when corporate fraudsters get epically outed. But try to suspend your schadenfreude for just the moment. Because maybe you wouldn't have spent thousands of dollars to swim with pigs or head bang to Blink182 on the beach, but the Fyre Festival ordeal reveals just how vulnerable all of us are to manipulation online, where a hashtag and a finely cropped image can cause us to lose all sense of caveat emptor.

Social media's promise has always been its ability to connect anyone with a voice to a willing and eager audience. That's been a potent driver of social change in recent years, leading to the rise of groups like Black Lives Matter. But it's also led to the rise of dangerous alt-right conspiracy theories like Pizzagate, fake news websites like the Denver Guardian, and performance artists like Alex Jones posing as journalists. At a time when one website peddling pure fiction can look just as legitimate as another real one---and build just as large a social media following---it can take a lot longer for the audience to discover the emperor has no clothes (or luxury villas).",Social Media,WIRED,https://www.wired.com/2017/04/fyre-festival-fiasco-seen-coming/,"The Fyre Festival fiasco reveals just how vulnerable we are to manipulation online; an expertly curated alternate reality can lead us to lose all sense of caution, allowing us to be easily misled by fake news, conspiracy theories, and scam artists.","Information, Discourse & Governance"
483,Yet another massive Facebook fail: Quiz app leaked data on ~120M users for years – TechCrunch,"Facebook knows the historical app audit it’s conducting in the wake of the Cambridge Analytica data misuse scandal is going to result in a tsunami of skeletons tumbling out of its closet.

It’s already suspended around 200 apps as a result of the audit — which remains ongoing, with no formal timeline announced for when the process (and any associated investigations that flow from it) will be concluded.

CEO Mark Zuckerberg announced the audit on March 21, writing then that the company would “investigate all apps that had access to large amounts of information before we changed our platform to dramatically reduce data access in 2014, and we will conduct a full audit of any app with suspicious activity”.

But you do have to question how much the audit exercise is, first and foremost, intended to function as PR damage limitation for Facebook’s brand — given the company’s relaxed response to a data abuse report concerning a quiz app with ~120M monthly users, which it received right in the midst of the Cambridge Analytica scandal.

Because despite Facebook being alerted about the risk posed by the leaky quiz apps in late April — via its own data abuse bug bounty program — they were still live on its platform a month later.

It took about a further month for the vulnerability to be fixed.

And, sure, Facebook was certainly busy over that period. Busy dealing with a major privacy scandal.

Perhaps the company was putting rather more effort into pumping out a steady stream of crisis PR — including taking out full page newspaper adverts (where it wrote that: “we have a responsibility to protect your information. If we can’t, we don’t deserve it”) — vs actually ‘locking down the platform’, per its repeat claims, even though the company’s long and rich privacy-hostile history suggests otherwise.

Let’s also not forget that, in early April, Facebook quietly confessed to a major security flaw of its own — when it admitted that an account search and recovery feature had been abused by “malicious actors” who, over what must have been a period of several years, had been able to surreptitiously collect personal data on a majority of Facebook’s ~2BN users — and use that intel for whatever they fancied.

So Facebook users already have plenty reasons to doubt the company’s claims to be able to “protect your information”. But this latest data fail facepalm suggests it’s hardly scrambling to make amends for its own stinkingly bad legacy either.

Change will require regulation. And in Europe that has arrived, in the form of the GDPR.

Although it remains to be seen whether Facebook will face any data breach complaints in this specific instance, i.e. for not disclosing to affected users that their information was at risk of being exposed by the leaky quiz apps.

The regulation came into force on May 25 — and the javascript vulnerability was not fixed until June. So there may be grounds for concerned consumers to complain.

Which Facebook data abuse victim am I?

Writing in a Medium post, the security researcher who filed the report — self-styled “hacker” Inti De Ceukelaire — explains he went hunting for data abusers on Facebook’s platform after the company announced a data abuse bounty on April 10, as the company scrambled to present a responsible face to the world following revelations that a quiz app running on its platform had surreptitiously harvested millions of users’ data — data that had been passed to a controversial UK firm which intended to use it to target political ads at US voters.

De Ceukelaire says he began his search by noting down what third party apps his Facebook friends were using — finding quizzes were one of the most popular apps. Plus he already knew quizzes had a reputation for being data-suckers in a distracting wrapper. So he took his first ever Facebook quiz, from a brand called NameTests.com, and quickly realized the company was exposing Facebook users’ data to “any third-party that requested it”.

The issue was that NameTests was displaying the quiz taker’s personal data (such as full name, location, age, birthday) in a javascript file — thereby potentially exposing the identify and other data on logged in Facebook users to any external website they happened to visit.

He also found it was providing an access token that allowed it to grant even more expansive data access permissions to third party websites — such as to users’ Facebook posts, photos and friends.

It’s not clear exactly why — but presumably relates to the quiz app company’s own ad targeting activities. (Its privacy policy states: “We work together with various technological partners who, for example, display advertisements on the basis of user data. We make sure that the user’s data is pseudonymised (e.g. no clear data such as names or e-mail addresses) and that users have simple rights of revocation at their disposal. We also conclude special data protection agreements with our partners, in which they commit themselves to the protection of user data.” — which sounds great until you realize its javascript was just leaking people’s personally identified data… [facepalm])

“Depending on what quizzes you took, the javascript could leak your facebook ID, first name, last name, language, gender, date of birth, profile picture, cover photo, currency, devices you use, when your information was last updated, your posts and statuses, your photos and your friends,” writes De Ceukelaire.

He reckons people’s data had been being publicly exposed since at least the end of 2016.

On Facebook, NameTests describes its purpose thusly: “Our goal is simple: To make people smile!” — adding that its quizzes are intended as a bit of “fun”.

It doesn’t shout so loudly that the ‘price’ for taking one of its quizzes, say to find out what Disney princess you ‘are’, or what you could look like as an oil painting, is not only that it will suck out masses of your personal data (and potentially your friends’ data) from Facebook’s platform for its own ad targeting purposes but also, until recently, that your and other people’s information could have been exposed to goodness knows who, for goodness knows what nefarious purposes…

The Facebook-Cambridge Analytica data misuse scandal has underlined that ostensibly frivolous social data can end up being repurposed for all sorts of manipulative and power-grabbing purposes. (And not only can end up, but that quizzes are deliberately built to be data-harvesting tools… So think of that the next time you get a ‘take this quiz’ notification asking ‘what is in your fact file?’ or ‘what has your date of birth imprinted on you’? And hope ads is all you’re being targeted for… )

De Ceukelaire found that NameTests would still reveal Facebook users’ identity even after its app was deleted.

“In order to prevent this from happening, the user would have had to manually delete the cookies on their device, since NameTests.com does not offer a log out functionality,” he writes.

“I would imagine you wouldn’t want any website to know who you are, let alone steal your information or photos. Abusing this flaw, advertisers could have targeted (political) ads based on your Facebook posts and friends. More explicit websites could have abused this flaw to blackmail their visitors, threatening to leak your sneaky search history to your friends,” he adds, fleshing out the risks for affected Facebook users.

As well as alerting Facebook to the vulnerability, De Ceukelaire says he contacted NameTests — and they claimed to have found no evidence of abuse by a third party. They also said they would make changes to fix the issue.

We’ve reached out to NameTests’ parent company — a German firm called Social Sweethearts — for comment. Its website touts a “data-driven approach” — and claims its portfolio of products achieve “a global organic reach of several billion page views per month”.

Update: It has now sent the following statement: “As the data protection officer of social sweethearts, I would like to inform you that the matter has been carefully investigated. The investigation found that there was no evidence that personal data of users was disclosed to unauthorised third parties and all the more that there was no evidence that it had been misused. Nevertheless, data security is taken very seriously at Social Sweethearts and measures are currently being taken to avoid risks in the future.”

After De Ceukelaire reported the problem to Facebook, he says he received an initial response from the company on April 30 saying they were looking into it. Then, hearing nothing for some weeks, he sent a follow up email, on May 14, asking whether they had contacted the app developers.

A week later Facebook replied saying it could take three to six months to investigate the issue (i.e. the same timeframe mentioned in their initial automated reply), adding they would keep him in the loop.

Yet at that time — which was a month after his original report — the leaky NameTests quizzes were still up and running, meaning Facebook users’ data was still being exposed and at risk. And Facebook knew about the risk.

The next development came on June 25, when De Ceukelaire says he noticed NameTests had changed the way they process data to close down the access they had been exposing to third parties.

Two days later Facebook also confirmed the flaw in writing, admitting: “[T]his could have allowed an attacker to determine the details of a logged-in user to Facebook’s platform.”

It also told him it had confirmed with NameTests the issue had been fixed. And its apps continue to be available on Facebook’s platform — suggesting Facebook did not find the kind of suspicious activity that has led it to suspend other third party apps. (At least, assuming it conducted an investigation.)

Facebook paid out a $4,000 x2 bounty to a charity under the terms of its data abuse bug bounty program — and per De Ceukelaire’s request.

We asked it what took it so long to respond to the data abuse report, especially given the issue was so topical when De Ceukelaire filed the report. But Facebook declined to answer specific questions.

Instead it sent us the following statement, attributed to Ime Archibong, its VP of product partnerships:

A researcher brought the issue with the nametests.com website to our attention through our Data Abuse Bounty Program that we launched in April to encourage reports involving Facebook data. We worked with nametests.com to resolve the vulnerability on their website, which was completed in June.

Facebook also claims it received De Ceukelaire’s report on April 27, rather than April 22, as he recounts it. Though it’s possible the former date is when Facebook’s own staff retrieved the report from its systems.

Beyond displaying a disturbingly relaxed attitude to other people’s privacy — which risks getting Facebook into regulatory trouble, given GDPR’s strict requirements around breach disclosure, for example — the other core issue of concern here is the company’s apparent failure to enforce its own developer policy.

The underlying issue is whether or not Facebook performs any checks on apps running on its platform. It’s no good having T&Cs if you don’t have any active processes to enforce your T&Cs. Rules without enforcement aren’t worth the paper they’re written on.

Historical evidence suggests Facebook did not actively enforce its developer T&Cs — even if it’s now “locking down the platform”, as it claims, as a result of so many privacy scandals.

The quiz app developer at the center of the Cambridge Analytica scandal, Aleksandr Kogan — who harvested and sold/passed Facebook user data to third parties — has accused Facebook of essentially not having a policy. He contends it is therefore Facebook who is responsible for the massive data abuses that have played out on its platform — only a portion of which have so far come to light.

Fresh examples such as NameTests’ leaky quiz apps merely bolster the case Kogan made for Facebook being the guilty party where data misuse is concerned. After all, if you built some stables without any doors at all would you really blame your horses for bolting?",Social Media,TechCrunch,https://techcrunch.com/2018/06/28/facepalm-2/,"The NameTests quiz app scandal highlights the potential danger of social media data being exposed and misused, as well as Facebook's failure to enforce its own developer policies and protect its users' data. This demonstrates the need for stronger regulations to ensure that social media companies are held accountable for their data misuse.",Security & Privacy
484,Twitter grilled on policy approach that reinforces misogyny – TechCrunch,"Twitter has faced a barrage of awkward questions from the U.K. parliament over its ongoing failure to tackle violent abuse targeted at women.

Katy Minshall, the social media platform’s head of U.K. government public policy, admitted it needs to do more to safeguard women users — but claimed the company is “acutely aware” of the problems women experience on Twitter, saying it’s in the process of reviewing how it applies its policies to fix its long-running misogyny problem.

“We are acutely aware of the unique experience women have on Twitter and changes we may have to make in our policies to get that right,” she told the human rights committee session on free speech and democracy this afternoon. “We are very much aware of the real issue that women experience on our platform.”

Parliamentarians raised the issue of how unequally Twitter applies policies on hateful conduct depending on the sex being targeted, with MP Joanna Cherry accusing the company of displaying a pattern of relaxed tolerance to tweets containing violent attacks on women — citing research that has shown women in public life receive more abuse than men which she said was why she was focusing her questions for Twitter on abuse targeted at women.

Cherry contrasted Twitter’s failure to respond quickly to misogynistic abuse with examples of alacritous intolerance to tweets that raised the issue of male violence — citing examples of users who had had their Twitter accounts temporarily suspended for making factual, gender-based observations with a male flavor — such as that, on aggregate, men kill more than women.

Or tweets citing British law — which states that only a man can commit rape.

“There seem to be a number of mistakes here. And they seem to be mistakes that are failing to protect women. Do you accept that?” asked Cherry.

“There is clearly a number of steps that we want to take, we need to take, but we are in a different place to where we were even this time last year,” said Minshall initially, before simplifying her response to “clearly there’s an issue here for us to look at” later in the Q&A session.

Minshall was asked to look at several examples of violent tweets which had been directed at women, including tweets whose recipients had reported them to Twitter — only to be told they did not violate its hateful conduct policy.

Only later, after Cherry said feminist campaigners, journalists and she herself had tweeted about Twitter’s decision not to take down some of the misogynistic tweets did it reverse course and remove them.

Minshall admitted that one of the abusive tweets had been removed last night, after Cherry had tweeted to draw attention to it.

The abusive tweets discussed during this portion of the session appeared to have been targeted at cisgender women, i.e. at women whose gender identity matches their birth sex.

To be clear, the term ‘cisgender’ was not used during the questioning session. But ‘terf’ — a term that stands for ‘trans-exclusionary radical feminist’ and is used to refer to people who reject the rights of trans women — was directly referred to during the hearing, featuring in several of the tweets that Cherry raised with Twitter. The MP suggested the term “tends” to be applied to women.

Before raising these tweets Cherry prefaced her questions by saying they relate to “the very active debate on social media about trans rights” — which she said contains “considerable abuse on both sides of the argument”, emphasizing that she “deplores any abuse directed at trans people”.

Twitter has been a major conduit for online conflict between transgender women and those individuals who seek to deny their rights as women.

One tweet that Cherry asked Minshall to look at depicted a cartoon figure with a photo of a real hand holding a gun pointed at the viewer, atop the caption “shut the fuck up terf”. Another included a video game clip of a man repeatedly chopping a cisgender woman in the neck which had been attached to a tweet saying “what I do to terfs”. A third tweet included what Cherry dubbed “a very unpleasant representation of a male flaying a woman alive” — which she said had been sent to a woman after she had complained on Twitter about receiving one of the other violent tweets.

Minshall said she believed all the tweets Cherry raised as examples at this point of the hearing violated Twitter’s policies and should have been removed if they hadn’t already. Though said she doesn’t work in the safety team, caveating her response with: “I’m not the expert.”

Later in the session Minshall also said that trying to moderate a public discussion about transgender rights can be “difficult” — leading Cherry to respond that none of the counter examples she cited of tweets related to male violence that were quickly taken down by Twitter were in any way abusive towards transgender people.

Had one of the tweets she raised contained an attack on trans people Cherry emphasized she would “condemn the tweet”. (The tweet being referred to at this point read: “All rapists are men. In UK law rape is a crime only committed by a person with a penis.”)

“What I’m trying to understand is why, initially, the first tweet — the chopping in the neck — was ruled alright by Twitter. And why it took the intervention of a leading journalist, a leading feminist commentator and a member of parliament for it to be ruled not alright,” Cherry asked Minshall.

“We need to understand who is actually carrying out these decisions. Who is carrying out the mediation at Twitter. Is it done in the UK, is it done in America, who is it done by. Is there any attempt at gender balance within the teams of people looking at these tweets.”

Minshall said she could not answer the question of the team’s gender breakdown there and then — saying she would write to the committee with an answer.

Cherry also made the point that sex is a protected characteristic in UK law, and pressed Minshall several times on why Twitter’s hateful conduct policy only applies to gender — asking Minshall at one point whether ‘terf’ is a gendered term “in the same way that bitch and cunt are gendered terms” (Minshall agreed it is gendered).

“Can you tell us why Twitter has chosen to exclude sex from their hateful conduct policy as a protected characteristic?” Cherry asked. “I’m wondering if that’s what could be going wrong here? That the training is not covering the fact that sexist, misogynistic, demeaning behavior should be treated as seriously as abuse of, for example, trans people.”

Minshall said Twitter’s hateful conduct policy is based upon United Nations definitions, arguing the current policy that protects gender should also protect against misogyny — while admitting there’s still an asymmetrical burden on female Twitter users to report abuse. She also agreed to follow up with the committee to explain why Twitter’s policy does not include sex as a protected characteristic too.

“There’s a lot that we want to do to reduce the burden on reporters,” she said. “We have rules in place where it would be a breach to target someone based on the fact that they’re a woman — where we need to do far more is to be proactive in reducing the burden on victims to report that to us.”

At an earlier point during the session Minshall noted that Twitter is reviewing its policy on harassment — saying it’s concerned about the risk of women being stalked via the platform by ex-partners.

“There is an issue specific to women, typically ex-partners, stalking them on Twitter in ways that have traditionally been difficult to detect in our rules — and we want to do better on that,” she added.

This post was updated to clarify that ‘terf’ is a descriptor, not a term of abuse; and with additional context around Cherry’s line of questioning and the transgender rights debate online",Social Media,TechCrunch,https://techcrunch.com/2019/05/01/twitter-grilled-on-policy-approach-that-reinforces-misogyny/,"Twitter has been accused of failing to tackle violent abuse targeted at women, particularly in regards to their unequal application of policies and their relaxed tolerance to tweets containing violent attacks on women. The company has acknowledged the issue and is in the process of reviewing how it applies its policies to address its long-running misogyny problem.",Equality & Justice
485,YouTube under pressure to ban UK far-right activist after live-streamed intimidation – TechCrunch,"The continued presence of a U.K. far-right activist on YouTube’s platform has been raised by the deputy leader of the official opposition during ministerial questions in the House of Commons today.

Labour’s Tom Watson put questions to the secretary of state for digital, Jeremy Wright, regarding Stephen Yaxley-Lennon’s use of social media for targeted harassment of journalists.

This follows an incident on Monday night when Yaxley-Lennon used social media tools to live-stream himself banging on the doors and windows of a journalist’s home in the middle of the night.

“Every major social media platform other than YouTube has taken down Stephen Yaxley-Lennon’s profile because of his hateful conduct,” said Watson, before recounting how the co-founder of the Far Right English Defence League — who goes by the made-up name “Tommy Robinson” on social media — used social media live-streaming tools to harass journalist Mike Stuchbery on Monday night.

Stuchbery has since written about the incident for Independent.

A journalist intimidated at home at 5am, live streamed. Women colleagues facing rape and death threats installing panic buttons. The torrent of online abuse undermines democracy and free speech. We must act to stop the online world turning into a cesspit of hate. pic.twitter.com/6G4fSjEue7 — Tom Watson (@tom_watson) March 7, 2019

As we reported on Monday, Facebook removed the live stream for violating its policies after it was reported but not before Stuchbery had received a flood of abusive messages from other Facebook users who were watching the stream online.

Yaxley-Lennon appears to have been able to circumvent Facebook’s ban on his own account to live stream his intimidation of Stuchbery via Facebook Live by using another Facebook account with a fake name (which the company appears to have since suspended).

Following the incident, Stuchbery has reported receiving physical hate mail to his home address, which Yaxley-Lennon gave out during the live stream (an intimidation tactic that’s known as doxxing). He has also said he’s received further abuse online.

“Does the secretary of state think that it is right that YouTube, and the parent company Alphabet, continues to give this man a platform?” asked Watson, after highlighting another vlog Yaxley-Lennon has since uploaded to YouTube in which he warns other journalists “to expect a knock at the door.”

Wright responded by saying that “all internet companies, all platforms for this kind of speech need to take their responsibilities seriously.”

“I hope that YouTube will consider this very carefully,” he told the House of Commons. “Consider what [Yaxley-Lennon] has said. What I have said, and reconsider their judgement.”

“We all believe in freedom of speech. But we all believe too that that freedom of speech has limits,” Wright added. “And we believe that those who seek to intimidate others, to potentially of course break the law… that is unacceptable. That is beyond the reach of the type of freedom of speech that we believe should be protected.”

We’ve reached out to YouTube for comment.

Stephen Yaxley-Lennon was banned by Facebook last month for repeat violations of its policies on hate speech. Twitter banned Yaxley-Lennon a full year ago.

But he remains active on YouTube — where his channel has more than 350,000 subscribers.

The company has resisted calls to shutter his account, claiming the content Yaxley-Lennon posts to its platform is different to content he has posted elsewhere and thus he has not broken any of its rules. (Though YouTube did demonetize videos on his channel in January, saying they violated its ad policies.)

In a follow-up question, Watson raised the issue of online harassment more widely — asking whether the government would be including measures “to prevent hate figures, extremists and their followers from turning the online world into a cesspit of hate” in its forthcoming white paper on social media and safety, which it’s due to publish this winter — and thereby tackle a culture of hate and harassment online that he said is undermining democracy.

Wright said he would “consider” Watson’s suggestion, though he stressed the government must protect the ability for people to carry out robust debate online — and “to discuss issues that are sometimes uncomfortable and certainly controversial.”

But he went on to reiterate his earlier point that “no freedom of speech can survive in this country if we do not protect… people’s ability to feel free to say what they think, free of intimidation, free of the threat of violence.”

“Those who engage in intimidation or threats of violence should not find succour either online or anywhere else,” the minister added.

YouTube’s own community guidelines prohibit “harassment and cyberbullying,” so its continued silence on Yaxley-Lennon’s misuse of its tools does look inconsistent. (YouTube previously banned the InfoWars conspiracy theorist Alex Jones for violating its policies, for example, and there’s more than a passing resemblance between the two “hate preachers”).

Moreover, as Watson noted in parliament, Yaxley-Lennon’s most recent video contains a direct threat to doorstep and doxx journalists who covered his harassment of Stuchbery. The video also contains verbal abuse targeted at Stuchbery. However, YouTube told us the video does not violate its policies.

In one of the live streams recorded outside Stuchbery’s home, Yaxley-Lennon can also be heard making allegations about Stuchbery’s sexual interests that the journalist has described as defamatory.

YouTube previously declined to make a statement about Yaxley-Lennon’s continued presence on its platform. It has not responded to our repeated requests for follow-up comment about the issue since Monday.

We’ll update this post if it does provide a statement following the government’s call to rethink its position on giving Yaxley-Lennon a platform.

This post was updated with information from YouTube regarding Yaxley-Lennon’s latest video.",Social Media,TechCrunch,https://techcrunch.com/2019/03/07/youtube-under-pressure-to-ban-uk-far-right-activist-after-livestreamed-intimidation/,"The presence of a far-right activist on YouTube's platform has raised concerns over his use of social media for targeted harassment and intimidation of journalists, leading to the government calling on YouTube to rethink its decision to give this person a platform.",Equality & Justice
486,Women Were the Trending Topic of the Games,"Who were the most talked-about athletes in the 2012 Summer Games?

If we’re talking daily mentions across the most active social media networks, that honor goes to Michael Phelps, Gabby Douglas, Usain Bolt, Ryan Lochte, Misty May-Treanor and Kerri Walsh Jennings. Pretty predictable, as they were some of the biggest winners at the Games.

A more striking measure of social media success is the percentage of increase in mentions. Content-sharing platform AddThis calculated the number of mentions (.jpg) athletes received in the two weeks before the Olympics, then tracked the mentions they received during the 2012 Summer Games. That yielded some interesting results.

The five athletes with the greatest percentage increase in mentions were women, most of whom didn't have a large fan base prior to the Games. That suggests they captured the hearts of sports fans the world over as the Olympics went on.

Unfortunately, people were as likely to discuss their physical appearance as their athletic prowess.

Without a doubt, the 2012 Summer Games — often hailed as the ""year of the woman"" — were monumental for women of all nations and cultures. For the first time, every country participating in the Games had at least one female athlete in London. The inclusion of women's boxing meant women were, for the first time, competing in every event at the Games. Nearly 45 percent of the 10,500-plus athletes were women.

It was against this backdrop that medal winners Gabby Douglas, Jordyn Wieber, Missy Franklin, Aly Raisman and Walsh Jennings dominated the social media conversation. Douglas — the first African-American gymnast to win the individual all-around gold medal — led the way with a staggering 31,234 percent increase in mentions. She also won the title of ""Most-Clicked Athlete"" on NBCOlympics.com, crushing Phelps by more than 11 million views.

But unfortunately, that rapid rise in internet popularity isn’t exclusively due to their athletic skills and Olympic victories. The Social Media Report produced by AddThis doesn’t provide the context of these online conversations, so we wondered how much of the increase in mentions can be attributed to trolls tweeting about Douglas’ hair or commenting on how Walsh Jennings looks in a bikini.",Social Media,WIRED,https://www.wired.com/2012/08/women-social-media-olympics/,"The increase in online mentions of some of the most successful female athletes in the 2012 Summer Games was unfortunately accompanied by comments about their physical appearance, rather than their athletic feats. This suggests that, even in a year that was heralded as the ""year of the woman"", the female athletes' achievements were not being fully appreciated.",Equality & Justice
487,Dutch court orders Facebook to ban celebrity crypto scam ads after another lawsuit – TechCrunch,"A Dutch court has ruled that Facebook can be required to use filter technologies to identify and pre-emptively take down fake ads linked to crypto currency scams that carry the image of a media personality, John de Mol, and other well known celebrities.

The Dutch celerity filed a lawsuit against Facebook in April over the misappropriation of his and other celebrities’ likeness to shill Bitcoin scams via fake ads run on its platform.

In an immediately enforceable preliminary judgement today the court has ordered Facebook to remove all offending ads within five days, and provide data on the accounts running them within a week.

Per the judgement, victims of the crypto scams had reported a total of €1.7 million (~$1.8M) in damages to the Dutch government at the time of the court summons.

The case is similar to a legal action instigated by UK consumer advice personality, Martin Lewis, last year, when he announced defamation proceedings against Facebook — also for misuse of his image in fake ads for crypto scams. Lewis withdrew the suit at the start of this year after Facebook agreed to apply new measures to tackle the problem: Namely a scam ads report button. It also agreed to provide funding to a UK consumer advice organization to set up a scam advice service.

In the de Mol case the lawsuit was allowed to run its course — resulting in today’s preliminary judgement against Facebook.

It’s not yet clear whether the company will appeal but in the wake of the ruling Facebook has said it will bring the scam ads report button to the Dutch market early next month.

In court, the platform giant sought to argue that it could not more proactively remove the Bitcoin scam ads containing celebrities’ images on the grounds that doing so would breach EU law against general monitoring conditions being placed on Internet platforms.

However the court rejected that argument, citing a recent ruling by Europe’s top court related to platform obligations to remove hate speech, also concluding that the specificity of the requested measures could not be classified as ‘general obligations of supervision’.

It also rejected arguments by Facebook’s lawyers that restricting the fake scam ads would be restricting the freedom of expression of a natural person, or the right to be freely informed — pointing out that the ‘expressions’ involved are aimed at commercial gain, as well as including fraudulent practices.

Facebook also sought to argue it is already doing all it can to identify and take down the fake scam ads — saying too that its screening processes are not perfect. But the court said there’s no requirement for 100% effectiveness for additional proactive measures to be ordered.

Its ruling further notes a striking reduction in fake scam ads using de Mol’s image since the lawsuit was announced.

Facebook’s argument that it’s just a neutral platform was also rejected, with the court pointing out that its core business is advertising. It also took the view that requiring Facebook to apply technically complicated measures and extra effort, including in terms of manpower and costs, to more effectively remove offending scam ads is not unreasonable in this context.

The judgement orders Facebook to remove fake scam ads containing celebrity likenesses from Facebook and Instagram within five days of the order — with a penalty of €10k per day that Facebook fails to comply with the order, up to a maximum of €1M (~$1.1M).

The court order also requires that Facebook provides data to the affected celebrity on the accounts that had been misusing their likeness within seven days of the judgement, with a further penalty of €1k per day for failure to comply, up to a maximum of €100k.

Facebook has also been ordered to pay the case costs.

Responding to the judgement in a statement, a Facebook spokesperson told us:

We have just received the ruling and will now look at its implications. We will consider all legal actions, including appeal. Importantly, this ruling does not change our commitment to fighting these types of ads. We cannot stress enough that these types of ads have absolutely no place on Facebook and we remove them when we find them. We take this very seriously and will therefore make our scam ads reporting form available in the Netherlands in early December. This is an additional way to get feedback from people, which in turn helps train our machine learning models. It is in our interest to protect our users from fraudsters and when we find violators we will take action to stop their activity, up to and including taking legal action against them in court.

One legal expert describes the judgement as “pivotal“. Law professor Mireille Hildebrandt told us that it provides for as an alternative legal route for Facebook users to litigate and pursue collective enforcement of European personal data rights. Rather than suing for damages — which entails a high burden of proof.

Injunctions are faster and more effective, Hildebrandt added.

This is a tort case, not based on GDPR. Enforcement of GDPR obligations can also be ensured via such penalty payments (much more effective than fines), ex art. 79 GDPR, which basically requires that tort actions can be filed against controllers if unlawful processing 4/4 — Mireille Hildebrandt (@mireillemoret) November 12, 2019

“It clearly demonstrates that Facebook is an advertising platform that can be held liable under Dutch tort law for not taking adequate measure to remove fake ads, provided certain conditions apply (such as that the request for an injunction is sufficiently specific),” she said. “It also clearly demonstrates that filing an injunction to stop unlawful behaviour based on tort law can be very efficient, without having to prove damages, because the injunction can be enforced by way of penalty payments for each day that it is not complied with.”

“I believe that this is the kind of action Member State should enable under art. 79 GDPR [General Data Protection Regulation] in the case of unlawful processing of personal data,” Hildebrandt added.

“Art. 80 GDPR allows data subjects to mandate their right to file such an injunction to a dedicated NGO (thus enabling collective action). This is not the case for a tort actions suing for damages, in that case the GDPR leaves it up to the Member States whether or not to allow such mandating. On top of that suing for damages is very cumbersome in the case of unlawful processing, because it is usually very difficult to prove damages and to prove causality.”

The judgement also raises questions around the burden of proof for demonstrating Facebook has removed scam ads with sufficient (increased) accuracy; and what specific additional measures it might deploy to improve its takedown rate.

Although the introduction of the ‘report scam ad button’ does provide one clear avenue for measuring takedown performance.

The button was finally rolled out to the UK market in July. And while Facebook has talked since the start of this year about ‘envisaging’ introducing it in other markets it hasn’t exactly been proactive in doing so — up til now, with this court order.

This report was updated with additional comment",Social Media,TechCrunch,https://techcrunch.com/2019/11/12/dutch-court-orders-facebook-to-ban-celebrity-crypto-scam-ads-after-another-lawsuit/,"The main undesirable consequence of Social Media being discussed here is the misuse of celebrities' images to shill fraudulent crypto currency scams, which has led to a court ruling in the Netherlands requiring Facebook to use filter technologies to identify and pre-emptively take down fake ads linked to these scams. The ruling has also ordered Facebook to provide data on the",Security & Privacy
488,"Facebook will not remove deepfakes of Mark Zuckerberg, Kim Kardashian and others from Instagram – TechCrunch","Facebook will not remove deepfakes of Mark Zuckerberg, Kim Kardashian and others from Instagram

Facebook will not remove the faked videos featuring Mark Zuckerberg, Kim Kardashian and President Donald Trump from Instagram, the company said in a statement.

Earlier today, Vice News reported on the existence of videos created by the artists Bill Posters and Daniel Howe and video and audio manipulation companies including CannyAI, Respeecher and Reflect.

The work, featured in a site-specific installation in the UK as well as circulating in video online, was the first test of Facebook’s content review policies since the company’s decision not to remove a manipulated video of House Speaker Nancy Pelosi received withering criticism from Democratic political leadership.

“We have said all along, poor Facebook, they were unwittingly exploited by the Russians,” Pelosi said in an interview with radio station KQED, quoted by The New York Times. “I think they have proven — by not taking down something they know is false — that they were willing enablers of the Russian interference in our election.”

After the late May incident Facebook’s Neil Potts testified before a smorgasbord of international regulators in Ottawa about deep fakes, saying the company would not remove a video of Mark Zuckerberg. This appears to be the first instance testing the company’s resolve.

Pentus-Rosimannus: If instead of Pelosi, a doctored video of Mark Zuckerberg was slowed down to make him seem impaired, would you leave it up? Potts: Yes. — Alex Boutilier (@alexboutilier) May 28, 2019

“We will treat this content the same way we treat all misinformation on Instagram. If third-party fact-checkers mark it as false, we will filter it from Instagram’s recommendation surfaces like Explore and hashtag pages,” said an Instagram spokesperson in an email to TechCrunch.

The videos appear not to violate any Facebook policies, which means that they will be subject to the treatment any video containing misinformation gets on any of Facebook’s platforms. So the videos will be blocked from appearing in the Explore feature and hashtags won’t work with the offending material.

Facebook already uses image detection technology to find content that has been debunked by its third-party fact checking program on Instagram. When misinformation is only present on Instagram the company is testing the ability to promote links into the fact-checking product on Facebook.

“Spectre interrogates and reveals many of the common tactics and methods that are used by corporate or political actors to influence people’s behaviours and decision making,” said Posters in an artist’s statement about the project. “In response to the recent global scandals concerning data, democracy, privacy and digital surveillance, we wanted to tear open the ‘black box’ of the digital influence industry and reveal to others what it is really like.”

Facebook’s consistent decisions not to remove offending content stands in contrast with YouTube which has taken the opposite approach in dealing with manipulated videos and other material that violate its policies.

YouTube removed the Pelosi video and recently took steps to demonetize and remove videos from the platform that violated its policies of hate speech — including a wholesale purge of content about Nazism.

These issues take on greater significance as the U.S. heads into the next Presidential election in 2020.

“In 2016 and 2017, the UK, US and Europe witnessed massive political shocks as new forms of computational propaganda employed by social media platforms, the ad industry, and political consultancies like Cambridge Analytica [that] were exposed by journalists and digital rights advocates,” said Howe, in a statement about his Spectre project. “We wanted to provide a personalized experience that allows users to feel what is at stake when the data taken from us in countless everyday actions is used in unexpected and potentially dangerous ways.”

Perhaps, the incident will be a lesson to Facebook in what’s potentially at stake as well.",Social Media,TechCrunch,https://techcrunch.com/2019/06/11/facebook-will-not-remove-deepfakes-of-mark-zuckerberg-kim-kardashian-and-others-from-instagram/,"The recent incident with the deepfakes of Mark Zuckerberg, Kim Kardashian and President Donald Trump on Instagram demonstrates the dangers of Social Media, as it reveals the tactics used by corporate or political actors to influence people's decisions and behaviours. This highlights the need for greater regulation of Social Media platforms to ensure that manipulated content does not spread unchecked.","Information, Discourse & Governance"
489,"From Kitchen to Bathroom, Social Media Invades Our Lives","Sharing—up to a point—is cool. But now social media has us sharing a friend of a friend’s commode, gastrointestinal bacteria, and “gifted” vending machine sodas.",Social Media,WIRED,https://www.wired.com/2011/11/st-socialmediafication/,"Social media has led to an oversharing of personal information that can be detrimental to our privacy, such as bathroom habits, bacteria levels, and even the contents of our drinks.",Security & Privacy
490,Feds Monitoring Social Media Does More Harm Than Good,"For privacy wonks and casual observers alike, border screening and surveillance has become an increasing area of critical concern over the last year. Around the world, invasive governments have particularly threatened people's digital privacy. That extends to the US, where Customs and Border Protection has expanded its demands and searches as well. And a fraught situation for travelers is even more so for US immigrants who are having more and more of their digital and social media footprint monitored by the Department of Homeland Security.

The agency's recent initiatives came into focus last week, when DHS posted updated language in the Federal Register about collecting “social media handles, aliases, associated identifiable information, and search results” on immigrants, including naturalized citizens and permanent residents. DHS also issued a notice about changes to its Intelligence Records System database that will store “public-source data (including information from social media)” and will gather information from a host of sources, including “commercial data providers and public sources such as social media, news media outlets, and the Internet.”

These initiatives, which began during the Obama administration, claim to be part of standard domestic security screening and background checking. But immigration experts and digital privacy advocates alike are deeply skeptical about how effective social media checks really are. Particularly, they cite the open-ended scope of the surveillance, and potential chilling effects of the dragnet, as critical drawbacks that should give the federal government pause.

""Admissions decisions should be based on specific criteria defined in laws or regulations,"" says Edward Hasbrouck, a travel expert and consultant to the freedom of movement group The Identity Project. ""What can you and can’t you say on Facebook if you want to be admitted to the country? We’re hearing over and over that everybody who’s not a citizen is afraid to say anything on social media because they don’t know how it might be held against them.""

Overreach

Ambiguity over what DHS will collect and why has immediate free speech implications for immigrants and potential immigrants. And language about collecting ""social media handles, aliases, [and] associated identifiable information"" hints at a more expansive agenda than simply scoping out people's public personas online. DHS seems to be asking people to list the pseudonyms and anonymizers they have used online, potentially exposing past speech they intended to disassociate from their real name.

""Yes, our rights at the border including for citizens are more limited than in other parts of life, but this is a gross invasion of your privacy and is far beyond the boundaries of a reasonable search,"" says Nuala O’Connor, the president of the digital rights group Center for Democracy & Technology and a former chief privacy officer at DHS. ""With these widespread fishing expeditions, who knows when or where that data about you will pop up again, particularly when you're asked to divulge spaces online where you thought you might have been anonymous or pseudonymous.""

Casting such a wide net also inevitably impacts bystanders whom subjects interact with online, creating implications for freedom of association as well.",Social Media,WIRED,https://www.wired.com/story/dhs-social-media-immigrants-green-card/,"The US government's expanded use of social media surveillance of immigrants and potential immigrants has serious implications for freedom of speech, privacy, and association, with a wide net that can potentially be used to target innocent bystanders.",Security & Privacy
491,The network effect is anti-competitive – TechCrunch,"A U.S. federal judge last week struck down Apple rules restricting app developers from selling directly to customers outside the App Store.

Apple’s stock fell 3% on the news, which is being regarded as a win for small and midsize app developers because they’ll be able to build direct billing relationships with their customers. But Apple is just one of many Big Tech companies that dominate their sector.

The larger issue is how this development will impact Amazon, Facebook, Grubhub and other tech giants with online marketplaces that use draconian terms of service to keep their resellers subservient. The skirmish between Apple and small and midsize app developers is just a smaller battle in a much larger war.

App makers pay up to 30% on every sale they make on the Apple App Store. Resellers on Amazon pay a monthly subscription fee, a sales commission of 8% to 15%, fulfillment fees and other miscellaneous charges. Grubhub charges restaurants 15% of every order, a credit card processing fee, an order processing fee and a 10% delivery commission.

Like app developers, online resellers and social media influencers are all falling for the same big lie: that they can build a sustainable business with healthy margins on someone else’s platform. The reality is the App Store, online marketplaces and even social networks that dominate their sectors have the unilateral power to selectively deplatform and squeeze their users, and there’s not much anyone can do about it.

Healthy competition exists inside the App Store and among marketplace resellers and aspiring social media influencers. But no one seems to be talking about the real elephants in the room, which are the social networks and online marketplace providers themselves. In some respects, they’ve become almost like digital dictators with complete control over their territories.

It’s something every small and midsize business that gets excited about some new online service catering to their industry should be aware of because it directly impacts their ability to grow a stable business. The federal judge’s decision suggests the real goal in digital business is a direct billing relationship with the end user.

On the internet, those who are able to lead a horse to water and make them drink — outside the walled gardens of digital marketplace operators like Uber, Airbnb and Udemy — are the true contenders. In content and e-commerce, this is what most small and midsize companies don’t realize. Your own website or owned media, at a top-level domain that you control, is the only unfettered way to sell direct to end users.

Mobile app makers on Apple’s App Store, resellers on Amazon and aspiring content creators on Instagram, YouTube and TikTok are all subject to the absolute control of digital titans who are free to govern by their own rules with unchecked power.

For access to online marketplaces and social networks, we got a raw deal. We’re basically plowing their fields like digital sharecroppers. Resellers on Amazon are forced to split their harvest with a landlord who takes a gross percentage with no caps. Amassing followers on TikTok is building an audience that’s locked inside their venue.

These tech giants — all former startups that built their audiences from scratch — are free to impose and selectively enforce oppressive rules. If you’re a small fry, they can prohibit you from asking for your customer’s email address and deplatform you for skimming, but look the other way when Spotify and The New York Times do the same thing. Both were already selling direct and through the App Store prior to Friday’s ruling.

How is that competitive? Even after the ruling, Big Tech still gets to decide who they let violate their terms of service and who they deplatform. It’s not just their audience. It’s their universe, their governance, their rules and their enforcement.

In the 1948 court case United States v. Paramount Pictures, the Supreme Court ruled that film studios couldn’t own their own theaters because that meant they could exclusively control what movies were screened. They stifled competition by controlling what films made it to the marquee, so SCOTUS broke them up.

Today, social networks control what gets seen on their platforms, and with the push of a button, they can give the hook to whoever they want, whenever they want. The big challenge that the internet poses to capitalism is that the network effect is fundamentally anti-competitive. Winner-take-all markets dominated by tech giants look more like government-controlled than free-market economies.

On the one hand, the web gives us access to a global marketplace of buyers and sellers. On the other, a few major providers control the services that most people use to do business, because they don’t have the knowledge or resources to stand up a competitive website. But unless you have your own domain and good search visibility, you’re always in danger of being deplatformed and losing access to your customers or audience members with no practical recourse.

The network effect is such that once an online marketplace becomes dominant, it neutralizes the competitive market, because everyone gravitates to the dominant service to get the best deal. There’s an inherent conflict between the goals of a winner-takes-all tech company and the goals of a free market.

Dominant online marketplaces are only competitive for users. Meanwhile, marketplace providers operate with impunity. If they decide they want to use half-baked AI or offshore contractors to police their terms of service and shore up false positives, there’s no practical way for users to contest. How can Facebook possibly govern nearly 3 billion users judiciously with around 60,000 employees? As we’ve seen, it can’t.

For app makers, online resellers and creators, the only smart option is open source on the open web. Instead of relying on someone else’s audience (or software for that matter), you own your online destination powered by software like WordPress or Discord, and you never have to worry about getting squeezed when the founders go public or their platform gets bought by profit-hungry investment bankers. Only then can you protect your profit margins. And only then are the terms of service the laws of the land.

Politics aside, as former President Donald Trump’s deplatforming demonstrated, if you get kicked off Facebook and Twitter, there’s really nowhere else to go. If they want you out, it’s game over. It’s no coincidence Trump lost his Facebook and Twitter accounts on the same day the Republicans lost the Senate. If the GOP takes back the Senate, watch Trump get his social media accounts back. Social networks ward off regulators by appeasing the legislative majority.

So don’t get too excited about the new Amazon Influencer Program. If you want to build a sustainable digital business, you need an owned media presence powered by software that doesn’t rake commissions, have access to your customer contact information and has an audience that can’t be commandeered with an algorithm tweak.",Social Media,TechCrunch,https://techcrunch.com/2021/09/14/the-network-effect-is-anti-competitive/,"The main issue with social media is that it gives tech giants such as Facebook and Twitter unchecked power over their users, allowing them to deplatform them and squeeze their profit margins. This anti-competitive network effect means that the only way to have control over your digital presence and build a sustainable business is to own your domain and have access to an",Economy
492,"Facebook Tweaks Newsfeed to Favor Content from Friends, Family","In November, Facebook CEO Mark Zuckerberg started sprinkling a new phrase, or perhaps a new idea, into his quarterly call with investors. “It's important to remember that Facebook is about bringing people closer together and enabling meaningful social interactions,” he said. Research, he continued, demonstrates that interactions with friends and family on social media is particularly “meaningful.” The goal of his service is to “encourage meaningful social interactions” and to connect in ways that lead to “meaningful interactions” and let us “build meaningful relationships.”

Clearly something was up. In December, Facebook researchers worked the word “meaningful” seven times into a blog post about the value of social media. “We want Facebook to be a place for meaningful interactions,” they wrote, explaining findings that the passive use of social media can be alienating but active use can be beneficial.

Thursday, Zuckerberg more fully explained how this quest for meaning will be worked into the core of its platform: Facebook is changing the algorithm that powers its newsfeed, the service at its core and the mechanism that increasingly determines how news and information spread throughout the world.

“We built Facebook to help people stay connected and bring us closer together with the people that matter to us. Research shows that strengthening our relationships improves our well-being and happiness,” Zuckerberg wrote in a blog post. “But recently we've gotten feedback from our community that public content -- posts from businesses, brands and media -- is crowding out the personal moments that lead us to connect more with each other. Based on this, we're making a major change to how we build Facebook. I'm changing the goal I give our product teams from focusing on helping you find relevant content to helping you have more meaningful social interactions.”

The upshot: The newsfeed algorithm will now give less weight to the popularity of posts and more weight to posts that encourage users to interact and comment. One of the big criticisms of Facebook in the past 18 months is that the content we see in the newsfeed is driven too much by Facebook’s obsession with persuading people to spend as much time as possible on Facebook. The more time people spend in the newsfeed, the more ad revenue Facebook makes. That may be good for Facebook, but, according to an increasingly loud chorus of critics, it’s not so good for humanity.

That’s changing. Popularity may have been the most important indicator when Facebook was small. But as Facebook has become the world’s news and information network, it’s helped make the newsfeed feel like a third-rate tabloid that not only drives the most extreme and polarizing information to the top our feeds, but encourages creators of that content to be more extreme and polarizing to get noticed.

Now, instead of optimizing for how much time users spend on Facebook, Zuckerberg says he wants the time users do spend be “time well spent.” What that means, according to Adam Mosseri, Facebook’s newsfeed boss, is that video, news, and other content from formal Facebook pages will get less prominence than posts from friends and family. It means that the number of comments on a post will count more than the number of Likes, and it means that posts where people have taken the time to write long comments will get more weight than those with only short comments. News and video will continue to appear in newsfeed, but the number of friends sharing it will matter more than its overall popularity.",Social Media,WIRED,https://www.wired.com/story/facebook-tweaks-newsfeed-to-favor-content-from-friends-family/,"Facebook's newsfeed algorithm had been prioritizing content based on popularity, leading to increased polarizing and extreme content, and an overall decrease in meaningful social interactions.",Social Norms & Relationships
493,6 Tales of Censorship in the Golden Age of Free Speech,"“In today’s networked environment, when anyone can broadcast live or post their thoughts to a social network, it would seem that censorship ought to be impossible,"" Zeynep Tufekci writes in our special issue about online free speech. But while the social internet gives everyone a voice, it also has countless ways of punishing people for speaking.

February 2018. Subscribe to WIRED. Sean Freeman

An African American writer calls out racist hate speech—and gets suspended from Facebook. A young adult author watches her unpublished novel ignite a firestorm on Twitter before anyone has even read it. A Muslim civil rights attorney self-censors, and then finds herself hoping that a white man will say what she was thinking. A well-known conservative firebrand suddenly becomes one of the biggest targets of far-right trolls. A Google engineer writes a controversial memo, and instantly becomes a villain to one army of online readers and a hero to another.

These are just a few stories—told in the subjects' own words—that capture what it’s like to live and post in this, our corrosive, divisive, democracy-poisoning golden age of free speech.

Magda Antoniuk

HOLLY O'REILLY

Songwriter and activist

On being blocked by Trump, and suing him for it

I had an alert that would go off whenever Trump tweeted, and I would reply to most of his tweets. I think it was a Sunday morning: I posted a GIF of the Pope kind of looking at Trump funny, and my tweet said, “This is pretty much how the whole world sees you.”

After that, my phone was very quiet all day. I thought, well, maybe he’s golfing. Then I came back to my computer in the evening and saw that he had actually blocked me. And I just laughed. I’m nobody. I can’t be more than a gnat to him. I felt incredulous, and then amused, and then concerned, all within moments of each other. Then I started thinking, you know, this is something that shouldn’t happen.

The things that I want to say are directed not just to Trump but to the other people who are on his feed. If they’re watching Fox News and listening to Rush Limbaugh and following Trump’s tweets, then Twitter is at least a place where they can get an opposing opinion. But he’s blocked people who disagree with him. When you look at his feed now, it’s mainly just people who are praising Dear Leader. That’s the part that bothers me. So when the Knight First Amendment Institute contacted me, kind of out of the blue, and asked if I would be interested in talking to them about taking part in a lawsuit against Trump, I said sure. Public officials should not be able to block you on social media.

—As told to Chelsea Leu

Magda Antoniuk

LAURA MORIARTY

Young-adult novelist

On being deemed “problematic”",Social Media,WIRED,https://www.wired.com/story/free-speech-issue-censorship/,"Social media has the potential to be a powerful tool for amplifying voices and sparking conversations, but it also has a darker side. By giving people the ability to instantly spread their opinions and views, it can create a dangerous echo chamber of opinion and fact, where individuals can be labeled “problematic” for expressing unpopular opinions or challenging",Equality & Justice
494,YouTube and Snapchat were asked to defend their apps’ age ratings in Senate hearing – TechCrunch,"YouTube and Snapchat were asked by lawmakers to defend whether or not their respective social media apps were appropriately rated on the U.S. app stores, given the nature of the content they hosted. In a line of questioning led by Senator Mike Lee (R-UT), Snap was given specific examples of the types of content found on its app that seemed to be inappropriate for younger teens. Both companies were also asked to explain why their app would be rated for different age groups on the different app stores.

For example, the YouTube app on the Google Play Store is rated “Teen,” meaning its content is appropriate for ages 13 and up, while the same app on Apple’s App Store is rated “17+,” meaning the content is only appropriate for users ages 17 and up, the senator pointed out.

Leslie Miller, YouTube’s Vice President of Government Affairs and Public Policy, who attended the hearing on behalf of YouTube, responded that she was “unfamiliar with the differences” the senator had just outlined with his question.

Snapchat’s Vice President of Global Public Policy Jennifer Stout, also wasn’t sure about the discrepancy when asked the same question.

In Snapchat’s case, the app is rated 12 and up on the Apple App Store but is rated “Teen” (13 and up) on the Google Play Store. She said she believed it was because Snapchat’s content was deemed “appropriate for an age group of 13 and above.”

The inconsistency with app ratings can make it hard for a parent to decide if an app is, in fact, appropriate for their kids, based on how strict their own household rules are on this sort of thing. But the issue ultimately comes down to each platform having different policies around app ratings. And because the app stores’ ratings aren’t necessarily consistent with similar ratings across other industries, like film and TV, parents today will often look to third-party resources, like Common Sense Media, news articles, online guides, or even anecdotal advice, when deciding whether or not to let their kids use a certain app.

Lee pushed on this point with Snapchat, in particular. He said his staff entered a name, birth date, and email to create a test account for a 15-year old child. They didn’t add any further content preferences for the account.

“When they opened the Discover page on Snapchat with its default settings, they were immediately bombarded with content that I can most politely describe as wildly inappropriate for a child, including recommendations for, among other things, an invite to play an online sexualized video game that’s marketed itself to people who are 18 and up; tips on quote, ‘why you shouldn’t go to bars alone;’ notices for video games that are rated for ages 17 and up; and articles about porn stars,” said Lee.

He wanted to know how Snap determined this content was appropriate for younger teens, as per the app’s rating on the app stores.

Stout responded by saying that Snapchat’s Discover section was a closed platform where the company chooses the publishers.

“We do select — and hand-select — partners that we work with. And that kind of content is designed to appear on Discover and resonate with an audience that is 13 and above,” she explained. Stout also seemed surprised that Discover would include the sort of inappropriate content the senator described. “I want to make clear that content and community guidelines suggest that any online sexual video games should be age-gated to 18 and above,” Stout said, adding that she was “unclear” why they would display by default for a younger user by default.

She went on to note that Snap’s publisher guidelines indicate the content must be accurate and fact-checked.

Lee’s response indicated he thought she was missing the point.

“I’m sure the articles about the porn stars were accurate and fact-checked,” he barked back. “And I’m sure that the tips on why you shouldn’t go to bars alone are accurate and fact-checked, but that’s not my question. It’s about whether it’s appropriate for children ages 13 and up as you certified,” he said.

The senator’s questions harken back to the ongoing moral panics from parents over teens’ use of social media — panics that have particularly impacted Snapchat with its ephemerality primed for teenage sexting.

However, the backlash over inappropriate Discover content actually spurred Snapchat to take action in the past — ahead of its IPO, of course.

The company in 2017 said it would begin to take a harder line on the sort of risqué and misleading images that had overrun the Discover section by implementing an updated set of publisher guidelines, which Stout also referenced during the hearing. But user reviews of the app in the years that followed have pointed out that Snapchat’s news section is filled with, as some young people described it: “cringe-worthy content” and “nonsense,” or “gossip, sex, and drugs.”

“The page is driven by clickbait, reality television news, and influencer fodder, with a smattering of TV networks and reputable news publishers, including ESPN, the Washington Post, and The Wall Street Journal,” a Vox report stated in June 2021. “Users complain that they’re frequently served tabloid-like content about influencers they don’t care about, or nonsensical clickbait that is best described as ‘internet garbage,'” it said.

This is likely the sort of content the senator’s team stumbled upon, too.

Of course, Snap has the right to curate whatever sort of publisher content it wants to — but there is a question here as to how well it’s actually curating and age-gating the content when it comes to its younger teenage users and whether that’s something that should be better regulated by law, not just left to the companies themselves to police.

“What kind of oversight do you conduct on this?,” Lee wanted to know about Snap’s curation of Discover.

“We use a variety of human review as well as automated review,” said Stout, adding that Snap would be interested in talking to his staff to find out what sort of content they saw that violated its guidelines.

(As an aside, reporters know well how this is often the response a big tech company will give when confronted with a direct example of something that contradicts their guidelines. They want the journalists to do the moderation work for them by itemizing the specific issues. They’ll then follow through and block or remove the violating pieces in question while downplaying the examples as a one-off they just missed by accident — instead of acknowledging this could be an example of a systemic problem. That’s not to say this is the situation with Snap, per se, but the tactic of acting shocked and surprised then demanding to know the examples is a familiar one.)

“While I would agree with you tastes vary when it comes to the kind of content that is promoted on Discover, there is no content there that is illegal. There is no content there that is hurtful,” Stout stated.

But Lee, whose questions mirror those from parents who believe age-inappropriate content is, in fact, hurtful, would likely disagree with that assessment.

“These app ratings are inappropriate,” Lee concluded. “We all know there’s content on Snapchat and on YouTube, among many other places, that’s not appropriate for children ages 12 or 13 and up.”

In a separate line of questing led by Senator Maria Cantwell (D-WA), Snap was also pressed on whether or not advertisers in Snapchat Discover understood what sort of content they’re being placed next to.

Stout said they did.

TikTok was not questioned by Lee on the nature of its content or app store age rating, but later in the hearing did note, in response to Senator Ben Ray Luján (D-NM), that its content for younger users was curated with help from Common Sense Networks and is “an age-appropriate experience.”",Social Media,TechCrunch,https://techcrunch.com/2021/10/26/youtube-and-snapchat-were-asked-to-defend-their-apps-age-ratings-in-senate-hearing/,"The inconsistency in app ratings can make it hard for parents to decide if an app is appropriate for their kids, and the issue of whether or not there is inappropriate content for younger teens on social media apps, such as Snapchat and YouTube, is a matter of much debate. The questions raised by lawmakers in this hearing highlighted the concerns of parents around",Social Norms & Relationships
495,Facebook’s Plan to Monopolize The Internet In India Should Be Defeated – TechCrunch,"Despite rebranding its free Internet.org ‘walled garden’ of apps plan in India under the new name of “Free Basics,” Facebook remains in direct violation of an open Internet. Facebook’s first attempt with Indian carrier Airtel was rolled-back soon after its release in April thanks to huge public outrage on social media, ironically.

India’s prolific tech entrepreneur Vishal Gondal has posted his thoughts and has rightfully called it evil.

Facebook thinks that by adopting a seemingly innocuous name such as “Free Basics” and launching a massive advertisement campaign all over India (including daily full-page ads in newspapers and countless billboards), it can advance its Internet-splitting plan in the days leading up to the final decision from the Telecom Regulatory Authority of India (TRAI).

Furthermore, Mark Zuckerberg has resorted to deceptive narratives to promote his efforts.

In this September 25th post, he presents how rural- and development friendly Internet.org and Free Basics are and suggesting the farmer couldn’t have benefited without their program.

However, the farmer in question has been on Facebook and the full Internet for at least 2 years before Internet.org was launched, where he could access a million times more information.

Here’s his page — active since 2013.

Facebook has never even advertised its own core product in India this heavily. Why would it spend this much cash on a “charity endeavor,” as they’re misleadingly framing it?

The primary mobile apps Facebook is pushing through “Free Basics” are their own, and all other apps, as innovative and helpful as they might be, would be subject to guidelines and an approval process created by Facebook.

In such a digitally emerging country like India, Facebook would essentially play gatekeeper, as millions of first-time Internet users on mobile would only experience a “Facebook version of the Internet.”

The competitive advantages for Facebook’s own apps and others it allows into “Free Basics” are so drastic that even Indian startups that Facebook has approached to bring onboard — and that would obviously benefit significantly from doing so — have declined in order to remain ethical and support a fair ecosystem, as Quartz has reported.

I’ve seen this “charity-with-hidden-agenda” model before in India, Africa, and other developing countries where politicians and international companies using non-profits as fronts to their commercial and political agenda to exploit the poor in the name of free goods and subsidies.

As Naveen Patnaik, Chief Minister of the Indian state Odisha, said: “If you dictate what the poor should get, you take away their rights to choose what they think is best for them.”

Let India’s poor choose what they want to use. Either open up “Free Basics” to every app and enforce a data cap, or just let the poor pay for the services themselves. Free market guru C.K. Prahalad showed the world in his seminal Gates-recommended book TheFortune at the Bottom of the Pyramid that the poor are viable customers.

My recent book, co-authored with Carol Realini, Financial Inclusion at theBottom of the Pyramid also contains dozens of examples of great innovations brought to the bottom of the pyramid at “radical affordability” without any need for preferential treatment given to one service over another.

And definitely not to the extent that one vendor dominates an entire region, thwarts innovation, and restricts consumer choice. Facebook is in critical need for its next one billion as its growth slows down in the United States, but stooping down to such desperate measures to gain market-share is unacceptable.

In Zuckerberg’s recent op-ed in Times of India, he asks, “who could possibly be against this?”

Nearly every Indian startup is against it including the very companies he is trying to recruit for “Free Basics,” dozens of Indian elected officials, the founder of the Internet Sir Tim Berners-Lee, and countless others in India and abroad in support of net neutrality have all voiced their concern against internet.org’s walled-garden.",Social Media,TechCrunch,https://techcrunch.com/2015/12/30/facebooks-plan-to-monopolize-the-internet-in-india-should-be-defeated/,"Facebook's attempts to rebrand its 'walled garden' Internet.org plan in India as ""Free Basics"" is in direct violation of open Internet and is an attempt to dominate the region, stifle innovation, and restrict consumer choice. This is unacceptable, and has been met with widespread criticism from Indian startups, elected officials, and Sir","Information, Discourse & Governance"
496,Everyone Loses When Your Employer Owns Your Facebook Account,"Listen up, America. You too, snowflakes. After being ""banned permanently"" from conservative media site The Blaze, Tomi Lahren could lose the very thing that elevated her from small-time PC-culture critic to an omnipresent conservative commentator: her 4.2 million Facebook fans. See, The Blaze owns Lahren's verified Facebook page. She's negotiating to keep the page and its followers as part of her severance package, but she also seems likely to lose. And while you might not agree with Lahren's politics—she was lambasted by critics for calling the Black Lives Matter movement ""the new KKK,"" among other things—this is actually a terrible, horrible, no-good, very bad thing.

That's because the phenomenon isn't particular to Lahren. Several former BuzzFeed employees have left the company, only for their BuzzFeed-associated fan pages, complete with their names and faces, to remain active for years. While BuzzFeed doesn't actively post to the those pages, beyond changing the cover photo, it has the legal right to do so—as The Blaze would with Lahren's page, which has been frozen since her initial suspension for expressing pro-choice sentiments on March 20. (Neither company responded to requests for comment.) Claiming you're ""Authentic. Unfiltered. Fearless,"" as The Blaze does, gets a lot more difficult when you own and operate a social media sock puppet.

From an employment-law standpoint, maintaining a Facebook page in an employee's name is fair game, as long as such a stipulation is in a person's contract. ""If a mall says 'no skateboarding,' you and I can't sign a contract saying you'll skateboard there. That's an invalid contract,"" says Nancy Kim, who teaches contract and technology law at California Western School of Law. ""But if this doesn't break Facebook's rules, it's enforceable."" And even though Facebook's community standards don't allow fake or misleading accounts, some wiggle room exists for public figures. According to a Facebook spokesperson familiar with the situation, if Lahren or a BuzzFeed contributor designated their employer as an authorized representative, then those public accounts are totally kosher—and that authorization doesn't necessarily end when your term of employment does. So nobody's breaking the law here.

Still, that loophole in Facebook's rules is ostensibly meant for busy celebrities with dedicated social media teams, not companies trying to glom on to a platform that a millennial internet-celeb worked hard to cultivate. And things get even more confusing thanks to the way BuzzFeed names its employee-affiliated Facebook pages. The ""BuzzFeed Gaby"" account, for example, inextricably links Gaby Dunn to the company and creates the misleading impression that she still works at BuzzFeed, despite having left the company for YouTube. (BuzzFeed has given the BuzzFeed Gaby fan page a ""BuzzFeed Alumni"" label__ __to signal she no longer works there, but the account still lives on with 120,000 followers. dwarfing Dunn's non-Buzzfeed Facebook page and its 5,000 followers.)

And if you ask a former BuzzFeed employee, that's what they're most concerned about. ""I didn't want people to stumble upon it after I left. That happened to Matt Bellassai,"" says YouTuber Safiya Nygaard, who left her job as a BuzzFeed video producer in January. ""People still comment on that Facebook page asking when he's going to start posting his series again."" So Nygaard asked BuzzFeed to disable her ""BuzzFeed Safiya"" account before she tendered her resignation, and the company made it private.",Social Media,WIRED,https://www.wired.com/2017/04/tomi-lahren-the-blaze-sock-puppet/,"Even after employees leave their employer, they can still lose control of their social media accounts, which have been linked to their former employer, leaving them in a vulnerable situation.",User Experience & Entertainment
497,Here’s Twitter’s position on Alex Jones (and hate-peddling anti-truthers) — hint: It’s a fudge – TechCrunch,"The number of tech platforms taking action against Alex Jones, the far right InfoWars conspiracy theorist and hate speech preacher, has been rising in recent weeks — with bans or partial bans including from Google, Apple and Facebook.

However, as we noted earlier, Twitter is not among them. Although it has banned known hate peddlers before.

Jones continues to be allowed a presence on Twitter’s platform — and is using his verified Twitter account to scream about being censored all over the mainstream place, hyperventilating at one point in the past 16 hours that ‘censoring Alex Jones is censoring everyone’ — because, and I quote, “we’re all Alex Jones now”.

(Fact check: No, we’re not… And, Alex, if you’re reading this, we suggest you take heart from the ideas in this Onion article and find a spot in your local park.)

We asked Twitter why it has not banned Jones outright, given that its own rules service proscribe hate speech and hateful conduct…

Abuse: You may not engage in the targeted harassment of someone, or incite other people to do so. We consider abusive behavior an attempt to harass, intimidate, or silence someone else’s voice. Hateful conduct: You may not promote violence against, threaten, or harass other people on the basis of race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, disability, or serious disease. Read more about our hateful conduct policy.

Add to that, CEO Jack Dorsey has made it his high profile mission of late to (try to) improve conversational health on the platform. So it seems fair to wonder how Twitter continuing to enable a peddler of toxic lies and hate is going to achieve that?

While Twitter would not provide a statement about Jones’ continued presence on its platform, a spokesman told us that InfoWars and Jones’ personal account are not in violation of Twitter (or Periscope’s) ToS. At least not yet. Though he pointed out it could of course take action in the future — i.e. if it’s made aware of particular tweets that violate its rules.

Twitter’s position therefore appears to be that the content posted by InfoWars to other social media platforms is different to the content Jones posts to Twitter itself — ergo, its (hedgy & fudgy) argument essentially boils down to saying Jones is walking a fine enough line on Twitter itself to avoid a ban, because he hasn’t literally tweeted content that violates the letter of Twitter’s ToS.

(Though he has tweeted stuff like “the censorship of Infowars just vindicates everything we’ve been saying” — and given the hate-filled, violently untruthful things he has been saying all over the Internet, he’s essentially re-packaged all those lies into that single tweet, so… )

To spell out Twitter’s fudge: The fact of Jones being a known conspiracy theorist and widely visible hate preacher is not being factored into its ToS enforcement decisions. (Which does appear to contradict one of Twitter’s own policy shifts, announced last year, to take into account off-platform behavior, as others have pointed out.)

The company says it’s judging the man by his output on Twitter — which means it’s failing to take into account the wider context around Jones’ tweets, i.e. all the lies and hate he peddles elsewhere (and indeed all the insinuating nods and dog whistles he makes to his followers on Twitter) — and by doing so it is in fact enabling the continued spread of hate via the wink-wink-nod-nod back door.

Twitter’s spokesman did not want to engage in a lengthy back and forth conversation, healthy or otherwise, about Jones/InfoWars so it was not possible to get a response from the company on that point.

However it does argue, i.e. in defense of its fudged position, that keeping purveyors of false news on its platform allows for an open, real-time debate which in turn allows for their lies to be challenged and debunked by people who are in their right minds — so, basically, this is the ‘fight bad speech with more speech argument’ that’s so beloved of people already enjoying powerful privilege.

The problem with that argument (actually, there are many) is it does not factor in the human cost; the people suffering directly because toxic lies impact their lives. Nor the cost to truth itself; To belief in the veracity and authenticity of credible sources of information which are under sustained and vicious attack by anti-truthers like Jones; The corrosive impact on professional journalism from lies being packaged and peddled under the lying banner of self-styled ‘truth journalism’ that Jones misappropriates. Nor the cost to society from hate speech whose very purpose is to rip up the social fabric and take down civic values — and, in the case of Jones’ particular bilious flavor, to further bang the drum of abuse via the medium of toxic disinformation — to further amplify and spread his pollution, via the power of untruth — to whip up masses of non-critically thinking conspiracy-prone followers. I could go on. (I have here.)

The amplification effect of social media platforms — combined with cynical tricks used by hate peddlers to game algorithms, such as bots retweeting and liking content to make it seem more popular than it is — makes this stuff a major, major problem.

‘Bad speech’ on such powerful platforms can become not just something to roll your eyes at and laughingly dismiss, but a toxic force that bullies, beats down and drowns out other types of speech — perhaps most especially truthful speech, because falsehood flies (and online it’s got rocket fuel) — and so can have a very deleterious impact on conversational health.

Really, it needs to be handled in a very different way. Which means Twitter’s position on Jones, and hateful anti-truthers in general, looks both flawed and weak.

It’s also now looking increasingly isolated, as other tech platforms are taking action.

Twitter’s spokesman also implied the company is working on tuning its systems to actively surface high quality counter-narratives and rebuttals to toxic BS — such as in replies to known purveyors of fake news like InfoWars.

But while such work is to be applauded, working on a fix also means you don’t actually have a fix yet. Meanwhile the lies you’re not stopping are spreading on your platform — at horrible and high cost to people and society.

It’s hard to see this as a defensible position.

And while Twitter keeps sitting on its fence, Jones’ hate speech and toxic lies, broadcast to millions as a weapon of violent disinformation, have got his video show booted from YouTube (which, after first issuing a strike yesterday then terminated his page for “violating YouTube’s Community Guidelines”).

The platform had removed ads from his channel back in March — but had not then (as Jones falsely claimed at the time) banned it. That decision took another almost half year for YouTube to arrive at.

Also yesterday, almost all of Jones’ podcasts were pulled by Apple, with the company saying it does not tolerate hate speech. “We believe in representing a wide range of views, so long as people are respectful to those with differing opinions,” it added.

Earlier this month, music streaming service Spotify also removed some of Jones’ podcasts for violating its hate-speech policy.

Even Facebook removed a bunch of Jones’ videos late last month, for violating its community standards — albeit after some dithering, and what looked like a lot of internal confusion.

The social media behemoth also imposed a 30-day ban on Jones’ personal account for posting the videos, and served him a warning notice for the InfoWars Facebook Page he controls.

Facebook later clarified it had banned Jones’ personal profile because he had previously received a warning — whereas the InfoWars Page had not, hence the latter only getting a strike.

There have even been bans from some unlikely quarters: YouPorn just announced action against Jones for a ToS violation — nixing his ability to try to pass off anti-truth hate preaching as a porn alternative on its platform.

Pinterest, too, removed Jones’ ‘hate, lies & supplements’ page after Mashable made enquiries.

So, uh, other responses than Twitter’s (of doing nothing) are widely possible.

On Twitter, Jones also benefits from being able to distinguish his account from any would-be imitators or satirists, because he has a verified account — denoted on the platform by a blue check mark badge.

We asked Twitter why it hasn’t removed Jones’ blue badge — given that the company has, until relatively recently, been rethinking its verification program. And last year it actively removed blue badges from a number of white supremacists because it was worried it looked like it had been endorsing them. Yet Jones — who spins the gigantic lie of ‘white genocide’ — continues to keep his.

Twitter’s spokesman pointed us to this tweet last month from product lead, Kayvon Beykpour, who wrote that updating the program “isn’t a top priority for us right now”.

We've heard some questions recently about the status of Verification on Twitter, so wanted to address directly. Updating our verification program isn’t a top priority for us right now (election integrity is). Here’s some history & context, and how we plan to put it on our roadmap — Kayvon Beykpour (@kayvz) July 17, 2018

Beykpour went on to explain that while Twitter had “paused” public verification last November (because “we wanted to address the issue that verifying the authenticity of an account was being conflated with endorsement”), it subsequently paused its own ‘pause for thought’ on having verified some very toxic individuals, with Beykpour writing in an email to staff in July:

Though the current state of Verification is definitely not ideal (opaque criteria and process, inconsistency in our procedures, external frustration from customers), I don’t believe we have the bandwidth to address this holistically (policy, process, product, and a plan around how & when these fit together) without coming at the cost of our other priorities and distracting the team.

At the same time Beykpour admits in the thread that Twitter has been ‘unpausing’ its pause on verification in some circumstances (“we still verify accounts ad hoc when we think it serves the public conversation & is in line with our policy”); but not, evidently, going so far as to unpause its pause on removing badges from hateful people who gain unjustified authenticity and authority from the perceived endorsement of Twitter verification — such as in ‘ad hoc’ situations where doing so might be terribly, terribly appropriate. Like, uh, this one.

Beykpour wrote that verification would be addressed by Twitter post-election. So it’s presumably sticking to its lack of having a policy at all right now, for now. (“I know this isn’t the most satisfying news, but I wanted to be transparent about our priorities,” he concluded.)

Twitter’s spokesman told us it doesn’t have anything further to share on verification at this point.

Jones’ toxic activity on social media has included spreading the horrendous lie that children who died in the Sandy Hook U.S. school shooting were ‘crisis actors’.

So, for now, a man who lies about the violent death of little children continues to be privileged with a badge on his not-at-all-banned Twitter account.

Two of the parents of a child who died at the school wrote an open letter to Facebook’s founder, Mark Zuckerberg, last month, describing how toxic lies about the school shooting spread via social media had metastasized into violent hate and threats directed at them.

“Our families are in danger as a direct result of the hundreds of thousands of people who see and believe the lies and hate speech, which you have decided should be protected,” wrote Lenny Pozner and Veronique De La Rosa, the parents of Noah, who died on 14 December, 2012, at the age of six.

“What makes the entire situation all the more horrific is that we have had to wage an almost inconceivable battle with Facebook to provide us with the most basic of protections to remove the most offensive and incendiary content.”",Social Media,TechCrunch,https://techcrunch.com/2018/08/07/heres-twitters-position-on-alex-jones-and-hate-peddling-anti-truthers-hint-its-a-fudge/,"The amplification effect of social media platforms, combined with cynical tricks used by hate peddlers to game algorithms, such as bots retweeting and liking content to make it seem more popular than it is, has enabled a major problem of toxic lies and hate speech being spread on these powerful platforms with horrible and high cost to people and society. This is",Equality & Justice
498,"Instagram tackles self-harm and suicide with new reporting tools, support options – TechCrunch","While Twitter continues to struggle with rampant bullying and abuse, Instagram is instead stepping up its game when it comes to keeping users safe from harm. The Facebook-owned photo-sharing network this week began rolling out a new reporting tool that lets its users anonymously flag friends’ posts about self-harm. This, in turn, will prompt a message from Instagram to the user in question, offering support, including tips as well as access to a help line, all of which can be accessed directly in the app itself.

The system itself is fairly simple to use, but addresses a serious need on the social network which continues to have a large teenage and young adult audience.

When you see a post where a friend is threatening self-harm or suicide, you may not always feel comfortable broaching the subject with them. Or you may not know what to say. In other cases, you may be following accounts of people you don’t know that well (or at all), and don’t feel it’s your place to speak out.

Instagram is instead offering a different option. By anonymously flagging the post, the friend will be sent a support message that reads, “Someone saw one of your posts and thinks you might be going through a difficult time. If you need support, we’d like to help.”

The recipient can then click through to see a list of support options, which includes a suggestion to message or call a friend, access more general tips and support or contact a help line — which will vary as determined by the user’s location. 40 organizations around the world are partners on the help line side of the system.

The company also says it worked with National Eating Disorders Association, Dr. Nancy Zucker (Associate Professor of Psychology and Neuroscience at Duke), and Forefront, which is led by academic researchers at the University of Washington, on the new tools. Other organizations including The National Suicide Prevention Lifeline and Save.org (US), Samaritans (UK), beyond blue and headspace (Australia) provided input, too.

What’s interesting about Instagram’s tool is that it isn’t only triggered by anonymous reporting. Instagram’s app will also direct users to the support message when they search the service for certain hashtags, like the banned search term #thinspo, for example, which is associated with eating disorders.

Dealing with abuse (and self-directed abuse)

The move is one of several changes Instagram has made in recent days to limit the abuses on its network. In September, it also made it possible for anyone to filter their comments using customizable blocklists — meaning, they could disallow anyone from posting certain explicit words or bullying terms in their Instagram comments.

Steps like these are critical to establishing the community’s tone. Unregulated free speech can devolve into anonymous bullying — as is now so prevalent on Twitter that it has at least partially impacted the network’s ability to find an acquirer. (Reportedly, Disney stepped away from an acquisition because of this problem.)

Posts about self-harm are a different matter than bullying, of course, but they fall under the broader umbrella of user protection and safety.

They are about establishing a community where it’s safe to share, but also one where certain types of sharing can be moderated for potential problems — whether that’s someone posting harmful comments directed at another person, or someone posting harmful comments directed at themselves.

Not having these sorts of limitations and policies in place can lead to dangerous results, especially when networks cater to a younger audience. For instance, the teen Q&A network became so well-known for bullying back in the day, it ended up being a contributing factor in a series of teen suicides.

Over the years, the major social media companies have learned from past tragedies to offer better ways to protect their users. Many partner with support organizations to craft and run PSAs when users search for harmful terms — Tumblr, Pinterest and Instagram have all done so. And most today use a combination of automated systems, human moderators and flagging tools to address problems that range from user-led searches for concerning terms and tags (like thinspo or suicide) to prompts that are triggered by users’ posts themselves.

Instagram’s new flagging tools are designed after those that are already in place on parent company Facebook, and an example of how the smaller network can benefit from the infrastructure Facebook already has in place to address problems related to self-harm.

Last year, Facebook launched an almost identical tool — right down to the wording — for Facebook users in the U.S. And earlier in 2016, it rolled this out to international users as well.

Along with the launch of the new tools, Instagram also partnered with Seventeen on a campaign focused on body image and self-confidence, National Body Confidence Day. This is running now under the hashtag, #PerfectlyMe. The November issue of Seventeen will also include 16 pages in the magazine in support of body confidence and #PerfectlyMe.",Social Media,TechCrunch,https://techcrunch.com/2016/10/19/instagram-tackles-self-harm-and-suicide-with-new-reporting-tools-support-options/,"The main undesirable consequence of social media discussed here is the prevalence of bullying and abuse, which can lead to serious mental and emotional harm for users, especially those in younger demographics. Instagram is taking steps to address this by introducing a new anonymous flagging tool that allows users to alert the network to posts about self-harm, as well as partnering",Social Norms & Relationships
499,How to Check If Your Facebook Account Got Hacked—and How Badly,"At the end of last month, Facebook made a bombshell disclosure: As many as 90 million of its users may have had their so-called access tokens—which keep you logged into your account, so you don't have to sign in every time—stolen by hackers. Friday, the company put the actual number at 30 million. Here's how to see if you were one of them, and if so, what the hackers got from your account.

There might understandably be some confusion around the matter; a few weeks ago, Facebook logged out 90 million of its users out of an abundance of caution, making them reset their passwords and negating the access token hack. Over the next few days, Facebook will insert a customized message into the News Feeds of the 30 million people whose accounts were actually impacted, based on the extent of the damage.

""People’s accounts have already been secured by the action we took two weeks ago to reset the access tokens for people who were potentially exposed—no one needs to log out again, and no one needs to change their password,"" says Guy Rosen, Facebook's vice president of product management. ""We’ll be explaining what information the attackers may have accessed as well as steps they can take to help protect themselves from any suspicious emails or text messages or calls that could potentially result from this kind of information being exposed. ""

If you don't want to wait for the message to hit your News Feed to find out if you're okay, go ahead and see if you were among those hit at this page. Scroll past the background paragraph, and you’ll see a header that reads Is my Facebook account impacted by this security issue?

From there, you’ll see one of three outcomes. If it says that based on what Facebook knows so far, you’re not impacted, you should be in the clear pending any revelations. The company says that one million of the 30 million people who had their access tokens stolen didn’t have any of their data comprised.

The remaining 29 million users will see one of two messages, depending on the extent of the damage. Fifteen million of them had their name, email addresses, and phone number accessed by hackers. While that’s not ideal by any accounting, the remaining 14 million Facebook users are left with a much worse result.

In addition to the basic contact information above, the list of details hackers accessed is long: username, date of birth, gender, devices you used Facebook on, and your language settings, at the very least. If you filled out the relationship status, religion, hometown, current city, work, education, or website sections of your profile, they got that too. And most unsettling of all, they could have accessed the 10 most recent locations you checked into or were tagged in, and the 15 most recent searches you’ve entered into the Facebook search bar.

""No one needs to log out again, and no one needs to change their password."" Guy Rosen, Facebook

Facebook says they’ve seen no signs yet that attackers used its access tokens to infiltrate third-party apps and services, as was technically possible. And it maintains that no account passwords or credit card information was compromised. But the amount of information, and its sensitive nature, should be a boon to phishers and scammers for years to come. You can change your password or cancel a credit card. Your hometown will always be just that. And where you’ve been and whom you’ve searched for are deeply personal parts of your life, both online and in the real world.

Facebook at least acknowledges this in its support page, offering some advice about how to avoid phishing attempts, like being “cautious of unwanted phone calls, text messages or emails from people you don't know.” Presumably, you were doing this anyway. The rest of the advice is similarly rudimentary, but that’s in part because there’s only so much you can do to stop that kind of attack. If a determined phisher wants to get you, they almost certainly will eventually. Especially if they have access to the kind of data that Facebook’s security fail has given away.

More Great WIRED Stories",Social Media,WIRED,https://www.wired.com/story/facebook-hack-check-if-account-affected/,"The Facebook access token hack has exposed up to 30 million users to potential phishing and scam attempts, as hackers have gained access to personal information such as names, email addresses, phone numbers, relationship statuses, religious affiliations, hometowns, current cities, workplaces, educational backgrounds, website URLs and recent locations visited, as well as searched",Security & Privacy
500,Banning digital political ads gives extremists a distinct advantage – TechCrunch,"Jack Dorsey’s announcement that Twitter will no longer run political ads because “political messages reach should be earned, not bought” has been welcomed as a thoughtful and statesmanlike contrast to Mark Zuckerberg’s and Facebook’s greedy acceptance of “political ads that lie.” While the 240-character policy sounds compelling, it’s both flawed in principle and, I fear, counterproductive in practice.

First: like it or hate it, the U.S. political system is drowning in money. In 2018, a non-presidential year, it is estimated that over $9B was spent on the U.S. elections. And unless laws change, more will continue to flow. Banning digital ads will not reduce the amount of money in politics, and will simply shift it to less transparent channels. In an ideal world, it would be great if all “political messages were earned and not bought,” but that is not how our system works. Candidates, Super PACs, C4s and others already allow the majority of their budgets to be swallowed up by other, less visible, accountable and cost-effective, channels — including television, mail, telephone, and radio.

More likely, at least some of the money will end up with even less transparent organizations that aren’t deemed “political,” but very much are.

Second, banning digital political ads will not only hurt the very candidates people should want to help, it will also damage our democratic process. Analog mediums are significantly more expensive and inefficient than digital ones, so candidates who have a lot of money and/or have spent time cultivating their followings will continue to dominate. In other words, incumbent candidates, rich people and reality TV stars enjoy an outsized advantage when digital advertising is denied.

A recent Stanford study found that, at the state house level, more than 10 times as many candidates advertise on Facebook than advertise on television. The research found that digital ads lowers advertising costs, which expands the set of candidates for whom advertising — and thus the potential to reach voters and seriously contest an election — is a real possibility.

Lesser well-known, but often highly-qualified candidates at the state, local and federal level are precisely the people who have been celebrated for their new perspectives, creative ideas and commitment to shake up the system. People who put their heads down, do good work in their communities and decide to run because they want to make a difference will be the ones that are disadvantaged.

You know who gets plenty of earned media opportunities? Donald Trump. He will be fine. In fact, he will be better than fine because we’ve just handed him and more extremist candidates like him a distinct advantage.

Democracy is about the combination of free speech and transparency. As the old adage goes, sunlight is the best disinfectant, so here are a few ideas that would be more effective than a ban:

Adding a “nutrition label” to political ads offers a more accessible, understandable and consistent way to identify the identities of the funder, their location, their budget and their target audience. This should be easily accessed, in any political ad via one click, just like we know where to find nutrition information on food we buy.

Enhance “consumer beware” acknowledgments so that if digital political ads remain exempt from fact-checking (as they mostly are on television), platforms have a duty to make that clear with visual signals and user education.

Ultimately, decisions about what is permissible political speech and appropriate distribution and targeting is too important to be left to technology platforms and their conceptions of the public interest.

Do we want Google, Facebook and Twitter making the rules for all political ads and being responsible for enforcing them? What we need is a true oversight body — one with teeth. If non-political advertisers make false claims about their own products or those of their competitors, they can be fined by the FTC. This is an acknowledgment, not only that consumers need accurate facts, but also that companies can not police themselves. This is far too much power for them.

This isn’t a way to let technology companies off the hook, as there is plenty more they can do as noted above. But we need a truly independent organization overseeing political ads — the rules that govern them and holding organizations accountable to following those rules. Is this the FEC? I’m not sure.

As I write this today, I worry that no agency truly has the capacity or the expertise to create these rules and challenge bad campaign practices. We should remedy this post-haste and get to finding true solutions. The alternative seems easier and even principled to fight for, but the unintended consequences will be swift — a government full of the types of people who we say we don’t want.",Social Media,TechCrunch,https://techcrunch.com/2019/11/08/banning-digital-political-ads-gives-extremists-a-distinct-advantage/,"Twitter's ban on political ads will not reduce money in politics, but instead shift it to less transparent channels, potentially disadvantaging lesser-known candidates and giving an outsized advantage to incumbents, wealthy people, and reality TV stars. Furthermore, without strong oversight, technology companies will be left to create and enforce rules for political ads, giving",Politics
501,Parler jumps to No. 1 on App Store after Facebook and Twitter ban Trump – TechCrunch,"Users are surging on small, conservative, social media platforms after President Donald Trump’s ban from the world’s largest social networks, even as those platforms are seeing access throttled by the app marketplaces of tech’s biggest players.

The social network, Parler, a network that mimics Twitter, is now the number one app in Apple’s app store and Gab, another conservative-backed service, claimed that it was seeing an explosion in the number of signups to its web-based platform as well.

Parler saw approximately 210,000 installs globally on Friday 1/8, up 281% from approximately 55,000 on 1/7, according to data from the analytics service Sensor Tower . “In the U.S., the app saw approximately 182,000 first-time downloads on 1/8, up 355% from about 40,000 installs on 1/7. Since Wednesday, the app has seen approximately 268,000 installs from across U.S. app stores,” a press rep from Sensor Tower wrote in an email.

Parler’s ballooning user base comes at a potentially perilous time for the company. It has already been removed from Google’s Play store and Apple is considering suspending the social media app as well if it does not add some content moderation features.

Both Parler and Gab have billed themselves as havens for free speech, with what’s perhaps the most lax content moderation online. In the past the two companies have left up content posted by an alleged Russian disinformation campaign, and allow users to traffic in conspiracy theories that other social media platforms have shut down.

The expectation with these services is that users on the platforms are in charge of muting and blocking trolls or offensive content, but, by their nature, those who join these platforms will generally find themselves among like-minded users.

Their user counts might be surging, but would-be adopters may soon have a hard time finding the services.

On Friday night, Google said that it would be removing Parler from their Play Store immediately — suspending the app until the developers committed to a moderation and enforcement policy that could handle objectionable content on the platform.

In a statement to TechCrunch, a Google spokesperson said:

“In order to protect user safety on Google Play, our longstanding policies require that apps displaying user-generated content have moderation policies and enforcement that removes egregious content like posts that incite violence. All developers agree to these terms and we have reminded Parler of this clear policy in recent months. We’re aware of continued posting in the Parler app that seeks to incite ongoing violence in the US. We recognize that there can be reasonable debate about content policies and that it can be difficult for apps to immediately remove all violative content, but for us to distribute an app through Google Play, we do require that apps implement robust moderation for egregious content. In light of this ongoing and urgent public safety threat, we are suspending the app’s listings from the Play Store until it addresses these issues.“

On Friday, Buzzfeed News reported that Parler had received a letter from Apple informing them that the app would be removed from the App Store within 24 hours unless the company submitted an update with a moderation improvement plan. Parler CEO John Matze confirmed the action from Apple in a post on his Parler account where he posted a screenshot of the notification from Apple.

“We want to be clear that Parler is in fact responsible for all the user generated content present on your service and for ensuring that this content meets App Store requirements for the safety and protection of our users,” text from the screenshot reads. “We won’t distribute apps that present dangerous and harmful content.

Parler is backed by the conservative billionaire heiress Rebekah Mercer, according to a November report in The Wall Street Journal. Founded in 2018, the service has experienced spikes in user adoption with every clash between more social media companies and the outgoing President Trump. In November, Parler boasted some 10 million users, according to the Journal.

Users like Fox Business anchor Maria Bartiromo and the conservative talk show host Dan Bongino, a wildly popular figure on Facebook who is also an investor in Parler, have joined the platform. In the Journal article Bongino called the company “a collective middle finger to the tech tyrants.”

It’s worth noting that Parler and Gab aren’t the only companies to see users numbers soar after the Trump bans. MeWe Network, OANN, Newsmax and Rumble have also seen adoption soar, according to data from the analytics company Apptopia.

The company noted that Parler was the #1 app on the iOS app store for two days surging from 18th on Thursday and 592 on Wednesday. Overall, the app was the 10th most downloaded social media app in 2020 with 8.1 million new installs.

“It is an event driven app though,” a company analyst noted. “After events like the election, BLM protests, Twitter first applying labels to Trump’s Tweets, we see bursts of downloads and usage but it will then drop off.”

Sarah Perez and Lucas Matney contributed additional reporting to this article.",Social Media,TechCrunch,https://techcrunch.com/2021/01/09/parler-jumps-to-no-1-on-app-store-after-facebook-and-twitter-bans/,"The surge in users on conservative-backed small social media platforms like Parler and Gab, due to President Trump's ban from larger social networks, has led to Google and Apple potentially removing the apps from their app marketplaces due to their lax content moderation policies that permit users to post dangerous and harmful content.",Security & Privacy
502,The Condom Snorting Challenge Is Tide Pods' Final Revenge,"Hello, fellow teens! Sorry to interrupt your latest obsession. According to various news outlets, you're all busy setting up webcams, then holding a flaccid condom to your nostril and inhaling until it jellyfishes out of your nasopharynx and into the back of your throat like a latex loogie from hell. You've just completed the Condom Snorting Challenge, along with untold thousands of other teens risking asphyxiation for likes.

Sorry, is that not what you're doing? Is it far more likely that you and the rest of your frivolous, moral-panic inducing compatriots don't actually exist? Do accept our apologies.

While the Condom Snorting Challenge is many things, it is most certainly not a trend. Unlike January's Tide Pod Challenge, there was no critical mass of teenagers talking about flossing their nostrils with contraceptives—but that didn't stop people from claiming otherwise. In this case, the trailer of internet breadcrumbs ends in San Antonio, Texas. But the true culprits here are threefold: Tide Pods, the internet's inability to not watch disgusting things, and some media outlets' endless hunt for relevance.

If condom-snorting ever qualified as a trend, it was probably in 2013. (Even that had predecessors: videos dating to May 2012; internet lore dating back to 2006; and a university newspaper article describing the act way back in 1993.) Then, it was called the Condom Challenge, before the (realer, bigger) trend of dropping a condom full of water on your head usurped that name. A young woman named Savannah Strong seems to have sparked the blaze almost exactly five years ago, with a (since deleted) YouTube video.

So why is this happening now? One answer is that in late March, a group of parents from the San Antonio area attended a workshop called “Dares, Drugs, and Dangerous Teen Trends,” in which state education specialist Stephen Enriquez told them that—in a hypothetical worst-case scenario—their teens could possibly start huffing rubbers as other teenagers have in the past. And as sometimes happens, this particular local news stories became inflated via the infinite bellows of social media and meta-aggregation.

Some of the resulting coverage managed to be useful and well-intentioned, like Dr. Bruce Y Lee’s piece for Forbes. “Regardless of how popular the challenge may actually be, it is out there for people to see and imitate,” Lee wrote. “People may not understand the relevant anatomy and the implications of disrupting parts of the body. Even though something looks like an opening or a tube doesn’t mean that it is just an opening or a tube.”

However, the medical warning was completely unnecessary. If you dig back into the coverage, you eventually realize that there's no actual trend underneath the stories—it's just trend pieces all the way down. As The Washington Post reported earlier this week, the organization Enriquez works for has been using the same curriculum for years. Even the original news report, by San Antonio Fox affiliate KABB, never calls it a trend. Yet, publications from Newsweek to Fox to Yahoo called it one, pretty much just because they could find evidence of multiple teenagers snorting condoms. Nor did it stop with US media: America’s Condom Snorting Challenge panic, we regret to inform you, is actually international news.) It was so perfect a trend the internet practically willed it into being.",Social Media,WIRED,https://www.wired.com/story/condom-snorting-challenge/,"The Condom Snorting Challenge is not actually a trend, but due to the internet's inability to not watch disgusting things, and some media outlets' endless hunt for relevance, it has been grossly exaggerated and turned into a moral panic.",Social Norms & Relationships
503,Our Minds Have Been Hijacked by Our Phones. Tristan Harris Wants to Rescue Them,"Sometimes our smart phones are our friends, sometimes they seem like our lovers, and sometimes they’re our dope dealers. And no one, in the past 12 months at least, has done more than Tristan Harris to explain the complexity of this relationship. Harris is a former product manager at Google who has gone viral repeatedly by critiquing the way that the big platforms—Apple, Facebook, Google, YouTube, Snapchat, Twitter, Instagram—suck us into their products and take time that, in retrospect, we may wish we did not give. He’s also launched a nonprofit called Time Well Spent, which is devoted to stopping “tech companies from hijacking our minds.” Today, the TED talk he gave last April was released online. In it, he proposes a renaissance in online design that can free us from being controlled and manipulated by apps, websites, advertisers, and notifications. Harris expanded on those ideas in a conversation with WIRED editor in chief Nicholas Thompson. The conversation has been edited for clarity and concision.

Nicholas Thompson: You’ve been making the argument that big internet platforms influence us in ways we don’t understand. How has that idea taken off?

Tristan Harris: It started with 60 Minutes and its piece reviewing the ways the tech industry uses design techniques to keep people hooked to the screen for as long and as frequently as possible. Not because they’re evil but because of this arms race for attention. And that led to an interview on the Sam Harris podcast about all the different ways technology is persuading millions of people in ways they don’t see. And that went viral through Silicon Valley. I think several million people listened to it. So this conversation about how technology is hijacking people is really catching on.

NT: What's the scale of the problem?

TH: Technology steers what 2 billion people are thinking and believing every day. It’s possibly the largest source of influence over 2 billion people’s thoughts that has ever been created. Religions and governments don’t have that much influence over people’s daily thoughts. But we have three technology companies who have this system that frankly they don’t even have control over—with newsfeeds and recommended videos and whatever they put in front of you—which is governing what people do with their time and what they’re looking at.

And when you say “three companies” you mean?

If we’re talking about just your phone, then we’re talking about Apple and Google because they design the operating systems, the phone itself, and the software in the phone. And if we’re talking about where people spend their time on the phone, then we’re talking about Facebook, YouTube, Snapchat and Instagram because that’s where people spend their time.

So you’ve started this big conversation. What's next?

Well, the TED talk I gave in April was only heard by conference attendees, but now it's available online. It basically suggests three radical changes that we need to make to technology. But before understanding what those changes are, we have to understand the problem. Just to reiterate, the problem is the hijacking of the human mind: systems that are better and better at steering what people are paying attention to, and better and better at steering what people do with their time than ever before. These are things like “Snapchat streaks,” which is hooking kids to send messages back and forth with every single one of their contacts every day. These are things like autoplay, which causes people to spend more time on YouTube or on Netflix. These are things like social awareness cues, which by showing you how recently someone has been online or knowing that someone saw your profile, keep people in a panopticon.",Social Media,WIRED,https://www.wired.com/story/our-minds-have-been-hijacked-by-our-phones-tristan-harris-wants-to-rescue-them/,"Social media has created a system that hijacks the human mind and steers our attention, encouraging us to spend more time on platforms than we would like, and to be constantly aware of the actions of others. This can lead to a panopticon-like state of surveillance that can be detrimental to our mental health and social well-being",User Experience & Entertainment
504,Trump's Campaign Can't Just Erase History on the Internet,"President Trump's overhauled campaign website looks a lot like the original: the resident in a suit and red tie, embedded tweets pillorying #FakeNews, and “Make America Great Again” hats for sale in every color (plus camo, of course). But what really stands out is what's missing: the entire archive of content published on the site prior to January.

The Trump campaign didn't respond to WIRED's request for comment, but it's hard not to see the move as a ham-fisted way of obscuring the president's most controversial campaign promises, particularly his vow in a press release to ban Muslims from entering the country. But the web doesn't work that way. Even when you're president, the internet never forgets.

The purge began Monday, after one White House reporter asked press secretary Sean Spicer why the campaign website still included references to the Muslim ban. That same day, during oral arguments in the federal appeals case over the Trump administration's executive order barring travelers from six Muslim-majority countries, Fourth Circuit Court of Appeals Judge Robert King also pressed Justice Department lawyer Jeffrey Wall about the site. Wall argued that the current ban doesn't discriminate against people on religious grounds, but King insisted the press release contradicts that claim. ""He has never repudiated what he said about the Muslim ban,"" Judge King said of the president. ""It is still on his website.""

Within hours it was gone. Within a day, so was every other pesky press release that might someday prove incriminating. Except that a quick scan of the Internet Archive's Wayback Machine still surfaces the release in its entirety. Google ""Trump Muslim ban press release"" (don't include the quotation marks), and you'll still get a link to the URL as well as a cached version of the press release visible in its entirely. That's to say nothing of the innumerable screenshots of the site currently cluttering the cloud and countless hard drives around the world, nor this tweet, which is still live:

For his supporters, Trump's biggest asset as a candidate was always his willingness to ""tell it like it is""---to speak first, worry about the consequences later. But as a president forced to answer for those statements in court, Trump is finding that there's no scrubbing the internet's long, limitless memory. That's particularly challenging for Trump, the first president in history to use social media to provide a steady and seemingly unfiltered stream of declarations. He simply has a lot more public statements to answer for.

But attempting to hide any of it won't do the president any good, and not only because anyone with a Wi-FI connection can easily dig it back up again. Thanks to the Streisand effect, attempting to hide just about anything in the digital age has a habit of bringing even more attention to whatever it is you're trying to hide. The internet doesn't just remember. It doesn't let you forget.",Social Media,WIRED,https://www.wired.com/2017/05/trumps-campaign-cant-just-erase-history-internet/,"Attempting to hide any statement made on social media in the digital age will only bring more attention to it, due to the Streisand effect. This is particularly challenging for President Trump, who has a lot more public statements to answer for.",Discourse & Governance
505,Facebook to test downranking political content in News Feed – TechCrunch,"After years of optimizing its products for engagement, no matter the costs, Facebook announced today it will “test” changes to its News Feed focused on reducing the distribution of political content. The company qualified that these tests will be temporary, impact a small percentage of people and will only run in select markets, including the U.S., Canada, Brazil and Indonesia.

The point of the experiments, Facebook says, is to explore a variety of ways it can rank political content in the News Feed using different signals, in order to decide on what approach it may take in the future.

It also notes that COVID-19 information from authoritative health organizations like the CDC and WHO, as well as national and regional health agencies and services, will be exempt from being downranked in the News Feed during these tests. Similarly, content from official government agencies will not be impacted.

The tests may also include a survey component, where Facebook asks impacted users about their experience.

Facebook’s announcement of the tests is meant to sound underwhelming because any large-scale changes would be an admission of guilt, of sorts. Facebook has the capacity to make far greater changes — when it wanted to downrank publisher content, it did so, decimating a number of media businesses along the way. In previous years, it also took harder action against low-quality sites, scrapers, clickbait, spam and more.

The news of Facebook’s tests comes at a time when people are questioning social media’s influence and direction. A growing number of social media users now believe tech platforms have been playing a role in radicalizing people, as their algorithms promote unbalanced views of the world, isolate people into social media bubbles and allow dangerous speech and misinformation to go viral.

In a poll this week, reported by Axios, a majority of Americans said they now believe social media radicalizes, with 74% also saying misinformation is an an extremely or very serious problem. Another 76% believe social media was at least partially responsible for the Capitol riot, which 7 in 10 think is the result of unchecked extreme behavior online, the report noted.

Meanwhile, a third of Americans regularly get their news from Facebook, according to a study from Pew Research Center, which means they’re often now reading more extreme viewpoints from fringe publishers, a related Pew study had found.

Elsewhere in the world, Facebook has been accused of exacerbating political unrest, including the deadly riots in Indonesia, genocide in Myanmar, the spread of misinformation in Brazil during elections and more.

Facebook, however, today argues that political content is a small amount of the News Feed (e.g. 6% of what people in the U.S. see) — an attempt to deflect any blame for the state of the world, while positioning the downranking change as just something user feedback demands that Facebook explore.",Social Media,TechCrunch,https://techcrunch.com/2021/02/10/facebook-to-test-downranking-political-content-in-news-feed/,"Social media has been accused of exacerbating political unrest and spreading misinformation, leading to deadly riots and even genocide in some cases. A majority of Americans now believe it radicalizes people and is partially responsible for the Capitol riot.",Politics
506,Facebook says Russia did try to meddle in Brexit vote – TechCrunch,"BuzzFeed has obtained a statement from Facebook in which the tech giant admits, for the first time, that some Russia-linked accounts may have used its platform to try to interfere in the UK’s European Union referendum vote in June 2016.

Which means Russian agents weren’t just using Facebook to meddle in the 2016 US presidential election, and in other recent elections in the West — such as those in France and Germany.

Elections are of course a huge deal but the result can at least be reversed at the ballot box in time. The in/out Brexit referendum in the UK was no such standard vote. And there is no standard process for reversing the result.

So if Kremlin agents also used Facebook to influence people in the UK to vote for Brexit that would be hugely significant — and further evidence that social media’s connective tissue can be used to drive and inflame societal divisions.

“To date, we have not observed that the known, coordinated clusters in Russia engaged in significant coordination of ad buys or political misinformation targeting the Brexit vote,” a Facebook spokesperson told BuzzFeed in a carefully worded statement.

Which begs the question how much Russian Facebook activity did target the Brexit vote? We asked Facebook how many socially divisive Russian-backed ads ran before Brexit. Facebook declined to comment.

While its claim not to have found “significant coordination” of Russian activity ahead of the Brexit vote might sound like ‘case closed’ on the EU referendum front, the company has consistently sought to play down the impact of Facebook-distributed Russian misinformation — with CEO Mark Zuckerberg initially describing it as a “pretty crazy idea” that fake news could have influenced voters in the US election.

Nearly half a year later, after conducting an internal investigation, Facebook conceded there had been a Russian disinformation campaign during the US election — but claimed the reach of the operation was “statistically very small” in comparison with overall political activity and engagement.

Then in September another tidbit came out when it said it now believed potential pro-Kremlin entities could have spent up to $150,000 on its platform to buy 3,000 ads to between 2015 and 2017. It said the ads were tied to 470 accounts — some linked to a known Russian troll farm called the Internet Research Agency.

It also agreed to share the Russian backed US political ads with congressional investigators looking into US election-related disinformation. Though it rejected calls to make all the ads public.

Finally, at the end of last month, about a year after its CEO’s denial of the potency of political disinformation on his mega platform, Facebook admitted Russian-backed content could have reached as many as 126 million people in the US.

It now estimates the number of pieces of divisive content at 80,000, after being asked by congressional investigators to report not just direct Russian-bought ads but organic posts, images, events and more, which can also of course become viral vehicles of disinformation on Facebook’s algorithmically driven platform.

So there’s a reason to be cautious about accepting at face value the company’s claim now that Russian Brexit meddling existed on its platform but was not significant.

Giving a speech yesterday, the UK prime minister set out in no uncertain tones her conviction that Russia has been using social media platforms to try to interfere with Western democracies, directly accusing Vladimir Putin of seeking to sow social division by “weaponizing information” and planting fake stories.

Multiple Twitter accounts previously linked to Russia’s Internet Research Agency have also been identified as engaging in Brexit-related tweeting, according to the Times — linking Russian-backed election meddling troll activity to the UK’s EU referendum vote too.

On Friday, Wired detailed some of the Russian-backed Twitter accounts and 2016 Brexit-related tweets — including tweets apparently seeking to conflate Islam with terrorism, and others aiming to stir up anti-immigrant sentiment such as by spreading racial slurs.

We asked Twitter how many accounts it has linked to pro-Kremlin entities that were also tweeting about Brexit ahead of the referendum vote. At the time of writing the company had not responded.

Meanwhile Russia continues to amuse itself with a spot of public Twitter trolling of the UK PM…

The Russian Foreign Ministry's official Twitter account is trolling our prime minister.

Can someone explain what this means? Hybrid warfare's over? We're now deep into postmodern warfare? pic.twitter.com/la97GLe7BL — Carole Cadwalladr (@carolecadwalla) November 14, 2017

A UK parliamentary committee which is investigating fake news has previously requested data from Twitter and Facebook on Russian accounts which posted about the EU referendum.

Commenting on the cache of Russian tweets now linked to Brexit, Damian Collins, the MP leading the inquiry, told Wired: “I think it shows that Russian-controlled accounts have been politically active in the UK as well as America. This could just be the tip of the iceberg because we’ve only really just started looking and doing a proper detailed study of what accounts linked to Russian organisations have been doing politically.”

The UK’s Brexit vote was both a shock result and a close one, with 51.9 per cent voting to leave the EU vs 48.1 per cent voting remain.

It caused huge immediate political upheaval — with the then UK Prime Minister resigning immediately. There was also major drop in the value of pound sterling. (The pound remains down around 11 per cent vs the dollar and 15 per cent vs the euro.)

While Brexit-based uncertainty continues to impact almost every aspect of day-to-day political activity in the UK, given the scale of the task facing ministers to try to unpick more than 40 years of EU agreements — clearly deflecting the government from being able to pursue a wider policy agenda as ministers’ fixed firefighting focus is on trying to enact Brexit without causing even greater disruption to UK businesses and citizens.

Scores of European ministers and civil servants are also having to expend further resources to manage Brexit vis-a-vis their own sets of priorities and to shape whatever comes after.

The incentive for Russia to have sought to run a disinformation campaign to encourage disunity in the European Union by encouraging a vote for Brexit is clear: Instability weakens your opponents.

Whether Putin’s agents were merely dabbling with Brexit disinformation as they geared up for a more major disinformation push focused on the US election remains to be seen. But given the closeness of the Brexit vote — and the long term disruption Brexit will undoubtedly cause — then any Russia-backed interference deserves to be quantified in full.

So we’re all looking at you, Facebook.",Social Media,TechCrunch,https://techcrunch.com/2017/11/14/facebook-says-russia-did-try-to-meddle-in-brexit-vote/,"Social media has been increasingly used by Russian agents to meddle in Western elections, and the UK's Brexit referendum is a prime example of this divisiveness. The potential for Kremlin-backed influence on the result is huge, and the long-term disruption caused by Brexit makes it even more concerning. As such, Facebook and Twitter need to",Politics
507,"After the Las Vegas Mass Shooting, Watch Out For Hoaxes and Bad Info","Sunday night, a gunman on the 32nd floor of the Mandalay Bay Resort and Casino in Las Vegas fired ammunition into a large crowd at a music concert adjacent to the hotel. At least 58 people were killed and more than 500 were wounded. The gunman, 64-year-old Stephen Paddock, was found dead when law enforcement entered his hotel room. That sums up most of what we know. Any information beyond that? Be very, very careful before you believe it.

It’s an unfortunate reality that every tragedy breeds not just misunderstandings but outright conspiracies and lies. This has always been true, a collision of imperfect information and the impulse to rationalize, to make the previously unthinkable square with what we thought we knew about the world.

You could see it in the aftermath of the Boston Marathon bombing, when Reddit wrongly identified college student Sunil Tripathi as one of the attackers. You could see it during and after Hurricane Sandy upended New York, when Photoshopped images of sharks and a doctored shot of a scuba diver underwater in the Times Square subway station consumed Twitter.

And you can see it unfolding now, just hours after the Las Vegas shooting. Paddock has already been claimed by and assigned to any number of fringe or terrorist groups. As Buzzfeed points out, message boards like 4chan have already claimed that the Instagram account of an unrelated Stephen Paddock belongs to the shooter. Trolls have coalesced around the notion that the real shooter was another man altogether, a theory that was briefly lent legitimacy when it got a write-up in the conservative-leaning site Gateway Pundit. (They noted also that “he was a fan of Rachel Maddow, People’s Action, Democrats, MoveOn.org, etc…” on Facebook.)

That’s just a sampling. Misinformation has also swirled about Marilou Danley, an initial person of interest who turned out to be out of the country. And as happened in the wake of the Manchester bombing this past May, a number of Twitter accounts are circulating photos of missing “victims” who turn out to be YouTube celebrities, porn stars, and stock-photography models.

Some of these misunderstandings are benign; others are malicious. All meet the same counterproductive ends. The most alarming seek to fit Paddock into a preexisting narrative. The so-called ""alt-right"" on Twitter has advanced the theory that he was a member of the “antifa,” a group dedicated to fighting perceived acts of fascism. Going in the opposite direction, terrorist group ISIS claimed responsibility for the Las Vegas attack Monday morning, stating that Paddock “converted to Islam a few months ago.” Meanwhile, authorities have said they’ve seen no evidence that ties Paddock to an international terrorist organization.

At some point they may. There may be a thread that clearly explains Stephen Paddock, that brings a blurred horror into focus. They also may not. All that matters, though, is that they haven’t yet. And anyone who claims otherwise—about his motivations, or allegiances, or what specific gun he used, or any of a thousand currently unknown variables—is only making things worse.",Social Media,WIRED,https://www.wired.com/story/las-vegas-shooting-misinformation-hoaxes-conspiracies/,"Social media has become a platform for the spread of false information and conspiracy theories in the wake of tragic events such as the Las Vegas shooting, leading to misinformation and the furthering of malicious agendas.","Information, Discourse & Governance"
508,Facebook really doesn’t want you to read these emails – TechCrunch,"Oh hey, y’all, it’s Friday! It’s August! Which means it’s a great day for Facebook to drop a little news it would prefer you don’t notice. News that you won’t find a link to on the homepage of Facebook’s Newsroom — which is replete with colorfully illustrated items it does want you to read (like the puffed up claim that “Now You Can See and Control the Data That Apps and Websites Share With Facebook”).

The blog post Facebook would really prefer you didn’t notice is tucked away in a News sub-section of this website — where it’s been confusingly entitled: Document Holds the Potential for Confusion. And has an unenticing grey image of a document icon to further put you off — just in case you happened to stumble on it after all. It’s almost as if Facebook is saying “definitely don’t click here“…

So what is Facebook trying to bury in the horse latitudes of summer?

An internal email chain, starting September 2015, which shows a glimpse of what Facebook’s own staff knew about the activity of Cambridge Analytica prior to The Guardian’s December 2015 scoop — when the newspaper broke the story that the controversial (and now defunct) data analytics firm, then working for Ted Cruz’s presidential campaign, had harvested data on millions of Facebook users without their knowledge and/or consent, and was using psychological insights gleaned from the data to target voters.

Facebook founder Mark Zuckerberg’s official timeline of events about what he knew when vis-à-vis the Cambridge Analytica story has always been that his knowledge of the matter dates to December 2015 — when The Guardian published its story.

But the email thread Facebook is now releasing shows internal concerns being raised almost two months earlier.

This chimes with previous (more partial) releases of internal correspondence pertaining to Cambridge Analytica — which have also come out as a result of legal actions (and which we’ve reported on previously here and here).

If you click to download the latest release, which Facebook suggests it “agreed” with the District of Columbia Attorney General to “jointly make public,” you’ll find a redacted thread of emails in which Facebook staffers raise a number of platform policy violation concerns related to the “political partner space,” writing September 29, 2015, that “many companies seem to be on the edge- possibly over.”

Cambridge Analytica is first identified by name — when it’s described by a Facebook employee as “a sketchy (to say the least) data modelling company that has penetrated our market deeply” — on September 22, 2015, per this email thread. It is one of many companies the staffer writes are suspected of scraping user data — but is also described as “the largest and most aggressive on the conservative side.”

On September 30, 2015, a Facebook staffer responds to this, asking for App IDs and app names for the apps engaging in scraping user data — before writing: “My hunch is that these apps’ data-scraping is likely non-compliant.”

“It would be very difficult to engage in data-scraping activity as you described while still being compliant with FPPs [Facebook Platform Policies],” this person adds.

Cambridge Analytica gets another direct mention (“the Cambridge app”) on the same day. A different Facebook staffer then chips in with a view that “it’s very likely these companies are not in violation of any of our terms” — before asking for “concrete examples” and warning against calling them to ask questions unless “red flags” have been confirmed.

On October 13, a Facebook employee chips back into the thread with the view that “there are likely a few data policy violations here.”

The email thread goes on to discuss concerns related to additional political partners and agencies using Facebook’s platform at that point, including ForAmerica, Creative Response Concepts, NationBuilder and Strategic Media 21. Which perhaps explains Facebook’s lack of focus on CA — if potentially “sketchy” political activity was apparently widespread.

On December 11 another Facebook staffer writes to ask for an expedited review of Cambridge Analytica — saying it’s “unfortunately… now a PR issue,” i.e. as a result of The Guardian publishing its article.

The same day a Facebook employee emails to say Cambridge Analytica “is hi pri at this point,” adding: “We need to sort this out ASAP” — a month and a half after the initial concern was raised.

Also on December 11 a staffer writes that they had not heard of GSR, the Cambridge-based developer CA hired to extract Facebook user data, before The Guardian article named it. But other Facebook staffers chip in to reveal personal knowledge of the psychographic profiling techniques deployed by Cambridge Analytica and GSR’s Dr Aleksandr Kogan, with one writing that Kogan was their post-doc supervisor at Cambridge University.

Another says they are friends with Michal Kosinski, the lead author of a personality modeling paper that underpins the technique used by CA to try to manipulate voters — which they described as “solid science.”

A different staffer also flags the possibility that Facebook has worked with Kogan — ironically enough “on research on the Protect & Care team” — citing the “Wait, What” thread and another email, neither of which appear to have been released by Facebook in this “Exhibit 1” bundle.

So we can only speculate on whether Facebook’s decision — around September 2015 — to hire Kogan’s GSR co-founder, Joseph Chancellor, appears as a discussion item in the “Wait, What” thread…

Putting its own spin on the release of these internal emails in a blog post, Facebook sticks to its prior line that “unconfirmed reports of scraping” and “policy violations by Aleksandr Kogan” are two separate issues, writing:

We believe this document has the potential to confuse two different events surrounding our knowledge of Cambridge Analytica. There is no substantively new information in this document and the issues have been previously reported. As we have said many times, including last week to a British parliamentary committee, these are two distinct issues. One involved unconfirmed reports of scraping — accessing or collecting public data from our products using automated means — and the other involved policy violations by Aleksandr Kogan, an app developer who sold user data to Cambridge Analytica. This document proves the issues are separate; conflating them has the potential to mislead people.

It has previously also referred to the internal concerns raised about CA as “rumors.”

“Facebook was not aware that Kogan sold data to Cambridge Analytica until December 2015. That is a fact that we have testified to under oath, that we have described to our core regulators, and that we stand by today,” it adds now.

It also claims that after an engineer responded to concerns that CA was scraping data and looked into it they were not able to find any such evidence. “Even if such a report had been confirmed, such incidents would not naturally indicate the scale of the misconduct that Kogan had engaged in,” Facebook adds.

The company has sought to dismiss the privacy litigation brought against it by the District of Columbia which is related to the Cambridge Analytica scandal — but has been unsuccessful in derailing the case thus far.

The DC complaint alleges that Facebook allowed third-party developers to access consumers’ personal data, including information on their online behavior, in order to offer apps on its platform, and that it failed to effectively oversee and enforce its platform policies by not taking reasonable steps to protect consumer data and privacy. It also alleges Facebook failed to inform users of the CA breach.

Facebook has also failed to block another similar lawsuit that’s been filed in Washington, DC by Attorney General Karl Racine — which has alleged lax oversight and misleading privacy standards.",Social Media,TechCrunch,https://techcrunch.com/2019/08/23/facebook-really-doesnt-want-you-to-read-these-emails/,Social Media is being criticized for not taking adequate steps to protect consumer data and privacy and failing to inform users of potential breaches like the Cambridge Analytica scandal. Internal emails from 2015 suggest Facebook was aware of the potential policy violations and data scraping prior to The Guardian's December 2015 scoop.,Security & Privacy
509,Tech Companies Have the Tools to Confront White Supremacy,"Say you're a white supremacist who happens to hate Jewish people—or black people, Muslim people, Latino people, take your pick. Today, you can communicate those views online any number of ways without setting off many tech companies' anti-hate-speech alarm bells. And that's a problem.

As the tech industry walks the narrow path between free speech and hate speech, it allows people with extremist ideologies to promote brands and beliefs on their platforms, as long as the violent rhetoric is swapped out for dog whistles and obfuscating language. All the while, social media platforms allow these groups to amass and recruit followers under the guise of peaceful protest. The deadly riots in Charlottesville, Virginia, last weekend reveal they're anything but. Now it's up to those same tech companies to adjust their approaches to online hate—as companies like GoDaddy and Discord did on Monday, by shutting down hate groups on their services—or risk enabling more offline violence in the future.

A Platform for Hate

For the most part, as long as you’re not using an online service to directly threaten anyone or disparage groups of people based on their race, ethnicity, national origin, religious affiliation, sexual orientation, sex, gender, gender identity, or serious disability or disease—policies laid out by Facebook, Twitter, and YouTube—you can get away with practically anything. You can wrap your hate in lofty language about “the heritage, identity, and future of people of European descent,” as white nationalist Richard Spencer does through his supposed think tank, the National Policy Institute. On Twitter, meanwhile, sharing a gas-chamber meme garners just a one-week suspension.

“Social media has allowed [hate groups] to spread and share their messages in ways that was never before possible,” says Jonathan Greenblatt, CEO of the Anti-Defamation League, which has tracked anti-Semitism and hate for more than a century. “They’ve moved from the margins into the mainstream.”

This weekend’s white-supremacist march in Charlottesville, which left 32-year-old Heather Heyer dead after an apparent Nazi sympathizer rammed his vehicle into a crowd, injuring 19 others, was organized out in the open on the very platforms that claim to ban hate speech of any kind. The weekend’s “Unite the Right” rally had its own Facebook page. On Reddit, members of the subreddit r/The_Donald promoted the event in the days leading up to it. And bigots like former Ku Klux Klan leader David Duke used Twitter to issue foreboding warnings that the torch rally was “only the beginning.”

Under the banner of free speech, these tech companies allowed the rhetoric to not only live on their platforms but thrive there. That’s because they operate using a simultaneously fuzzy and overly narrow set of rules around what constitutes banned behavior.

Twitter overtly allows “controversial content,” including from white-supremacist accounts. It only takes action when those tweets threaten violence, incite fear in a group of people, or use explicit slurs.

Facebook, meanwhile, says that while it removes hate speech or any praise of violent acts and hate groups, it allows “people to use Facebook to challenge ideas, institutions, and practices. And we allow groups to organize peaceful protests or rallies for or against things.”",Social Media,WIRED,https://www.wired.com/story/charlottesville-social-media-hate-speech-online/,"Social media has enabled hate groups to spread their messages more widely than ever before, potentially leading to more offline violence. Companies like Facebook, Twitter, and YouTube are grappling with a tricky balance between free speech and hate speech, but it's up to them to adjust their approaches in order to prevent more violence in the future.",Equality & Justice
510,Zuckerberg explains why Facebook won’t take action on Trump’s recent posts – TechCrunch,"In a statement posted to Facebook late Friday afternoon, Mark Zuckerberg offered up an explanation of why his company did not contextualize or remove posts from the accounts associated with President Donald Trump that appeared to incite violence against American citizens.

“We looked very closely at the post that discussed the protests in Minnesota to evaluate whether it violated our policies,” Zuckerberg wrote. “Our policy around incitement of violence allows discussion around state use of force, although I think today’s situation raises important questions about what potential limits of that discussion should be.”

Facebook’s position stands in sharp contrast to recent decisions made by Twitter, with the approval of its chief executive, Jack Dorsey, to screen a tweet from the President on Thursday night using a “public interest notice” that indicated the tweet violated its rules glorifying violence. The public interest notice replaces the substance of what Trump wrote, meaning a user has to actively click through to view the offending tweet.

Critics excoriated Facebook and its CEO for its decision to take a hands off approach to the dissemination of misinformation and potential incitements to violence published by accounts associated with the President and the White House. Some of the criticism has even come from among the company’s employees.

“I have to say I am finding the contortions we have to go through incredibly hard to stomach,” one employee, quoted by The Verge, wrote in a comment on Facebook’s internal message board. “All this points to a very high risk of a violent escalation and civil unrest in November and if we fail the test case here, history will not judge us kindly.”

Zuckerberg defended Facebook’s position saying that it would not take any action on the posts from the President because “we think people need to know if the government is planning to deploy force.”

Facebook’s chief executive also drew a sharp contrast between Facebook’s response to the controversy and that of Twitter, which has provided a fact check for one of the President’s tweets and hidden Thursday’s tweet behind a warning label for violating its policies on violence.

“Unlike Twitter, we do not have a policy of putting a warning in front of posts that may incite violence because we believe that if a post incites violence, it should be removed regardless of whether it is newsworthy, even if it comes from a politician,” wrote Zuckerberg.

Twitter explained its decision in a statement. “This Tweet violates our policies regarding the glorification of violence based on the historical context of the last line, its connection to violence, and the risk it could inspire similar actions today,” the company said.

Twitter Comms ✔@TwitterComms We have placed a public interest notice on this Tweet from @realdonaldtrump. https://twitter.com/realDonaldTrump/status/1266231100780744704 … Donald J. Trump ✔@realDonaldTrump Replying to @realDonaldTrump ….These THUGS are dishonoring the memory of George Floyd, and I won’t let that happen. Just spoke to Governor Tim Walz and told him that the Military is with him all the way. Any difficulty and we will assume control but, when the looting starts, the shooting starts. Thank you! 63.4K Twitter Ads info and privacy

“We’ve taken action in the interest of preventing others from being inspired to commit violent acts, but have kept the Tweet on Twitter because it is important that the public still be able to see the Tweet given its relevance to ongoing matters of public importance,” the Twitter statement continued.

Perhaps, as Zuckerberg suggests, Facebook will have an opportunity to provide some answers to the questions around what the limits should be around allowing the state discussion of incitements to violence. For now, the company’s response only begs more questions.

A link to the full post from Zuckerberg follows below:",Social Media,TechCrunch,https://techcrunch.com/2020/05/29/zuckerberg-explains-why-facebook-wont-take-action-on-trumps-recent-posts/,Facebook's decision not to take any action on posts from President Donald Trump that appeared to incite violence against American citizens is a controversial one that has been heavily criticized by social media users and even members of Facebook's own staff.,Equality & Justice
511,SafeToNet demos anti-sexting child safety tool – TechCrunch,"With rising concern over social media’s ‘toxic‘ content problem, and mainstream consumer trust apparently on the slide, there’s growing pressure on parents to keep children from being overexposed to the Internet’s dark sides. Yet pulling the plug on social media isn’t exactly an option.

UK startup SafeToNet reckons it can help, with a forthcoming system of AI-powered cyber safety mobile control tools.

Here at Mobile World Congress it’s previewing an anti-sexting feature that will be part of the full subscription service — launching this April, starting in the UK.

It’s been developing its cyber safety system since 2016, and ran beta testing with around 5,000 users last year. The goal is to be “protecting” six million children by the end of this year, says CEO Richard Pursey — including via pursuing partnerships with carriers (which in turn explains its presence at MWC).

SafeToNet has raised just under £9 million from undisclosed private investors at this point, to fund the development of its behavioral monitoring platform.

From May, the plan is to expand availability to English speaking nations around the world. They’re also working on German, Spanish, Catalan and Danish versions for launch in Q2.

So what’s at stake for parents? Pursey points to a recent case in Denmark as illustrative of the risks when teens are left freely using social sharing apps.

In that instance more than 1,000 young adults, many of them teenagers themselves, were charged with distributing child pornography after digitally sharing a video of two 15-year-olds having sex.

The video was shared on Facebook Messenger and the social media giant alerted US authorities — which in turn contacted police in Denmark. And while the age of consent is 15 in Denmark, distributing images of anyone under 18 is a criminal offense. Ergo sexting can get even consenting teens into legal hot water.

And sexting is just one of the online risks and issues parents now need to consider, argues Pursey, pointing to other concerns such as cyber bullying or violent content. Parents may also worry about their children being targeted by online predators.

“We’re a cyber safety company and the reason why we exist is to safeguard children on, in particular, social networking and messaging apps from all those things that you read about every day: Cyber bullying, abuse, aggression, sextortion, grooming,” he says.

“We come from the basis that existing parental control systems… simply aren’t good enough. They’ve not kept up to date with the digital world and in particular the world that kids socialize on. So Snapchat, Instagram, less so Facebook, but you get the idea.

“We’ve tackled this using a whole mixture of deep tech from behavioral analytics, sentiment analysis and so on, all using machine learning, to be able to contextualize messages that children send, share and receive. And then block anything harmful. That’s the mission.”

Once the SafeToNet app is installed on a child’s device, and linked with their parents’ SafeToNet account, the software scans for any inappropriate imagery on their device. If it finds anything it will quarantine it and blur the content so it no longer presents a sharing risk, says Pursey.

The software runs continuously in the background on the device so it can also step in in real-time to, for instance, block access to a phone’s camera if it believes the child might be about to use it for sexting.

It’s able to be so reactive because it’s performing ongoing sentiment analysis of everything being typed on the device via its own keyboard app — and using its visibility into what’s being sent and received, how and by whom, to infer a child might be about to send or see something inappropriate.

Pursey says the AI system is designed to learn the child’s normal device usage patterns so it can also alert parents to potential behavioral shifts signaled by their online activity — which in turn might represent a problem or a risk like depression or aggression.

He says SafeToNet’s system is drawing on research into social behavioral patterns, including around digital cues like the speed and length of reply, to try to infer psychological impacts.

If that sounds a little Black Mirror/Big Brother, that’s kind of intentional. Pursey says it’s deliberately utilizing the fact that the children who are its users will know its system is monitoring their device to act as a moderating impulse and rein in risky behaviors.

Its website specifies that children have to agree to the software being installed, and kids will obviously be aware it’s there when it pops up the first notification related to something problematic that they’re trying to do.

“If children know they’re being watched they automatically adjust their behavior,” he says. “We’re using a high degree different methods to deploy our software but it is based upon research working with universities, child welfare support groups, even a priest we’ve been talking to.”

On the parent side, the system hands them various controls, such as enabling them to block access to certain apps or groups of apps for a certain time period, or lock out their kids’ devices so they can’t be used at bedtime or during homework hours. Or ground access to a device entirely for a while.

Though, again, SafeToNet’s website suggests parents use such measures sparingly to avoid the tool being used to punish or exclude kids from socializing digitally with their friends.

The system can also report on particular apps a child is using that parents might not even know could present a concern, says Pursey, because it’s tracking teen app usage and keeping an eye on fast-changing trends — be it a risky meme or something worse.

But he also claims the system is designed to respect a child’s privacy, and Pursey says the software will not share any of the child’s content with their parents without the child’s say so. (Or, in extremis, after a number of warnings have been ignored by the child.)

That’s also how he says it’s getting around the inevitable problem of no automated software system being able to be an entirely perfect content monitoring guardian.

If/when the system generates a false positive — i.e. the software blocks content or apps it really shouldn’t be blocking — he says kids can send a request to their parents to unlock, for example, an image that wasn’t actually inappropriate, and their parents can then approve access to it.

Another privacy consideration: He says SafeToNet’s monitoring systems are designed to run without any of its employees accessing or viewing children’s content. (You can read the company’s Privacy Policy here. They’ve also written a plain English version. And published a privacy impact assessment.)

Though the vast majority (circa 80%) of the data processing it needs to do to run this pervasive monitoring system is being done in the cloud right now. So it obviously cannot guarantee its systems and the data being processed there are safe from hacking risks.

Asked about the company’s intentions towards the user data it’s collecting, Pursey says SafeToNet will not be selling usage data in any form whatsoever. Activity data collected from users will only be used for making improvements to the SafeToNet service itself, he emphasizes.

But isn’t deploying background surveillance of children’s digital devices something of a sledgehammer to crack a nut approach to online safety risks?

Shouldn’t parents really be engaging in ongoing and open conversations with their children in order to equip them with the information and critical thinking for them to be able to assess Internet risks and make these kind of judgement calls themselves?

Pursey argues that risks around online content can now be so acute, and kids’ digital worlds so alien to parents, that they really do need support tools to help them navigate this challenge.

SafeToNet’s website is also replete with warnings that parents should not simply tune out once they have the system installed.

“When you realize that the teenage suicide rate is through the roof, depression, all of these issues you read about every day… I don’t think I would use that phrase,” he says. “This isn’t about restricting children it’s actually about enabling their access to social media.

“The way we look at is the Internet is an incredibly powerful and wonderful thing. The problem is is that it’s unregulated, it’s out of control. It’s a social experiment that nobody on the planet knows how it’s going to come out the other end.”

“I’ve seen a 10 year old girl hang herself in a cupboard,” he adds. “I’ve seen it. I saw it online. I’ve seen two 12 year old boys hang themselves. This morning I saw a film of two Russian girls jumped off a balcony to their death.

“I’ve seen a man shot in the head. I’ve seen a man — two men, actually — have their heads chopped off. These are all things that six year old kids can stumble across online. When you’ve seen those sorts of things you can’t help be affected by them.”

What about the fact that, as he says, surveillance impacts how people behave? Isn’t there a risk of this kind of pervasive monitoring ending up constraining children’s sense of freedom to experiment and explore boundaries, at a crucial moment when they are in the process of forming their identities?

A child may also be thinking about their own sexuality and wanting private access to information to help them try to understand their feelings — without necessarily wanting to signpost all that to their parents. A system that’s monitoring what they’re looking at and intervening in a way that shuts down exploration could risk blocking natural curiosity and even generate feelings of isolation and worse.

“Children are trying to determine their identity, they’re trying to work out who they are but… we’re not there to be the parent,” Pursey responds on that. “We’re they’re to advise, to do the safeguarding… But [parents’ job] is to try and make sure that their children are well balanced and well informed, and can handle the challenges that life brings.

“Our job is certainly not to police them — quite the opposite. It’s to enable them, to give them the freedom to do these things. Rather than sledgehammer to crack a nut, which is the existing parental control systems. In my opinion they cause more harm than they actually save or protect. Because parents don’t know how to use them.”

SafeToNet’s software will work across both Android and iOS devices (although Pursey says it was a lot easier to get it all working on Android, given the open nature of the platform vs Apple’s more locked down approach). Pricing for the subscription will be £4.99 monthly per family (with no limit on the number of devices), or £50 if paid up front for a year.",Social Media,TechCrunch,https://techcrunch.com/2018/02/26/safetonet-demos-anti-sexting-child-safety-tool/,"Parents of children using social media may be at risk of their children facing cyberbullying, sexting, and other online dangers, and SafeToNet is a subscription-based software that can help parents monitor their children's behavior and protect them from potential risks.",Security & Privacy
512,Facebook’s content moderation system under fire again for child safety failures – TechCrunch,"Facebook has again been criticized for failing to remove child exploitation imagery from its platform following a BBC investigation into its system for reporting inappropriate content.

Last year the news organization reported that closed Facebook groups were being used by pedophiles to share images of child exploitation. At the time Facebook’s head of public policy told it he was committed to removing “content that shouldn’t be there”, and Facebook has since told the BBC it has improved its reporting system.

However, in a follow-up article published today, the BBC again reports finding sexualized images of children being shared on Facebook — the vast majority of which the social networking giant failed to remove after the BBC initially reported them.

The BBC said it used the Facebook report button to alert the company to 100 images that appeared to break its guidelines against obscene and/or sexually suggestive content — including from pages that it said were explicitly for men with a sexual interest in children.

Of the 100 reported images only 18 were removed by Facebook, according to the BBC. It also found five convicted pedophiles with profiles and reported them to Facebook via its own system but says none of the accounts were taken down — despite Facebook’s own rules forbidding convicted sex offenders from having accounts.

In response to the report, the chairman of the UK House of Commons’ media committee, Damian Collins, told the BBC he has “grave doubts” about the effectiveness of Facebook’s content moderation systems.

“I think it raises the question of how can users make effective complaints to Facebook about content that is disturbing, shouldn’t be on the site, and have confidence that that will be acted upon,” he said.

In a further twist, the news organization was subsequently reported to the police by Facebook after sharing some of the reported images directly with Facebook when it asked to send examples of reported content that had not been removed.

TechCrunch understands Facebook was following CEOP guidelines at this point — although the BBC claims it only sent images after being asked by Facebook to share examples of reported content. However viewing or sharing child exploitation images is illegal in the UK. The BBC would have to have sent Facebook links to illegal content, rather than shared images directly to avoid being reported — so it’s possible this aspect of the story boils down to a miscommunication.

Facebook declined to answer our questions — and declined to be interviewed on a flagship BBC news program about its content moderation problems — but in an emailed statement UK policy director, Simon Milner, said: “We have carefully reviewed the content referred to us and have now removed all items that were illegal or against our standards. This content is no longer on our platform. We take this matter extremely seriously and we continue to improve our reporting and take-down measures. Facebook has been recognized as one of the best platforms on the internet for child safety.”

“It is against the law for anyone to distribute images of child exploitation. When the BBC sent us such images we followed our industry’s standard practice and reported them to CEOP. We also reported the child exploitation images that had been shared on our own platform. This matter is now in the hands of the authorities,” he added.

The wider issue here is that Facebook’s content moderation system remains very clearly very far from perfect. And contextual content moderation is evidently a vast problem that requires far more resources that are being devoted to it by Facebook. Even if the company employs “thousands” of human moderators, distributed in offices around the world (such as Dublin for European content) to ensure 24/7 availability, it’s still a drop in the ocean for a platform with more than a billion active users sharing multiple types of content on an ongoing basis.

Technology solutions can be part of the solution — such as Microsoft’s PhotoDNA cloud service, which can identify known child abuse images, for example — but such systems can’t help identify unknown obscene material. It’s a problem that necessitates human-moderation and enough human moderators to review user reports in a timely fashion so that problem content can be identified accurately and removed promptly — in other words, the opposite of what appears to have happened in this instance.

Facebook’s leadership cannot be accused of being blind to concerns about its content moderation failures. Indeed, CEO Mark Zuckerberg recently discussed the issue in an open letter — conceding the company needs to “do more”. He also talked about his hope that technology will be able to take a bigger role in fixing the problem in future, arguing that “artificial intelligence can help provide a better approach”, and saying Facebook is working on AI-powered content flagging systems to scale to the ever-growing challenge — although he also cautioned these will take “many years to fully develop”.

And that’s really the problem in a nutshell. Facebook is not putting in the resources needed to fix the current problem it has with moderation — even as it directs resources into trying to come up with possible future solutions where AI-moderation can be deployed at scale. But if Zuckerberg wants to do more right now, the simple fix is to employ more humans to review and act on reports.",Social Media,TechCrunch,https://techcrunch.com/2017/03/07/facebooks-content-moderation-system-under-fire-for-child-safety-failures/,"Facebook has come under fire for failing to remove child exploitation imagery from its platform, as a BBC investigation revealed that many images reported were not taken down and five convicted pedophiles with profiles were not removed despite Facebook's own rules. This highlights the inefficiency of Facebook's content moderation system and the need for more resources to be devoted to the issue",Security & Privacy
513,Facebook has removed 4 Infowars pages — but not because of fake news – TechCrunch,"There’s yet more Alex Jones/Infowars news. Facebook yanked four of the conspiracy theorist’s videos from its platform last week, and now it has finally taken more stringent action after it removed four Infowars pages from the social network entirely.

Over the weekend Spotify, Stitcher and Apple all removed Infowars audio content from their platforms days after YouTube and then Facebook pulled four videos that were found to violate community standards.

A refresher for those who need it: Infowars has broadcast a range of conspiracy theories which have included claims 9/11 was an inside job and alternate theories to the San Bernardino shootings, while it has encouraged harassment of families of victims of the Sandy Hook shooting among other things.

Yet despite much attention on the organization and its use of social media, Facebook’s efforts to handle Infowars have been confusing.

One of the four videos it removed had actually been cleared following a complaint a month ago, while the video purge saw Facebook hand a 30-day ban to Jones’ personal account but the Infowars page — where the content was posted — was able to continue on as normal. That was down to the Facebook system of warnings/accumulated warnings for content violations and nothing to do with peddling fake news. That’s apparently ok.

Indeed, the four Infowars pages that have been “unpublished” — the Alex Jones Channel Page, the Alex Jones Page, the InfoWars Page and the Infowars Nightly News Page — were punished for “repeated violations of Community Standards and accumulating too many strikes” after more videos and content were reported to Facebook by users of the social network.

“Upon review, we have taken [the pages] down for glorifying violence, which violates our graphic violence policy, and using dehumanizing language to describe people who are transgender, Muslims and immigrants, which violates our hate speech policies,” the company explained in an announcement.

Facebook didn’t provide details of exactly which videos violated its policies and how, but it did say explicitly that its action were not related to fake news.

“Much of the discussion around Infowars has been related to false news, which is a serious issue that we are working to address by demoting links marked wrong by fact checkers and suggesting additional content, none of the violations that spurred today’s removals were related to this,” it said in a statement.

Facebook has opted to remain news-neutral, in the sense that only issues warnings based on community standards.

That’s a controversial stance — it is instead pursuing a policy of fact-checking information and letting users make their own mind — but irrespective of whether you agree with that approach, its actions over the past week are problematic because they don’t scale. They rely squarely on the community flagging content in the first instance.

It isn’t clear why Facebook wasn’t able to conduct a more thorough analysis of these Infowars pages last week, when the initial complaints first rolled in. You’d imagine that there’s been enough interest in the topic to warrant a proactive investigation.

Instead, it has taken another week and more reporting of content from users to reach the inevitable conclusion that Infowars has more than just four offensive videos (!) and therefore its pages should be removed(!).

Facebook has chosen to police content based on community guidelines and not the accuracy of information, but the fact it takes so long to take action on the most obvious bad actors doesn’t bode well for finding other, less obvious pages lurking out there that also fall foul of its standards.

Based on that system, it will always be playing catch up. Given the damage that false information can have across its services — from swaying elections to encouraging lynchings, religious violence and more — that simply isn’t good enough.",Social Media,TechCrunch,https://techcrunch.com/2018/08/06/facebook-has-removed-4-infowars-pages/,"Facebook's policy of policing content based on community guidelines, rather than accuracy of information, means it takes too long to take action on bad actors, leading to the potential for false information to spread and cause damage such as swaying elections and encouraging violence.","Information, Discourse & Governance"
514,The Story Of An Internet Troll With A Surprising Twist – TechCrunch,"You get a thick skin when messing around on the Internet and that can go either way. You can react, like a nerve burned raw, or you can become a troll. This is a story of both.

I’ll paraphrase the story here, but go and read it yourself. It’s quite short and very concise. It shows a man’s descent into fear and his eventual recovery and uncovers part of the Internet that, in a word, horrifies me.

Leo Traynor is a political consultant and writer in Ireland. As he began to use Twitter, he would get random followers who then proceeded to harass him, screaming garbage like “Your husband is scum. A rotten b*stard and you’re a wh*re” to his wife and casting anti-Semitic aspersions with regularity. He’d block the account and another would pop up in its place. He’d block that one and he’d get more abuse.

Finally, Traynor quit Twitter. That’s when he got a box of ashes and a note saying “Say hello to your relatives from Auschwitz.” The Troll knew where he lived.

Traynor grew paranoid. He feared for his wife and his family’s life. However, with a bit of digging with the help of an IT-savvy friend, he grabbed three IP addresses and some information that pointed him to a friend’s house – and pinpointed the friends’ 17-year-old son.

Traynor writes:

I spoke to my friend at length. He told me how his son was always glued to his laptop, tablet or smartphone. How he couldn’t watch a TV show without tweeting about it simultaneously. About how he’d become engrossed in conspiracy sites. It also became clear that the other two IP addresses had been used by his son.He was horrified at what his son had done. Horrified, but not surprised. He wanted to call the authorities there and then and turn him in. But I said no.

He went to the friend’s house to meet his troll. The boy – although I would argue he’s a bit older than a boy – was oblivious until Traynor produced a list of all of the attacks. The boy’s parents wanted to call the police but Traynor refused. He turned this into a teaching lesson.

Before he left, Traynor faced his tormenter.

He stood. I said ” Look at me. I’m a middle aged man with a limp and a wheeze and a son and a wife that I love. I’m not just a little avatar of an eye. You’re better than this. You have a name of your own. Be proud of it. Don’t hide it again and I won’t ruin it if you play ball with your parents. Now shake hands.”

Trolling is easy. Anonymous – or even non-anonymous – vitriol on the Internet is the simplest, most primitive method of communication known and it is also the most useless. It is, in short, garbage.

Many of us, myself included, dash off angry comments and other bullshit in forums and social media. But why? Must we really release our impotent rage? Must we be like an old racist at an empty kitchen table, alone and unwanted, disintegrating into a soup of hatred. Are we infants?

I posit we aren’t.

Traynor met his troll and I think his story is educational and important. We’re all better than this. I have to hope that’s true.

via BB",Social Media,TechCrunch,https://techcrunch.com/2012/09/25/the-story-of-an-internet-troll-with-a-surprising-twist/,"The story discusses the negative consequences of trolling and anonymous vitriol on social media. It highlights how easy it can be to release impotent rage online and how it can quickly become a destructive force, with potentially dangerous consequences. The story also shows that we should take responsibility for our actions online, instead of hiding behind our screens.",Social Norms & Relationships
515,How much more abuse do female politicians face? A lot.,"On Monday, the Institute for Strategic Dialogue, a London-based think tank that researches extremism, released some timely data showing that some of the same politicians calling out Twitter’s inaction are indeed facing more attacks online than other politicians.

The study: Researchers collected publicly tagged mentions on Twitter and Facebook for a handful of politicians for two weeks in June and July, scrutinizing them manually and using AI to identify abusive posts.

Overall, researchers found that women and people of color were “far more likely than men to be abused on Twitter.” They found that women received an average of 12% more abuse on Facebook than male politicians. Between 5% and 10% of mentions of most male politicians were considered abusive, while mentions of female politicians on Twitter contained abuse between 15% and 39% of the time.

Overall, women were targeted much more personally by the tweets in the study. While male politicians primarily faced abuse that used general terms, women—and in particular, Representatives Alexandria Ocasio-Cortez and Nancy Pelosi—were attacked with deeply personal and gendered language.

The conclusion: The fact that these groups face much more harassment should be surprising to literally nobody who has been on Twitter. But the study focuses on such a small number of politicians that it is easy to read too much into the specifics.

However, the report helps quantify one of the platform’s longest-running issues at a crucial moment, as Kamala Harris, the Democrats’ vice presidential candidate, prepares to debate Vice President Mike Pence, and as activists raise alarms about online voter suppression campaigns.

The findings may not really be new, but they reinforce what women and people of color have been saying about Facebook, Twitter, and other major social-media companies for years: that while the sites may increasingly have policies banning abusive behavior, the enforcement of those policies often leaves its most common targets open to sustained, coordinated harassment.",Social Media,MIT,https://www.technologyreview.com/2020/10/06/1009406/twitter-facebook-online-harassment-politicians/,"The study by the Institute for Strategic Dialogue confirms the long-held belief that women and people of color suffer more online abuse than men, and that the enforcement of bans on such behaviour often leaves its most common targets open to sustained harassment.",Equality & Justice
516,A huge database of Facebook users’ phone numbers found online – TechCrunch,"Hundreds of millions of phone numbers linked to Facebook accounts have been found online.

The exposed server contained more than 419 million records over several databases on users across geographies, including 133 million records on U.S.-based Facebook users, 18 million records of users in the U.K., and another with more than 50 million records on users in Vietnam.

But because the server wasn’t protected with a password, anyone could find and access the database.

Each record contained a user’s unique Facebook ID and the phone number listed on the account. A user’s Facebook ID is typically a long, unique and public number associated with their account, which can be easily used to discern an account’s username.

But phone numbers have not been public in more than a year since Facebook restricted access to users’ phone numbers.

TechCrunch verified a number of records in the database by matching a known Facebook user’s phone number against their listed Facebook ID. We also checked other records by matching phone numbers against Facebook’s own password reset feature, which can be used to partially reveal a user’s phone number linked to their account.

Some of the records also had the user’s name, gender and location by country.

This is the latest security lapse involving Facebook data after a string of incidents since the Cambridge Analytica scandal, which saw more than 80 million profiles scraped to help identify swing voters in the 2016 U.S. presidential election.

Since then the company has seen several high-profile scraping incidents, including at Instagram, which recently admitted to having profile data scraped in bulk.

This latest incident exposed millions of users’ phone numbers just from their Facebook IDs, putting them at risk of spam calls and SIM-swapping attacks, which relies on tricking cell carriers into giving a person’s phone number to an attacker. With someone else’s phone number, an attacker can force-reset the password on any internet account associated with that number.

Sanyam Jain, a security researcher and member of the GDI Foundation, found the database and contacted TechCrunch after he was unable to find the owner. After a review of the data, neither could we. But after we contacted the web host, the database was pulled offline.

Jain said he found profiles with phone numbers associated with several celebrities.

Facebook spokesperson Jay Nancarrow said the data had been scraped before Facebook cut off access to user phone numbers.

“This data set is old and appears to have information obtained before we made changes last year to remove people’s ability to find others using their phone numbers,” the spokesperson said. “The data set has been taken down and we have seen no evidence that Facebook accounts were compromised.”

Facebook later claimed the server contained “about 220 million” records.

But questions remain as to exactly who scraped the data, when it was scraped from Facebook and why.

Facebook has long restricted developers‘ access to user phone numbers. The company also made it more difficult to search for friends’ phone numbers. But the data appeared to be loaded into the exposed database at the end of last month — though that doesn’t necessarily mean the data is new.

This latest data exposure is the most recent example of data stored online and publicly without a password. Although often tied to human error rather than a malicious breach, data exposures nevertheless represent an emerging security problem.

In recent months, financial giant First American left data exposed, as did MoviePass and the Senate Democrats.",Social Media,TechCrunch,https://techcrunch.com/2019/09/04/facebook-phone-numbers-exposed/,"This latest incident has exposed millions of users' phone numbers, putting them at risk of spam calls and SIM-swapping attacks, which rely on tricking cell carriers into giving a person's phone number to an attacker.",Security & Privacy
517,"2017: The Year Women Reclaimed the Web, From the Women's March to #MeToo","Over the last year, the social media platforms that dominate the web have made fools out of anyone who believed in their fundamental goodness. Neo-Nazis used Facebook groups to organize a hate rally in Charlottesville; Russian trolls used digital ads to drive a wedge through the American electorate. A man livestreamed a murder on Facebook, and the President of the United States used his Twitter account to spread misleading propaganda about Muslims, and levy threats against both the free press and private citizens.

But if there was one bright spot in all this darkness—one series of moments when the web actually did live up to the most optimistic expectations—it was that in the year 2017, women took back the very platforms that have been used to torment and troll them for so long, and built a new-wave women’s movement on top of them.

The first glimpse of just how powerful this movement would become came just three weeks into the New Year, as stuffed Metro cars and fully loaded busses made their way toward the National Mall in Washington DC the day after President Trump's inauguration. Miles across the river, on the Virginia side of the Potomac, you could see a trail of women making their way toward the march, dressed in pink knit hats, carrying signs that read, “Keep your tiny hands off my rights,” and “Grab him by the tax return.” It was obvious that the Women’s March would be a historic event before it even started, one that united millions of women around the world not just in protest of Trump, but in protest of a society that allows powerful men to become even more so despite alleged crimes against women.

In the year 2017, women took back the very platforms that have been used to torment and troll them for so long.

That chapter in the history books might never have been written, though, if a woman named Teresa Shook hadn’t created a Facebook event on election night 2016, calling for women to march on Washington. The event took off in a way entirely unique to this digital age; by the next morning, Shook had amassed 10,000 RSVPs. As Jenna Arnold, an advisor to the Women’s March, told WIRED days before the event, ""It would be hard to say that we would have had this kind of success without an existing platform like Facebook.”

With their voices echoing through the streets of Washington, DC that day, women started the year by promising a reckoning. As the weeks passed, they quickly delivered. Within a month, a woman named Susan Fowler wrote the blog post heard 'round the tech world: ""Reflecting On One Very, Very Strange Year At Uber."" It laid bare the toxic culture at Uber, in which Fowler says she was regularly sexually harassed, punished for reporting her manager to human resources, and ignored when she asked how Uber planned to address massive departures of women from the company. For once, Fowler's voice was actually heard.

Her blog post inspired other women at Uber to tell their own stories of harassment on platforms like Medium. With each viral post, pressure on Uber mounted, until the company finally hired former attorney general Eric Holder to investigate the rideshare giant's sexist culture. That investigation led to the firing of 20 staffers, and contributed to founder Travis Kalanick's resignation as CEO. And when board member David Bonderman made a sexist comment during a board meeting about sexism, he too was forced to resign.",Social Media,WIRED,https://www.wired.com/story/year-women-reclaimed-the-web/,"In 2017, social media platforms saw a rise in the use of hate speech, trolling and propaganda to spread misinformation and division. This culminated in the Women's March which highlighted the need to tackle this abuse, and subsequent movements which exposed the prevalence of workplace harassment and sexism.",Equality & Justice
518,Twitter says it may “refine” its policies after reversing position on Blackburn campaign ad – TechCrunch,"Twitter says it may “refine” its policies after reversing position on Blackburn campaign ad

For the second time in less than three weeks, Twitter has said it will look at its policies following controversy over tweets by a politician. On Tuesday, Twitter reversed a decision it made the day before to block a campaign video from Rep. Marsha Blackburn, a Republican representative from Tennessee, for breaking its ad policies.

In a media statement, a Twitter spokesperson said “While we initially determined that a small portion of the video used potentially inflammatory language, after reconsidering the ad in the context of the entire message, we believe that there is room to refine our policies around these issues.”

Last month, Twitter promised an update to its “public-facing rules” after explaining that it allowed a tweet by President Donald Trump about North Korea, which critics believed broke the platform’s user policies against inciting violence, to stay up because of its “newsworthiness.”

According to a report by Buzzfeed News, Twitter sent an email to the Blackburn campaign explaining it would allow the video ad if it removed a line mentioning the sale of “baby body parts,” which refer to allegations by anti-abortion against Planned Parenthood that have been discredited by multiple state investigations.

In the email, which was obtained by Buzzfeed, Twitter told the campaign that “The line in this video specific to ‘stopped the sale of baby body parts’ has been deemed an inflammatory statement that is likely to evoke a strong negative reaction. If this is omitted from the video it will be permitted to serve.”

.@Twitter shut down our video ad, claiming it’s “inflammatory” & “negative.” Join me in standing up to Silicon Valley → RETWEET our message! pic.twitter.com/K3w4AMgW6i — Marsha Blackburn (@VoteMarsha) October 9, 2017

Ironically (but not surprisingly), Twitter’s initial decision to ban the ad gave it more publicity, with former White House press secretary Sean Spicer tweeting that “Twitter continues campaign against GOP.”

Blackburn also used the ban as an opportunity to attack Twitter, Silicon Valley and the “liberal elite” on Twitter.

The conservative revolution won’t be stopped by @Twitter and the liberal elite. Donate to my Senate campaign today! https://t.co/McCubzaFQD — Marsha Blackburn (@VoteMarsha) October 10, 2017

While critics across the political spectrum have long accused Twitter of applying its policies in an arbitrary way, the divisive political atmosphere in the U.S. has put even more pressure on social media companies like Twitter and Facebook to justify how they enforce their content policies.

Adding another layer of complexity to the issue are ongoing Congressional investigations over how much major tech companies like Twitter, Facebook and Google knew about ads that were bought and placed by a Russian company to influence the 2016 presidential election.

Instead of providing clarity, however, Twitter’s promises to “update” or “refine” its policies may add to the confusion, especially if it doesn’t also provide more transparency to how they are applied. TechCrunch has asked Twitter when it will give more specific information about policy changes to users.",Social Media,TechCrunch,https://techcrunch.com/2017/10/10/twitter-says-it-may-refine-its-policies-after-reversing-position-on-blackburn-campaign-ad/,"The ongoing political atmosphere in the US has put pressure on social media companies such as Twitter and Facebook to justify their content policies. However, promises by Twitter to update or refine their policies may add to the confusion, and there is a need for more transparency in how these policies are applied.","Information, Discourse, and Governance"
519,Facebook and Instagram will now allow users to hide ‘Like’ counts on posts – TechCrunch,"Facebook this week will begin to publicly roll out the option to hide Likes on posts across both Facebook and Instagram, following earlier tests beginning in 2019. The project, which puts the decision about Likes in the hands of the company’s global user base, had been in development for years, but was deprioritized due to the COVID-19 pandemic and the response work required on Facebook’s part, the company says.

Originally, the idea to hide Like counts on Facebook’s social networks was focused on depressurizing the experience for users. Often, users faced anxiety and embarrassment around their posts if they didn’t receive enough Likes to be considered “popular.” This problem was particularly difficult for younger users who highly value what peers think of them — so much so that they would take down posts that didn’t receive enough Likes.

Like-chasing on Instagram, especially, also helped create an environment where people posted to gain clout and notoriety, which can be a less authentic experience. On Facebook, gaining Likes or other forms of engagement could also be associated with posting polarizing content that required a reaction.

As a result of this pressure to perform, some users grew hungry for a “Like-free” safer space, where they could engage with friends or the wider public without trying to earn these popularity points. That, in turn, gave rise to a new crop of social networking and photo-sharing apps such as Minutiae, Vero, Dayflash, Oggl and, now, newcomers like Dispo and newly viral Poparazzi.

Though Facebook and Instagram could have chosen to remove Likes entirely and take its social networks in a new direction, the company soon found that the metric was too deeply integrated into the product experience to be fully removed. One key issue was how the influencer community today trades on Likes as a form of currency that allows them to exchange their online popularity for brand deals and job opportunities. Removing Likes, then, is not necessarily an option for these users.

Instagram realized that if it made a decision for its users, it would anger one side or the other — even if the move in either direction didn’t really impact other core metrics, like app usage.

“How many likes [users] got, or other people got — it turned out that it didn’t actually change nearly as much about the experience, in terms of how people felt or how much they use the experience, as we thought it would. But it did end up being pretty polarizing,” admitted Instagram head, Adam Mosseri. “Some people really liked it and some people really didn’t.”

“For those who liked it, it was mostly what we had hoped — which is that it depressurized the experience. And, for those who didn’t, they used Likes to get a sense for what was trending or was relevant on Instagram and on Facebook. And they were just super annoyed that we took it away,” he added. This latter group sometimes included smaller creators still working on establishing a presence across social media, though larger influencers were sometimes in favor of Like removals. (Mosseri name-checked Katy Perry as being pro Like removals, in fact.)

Ultimately, the company decided to split the difference. Instead of making a hard choice about the future of its online communities, it’s rolling out the “no Likes” option as a user-controlled setting on both platforms.

On Instagram, both content consumers and content producers can turn on or off Like and View counts on posts — which means you can choose to not see these metrics when scrolling your own Feed and you can choose whether to allow Likes to be viewed by others when you’re posting. These are configured as two different settings, which provides for more flexibility and control.

On Facebook, meanwhile, users access the new setting from the “Settings & Privacy” area under News Feed Settings (or News Feed Preferences on desktop). From here, you’ll find an option to “Hide number of reactions” to turn this setting off for both your own posts and for posts from others in News Feed, groups and Pages.

The feature will be made available to both public and private profiles, Facebook tells us, and will include posts you’ve published previously.

Instagram last month restarted its tests on this feature in order to work out any final bugs before making the new settings live for global users, and said a Facebook test would come soon. But it’s now forging ahead with making the feature available publicly. When asked why such a short test, Instagram told TechCrunch it had been testing various iterations on this experience since 2019, so it felt it had enough data to proceed with a global launch.

Mosseri also pushed back at the idea that a decision on Likes would have majorly impacted the network. While removal of Likes on Instagram had some impact on user behavior, he said, it was not enough to be concerning. In some groups, users posted more — signaling that they felt less pressure to perform, perhaps. But others engaged less, Mosseri said.

“Often people say, ‘oh, this has a bunch of Likes. I’m gonna go check it out,’ ” the exec explained. “Then they read the comments, or go deeper, or swipe to the carousel. There’s been some small effects — some positive, some negative — but they’ve all been small,” he noted. Instagram also believes users may toggle on and off the feature at various times, based on how they’re feeling.

In addition, Mosseri pointed out, “there’s no rigorous research that suggests Likes are bad for people’s well-being” — a statement that pushes back over the growing concerns that a gamified social media space is bad for users’ mental health. Instead, he argued that Instagram is still a small part of people’s day, so how Likes function doesn’t affect people’s overall well-being.

“As big as we are, we have to be careful not to overestimate our influence,” Mosseri said.

He also dismissed some of the current research pointing to negative impacts of social media use as being overly reliant on methodologies that ask users to self-report their use, rather than measure it directly.

In other words, this is not a company that feels motivated to remove Likes entirely due to the negative mental health outcomes attributed to its popularity metrics.

It’s worth mentioning that another factor that could have come into play here is Instagram’s plan to make a version of its app available to children under the age of 13, as competitor TikTok did following its FTC settlement. In that case, hiding Likes by default — or perhaps adding a parental control option — would necessitate such a setting. Instagram tells TechCrunch that, while it’s too soon to know what it would do with a kids app, it will “definitely explore” a no Likes by default option.

Facebook and Instagram both told TechCrunch the feature will roll out starting on Wednesday but will reach global users over time. On Instagram, that may take a matter of days.

Facebook, meanwhile, says a small percentage of users will have the feature Wednesday — notified through an alert on News Feed — but it will reach Facebook’s global audience “over the next few weeks.”",Social Media,TechCrunch,https://techcrunch.com/2021/05/26/facebook-and-instagram-will-now-allow-users-to-hide-like-counts-on-posts/,"Social media has come under criticism for inducing feelings of anxiety, embarrassment, and competition amongst its users, as well as encouraging them to post polarizing content for the sake of garnering Likes. To combat this, Facebook and Instagram are rolling out the option to hide Likes, giving users the freedom to choose whether or not to view them.",Social Norms & Relationships
520,No Signal: Egypt blocks the encrypted messaging app as it continues its cyber crackdown – TechCrunch,"No Signal: Egypt blocks the encrypted messaging app as it continues its cyber crackdown

After a week of blocking the secure messaging app Signal in Egypt the service is back online thanks to new features added by its parent company Open Whisper Systems.

Last week Egyptian users raised the alarm about their inability to access the highly encrypted app popular among activists, including important whistleblower Edward Snowden.

Egypt has been increasingly tightening controls on speech all year, and the move against Signal is just the latest attempt to stifle dissent and impede open journalism.

“Signal is important as a means of secure communications without third parties knowing who I’m contacting,” said prominent Egyptian blogger and Global Voices board member Mohamed ElGohary.

ElGohary was one of the first activists to report the lack of access on Twitter.

“I was trying to message a friend on Signal, and it said ‘unable to send’. I tried other friends, same issue. The failing to send also happened on another ISP. When I tried to use it on VPN, it worked. So I concluded that something happened in the scope of Egypt,” ElGohary told TechCrunch.

The app, available on IOS, Android and Desktop, uses built-in end-to-end encryption to prevent third parties (like governments) from seeing the content being sent. It is popular among Egyptian activists and journalists in protecting their sources.

Dear world, as of yesterday both Signal & Telegram apps are not working in Egypt. Among all My losses, I’ve lost access to secure chatting. — Nora Younis (@NoraYounis) December 17, 2016

Egypt has blocked other VoIP apps such as Skype and Whatsapp before, but these for-profit communications services didn’t offer quite the same level of encryption and privacy features as Signal.

Other countries in the Middle East and North Africa (like Morocco) have also limited access to anonymous messaging services and Turkey in recent days has blocked social media in light of the recent assassination of the Russian ambassador.

“These disruptions are not uniform and the causes behind them are not clear,” said Rasha Abdulla, professor of communications at American University in Cairo, who authored a book on the internet in the Arab world. She uses Signal herself and was able to access the app when other users could not.

The Ministry of Communications & Information Technology has not confirmed or denied its block of Signal. TechCrunch reached out to the ministry several times to no avail.

With the recent targeting of Signal, security fears have been raised over what the San Francisco-based service provider Open Whisper Systems has termed “censoring access.”

In an update of the status of the app, Open Whisper Systems included “support for censorship circumvention in Egypt and the UAE” as a new feature using domain fronting.

“The idea is that to block the target traffic, the censors would also have to block those entire services. With enough large scale services acting as domain fronts, disabling Signal starts to look like disabling the internet,” according to a technical note issued by the company.

Authorities famously cut internet access in the midst of the 2011 revolution that toppled 30 years of autocratic rule under former president Hosni Mubarak.

Can anyone subscribed to @TEDataEgypt check if they can access the site https://t.co/U7wptEmr4H, they claim I’m the only person who can’t. — Wael Eskandar (@weskandar) December 18, 2016

“Attempts to curtail freedom online, whether by blocking content or by user violations, is an obvious way to fight the effect of social media that was demonstrated in 2011 and in the couple of years after. The online world provided a free space for people to discuss and organize in a way that was unprecedented,” Abdulla said.

Digital rights group Privacy International claimed in a report issued earlier this year that at the height of the Arab revolutions Egypt bought surveillance technologies from European companies, including the Italian firm Hacking Team.

The disruption of Signal’s service comes at a time especially when freedom of expression is regularly curbed. A Facebook page administrator was arrested for “publishing false news” and authorities shut down 163 Facebook pages for their incitement to violence.

Egypt has intensified its cyber crackdown under president Abdel Fattah el-Sisi. In recent years authorities have blocked Facebook’s controversial Free Basics program for not allowing it to spy on users, imprisoned citizens for satirical Facebook posts and reportedly used Deep Packet Inspection technology allowing for extensive surveillance of Egyptians’ online activities.

In the wake of the Islamic State (IS) terrorist attack earlier this month on a cathedral in Cairo killing 27 people, Egyptian political parties have renewed calls for parliament to pass a repressive cybersecurity law that can carry the death sentence in some cases.

“Unfortunately, most legislation in the Arab world is done with the intention of control rather than regulation” explained Abdulla of the law’s severity.

“It also places part of the responsibility on ISPs, so that every layer of the society is policing another,” Abdulla said.",Social Media,TechCrunch,https://techcrunch.com/2016/12/26/1431709/,"Egypt has been intensifying an online crackdown of speech and dissent, blocking the encrypted messaging app Signal as the latest attempt to impede open journalism and stifle dissent. This comes at a time when authorities have already blocked other VoIP apps and Facebook pages, and are now pushing for a repressive cybersecurity law that can carry the death sentence in some cases",Security & Privacy
521,Shane Dawson’s Jake Paul Documentary Shows the Price of Giving People What They Want,"Youtuber Jake Paul is sort of like Gen Z’s Johnny Knoxville—if, in between ill-advised stunts, Johnny Knoxville rapped about how you should buy his merch and spit on his girlfriend on camera. He and his brother Logan (who infamously filmed a dead body in Japan’s Aokigahara forest earlier this year) leave chaos and scandal in their wake online and off. Jake, especially, is douchebagus americanus var. bro, one of the most hated people on YouTube, but only according to people over the age of 16. To a devoted audience of tweens, he is their universe.

Enter Shane Dawson, a YouTube megastar in his own right, but one with a decade on Jake Paul—both in age and experience. At 30, he’s YouTube’s elder statesman, and his channel reflects the growth that comes with age. It’s morphed from a composite of silly (sometimes racist) sketches, to an equally dark period of making “cakes” out of oozing piles of fast food to what it’s become in the last three months: a vehicle for YouTube’s version of investigative reporting, complete with multipart documentaries deep-diving into the lives of YouTube’s most controversial stars. He’s profiled people like Jeffree Star and Tana Mongeau, with the kind of access only Dawson is likely to get. Whenever Dawson’s videos drop, they make YouTube stand still and gawp—they get millions and millions of views, inspire both positive and negative reaction videos, and generate more conversation than almost anything else on YouTube, except maybe the Paul brothers.

So when Dawson announced his next subject would be Jake Paul, the backlash was instantaneous, and thunderous. The series, which culminated this week in an almost two hour final episode, has more than 100 million views altogether.

The documentary is by turns engrossing and exhausting, sharp and extremely silly, clear eyed and disturbingly blinkered. It’s central question is at once a focused interrogation (Is Jake Paul a sociopath? And if he is, are all YouTubers?) and an existential crisis (Who made Jake Paul? Who even is Jake Paul?). Through this meandering gaze the series tackles America’s thorniest cultural issues, from the outrage-driven tendency to outcast social media celebrities after a scandal to casual racism and toxic masculinity— while acknowledging Dawson’s own place in the attention-seeking streaming ecosystem.

“Listen, I get it, okay? I shouldn’t be doing this. I shouldn’t be giving Jake Paul a platform,” Dawson says in the first episode. “But I also want to be doing shit on my channel that I find interesting.”

That ambivalence and helpless self-reproach characterizes the entire series, especially the finale, where Shane and the audience sit down and listen to Jake Paul speak his version of the truth for an hour, like a low budget version of The Bachelor’s “After the Final Rose.”

Airing this direct interaction is a radical action. By social media law, Jake Paul’s extremely bad behavior, which includes imperiling neighbors by setting giant fires and making racist and xenophobic jokes regularly, and directly impacts millions of preteens, has gotten him “canceled”—social media parlance for being made a pariah, a tactic designed to starve a problem person of oxygen. That law applies to the rarified world of YouTube celebrities most of all: Later in the series, Paul says that he feels like “the elephant in the room” at YouTube creator events because people pretend he isn’t there. By talking about and to Paul, Dawson is stepping outside YouTube’s norms, and the viewer joins him by watching. Everyone knows it’s not quite right to ignore him and even worse to give him attention, but that makes it harder to look away.",Social Media,WIRED,https://www.wired.com/story/shane-dawsons-youtube-documentary-shows-the-price-of-giving-people-what-they-want/,"The main undesirable consequence of social media discussed in this piece is a tendency to outcast people after a scandal, regardless of their actual wrongdoing. This is exemplified in the ""canceling"" of YouTube celebrity Jake Paul, which has made him an outcast in the online streaming world. This behavior is an example of how social media can",Social Norms & Relationships
522,How an internet lie about the Capitol invasion turned into an instant conspiracy theory,"Just as well-known, easily identifiable far-right figures livestreamed themselves invading the Capitol in Washington, DC, a lie started spreading around the Trump-supporting internet: What if the mob was actually a group of antifa activists trying to make the president’s supporters look bad? The rumor was false, and debunked repeatedly—not least by the words and actions of the MAGA personalities who were leading the charge in front of a live audience.

The lie had been seeded already, since false claims about antifa are peppered through the history of far-right online spaces. A typical conspiracy theory features an unfounded warning that buses loaded with protesters are being sent to cause trouble in small towns. President Trump himself has repeatedly promoted such claims, helping to turn anti-fascist protesters into go-to villains for his supporters.

That gave fuel to the latest rumor, false though it was. It rapidly made its way through social networks, broadcast news, and online media—and was amplified and supported by some Republican politicians.

According to data from media intelligence firm Zignal labs, at least 411,099 mentions of the lie appeared online in less than 24 hours. The rumor morphed and gained traction as more people contributed subplots, and it swerved through niche platforms and into the mainstream, where a Republican member of Congress blamed antifa for the insurrection.

Mentions about Antifa lie across social media platform

How it happened

As the congressional certification of electoral votes took place on Wednesday, a Trump rally outside the Capitol quickly turned into chaos. At around 2.30 p.m. EST, protesters moved through police lines and mobbed the building.

Around 3:30 p.m., Lin Wood, a well-known right-wing conspiracy theorist, posted on Parler, the social network that is popular among some Trump supporters. He claimed that the mob were antifa supporters, and that two separate images—one of a man from the Capitol mob and the other supposedly from “phillyantifa.org”—showed the same person. The post got 5.6 million views and over 56,000 upvotes. With that, the seed was planted.",Social Media,MIT,https://www.technologyreview.com/2021/01/07/1015858/capitol-invasion-antifa-conspiracy-lie/,"The spread of false claims, such as the one that antifa activists were behind the Capitol invasion, has been facilitated by social media, with hundreds of thousands of mentions of the lie online in just 24 hours. This has resulted in the propagation of dangerous conspiracy theories and misinformation, and allowed far-right figures to sow discord and manipulate public opinion","Information, Discourse & Governance"
523,"Instagram is killing its creepy stalking feature, the Following tab – TechCrunch","Instagram is removing its Following tab, a feature that became better known as a stalking tool than one to aid with new account discovery, as the company had intended. Today, Instagram says that its Explore tab is the go-to place to find new people, places and hashtags to follow. Meanwhile, the Following tab is now only used by a small number of people on a regular basis.

Combined, those factors were the key reasons behind its shutdown, we understand.

But the Following tab wasn’t just unhelpful and therefore ignored by Instagram users — a number of people forgot, or never even knew, it existed in the first place. There are numerous stories of people’s illicit or private activities being outed because of what they were liking and following on Instagram.

The tab was fairly notorious for the transparency it brought to our online lives. It was often a place where “micro-cheating” activity was caught, for example. That is, it could reveal when a person in a committed relationship was spending just a little too much time liking someone else’s posts, engaging with an ex’s content or networking with series of potential “backup partners” on a regular basis.

It also could easily reveal when someone’s main use case for Instagram was to like and follow IG “models” — something that didn’t seem like the kind of activity most would want to be offered in a trackable format. (And information that was particularly awkward to see when the person in question was not a close friend, but rather a work colleague of some sort.)

The tab was known to have impacted or even ended friendships, too — like in the case where someone wouldn’t respond to their friend’s texts, or claim they were “busy.” Meanwhile, they were consistently active on Instagram, and very obviously lying.

Gossip followers also liked the tab, as it would reveal who celebs were following — sometimes an indication of a new relationship, either personal or professional in nature.

Overall, the tab generally became known as a creepy tool, and not one that offered any significant benefit to users in terms of being a legitimate discovery mechanism.

Though Instagram is only announcing this change today, many users had already lost access to the tab. Back in August, for example, a big thread on Reddit saw users complaining their Following tab had disappeared. They assumed it was a bug, however.

After the tab’s removal, you’ll only see your own activity, as before, when you click the Heart button.

The company says the Following tab is being removed starting today, but it will take the rest of the week for the rollout to complete.",Social Media,TechCrunch,https://techcrunch.com/2019/10/07/instagram-is-killing-its-creepy-stalking-feature-the-following-tab/,"The Following tab on Instagram was notorious for its transparency, allowing users to easily uncover ""micro-cheating"" activities, inappropriate use of the platform, and gossip. It often revealed embarrassing and incriminating behavior, leading to strained friendships and relationships.",Social Norms & Relationships
524,'Torrent Tweets' Marries BitTorrent to Twitter,"Peer-to-peer file sharing is getting its hooks into Twitter, thanks to an update from BitTorrent, the company that makes the popular µTorrent client and which is seeking to make a big, legitimate business from a protocol most associated with copyright infringement.

Despite its name, peer-to-peer file sharing is often a solitary pursuit, where users' computers swap bits of files, while the users themselves remain anonymous to one another. BitTorrent is hoping to make this more social by using Twitter to make it easy for people downloading and uploading the same file to talk about it.

A new feature in the beta version of the µTorrent client called Torrent Tweets allows users to talk about a given download from within the application and see what everyone else is saying on Twitter, as demonstrated by the perfectly non-infringing screenshot above (the Yes Men encourage this version of their documentary to be distributed in this fashion).

This system works by creating a unique Twitter hashtag using bit torrent's own infohash, a unique identifier that fingerprints every file shared via BitTorrent.

The idea is to turn the often lonely world of the BitTorrent downloader into a global party where viewers and downloads can mingle to chat about the audio, video, software and text files available free of charge thanks to trackers hosted around the world.

That's a fine feature for those downloading works whose copyright owners, like the Yes Men, don't mind people sharing their files – or, say, a document from Wikileaks. But given that most people's Twitter feeds are public and archived for posterity by the Library of Congress, sharing with the world that you are downloading a copy of Hurt Locker could be an easy way to make yourself a target for an expensive copyright infringement lawsuit.

BitTorrent announced this app on Thursday, and so far, it's only available within the latest beta version of µTorrent (BitTorrent owns µTorrent). Company spokesman Simon Morris told Wired.com that this latest version won't leave beta status and become the default version for a while.

""Historically it takes a few months, incorporating some feedback cycles with our community and a ton of bug fixes,"" said Morris. ""Conservatively, I’d say some time in the fall of this year.""

Of course, BitTorrent's µTorrent is just one of the many BitTorrent clients out there, including the popular Vuze (formerly Azureus). Torrent Tweets won't truly become a standard for BitTorrent users until the rest of the clients incorporate it as well, which would be relatively straightforward to do, because it's mostly just a matter of writing simple code to turn BitTorrent infohashes into Twitter hashtags.

In his announcement of the new Torrent Tweets app, Morris encouraged the developers of other BitTorrent clients to integrate this functionality into their software as well. If they do, the marriage between Twitter and BitTorrent will be complete, for better or for worse.

Follow us for disruptive tech news: Eliot Van Buskirk and Epicenter on Twitter.

See Also:",Social Media,WIRED,https://www.wired.com/2010/08/bit-torrent-and-twitter/,"Using the new Torrent Tweets feature, users risk being targeted with an expensive copyright infringement lawsuit if they download copyrighted material, as their Twitter feeds are public and archived by the Library of Congress.",Security & Privacy
525,A majority of U.S. teens are taking steps to limit smartphone and social media use – TechCrunch,"A majority of U.S. teens are taking steps to limit smartphone and social media use

It’s not just parents who are worrying about their children’s device usage. According to a new study released by Pew Research Center this week, U.S. teens are now taking steps to limit themselves from overuse of their phone and its addictive apps, like social media. A majority, 54% of teens, said they spend too much time on their phone, and nearly that many – 52% – said they are trying to limit their phone use in various ways.

In addition, 57% say they’re trying to limit social media usage and 58% are trying to limit video games.

The fact that older children haven’t gotten a good handle on balanced smartphone usage points to a failure on both parents’ parts and the responsibilities of technology companies to address the addictive nature of our devices.

For years, instead of encouraging more moderate use of smartphones, as the tools they’re meant to be, app makers took full advantage of smartphones’ always-on nature to continually send streams of interruptive notifications that pushed users to constantly check in. Tech companies even leveraged psychological tricks to reward us each time we launched their app, with dopamine hits that keep users engaged.

Device makers loved this addiction because they financially benefited from app sales and in-app purchases, in addition to device sales. So they built ever more tools to give apps access to users’ attention, instead of lessening it.

For addicted teens, parents were of little help as they themselves were often victims of this system, too.

Today, tech companies are finally waking up to the problem. Google and Apple have now both built in screen time monitoring and control tools into their mobile operating systems, and even dopamine drug dealers like Facebook, Instagram and YouTube have begun to add screen time reminders and other “time well spent” features.

But these tools have come too late to prevent U.S. children from developing bad habits with potentially harmful side effects.

Pew says that 72% of teens are reaching for their phones as soon as they wake up; four-in-ten feel anxious without their phone; 56% report that not have their phone with them can make them feel lonely, upset or anxious; 51% feel their parents are distracted by phones during conversations (72% of parents say this is true, too, when trying to talk to teens); and 31% say phones distract them in class.

The problems are compounded by the fact that smartphones aren’t a luxury any longer – they’re in the hands of nearly all U.S. teens, 45% of whom are almost constantly online.

The only good news is that today’s teens seem to be more aware of the problem, even if their parents failed to teach balanced use of devices in their own home.

Nine-in-ten teens believe that spending too much time online is a problem, and 60% say it’s a major problem. 41% say they spend too much time on social media.

In addition, some parents are starting to take aim at the problem, as well, with 57% reporting they’ve set some screen time restrictions for their teens.

Today’s internet can be a toxic place, and not one where people should spend large amounts of time.

Social networking one the top activities taking place on smartphones, reports show.

But many of these networks were built by young men who couldn’t conceive of all the ways things could go wrong. They failed to build in robust controls from day one to prevent things like bullying, harassment, threats, misinformation, and other issues.

Instead, these protections have been added on after the fact – after the problems became severe. And, some could argue, that was too late. Social media is something that’s now associated with online abuse and disinformation, with comment thread fights and trolling, and with consequences that range from teen suicides to genocide.

If we are unable to give up our smartphones and social media for the benefits they do offer, at the very least we should be monitoring and moderating our use of them at this point.

Thankfully, as this study shows, there’s growing awareness of this among younger users, and maybe, some of them will even do something about it in the future – when they’re the bosses, the parents, and the engineers, they can craft new work/life policies, make new house rules, and write better code.",Social Media,TechCrunch,https://techcrunch.com/2018/08/24/a-majority-of-u-s-teens-are-taking-steps-to-limit-smartphone-and-social-media-use/,"Social media has been associated with online abuse, harassment, threats, misinformation, trolling, and in some cases, teen suicides and even genocide, making it clear that it should be monitored and moderated more carefully.",Social Norms & Relationships
526,Does Facebook Praise Kill Self-Control?,"One hateful Facebook comment might reduce you to tears, but a recent study found that the ""likes"" prompted by your status updates and photo posts might also have a negative impact, especially on your waistline and pocketbook.

Columbia Business School professor Keith Wilcox and University of Pittsburgh business professor Andrew Stephen studied people who use Facebook to stay in touch with their closest friends and found that the more likes and self-affirming comments they got, the more likely they were to reach for a cookie over a granola bar.

The study, titled “Are Close Friends the Enemy? Online Social Networks, Self-Esteem, and Self-Control,” was published in the Journal of Consumer Research and found that your self-esteem can soar while browsing your Facebook feed, but only if you have strong ties to your Facebook friends. If your feed is populated with updates and comments from people you’ve never met before, you won’t feel any better after reading their updates.

That boost in self-esteem can come with a price. For one experiment in the study, Wilcox and Stephen asked 84 study participants to either browse Facebook or read CNN.com for five minutes. Both groups then had to choose between a healthy granola bar or not-so-healthy chocolate chip cookie. The Facebook group was much more likely to go for the cookie, while the CNN group picked the granola bar.

The results surprised Wilcox, who knew from prior studies that spending time on social networks makes us feel better about ourselves. He expected those good feelings to help bolster self control, not diminish it. “People with high self-esteem typically have more self control, not less,” he says. “It seems the momentary increase in self control that the participants got from browsing Facebook for a few minutes creates a sense of entitlement to do what they want and, therefore, lower self control.”

The final experiment of the study found that those people who reported higher self-esteem and lower self control from browsing Facebook happen to have a higher body mass index and more credit card debt. Those with strong ties to their Facebook friends and who used the social network frequently had a higher BMI, were more likely to binge eat, and had several hundred dollars more in credit card debt than frequent users that barely know their Facebook friends and those that go on Facebook less often. Wilcox cautions that those findings do not necessarily mean that spending time on social networks causes any of those things.

These two business school professors did not draw any lines between the findings and how advertisers could exploit them, but you can be confident that one cookie company or another will take this golden opportunity to stuff your Facebook feed with more ads and your face with more cookies. After all, the cookie peddlers know you won't be able to resist the temptation.",Social Media,WIRED,https://www.wired.com/2013/01/self-control-and-facebook/,"The study conducted by Columbia Business School professor Keith Wilcox and University of Pittsburgh business professor Andrew Stephen showed that people with strong ties to their Facebook friends and frequent use of the social network had higher body mass index, were more likely to binge eat, and had more credit card debt than those who used the social network less often.",Social Norms & Relationships
527,‘The Real Facebook Oversight Board’ launches to counter Facebook’s ‘Oversight Board’ – TechCrunch,"Today a group of academics, researchers and civil rights leaders go live on with ‘The Real Facebook Oversight Board’ which is designed to criticize and discuss the role of the platform in the upcoming US election. The group includes Facebook’s ex-head of election security, leaders of the #StopHateForProfit campaign and Roger McNamee, early Facebook investor. Facebook launched its own ‘Oversight Board’ last November to deal with thorny issues of content moderation, but Facebook has admitted it will not be overseeing any of Facebook’s content or activity during the course of the US election, and will only adjudicate on issues after the event.

The press conference for the launch is streamed live today, below:



Facebook founder Mark Zuckerberg claimed last November that the Oversight Board was “an incredibly important undertaking” and would “prevent the concentration of too much decision-making within our teams” and promote “accountability and oversight”.

The move was seen as an acknowledgment of the difficulty of decision-making inside Facebook. Decisions on what controversial posts to remove fall on the shoulders of individual executives, hence why the Oversight Board will act like a ‘Supreme Court’ for content moderation.

However, the Oversight Board has admitted it will take up to three months to make a decision and will only make judgments about content that has been removed from the platform, not what stays up.

Facebook has invested $130 million in this board and announced its first board members in May, including ex-prime minister of Denmark, Helle Thorning-Schmidt and the ex-editor-in-chief of the Guardian, Alan Rusbridger.

The activist-led ‘Real Facebook Oversight Board’ includes the ex-President of Estonia, Toomas Henrik Ilves, an outspoken critic of Facebook and Maria Ressa, the journalist currently facing imprisonment in the Philippines for cyberlibel.

Board members also include Shoshana Zuboff, author of Surveillance Capitalism, Derrick Johnson, president of the NAACP, Yael Eisenstat, former head of election integrity at Facebook, Rashad Robinson, president of Color of Change, and Jonathan Greenblatt, CEO of the Anti-Defamation League.

This issue of how Facebook moderates its content and allows its users to be targetted by campaigns has become ever more pressing as the US election looms closer. It’s already been revealed by Channel 4 News in the UK that 3.5 million Black Americans were profiled and categorized on Facebook, and other social media, as needing to be deterred from voting by the Trump campaign.",Social Media,TechCrunch,https://techcrunch.com/2020/09/30/the-real-facebook-oversight-board-launches-to-counter-facebooks-oversight-board/,Social media platforms such as Facebook have come under fire for allowing certain users to be targeted by campaigns and for their lack of accountability and oversight when it comes to content moderation. This has led to the launch of the 'Real Facebook Oversight Board' which is designed to hold the platform to a higher standard of accountability and transparency during the US election.,"Information, Discourse & Governance"
528,Higher Ground Labs is betting tech can help sway the 2020 elections for Democrats – TechCrunch,"Higher Ground Labs is betting tech can help sway the 2020 elections for Democrats It's mobilizing the mobile phones of the millennial electorate

When Shomik Dutta and Betsy Hoover first met in 2007, he was coordinating fundraising and get-out-the-vote efforts for Barack Obama’s first presidential campaign and she was a deputy field director for the campaign.

Over the next two election cycles the two would become part of an organizing and fundraising team that transformed the business of politics through its use of technology — supposedly laying the groundwork for years of Democratic dominance in organizing, fundraising, polling and grassroots advocacy.

Then came Donald J. Trump and the 2016 election.

For both Dutta and Hoover, the 2016 outcome was a wake-up call against complacency. What had worked for the Democratic party in 2008 and 2012 wasn’t going to be effective in future election cycles, so they created the investment firm Higher Ground Labs to provide financing and a launching pad for new companies serving Democratic campaigns and progressive organizations.

“As the political world shifts from analog to digital, we need a lot more tools to capture that spend,” says Dutta. “Democrats are spending on average 70 cents of every dollar raised on television ads. We are addicted to old ways of campaigning. If we want to activate and engage an enduring majority of voters we have to go where they are (and that’s increasingly online) and we have to adapt to be able to have these conversations wherever they are.”

Social media and the rise of “direct to consumer” politics

While the Obama campaign effectively used the internet as a mobilization tool in its two campaigns, the lessons of social media and mobile technologies that offer a “direct-to-consumer” politics circumventing traditional norms have, in the ensuing years, been harnessed most effectively by conservative organizations, according to some scholars and activists.

“The internet is a tool and in that sense it’s neutral, but just like other communication tools from the past, people with more power, with more resources, with more organization, have been able to take advantage of it,” Jen Schradie, an assistant professor at the Observatoire sociologique du changement at Sciences Po in Paris, told Vox in an interview earlier this month.

Schradie is a scholar whose recent book, “The Revolution That Wasn’t,” contends that the internet’s early application as a progressive organizing tool has been overtaken by more conservative elements. “The idea of neutrality seems more true of the internet because the costs of distributing information are dramatically lower than with something like television or radio or other communication tools,” she said. “However, to make full use of the internet, you still need substantial resources and time and motivation. The people who can afford to do this, who can fund the right digital strategy, create a major imbalance in their favor.”

Schradie contends that a web of privately funded think tanks, media organizations, talk radio and — increasingly — mobile applications have woven a conservative stitch into the fabric of social media. The medium’s own tendency to promote polarizing and fringe viewpoints also served to amplify the views of pundits who were previously believed to be political outliers.

Essentially, these sites have enabled commentators and personalities to create a patchwork of “grassroots” organizations and media operations dedicated to reaching an audience receptive to their particular political message that’s funded by billionaire donors and apolitical corporate ad dollars.

Then there’s the technology companies, like Cambridge Analytica, which improperly used access to Facebook data for targeting purposes — also financed by these same billionaires.

“The last six years have witnessed millions and millions of dollars of private Koch money and Mercer money that have gone to pretty sophisticated data and media efforts to advance the Republican agenda,” says Dutta. “I want to even the scale.”

Dutta is referring to Charles and David Koch and Robert Mercer, the scions and founder (respectively) of two family dynasties worth billions. The Koch brothers support a web of political advocacy groups, while Mercer and his daughter were large backers of Breitbart News and Cambridge Analytica, two organizations that arguably provided much of the policy underpinnings and online political machinery for the Trump presidential campaign.

But there’s also the simple fact that Donald Trump’s digital strategy director, Brad Parscale, was able to effectively and inexpensively leverage the social media tools and data troves amassed by the Republican National Committee that were already available to the candidate who won the Republican primary. In fact, in the wake of Romney’s loss, Republicans spent years building up profiles of 200 million Americans for targeted messaging in the 2016 election.

“Who controls Facebook controls the 2016 election,” Parscale said during a speaking engagement at the Romanian Academy of Sciences, according to a report in Forbes.

Parscale, now the campaign manager for the president’s 2020 reelection campaign recalled, “These guys from Facebook walked into my office and said: ‘we have a beta … it’s a new onboarding tool … you can onboard audiences straight into Facebook and we will match them to their Facebook accounts,’ ” according to Forbes.

During the 2016 campaign, Hillary Clinton’s team made 66,000 visual ads, according to Parscale, while the Trump campaign made 5.9 million ads by leveraging social media networks and the language of memes. And in the run-up to the 2020 election, Parscale intends to go back to the same well. The Trump campaign has already spent more than $5 million on Facebook ads in the current election cycle, according to The New York Times — outspending every single Democratic candidate in the field and roughly all of the Democrats combined.

Reaching higher ground

Dutta and Hoover are working to offset this movement with investments of their own. Back in 2017, the two launched Higher Ground Labs, an early-stage company accelerator and investment firm dedicated to financing technology companies that could support progressive causes.

The firm has $15 million committed from investors, including Reid Hoffman, the co-founder of LinkedIn and a partner at Greylock; Ron Conway, the founder of SV Angel and an early backer of Google, Facebook and Twitter; Chris Sacca, an early investor in Uber; and Elizabeth Cutler, the founder of SoulCycle. Already, Higher Ground has invested in more than 30 companies focused on services like advocacy outreach, polling and campaign organizing — among others.

“It is vitally important that Democrats learn to do their campaigns online,” says Dutta. “The way you recruit volunteers; the way you poll sentiment; the way you target and mobilize voters has to be done with online tools and has to improve in the progressive movement and that’s the job of Higher Ground Labs to fix.”

For-profit companies have a critical role to play in election organizing and mobilization, Dutta says. Thanks to government regulation, only private companies are allowed to trade data across organizations and causes (provided they do it at fair market value). That means advocacy groups, unions and others can tap the information these companies collect — for a fee.

The Democratic Party already has one highly valued private company that it uses for its technology services. Formed from the merger of NGP Software and Voter Activation Network, two companies that got their start in the late 1990s and early 2000s, NGP VAN is the largest software and technology services provider for Democratic campaigns. It’s also a highly valued company, which received roughly $100 million in financing last year from the private equity firm Insight Venture Partners, according to people familiar with the investment. Terms of the deal were not disclosed.

“Our vision has been to build a platform that would break down the painful data silos that exist in the campaigns and nonprofit space, and to offer truly best-in-class digital, fundraising and organizing features that could serve both the largest and the smallest nonprofits and campaigns, all with one unified CRM,” wrote Stu Trevelyan, the chief executive of NGP VAN + EveryAction, in an August blogpost announcing the investment. “We’re so excited that others, like our new partners at Insight, share that vision, and we can’t wait to continue innovating and growing together in the coming years.”

Can startups lead the way?

Even as private equity dollars boost the firepower of organizations like NGP VAN, venture capitalists are financing several companies from the Higher Ground Labs portfolio.

Standouts like Hustle, which raised $30 million last May, show that investors are buying into the proposition that these companies can build lasting businesses serving Democratic and progressive political campaigns and corporate businesses that would also like to rally employees or personalize a marketing pitch to customers.

Then there are earlier stage companies that are gaining significant traction both on the political and commercial circuits.

These are companies like Change Research, an earlier-stage company that just launched from Higher Ground Labs accelerator last year. That company, founded by Mike Greenfield, a serial Silicon Valley entrepreneur who was the first data scientist working on the problem of fraud detection at PayPal, and Pat Reilly, a communications professional who worked with state and local Democratic politicians, is slashing the cost of political polling.

“I wanted to do something for American democracy to try and improve the state of things,” Greenfield said in an interview last year.

For Greenfield, that meant increasing access to polling information. He cited the test case of a Kansas special election in a district that Donald Trump had won by 27 points. Using his own proprietary polling data, Greenfield predicted that the Democratic challenger, James Thompson, would pose a significant threat to his Republican opponent, Mike Estes.

Estes went on to a 7% victory at the ballot, but Thompson’s campaign did not have access to polling data that could have helped inform his messaging and — potentially — sway the election, said Greenfield.

“Public opinion is used to ween out who can be most successful based on how much money they’re able to raise for a poll,” says Reilly. It’s another way that electoral politics is skewed in favor of the people with disposable income to spend what is a not-insignificant amount of money on campaigns.

Polls alone can cost between $20,000 to $30,000 — and Change Research has been able to cut that by 80% to 90%, according to the company’s founders.

“It’s safe to say that most of the world was stunned by the outcome [of the presidential election] because most polls predicted the opposite,” says Greenfield. “Being a good American and as a parent of a 10-year-old and a 12-year-old, providing forward-thinking candidates and causes with the kind of insight they needed to win up and down the ballot could not only be a good business, but really help us save our democracy.”

Change Research isn’t just polling for politicians. Last year, the company conducted roughly 500 polls for political candidates and advocacy groups.

“The way that I’ve described Change Research to investors is that we want to simultaneously move the world in a better direction and having a positive impact while building a substantial business,” says Greenfield. “We’re only going to work with candidates and causes that we’re aligned with.”

Being exclusively focused on progressive causes isn’t the liability that many in the broader business community would think, says Dutta. Many Democratic organizations won’t work with companies that sell services to both sides of the aisle.

For Higher Ground Labs, a stipulation for receiving their money is a commitment not to work with any Republican candidate. Corporations are okay, but conservative causes and organizations are forbidden.

“We’re in a moment of existential crisis in America and this Republican party is deeply toxic to the health and future of our country,” says Dutta. “The only path out of this mess is to vote Republicans out of office and to do that we need to make it easier for good candidates to run for office and to engage a broader electorate into voting regularly.”

UPDATE: An earlier version of this story mentioned Civis Analytics as a Higher Ground Labs portfolio company. That was incorrect.",Social Media,TechCrunch,https://techcrunch.com/2019/07/01/higher-ground-labs/,"The use of social media to spread conservative messages has been taken advantage of by people with more power and resources, creating an imbalance in their favor and potentially influencing the outcome of elections.",Politics
529,Justice Department’s threat to social media giants is wrong – TechCrunch,"Never has it been so clear that the attorneys charged with enforcing the laws of the country have a complete disregard for the very laws they’re meant to enforce.

As executives of Twitter and Facebook took to the floor of the Senate to testify about their companies’ response to international meddling into U.S. elections and addressed the problem of propagandists and polemicists using their platforms to spread misinformation, the legal geniuses at the Justice Department were focused on a free speech debate that isn’t just unprecedented, but also potentially illegal.

These attorneys general convened to confabulate on the “growing concern” that social media companies are stifling expression and hurting competition. What’s really at issue is a conservative canard and talking point that tries to make a case that private companies have a First Amendment obligation to allow any kind of speech on their platforms.

The simple fact is that they do not. Let me repeat that. They simply do not.

What the government’s lawyers are trying to do is foist a responsibility that they have to uphold the First Amendment onto private companies that are under no such obligation. Why are these legal eagles so up in arms? The simple answer is the decision made by many platforms to silence voices that violate the expressed policies of the platforms they’re using.

Chief among these is Alex Jones — who has claimed that the Sandy Hook school shooting was a hoax and accused victims of the Parkland school shooting of being crisis actors.

Last month a number of those social media platforms that distributed Jones finally decided that enough was enough.

The decision to boot Jones is their prerogative as private companies. While Jones has the right to shout whatever he wants from a soapbox in free speech alley (or a back alley, or into a tin can) — and while he can’t be prosecuted for anything that he says (no matter how offensive, absurd or insane) — he doesn’t have the right to have his opinions automatically amplified by every social media platform.

Almost all of the big networking platforms have come to that conclusion.

The technology-lobbying body has already issued a statement excoriating the Department of Justice for its ham-handed approach.

[The] U.S. Department of Justice (DOJ) today released a statement saying that it was convening state attorneys general to discuss its concerns that these companies were “hurting competition and intentionally stifling the free exchange of ideas.” Social media platforms have the right to determine what types of legal speech they will permit on their platforms. It is inappropriate for the federal government to use the threat of law enforcement to limit companies from exercising this right. In particular, law enforcement should not threaten social media companies with unwarranted investigations for their efforts to rid their platforms of extremists who incite hate and violence.

While the Justice Department’s approach muddies the waters and makes it more difficult for legitimate criticism and reasoned regulation of the social media platforms to take hold, there are legitimate issues that legislators need to address.

Indeed, many of them were raised in a white paper from Senator Mark Warner, which was released in the dog days of summer.

Or the Justice Department could focus on the issue that Senator Ron Wyden emphasized in the hours after the hearing:

Personal data is now the weapon of choice for political influence campaigns and we must NOT make it easier for foreign adversaries to seize these weapons. Beefing up protections and controls on personal privacy needs to be a national security priority. pic.twitter.com/FmlcK7fRWF — Ron Wyden (@RonWyden) September 5, 2018

Instead of focusing on privacy or security, attorneys general for the government are waging a Pyrrhic war against censorship that doesn’t exist and ignoring the real cold war for platform security.",Social Media,TechCrunch,https://techcrunch.com/2018/09/05/justice-departments-threat-to-social-media-giants-is-wrong/,"The Justice Department's attempts to force social media companies to allow all speech on their platforms, regardless of their policies, is misguided and potentially illegal. This puts these companies in an untenable position and distracts from the need for more robust privacy and security measures to protect users from malicious foreign influence campaigns.",Security & Privacy
530,"Cable Traffic: WikiLeaks, Facebook, and You","Part of what comes with the State Department retaining the name “cables” for its now-electronic internal messages is the expectation of secrecy. The various classifications of cables—top secret, secret, noforn, secret/noform, confidential—remind me of the privacy settings for information shared on Facebook. You might let only certain friends see your e-mail address and phone number, but friends of friends can see your status updates, and everyone can see your Wall.

These gradations of secrecy or privacy permit the illusion that you can say something you wouldn’t say to the whole world. And yet, the data is all in Facebook’s servers, and the company has a history of expanding its user base, changing its policies, and revising its defaults, with the collective effect of its users sharing information with more people than they intended or expected to. Revelations about what a Saudi sheikh really thinks about Israel and Iran have broader repercussions than the photos of a bong-wielding Ivy Leaguer with her heart set on a career in banking. Both are open secrets, at least in certain circles, and the betrayal or horror the subjects both feel once their private words and actions spiral out of control are probably quite similar.",Social Media,WIRED,https://www.wired.com/2010/12/cable-traffic-wikileaks-facebook-and-you/,"The consequences of Social Media can be dire, as private words and actions can easily get shared with more people than expected, leading to negative repercussions for the subjects not only personally, but politically as well.",Security & Privacy
531,Elon Musk dunks on Facebook and recommends Signal in wake of US Capitol insurrection attempt – TechCrunch,"Elon Musk, the tech billionaire set to likely soon become the world’s richest man, and one of the most influential voices in the world of tech entrepreneurship, continued his recent trend of criticizing Facebook with a Twitter post late Wednesday night, following the attempted insurrection by pro-Trump rioters at the U.S. Capitol building. Musk shared a meme suggesting the founding of Facebook ultimately led to the day’s disastrous and shameful events.

This is called the domino effect pic.twitter.com/qpbEW54RvM — Elon Musk (@elonmusk) January 7, 2021

Musk, who has himself used his massive reach (he has around 42.5 million followers on Twitter) to spread misinformation to his many followers, specifically around COVID-19 and its severity, also followed that up on Thursday morning with a reply expressing a lack of surprise at WhatsApp’s new Terms of Service and Privacy Policy, which will make sharing data from WhatsApp users back to Facebook mandatory for all on the platform.

The Tesla and SpaceX CEO also recommended that people instead use Signal, an encrypted messaging client that uses encryption by default and which is based on open-source standards. Side-note: If you do end up following Musk’s advice, you should also enable the app’s “disappearing messages” feature for an added layer of protection on both ends of the conversation.

Musk has a long history of opposing the use of Facebook, including the deletion of not just his own personal page, but also those of both Tesla and SpaceX, in 2018 during the original #deletefacebook campaign following the revelation of the Cambridge Analytica scandal.",Social Media,TechCrunch,https://techcrunch.com/2021/01/07/elon-musk-dunks-on-facebook-and-recommends-signal-in-wake-of-u-s-capitol-insurrection-attempt/,"Elon Musk has recently expressed concern about the potential for social media platforms like Facebook to lead to dangerous outcomes, such as the attempted insurrection at the US Capitol. His suggestion to avoid the platform's data sharing policies is to instead use an encrypted messaging app, such as Signal. He has also been a vocal critic of Facebook's implications in the",Security & Privacy
532,Facebook’s weapon amid chaos and controversy: misdirection – TechCrunch,"The New York Times’ bombshell report into the past three years at Facebook paint a grotesque picture of the company’s attempts to navigate a string of high-profile controversies by using unsavory, unethical and dark PR tactics.

The Times’ report, citing more than 50 sources, accuses the company of:

employing a Republican opposition research firm to “discredit activist protesters,” in part by linking them to the liberal billionaire George Soros;

using its business relationships to lobby a Jewish civil rights group to flag critics and protesters as anti-Semitic;

attempted to shift anti-Facebook rhetoric against its rivals to soak up the blame by planting stories with reporters;

posting “less specific” carefully crafted posts about Russian election interference amid claims that the company was slow to act;

and urging its senior staff to switch to Android (which Facebook denies) after Apple chief executive Tim Cook made critical remarks about Facebook’s data practices

Not a good look at all. The whole report is worth a read. Facebook responded with its own version of events, calling out “a number of inaccuracies” in the Times’ report.

Facebook, to be fair, has had a rough few years. To be unfair, much of it was of its own making. The Cambridge Analytica scandal. A firehose of criticism over its data practices and privacy issues. Election interference. Its involvement in Myanmar’s genocide. And a major data breach.

And then misinformation, misinformation and misinformation.

Facebook has shown that it can’t keep its users safe.

But instead of tackling the fires it had created for itself, the company took to discrediting and deflecting in an effort to distance or absolve itself from the responsibility of the mess that it helped create.

Facebook had an uncanny ability to throw out good headlines amid chaos. A day after a lawsuit accused the company of inflating its video figures that put some newsrooms out of business, a stream of headlines (including from TechCrunch!) from Facebook’s makeshift election war room pushed any lingering headlines to the bottom of the pile. With just weeks to go before the midterms, Facebook wanted to paint some good news that it was working to pilot better election campaign security efforts, even though critics said it was way too late. Every opportunity it got to say it took down some misinformation or “inauthentic behavior,” it took it — a mea culpa for its role in failing to prevent the spread of misinformation during the 2016 presidential election, or a cheap way to get some quick, positive headlines? Even the debut of its camera-enabled Facebook Portal product was tone-deaf, announced the same week as its data breach.

Coincidence? Maybe. Suspect? Definitely.

I really hope nobody is buying all this daft Facebook PR. First their undefinable ""War Room"" that's no more than a sign on a door, now a psuedo-celebrity hire to push the notion they even recognise ""global affairs"". All a smokescreen for doing nothing. — Peter Gothard (@petergothard) October 19, 2018

The health of the company — particularly its leadership — doesn’t look good.

The Times’ report is going to reignite needed conversation about whether the executive duopoly, chief executive Mark Zuckerberg and chief operating officer Sheryl Sandberg, are fit to keep running the company. Zuckerberg, who has about 60 percent of voting power, will make it near impossible to remove him from his leadership position.

This time, an apology tour isn’t going to cut it.",Social Media,TechCrunch,https://techcrunch.com/2018/11/15/chaos-controversy-facebook-fights-misinformation-with-misdirection/,"The New York Times' report paints a grim picture of Facebook's attempts to manipulate the public and deflect blame away from its own misdeeds, such as its involvement in election interference and the Cambridge Analytica scandal. This has raised serious doubts about whether the company's executive duopoly, Mark Zuckerberg and Sheryl Sandberg, are",Security & Privacy
533,Why Facebook’s angry emoji should interest the US SEC – TechCrunch,"Facebook may have a new name, but a rebrand won’t erase the multiple recent disclosures illustrating how destructive the company is for society — and how harmful it is for its own investors.

The revelations from Facebook whistleblower Frances Haugen were shocking but not surprising. According to Haugen, who worked for the tech giant on election security matters before leaving and turning over a large cache of documents to the press, lawmakers and regulators, Facebook consistently knew that its algorithms were harming society and the vulnerable, with reprehensible actions like pushing so-called “thin-spiration” to teenage girls, which can increase the likelihood of anorexia.

A recent analysis of internal Facebook documents showed that Facebook engineers treated emoji reactions — including the “angry” emoji — as five times more valuable than “likes,” favoring controversial posts to keep users engaged — and profits flowing.

This isn’t just a story of a company acting against the public interest and harming its own consumers; it is also a story about one acting against its investors. According to Haugen, the company misled its shareholders about basic business facts, from how it addresses safety to the size of its user base.

By serially failing to tell investors this kind of critical information, Facebook may have violated our securities laws and regulations. And Haugen has filed at least eight complaints with the Securities and Exchange Commission alleging that Facebook has broken the law for withholding material information related to the company’s internal research.

Meanwhile, a second unnamed whistleblower provided an affidavit to the SEC alleging that Facebook prioritizes growth and profits over hate speech and misinformation.

As notable as whistleblower allegations are, it is surprising that the role the SEC could play in regulating and containing Facebook hasn’t received top billing. While Facebook is a tech company, it is first and foremost a publicly traded company, and therefore subject to SEC regulations and oversight.

It was the Obama-era SEC that originally approved Facebook’s 2012 initial public offering although the process included a strange special class of shares that continued to give Mark Zuckerberg total effective control over the company even after it became public. Additionally, it was the Trump-era SEC that settled with Facebook over its failure to properly disclose to investors that it knew the scandalous data company Cambridge Analytica had accessed and misused the Facebook data of approximately 30 million Americans.

Asking the SEC to take a closer look at Facebook, a company that has settled with the commission within the last two and a half years over securities law violations, should be ordinary, reasonable and what we expect of our securities regulator.

Americans are lucky that the Biden administration has placed strong regulators in charge of the SEC and Federal Trade Commission by appointing Gary Gensler and Lina Khan their respective chairs. But just as Facebook and the other Big Tech behemoths have come to touch all aspects of our economy, politics and everyday life, the task of properly regulating and containing these firms is larger than any one or two agencies.

What we need to truly tackle the questions and threats posed by large tech firms is a whole-of-government approach. The Biden administration has made a good first step with its Council on Competition, but this must represent the first cut rather than a final product. It also means refusing to give large tech firms access to new markets to dominate, from fintech to currency to special government contracts, understanding that these companies would almost certainly use these opportunities to grow even more powerful at the expense of smaller firms and consumers.

We also need the engagement of Congress on these critical questions. On the same day that Haugen was testifying before a Senate subcommittee, the House Financial Services Committee held a hearing on SEC oversight. Despite several days of coverage of Haugen’s incendiary claims, the hearing went for hours without a single comment or question about the SEC’s role in holding Facebook accountable to its investors.

The failure of this key oversight opportunity represents a lack of imagination and coordination. To truly address the dangers of Big Tech, we need every member to be thinking about remedies for dealing with these gigantic companies, including pushing the Biden administration to do more.

Haugen is not the first whistleblower to emerge from Facebook, nor will she be the last. The time has passed for the U.S. federal government to meekly watch as Big Tech employees, shareholders, contractors and even the founders lay out the dangers these companies pose to Americans.

As the companies grow in size, power and danger, now is the time for the Biden administration to act boldly, aggressively and cohesively, beginning with the SEC taking up the cause and thoroughly investigating Facebook and fully enforcing our laws.",Social Media,TechCrunch,https://techcrunch.com/2021/10/29/why-facebooks-angry-emoji-should-interest-the-us-sec/,"Facebook's multiple recent disclosures illustrate how destructive it is for society and harmful for its investors, as internal documents show that the company prioritizes growth and profits over user safety and wellbeing. The SEC must thoroughly investigate Facebook and enforce laws to contain the dangers posed by Big Tech.",Equality & Justice
534,It’s still easy to find coronavirus mask ads on Facebook – TechCrunch,"Ads for face masks are still appearing on Facebook, Instagram and Google, according to a review of the platforms carried out by the Tech Transparency Project (TTP). This despite pledges by the platforms that they would stamp out ads which seek to profit from the coronavirus pandemic.

Facebook said on March 6 that it would temporarily ban commerce listings and advertisements for medical face masks, in an effort to combat price-gouging and misinformation during the COVID-19 crisis.

Google followed suit a few days later, saying it would temporarily ban all medical face mask ads “out of an abundance of caution.”

The risk of online misinformation exacerbating a global public health crisis has been front of mind for policymakers in many Western markets. Meanwhile front-line medical staff continue to face shortages of vital personal protective equipment, such as N95 masks, as they battle rising rates of infection.

There has also been concern that online sellers are attempting to cash in on a public health crisis by price gouging and/or targeting internet users with ads for substandard masks.

Early last week two senators urged the U.S.’ FTC to act, blasting Google for continuing to allow ads for face masks to be shown to internet users.

A week later and ads are still circulating.

The TTP — a research project by the nonprofit Campaign for Accountability, a group which focuses on exposing misconduct and malfeasance in public life — reported finding web users still being targeted with face mask ads on Google this week.

It also conducted a review of Facebook and Instagram, and was able to find more than 130 pages on Facebook listing masks for sale, including some using the platform’s e-commerce tools.

“One Facebook Page called ‘CoronaVirus Mask’ offers a ‘respiratory mask collection,’ with prices ranging from $32 to $37, and uses Facebook’s ‘Shop’ feature to display its merchandise and allow people to add purchases to their cart,” it writes in a blog post. “Facebook’s ‘check out on website’ button then directs users to complete the purchase on the seller’s website.”

“Facebook pages that use WhatsApp to establish contact with buyers are employing a tactic commonly used by wildlife and other traffickers, who often display goods on Facebook and then arrange the actual purchase through WhatsApp encrypted messages. The Facebook Page ‘Surgical Face Mask For Sale,’ for example, has a video showing boxes of medical masks and the seller’s WhatsApp number scrawled on a piece of paper,” it added.

“A visit to one of these Facebook pages often triggers recommendations for other pages selling face masks, a sign that the platform’s algorithms are actually amplifying the reach of these sketchy sellers. TTP, without logging into Facebook, went to the page for ‘Corona Mask Shop’ and was served up ‘Related Pages’ for ‘Corona Mask 247’ and ‘Corona MASK on sale.’ ”

TechCrunch conducted our own searches on Facebook today and while some obvious search terms returned no results a little tweaking of keywords choice and we were quickly able to find additional pages hawking face masks — such as the below example grabbed from a Facebook page calling itself “Face Mask Manufacturer.”

From this page Facebook’s algorithm then recommended more pages — with names like “Medical Masks” and “Dispo mask for sale” — which also appeared to be selling masks.

The TTP’s review also found mask ads circulating on Facebook-owned Instagram.

“One Instagram account for @coronavsmask reads, ‘Act now before it’s too late! GET your N95 Respiratory Face Mask NOW!’ It only has a single post but already counts over 6,300 followers,” it wrote. “An account created on March 14 called @handsanitizers_and_coronamask includes over a dozen posts offering such products.”

It also found “several” Instagram accounts that sell drugs had begun to incorporate medical face masks into their offerings.

At the time of writing Facebook had not responded to our request for comment on the findings. Update: The company has now sent this statement, attributed to a Facebook spokesperson:

Facebook is focused on preventing exploitation of this crisis for financial gain. Since COVID-19 was declared a public health emergency, Facebook has removed millions of ads and commerce listings for the sale of masks, hand sanitiser, surface disinfecting wipes and COVID-19 test kits. While enforcement is not perfect, we have put several automated detection mechanisms in place to block or remove this material from our platform.

In further searches the group was reproduced examples of Google’s third-party advertising display network serving ads for face masks alongside news stories related to the coronavirus — an issue highlighted by Sen. Mark Warner in a tweet last week when he blasted the company for “still running ads for facemasks and other coronavirus scams.”

Why is @Google still running ads for facemasks and other coronavirus scams? Despite promises from the company, all it takes is one Google search for ""coronavirus"" and ""mask"" and this is what you get. pic.twitter.com/2UsqviuQzt — Mark Warner (@MarkWarner) March 18, 2020

“The Facebook mask pages were searched and collected on March 17-18 using the terms “corona mask,” “N95” and “surgical mask” in Facebook’s search function,” a TTP spokesman told us when asked for more info about its review. “Of the more than 130 pages identified, 43 were created in the month of March, more than a dozen of those just days before TTP ran the searches.”

“We don’t have the same level of data from Instagram/Google. Instagram’s search function does not lend itself to the same search ability; it doesn’t bring up a list of accounts based on a single term like Facebook’s search function does. With Google, our goal was to show examples of Google-served ads; those were identified in news stories on March 18,” he added.

We reached out to Google for comment on the findings and a spokesman told us the company has a dedicated task force that has removed “millions” of ads in the past week alone — which he said had already led to a sharp decrease in face mask ads. But Google said “opportunistic advertisers” had been trying to run “an unprecedented number” of these ads on its platforms.

Here’s Google’s statement:

Since January, we’ve blocked ads for products that aim to capitalise on coronavirus, including a temporary ban on face mask ads. In the past few weeks, we’ve seen opportunistic advertisers try to run an unprecedented number of these ads on our platforms. We have a dedicated task force working to combat this issue and have removed millions of ads in the past week alone. We’re monitoring the situation closely and continue to make real-time adjustments to protect our users.

Google declined to specify how many people it has working to identify and remove mask ads, saying only that the taskforce is made up of members from its product, engineering, enforcement and policy teams — and that it’s been set up with coverage across time zones.

It also said the examples highlighted by TTP are already over a week old and do not reflect the impact of its newest enforcement measures.

The company told us it’s analysing both ad content and how they’re served to enhance its take-down capacity.",Social Media,TechCrunch,https://techcrunch.com/2020/03/26/its-still-easy-to-find-coronavirus-mask-ads-on-facebook/,"Social media platforms have been failing to completely stamp out ads for face masks, leading to price-gouging and misinformation during the COVID-19 crisis. This has raised concerns of online sellers attempting to cash in on the public health crisis and front-line medical staff facing shortages of vital personal protective equipment.",Equality & Justice
535,Most Americans say they can’t distinguish a social media bot from a human,"Most Americans say they can’t tell social media bots from real humans, and most are convinced bots are bad, according to a new study from Pew Research Center. Only 47 percent of Americans are somewhat confident they can identify social media bots from real humans. In contrast, most Americans surveyed in a study about fake news were confident they could identify false stories.

The Pew study is an uncommon look at what the average person thinks about these automated accounts that plague social media platforms. After surveying over 4,500 adults in the US, Pew found that most people actually don’t know much about bots. Two-thirds of Americans have at least heard of social media bots, but only 16 percent say they’ve heard a lot about them, while 34 percent say they’ve never heard of them at all. The knowledgeable tend to be younger, and men are more likely than women (by 22 percentage points) to say they’ve heard of bots. Since the survey results are self-reported, there’s a chance people are overstating or understating their knowledge of bots.

80 percent of people think bots are bad

Of those who have heard of bots, 80 percent say the accounts are used for bad purposes. Regardless of whether a person is a Republican or Democrat or young or old, most think that bots are bad. And the more that a person knows about social media bots, the less supportive they are of bots being used for various purposes, like activists drawing attention to topics or a political party using bots to promote candidates.

Overall, the report gives the sense that most Americans are worried and can’t do much to identify bots. Still, researchers have already come up with tools to combat bots. One of the more recent ones is an algorithm proposed by MIT Sloan academics that would distinguish bots from humans based on how they interact with other accounts. The general idea is that if an account is very active and others aren’t talking to it, it could be a bot.",Social Media,Verge,https://www.theverge.com/2018/10/15/17980026/social-media-bot-human-difference-ai-study,"Most Americans are concerned about social media bots, with 80% believing they are used for bad purposes. The study highlights the difficulty of recognizing bots, with only 47% of Americans being confident they can identify them. This has negative implications for the spread of misinformation and other malicious activities on social media.","Information, Discourse & Governance"
536,It's Time to Defund Social Media,"First, it would minimize the incentive to be an asshole. If you’re not rewarding people with clicks and likes for antagonistic behaviors, there’s less reason for them to keep doing it. This is a dynamic as old as trolldom. As long as something generates capital—whether economic or social—there’s no reason to stop. In fact, one’s livelihood might depend on keeping it up, and doing it even worse the next time.

Second, foregrounding the good-faith majority short-circuits the amplification feedback loops that normalize harm. I made this argument back in April in response to the anti-quarantine protests: when you frame a fringe movement as a mainstream one, it has a funny tendency to become exactly that. In the case of masks, propagating the anti-maskers’ arguments, even to condemn them, risks spreading those arguments to even more people who might be sympathetic. At the very least, it muddies the issue—if so many people are fighting about masks, does that mean there’s something here to fight about?

Another structural cause of our informational woes is embedded in straightforward-seeming ways to fix them. One of the most common is the assumption that calling attention to a harm will help to mitigate it; this is sometimes referred to as the “sunlight disinfects” model of media. All we need to do is show that the bad thing is happening—that Karen is at it again—and let the marketplace of ideas, that great Costco in the sky, handle the rest. People will use their critical thinking skills to compare being a Karen with not being a Karen, and the result will be fewer Karens. The problem is, the people most likely to arrive at this conclusion are the ones who already agree. Sharing mask freakout videos, or other content spotlighting anti-maskers, still amplifies their messages, however, looping us right back to all the ways the attention economy incentivizes the tyranny of the loudest. Such a system isn’t just good for Karens; it was built for Karens.

Fact-checking is another idea that sounds good on paper but is quite tricky in practice. Many approach the spread of false or misleading information as a case of people not having all the facts. If we only said the facts more loudly, we could stop the flow of bad information. In reality, the people who see masks as an encroachment on their rights, who think the threat of the virus has been overblown, or that Anthony Fauci is actually Bill Gates in a George Soros mask, don’t arrive at those conclusions because they’re low-information rubes. They’re often steeped in information. That information, however, is filtered through what Ryan Milner and I call deep memetic frames: sense-making apparatuses that structure how people see the world, and the ways that they respond to it.

As Milner and I illustrate throughout our book, fact checks aimed at deep memetic frames rarely have the intended effect—you can trace this from the Satanic Panics of the 1980s and 1990s to QAnon. The precise reasons why are complicated; research around the efficacy of fact checking is, let’s say, mixed. What is clear is that throwing facts at falsehood doesn’t magically change hearts and minds. If it did, we wouldn’t be in this mess.

So what’s the best way forward? How do we avoid pushing an already terrible situation to an even worse place? The answer is fundamental structural change. We need to reimagine what our networks can and should be. We need to put justice over profits. We need to defund social media. Individual people can’t do that on their own, of course. Even journalists are limited in the effects they can personally have; everyone’s a dollar sign to someone up the chain. Still, by identifying the systems we’re all embedded within and considering how those systems are fundamentally part of our problems, we can make choices—about the things we publicize, who we share them with, how we choose to frame them—that, at the very least, actively resist information dysfunction, rather than greasing its wheels.

Photographs: Duncan Andison/Getty images; Brendan O'Sullivan/Getty Images; Allen J. Schaben/Los Angeles Times/Getty Images",Social Media,WIRED,https://www.wired.com/story/its-time-to-defund-social-media/,"Social media has become a platform for the loudest and most antagonistic voices, and propagating their arguments, even to condemn them, risks spreading those arguments to even more people, normalizing harm and creating an incentive to be an asshole.","Information, Discourse & Governance"
537,The aftermath of the Parkland mass shooting exemplifies the ugly side of social media,"Uncovering and explaining how our digital world is changing — and changing us.

Want a perfect example of the kind of content challenges Facebook and Twitter are up against? Just look at what happened over the past few days in the aftermath of the mass shooting last week in Parkland, Fla., which left 17 dead and has reignited discussion about gun control in America.

In the week since, we’ve seen the worst social media has to offer.

Russian bots on Twitter tried to create animosity among critics and advocates of the Second Amendment. High school students who survived the shooting have been mocked online for standing up to politicians and calling for gun control. And now conspiracy theories are circulating on Facebook and Twitter to try and tear down those same students, calling them “crisis actors” and suggesting they’re puppets for liberal politicians.

What we’ve learned from Parkland is that, even in the wake of tragedy, divisive and troubling content still thrives on social media platforms. No one is safe from mockery and ridicule, including children and teenagers. And it’s not entirely clear what anyone can do about it.

Here’s one example: Conservative political commentator and author Dinesh D’Souza mocked grieving Parkland High School students on Twitter.

Worst news since their parents told them to get summer jobs https://t.co/Vg3mXYvb4c — Dinesh D'Souza (@DineshDSouza) February 20, 2018

Here’s another, calling Parkland student David Hogg — who has been identified as one of the most vocal and visible students from the school — an “attention whore.”

The tweets are insensitive and embarrassing. Are they against Twitter’s user guidelines? Probably not. But Twitter is full of tweets like these, stuff that feels gross or mean or uncomfortable, but doesn’t merit any formal action from the company because it’s not necessarily threatening or abusive.

This is how absurd, gaslighting ""crisis actor"" theories go viral.



One @facebook post from this person has 111,000+ shares. Another has 23,000.



This is one person, two posts.



Imagine the millions and millions of people crackpot theories like this are reaching and influencing. pic.twitter.com/VU7cKCJhXq — Micah Grimes (@MicahGrimes) February 20, 2018

It’s one of the tough challenges Facebook and Twitter deal with, and one of the reasons social media can feel like such an ugly and discouraging place.

We are KIDS - not actors. We are KIDS that have grown up in Parkland all of our lives. We are KIDS who feared for our lives while someone shot up our school. We are KIDS working to prevent this from happening again. WE ARE KIDS. — Jaclyn Corin (@JaclynCorin) February 20, 2018

We’ve also seen conspiracy theories run rampant. One Facebook user posted that Hogg was a “crisis actor” and not an actual Parkland student. The conspiracy post was supposedly shared more than 110,000 times in six hours, according to a screenshot from NBC News’s Micah Grimes. (It appears that Facebook has since removed the offending account.)

This other Facebook video, which calls Hogg a “crisis actor scumbag,” has more than 20,000 views. Even the President’s son, Donald Trump Jr, liked a tweet that suggested Hogg was “running cover” for his father, who is apparently a former FBI agent.

The harassment got bad enough that Marco Rubio, the Republican Senator from Florida, had to come out on Twitter in defense of Parkland’s students.

Claiming some of the students on tv after #Parkland are actors is the work of a disgusting group of idiots with no sense of decency — Marco Rubio (@marcorubio) February 20, 2018

Part of the problem is that Facebook’s and Twitter’s algorithms are set up to reward posts that receive lots of shares or comments — “engagement” that is considered a signal to show those videos to even more people. Even if Facebook can catch videos on the same day they’re posted, the Hogg video is evidence that misinformation can and will still spread like wildfire before anyone has the chance to take it down.

The Parkland shooting has created a perfect storm for the social media world we now live in. There have been positives — Facebook and Twitter have given high school students an incredible megaphone to come out and push for stricter gun control regulations. Teenagers who might otherwise be ignored are sending messages that are reaching millions and, in some cases, publicly challenging elected officials in an effort to get something done.

17 of my classmates are gone. That’s 17 futures, 17 children, and 17 friends stolen. But you’re right, it always has to be about you. How silly of me to forget. #neveragain https://t.co/i6Hldlo0Aq — Aly Sheehy (@Aly_Sheehy) February 18, 2018

I couldn’t be prouder to be a member of the #NeverAgain movement. We are receiving an unimaginable amount of support from all over the world and this is just the beginning. This is for the 17. ❤️ #MarchForOurLives — Sofie Whitney (@sofiewhitney) February 20, 2018

But it’s hard to have to witness the ugly side of social media. And in the wake of tragedy, it looks uglier than ever.",Social Media,Verge,https://www.theverge.com/2018/2/21/17037044/the-aftermath-of-the-parkland-mass-shooting-shows-social-media-s,"In the wake of the Parkland shooting, social media has presented the worst side of itself, with Russian bots, conspiracy theories, and trolls mocking and attacking the high school students who survived the shooting and are calling for gun control. This is a prime example of how digital platforms can be abused and highlight how digital platforms need to better manage the",Security & Privacy
538,Study: Russian Twitter bots sent 45k Brexit tweets close to vote – TechCrunch,"To what extent — and how successfully — did Russian backed agents use social media to influence the UK’s Brexit vote? Yesterday Facebook admitted it had linked some Russian accounts to Brexit-related ad buys and/or the spread of political misinformation on its platform, though it hasn’t yet disclosed how many accounts were involved or how many rubles were spent.

Today the The Times reported on research conducted by a group of data scientists in the US and UK looking at how information was diffused on Twitter around the June 2016 EU referendum vote, and around the 2016 US presidential election.

The Times reports that the study tracked 156,252 Russian accounts which mentioned #Brexit, and also found Russian accounts posted almost 45,000 messages pertaining to the EU referendum in the 48 hours around the vote.

Although Tho Pham, one of the report authors, confirmed to us in an email that the majority of those Brexit tweets were posted on June 24, 2016, the day after the vote — when around 39,000 Brexit tweets were posted by Russian accounts, according to the analysis.

But in the run up to the referendum vote they also generally found that human Twitter users were more likely to spread pro-leave Russian bot content via retweets (vs pro-remain content) — amplifying its potential impact.

From the research paper:

During the Referendum day, there is a sign that bots attempted to spread more leave messages with positive sentiment as the number of leave tweets with positive sentiment increased dramatically on that day. More specifically, for every 100 bots’ tweets that were retweeted, about 80-90 tweets were made by humans. Furthermore, before the Referendum day, among those humans’ retweets from bots, tweets by the Leave side accounted for about 50% of retweets while only nearly 20% of retweets had pro-remain content. In the other words, there is a sign that during pre-event period, humans tended to spread the leave messages that were originally generated by bots. Similar trend is observed for the US Election sample. Before the Election Day, about 80% of retweets were in favour of Trump while only 20% of retweets were supporting Clinton.

You do have to wonder whether Brexit wasn’t something of a dry run disinformation campaign for Russian bots ahead of the US election a few months later.

The research paper, entitled Social media, sentiment and public opinions: Evidence from #Brexit and #USElection, which is authored by three data scientists from Swansea University and the University of California, Berkeley, used Twitter’s API to obtain relevant datasets of tweets to analyze.

After screening, their dataset for the EU referendum contained about 28.6M tweets, while the sample for the US presidential election contained ~181.6M tweets.

The researchers say they identified a Twitter account as Russian-related if it had Russian as the profile language but the Brexit tweets were in English.

While they detected bot accounts (defined by them as Twitter users displaying ‘botlike’ behavior) using a method that includes scoring each account on a range of factors such as whether it tweeted at unusual hours; the volume of tweets including vs account age; and whether it was posting the same content per day.

Around the US election, the researchers generally found a more sustained use of politically motivated bots vs around the EU referendum vote (when bot tweets peaked very close to the vote itself).

They write:

First, there is a clear difference in the volume of Russian-related tweets between Brexit sample and US Election sample. For the Referendum, the massive number of Russian-related tweets were only created few days before the voting day, reached its peak during the voting and result days then dropped immediately afterwards. In contrast, Russian-related tweets existed both before and after the Election Day. Second, during the running up to the Election, the number of bots’ Russian-related tweets dominated the ones created by humans while the difference is not significant during other times. Third, after the Election, bots’ Russian-related tweets dropped sharply before the new wave of tweets was created. These observations suggest that bots might be used for specific purposes during high-impact events.

In each data set, they found bots typically more often tweeting pro-Trump and pro-leave views vs pro-Clinton and pro-remain views, respectively.

They also say they found similarities in how quickly information was disseminated around each of the two events, and in how human Twitter users interacted with bots — with human users tending to retweet bots that expressed sentiments they also supported. The researchers say this supports the view of Twitter creating networked echo chambers of opinion as users fix on and amplify only opinions that align with their own, avoiding engaging with different views.

Combine that echo chamber effect with deliberate deployment of politically motivated bot accounts and the platform can be used to enhance social divisions, they suggest.

From the paper:

These results lend supports to the echo chambers view that Twitter creates networks for individuals sharing the similar political beliefs. As the results, they tend to interact with others from the same communities and thus their beliefs are reinforced. By contrast, information from outsiders is more likely to be ignored. This, coupled by the aggressive use of Twitter bots during the high-impact events, leads to the likelihood that bots are used to provide humans with the information that closely matches their political views. Consequently, ideological polarization in social media like Twitter is enhanced. More interestingly, we observe that the influence of pro-leave bots is stronger the influence of pro-remain bots. Similarly, pro-Trump bots are more influential than pro-Clinton bots. Thus, to some degree, the use of social bots might drive the outcomes of Brexit and the US Election. In summary, social media could indeed affect public opinions in new ways. Specifically, social bots could spread and amplify misinformation thus influence what humans think about a given issue. Moreover, social media users are more likely to believe (or even embrace) fake news or unreliable information which is in line their opinions. At the same time, these users distance from reliable information sources reporting news that contradicts their beliefs. As a result, information polarization is increased, which makes reaching consensus on important public

issues more difficult.

Discussing the key implications of the research, they describe social media as “a communication platform between government and the citizenry”, and say it could act as a layer for government to gather public views to feed into policymaking.

However they also warn of the risks of “lies and manipulations” being dumped onto these platforms in a deliberate attempt to misinform the public and skew opinions and democratic outcomes — suggesting regulation to prevent abuse of bots may be necessary.

They conclude:

Recent political events (the Brexit Referendum and the US Presidential Election) have observed the use of social bots in spreading fake news and misinformation. This, coupled by the echo chambers nature of social media, might lead to the case that bots could shape public opinions in negative ways. If so, policy-makers should consider mechanisms to prevent abuse of bots in the future.

Commenting on the research in a statement, a Twitter spokesperson told us: “Twitter recognizes that the integrity of the election process itself is integral to the health of a democracy. As such, we will continue to support formal investigations by government authorities into election interference where required.”

Its general critique of external bot analysis conducted via data pulled from its API is that researchers are not privy to the full picture as the data stream does not provide visibility of its enforcement actions, nor on the settings for individual users which might be surfacing or suppressing certain content.

The company also notes that it has been adapting its automated systems to pick up suspicious patterns of behavior, and claims these systems now catch more than 3.2M suspicious accounts globally per week.

Since June 2017, it also claims it’s been able to detect an average of 130,000 accounts per day that are attempting to manipulate Trends — and says it’s taken steps to prevent that impact. (Though it’s not clear exactly what that enforcement action is.)

Since June it also says it’s suspended more than 117,000 malicious applications for abusing its API — and say the apps were collectively responsible for more than 1.5BN “low-quality tweets” this year.

It also says it has built systems to identify suspicious attempts to log in to Twitter, including signs that a login may be automated or scripted — techniques it claims now help it catch about 450,000 suspicious logins per day.

The Twitter spokesman noted a raft of other changes it says it’s been making to try to tackle negative forms of automation, including spam. Though he also flagged the point that not all bots are bad. Some can be distributing public safety information, for example.

Even so, there’s no doubt Twitter and social media giants in general remain in the political hotspot, with Twitter, Facebook and Google facing a barrage of awkward questions from US lawmakers as part of a congressional investigation probing manipulation of the 2016 US presidential election.

A UK parliamentary committee is also currently investigating the issue of fake news, and the MP leading that probe recently wrote to Facebook and Twitter to ask them to provide data about activity on their platforms around the Brexit vote.

And while it’s great that tech platforms finally appear to be waking up to the disinformation problem their technology has been enabling, in the case of these two major political events — Brexit and the 2016 US election — any action they have since taken to try to mitigate bot-fueled disinformation obviously comes too late.

While citizens in the US and the UK are left to live with the results of votes that appear to have been directly influenced by Russian agents using US tech tools.

Today, Ciaran Martin, the CEO of the UK’s National Cyber Security Centre (NCSC) — a branch of domestic security agency GCHQ — made public comments stating that Russian cyber operatives have attacked the UK’s media, telecommunications and energy sectors over the past year.

This follow public remarks by the UK prime minister Theresa May yesterday, who directly accused Russia’s Vladimir Putin of seeking to “weaponize information” and plant fake stories.

The NCSC is “actively engaging with international partners, industry and civil society” to tackle the threat from Russia, added Martin (via Reuters).

Asked for a view on whether governments should now be considering regulating bots if they are actively being used to drive social division, Paul Bernal, a lecturer in information technology at the University of East Anglia, suggested top down regulation may be inevitable.

“I’ve been thinking about that exact question. In the end, I think we may need to,” he told TechCrunch. “Twitter needs to find a way to label bots as bots — but that means they have to identify them first, and that’s not as easy as it seems.

“I’m wondering if you could have an ID on twitter that’s a bot some of the time and human some of the time. The troll farms get different people to operate an ID at different times — would those be covered? In the end, if Twitter doesn’t find a solution themselves, I suspect regulation will happen anyway.”",Social Media,TechCrunch,https://techcrunch.com/2017/11/15/study-russian-twitter-bots-sent-45k-brexit-tweets-close-to-vote/,"The use of politically motivated bots on social media can lead to ideological polarization, which makes reaching consensus on important public issues more difficult. Government regulation may be necessary to prevent the abuse of these bots and their potential to spread fake news and misinformation.","Information, Discourse & Governance"
539,"It was not consent, it was concealment – TechCrunch","Facebook’s response to the clutch of users who are suddenly woke — triggered to delve into their settings by the Facebook data misuse scandal and #DeleteFacebook backlash — to the fact the social behemoth is, quietly and continuously, harvesting sensitive personal data about them and their friends tells you everything you need to know about the rotten state of tech industry ad-supported business models.

Want to freak yourself out? I'm gonna show just how much of your information the likes of Facebook and Google store about you without you even realising it — Dylan Curran (@iamdylancurran) March 24, 2018

“People have to expressly agree to use this feature,” the company wrote in a defensively worded blog post at the weekend, defending how it tracks some users’ SMS and phone call metadata — a post it had the impressive brass neck to self-describe as a “fact check”.

“Call and text history logging is part of an opt-in feature for people using Messenger or Facebook Lite on Android. This helps you find and stay connected with the people you care about, and provides you with a better experience across Facebook.”

So, tl;dr, if you’re shocked to see what Facebook knows about you, well, that’s your own dumb fault because you gave Facebook permission to harvest all that personal data.

Not just Facebook either, of course. A fair few Android users appear to be having a similarly rude awakening about how Google’s mobile platform (and apps) slurp location data pervasively — at least unless the user is very, very careful to lock everything down.

But the difficulty of A) knowing exactly what data is being collected for what purposes and B) finding the cunning concealed/intentionally obfuscated master setting which will nix all the tracking is by design, of course.

Privacy hostile design.

No accident then that Facebook has just given its settings pages a haircut — as it scrambles to rein in user outrage over the still snowballing Cambridge Analytica data misuse scandal — consolidating user privacy controls onto one screen instead of the full TWENTY they had been scattered across before.

ehem

Insert your ‘stable door being bolted’ GIF of choice right here.

Another example of Facebook’s privacy hostile design: As my TC colleague Romain Dillet pointed out last week, the company deploys misleading wording during the Messenger onboarding process which is very clearly intended to push users towards clicking on a big blue “turn on” (data-harvesting) button — inviting users to invite the metaphorical Facebook vampire over the threshold so it can perpetually suck data.

Facebook does this by implying that if they don’t bare their neck and “turn on” the continuous contacts uploading they somehow won’t be able to message any of their friends…

That’s complete nonsense of course. But opportunistic emotional blackmail is something Facebook knows a bit about — having been previously caught experimenting on users without their consent to see if it could affect their mood.

Add to that, the company has scattered its social plugins and tracking pixels all around the World Wide Web, enabling it to expand its network of surveillance signals — again, without it being entirely obvious to Internet users that Facebook is watching and recording what they are doing and liking outside its walled garden.

According to pro-privacy search engine DuckDuckGo Facebook’s trackers are on around a quarter of the top million websites. While Google’s are on a full ~three-quarters.

So you don’t even have to be a user to be pulled into this surveillance dragnet.

In its tone-deaf blog post trying to defang user concerns about its SMS/call metadata tracking, Facebook doesn’t go into any meaningful detail about exactly why it wants this granular information — merely writing vaguely that: “Contact importers are fairly common among social apps and services as a way to more easily find the people you want to connect with.”

It’s certainly not wrong that other apps and services have also been sucking up your address book.

But that doesn’t make the fact Facebook has been tracking who you’re calling and messaging — how often/for how long — any less true or horrible.

This surveillance is controversial not because Facebook gained permission to data mine your phone book and activity — which, technically speaking, it will have done, via one of the myriad socially engineered, fuzzily worded permission pop-ups starring cutesy looking cartoon characters.

But rather because the consent was not informed.

Or to put it more plainly, Facebookers had no idea what they were agreeing to let the company do.

Which is why people are so horrified now to find what the company has been routinely logging — and potentially handing over to third parties on its ad platform.

Phone calls to your ex? Of course Facebook can see them. Texts to the number of a health clinic you entered into your phonebook? Sure. How many times you phoned a law firm? Absolutely. And so on and on it goes.

This is the rude awakening that no number of defensive ‘fact checks’ from Facebook — nor indeed defensive tweet storms from current CSO Alex Stamos — will be able to smooth away.

“There are long-standing issues with organisations of all kinds, across multiple sectors, misapplying, or misunderstanding, the provisions in data protection law around data subject consent,” says data protection expert Jon Baines, an advisor at UK law firm Mishcon de Reya LLP and also chair of NADPO, when we asked what the Facebook-Cambridge Analytica data misuse scandal says about how broken the current system of online consent is.

“The current European Data Protection Directive (under which [the UK] Data Protection Act sits) says that consent means any freely given specific and informed indication of their wishes by which a data subject signifies agreement to their personal data being processed. In a situation under which a data subject legitimately later claims that they were unaware what was happening with their data, it is difficult to see how it can reasonably be said that they had “consented” to the use.”

Ironically, given recent suggestions by defunct Facebook rival Path’s founder of a latent reboot to cater to the #DeleteFacebook crowd — Path actually found itself in an uncomfortable privacy hotseat all the way back in 2012, when it was discovered to have been uploading users’ address book information without asking for permission to do so.

Having been caught with its fingers in the proverbial cookie jar, Path apologized and deleted the data.

The irony is that while Path suffered a moment of outrage, Facebook is only facing a major privacy backlash now — after it’s spent so many years calmly sucking up people’s contacts data, also without them being aware because Facebook nudged them to think they needed to tap that big blue ‘turn on’ button.

Exploiting users’ trust — and using a technicality to unhook people’s privacy — is proving pretty costly for Facebook right now though.

And the risks of attempting to hoodwink consent out of your users are about to step up sharply too, at least in Europe.

Baines points out that the EU’s updated privacy framework, GDPR, tightens the existing privacy standard — adding the words “clear affirmative act” and “unambiguous” to consent requirements.

More importantly, he notes it introduces “more stringent requirements, and certain restrictions, which are not, or are not explicit, in current law, such as the requirement to be able to demonstrate that a data subject has given (valid) consent” (emphasis his).

“Consent must also now be separable from other written agreements, and in an intelligible and easily accessible form, using clear and plain language. If these requirements are enforced by data protection supervisory authorities and the courts, then we could well see a significant shift in habits and practices,” he adds.

The GDPR framework is also backed up by a new regime of major penalties for data protection violations which can scale up to 4% of a company’s global turnover.

And the risk of fines so large will be much harder for companies to ignore — and thus playing fast and loose with data, and moving fast and breaking things (as Facebook used to say), doesn’t sound so smart anymore.

As I wrote back in 2015, the online privacy lie is unraveling.

It’s taken a little longer than I’d hoped, for sure. But here we are in 2018 — and it’s not just the #MeToo movement that’s turned consent into a buzzword.",Social Media,TechCrunch,https://techcrunch.com/2018/03/28/it-was-not-consent-it-was-concealment/,"The main undesirable consequence of social media discussed here is the exploitation of user trust by companies such as Facebook and Google who continuously harvest sensitive personal data without users being aware of what they are agreeing to. This lack of informed consent has recently been brought to light due to the Cambridge Analytica data misuse scandal, and the GDPR framework will now",Security & Privacy
540,Is America’s national security Facebook and Google’s problem? – TechCrunch,"Outrage that Facebook made the private data of over 87 million of its U.S. users available to the Trump campaign has stoked fears of big US-based technology companies are tracking our every move and misusing our personal data to manipulate us without adequate transparency, oversight, or regulation.

These legitimate concerns about the privacy threat these companies potentially pose must be balanced by an appreciation of the important role data-optimizing companies like these play in promoting our national security.

In his testimony to the combined US Senate Commerce and Judiciary Committees, Facebook CEO Mark Zuckerberg was not wrong to present his company as a last line of defense in an “ongoing arms race” with Russia and others seeking to spread disinformation and manipulate political and economic systems in the US and around the world.

The vast majority of the two billion Facebook users live outside the United States, Zuckerberg argued, and the US should be thinking of Facebook and other American companies competing with foreign rivals in “strategic and competitive” terms. Although the American public and US political leaders are rightly grappling with critical issues of privacy, we will harm ourselves if we don’t recognize the validity of Zuckerberg’s national security argument.

Examples are everywhere of big tech companies increasingly being seen as a threat. US President Trump has been on a rampage against Amazon, and multiple media outlets have called for the company to be broken up as a monopoly. A recent New York Times article, “The Case Against Google,” argued that Google is stifling competition and innovation and suggested it might be broken up as a monopoly. “It’s time to break up Facebook,” Politico argued, calling Facebook “a deeply untransparent, out-of-control company that encroaches on its users’ privacy, resists regulatory oversight and fails to police known bad actors when they abuse its platform.” US Senator Bill Nelson made a similar point when he asserted during the Senate hearings that “if Facebook and other online companies will not or cannot fix the privacy invasions, then we are going to have to. We, the Congress.”

While many concerns like these are valid, seeing big US technology companies solely in the context of fears about privacy misses the point that these companies play a far broader strategic role in America’s growing geopolitical rivalry with foreign adversaries. And while Russia is rising as a threat in cyberspace, China represents a more powerful and strategic rival in the 21st century tech convergence arms race.

Data is to the 21st century what oil was to the 20th, a key asset for driving wealth, power, and competitiveness. Only companies with access to the best algorithms and the biggest and highest quality data sets will be able to glean the insights and develop the models driving innovation forward. As Facebook’s failure to protect its users’ private information shows, these date pools are both extremely powerful and can be abused. But because countries with the leading AI and pooled data platforms will have the most thriving economies, big technology platforms are playing a more important national security role than ever in our increasingly big data-driven world.

China, which has set a goal of becoming “the world’s primary AI innovation center” by 2025, occupying “the commanding heights of AI technology” by 2030, and the “global leader” in “comprehensive national strength and international influence” by 2050, understands this. To build a world-beating AI industry, Beijing has kept American tech giants out of the Chinese market for years and stolen their intellectual property while putting massive resources into developing its own strategic technology sectors in close collaboration with national champion companies like Baidu, Alibaba, and Tencent.

Examples of China’s progress are everywhere.

Close to a billion Chinese people use Tencent’s instant communication and cashless platforms. In October 2017, Alibaba announced a three-year investment of $15 billion for developing and integrating AI and cloud-computing technologies that will power the smart cities and smart hospitals of the future. Beijing is investing $9.2 billion in the golden combination of AI and genomics to lead personalized health research to new heights. More ominously, Alibaba is prototyping a new form of ubiquitous surveillance that deploys millions of cameras equipped with facial recognition within testbed cities and another Chinese company, Cloud Walk, is using facial recognition to track individuals’ behaviors and assess their predisposition to commit a crime.

In all of these areas, China is ensuring that individual privacy protections do not get in the way of bringing together the massive data sets Chinese companies will need to lead the world. As Beijing well understands, training technologists, amassing massive high-quality data sets, and accumulating patents are key to competitive and security advantage in the 21st century.

“In the age of AI, a U.S.-China duopoly is not just inevitable, it has already arrived,” said Kai-Fu Lee, founder and CEO of Beijing-based technology investment firm Sinovation Ventures and a former top executive at Microsoft and Google. The United States should absolutely not follow China’s lead and disregard the privacy protections of our citizens. Instead, we must follow Europe’s lead and do significantly more to enhance them. But we also cannot blind ourselves to the critical importance of amassing big data sets for driving innovation, competitiveness, and national power in the future.

In its 2017 unclassified budget, the Pentagon spent about $7.4 billion on AI, big data and cloud-computing, a tiny fraction of America’s overall expenditure on AI. Clearly, winning the future will not be a government activity alone, but there is a big role government can and must play. Even though Google remains the most important AI company in the world, the U.S. still crucially lacks a coordinated national strategy on AI and emerging digital technologies. While the Trump administration has gutted the white house Office of Science and Technology Policy, proposed massive cuts to US science funding, and engaged in a sniping contest with American tech giants, the Chinese government has outlined a “military-civilian integration development strategy” to harness AI to enhance Chinese national power.

FBI Director Christopher Wray correctly pointed out that America has now entered a “whole of society” rivalry with China. If the United States thinks of our technology champions solely within our domestic national framework, we might spur some types of innovation at home while stifling other innovations that big American companies with large teams and big data sets may be better able to realize.

America will be more innovative the more we nurture a healthy ecosystem of big, AI driven companies while also empowering smaller startups and others using blockchain and other technologies to access large and disparate data pools. Because breaking up US technology giants without a sufficient analysis of both the national and international implications of this step could deal a body blow to American prosperity and global power in the 21st century, extreme caution is in order.

America’s largest technology companies cannot and should not be dragooned to participate in America’s growing geopolitical rivalry with China. Based on recent protests by Google employees against the company’s collaboration with the US defense department analyzing military drone footage, perhaps they will not.

But it would be self-defeating for American policymakers to not at least partly consider America’s tech giants in the context of the important role they play in America’s national security. America definitely needs significantly stronger regulation to foster innovation and protect privacy and civil liberties but breaking up America’s tech giants without appreciating the broader role they are serving to strengthen our national competitiveness and security would be a tragic mistake.",Social Media,TechCrunch,https://techcrunch.com/2018/04/15/is-americas-national-security-facebook-and-googles-problem/,"The misuse of private data by social media companies poses a significant threat to privacy and civil liberties, and if not regulated properly, could lead to a self-defeating mistake of breaking up US tech giants without appreciating their crucial role in strengthening national competitiveness and security.",Security & Privacy
541,"In Twitter's Fight With Instagram, You Take the Hit","This is the latest dust-up in a fight between Twitter and Instagram that does more to hurt both companies' users than it benefits either business. It's all the more fascinating given how closely the two companies are related. Instagram chief Systrom was an intern at Ev Williams' podcasting startup Odeo, which Twitter grew out of. Twitter chairman and founder Jack Dorsey was an early investor in Instagram, and the photo-sharing service grew largely on the back of Twitter, thanks to its ability to export photos directly to other social networks.

But things have been frosty between the two companies recently. In April, Twitter's rival Facebook purchased Instagram. Then, in July, Instagram was an early casualty of Twitter's new API policy, after Twitter cut off Instagram's ability to access its API to look up friends. This was seen as a move to help keep Instagram, and by extension Facebook, from plundering Twitter's social graph. And in recent months, Twitter has been rumored to be working on its own photo filter and sharing feature, something that would compete directly with Instagram.

None of which should matter to the people who just want to use both companies' products in conjunction, but instead are being treated like chess pieces in a proxy war between Facebook and Twitter. As each company continues to further wall off products, that's just going to get worse, as each takes steps to make its product less interoperable with the other.

It's inevitable that these social-media companies are going to continue to bump up against the other's business models as they grow, evolve and become ever more diversified. One of the more effective defensive actions each can take to preserve their users and business is to make it hard for you to leave their service.

Twitter never wants you to leave its stream. When you see a photo you like, Instagram wants you to click on the heart in its stream rather than the star in Twitter's. And the best way to make sure you do that? It's to yank Twitter's ability to display Instagram media. In short, Twitter started this fight, and in retaliation Instagram brought out the big guns.

While each can argue that the company is just doing what is best for the business, all of those moves to reduce interoperability and cleave the services further apart just makes for more steps and more hassle for the people who have truly built both services into successful enterprises — which is to say the end users. And no matter who comes out ahead, in each of these battles, the people who use and love both networks are the clear and real losers.",Social Media,WIRED,https://www.wired.com/2012/12/in-instagram-vs-twitter-spat-you-are-the-loser/,"The fight between Twitter and Instagram has resulted in reduced interoperability and more hassle for the users, who are the real losers in this battle.",User Experience & Entertainment
542,Coronavirus Disrupts Social Media’s First Line of Defense,"Facebook users around the globe began to notice something strange happening on their feeds Tuesday night. Links to legitimate news outlets and websites, including The Atlantic, USA Today, the Times of Israel, and BuzzFeed, among many others, were being taken down en masse for reportedly violating Facebook’s spam rules. The problem impacted many people’s ability to share news articles and information about the developing coronavirus pandemic. Canadian pundit and podcast host Andrew Lawton said he was shocked to find that Facebook had wiped his episode archive and was barring him from sharing updates about Covid-19. “This is unreal,” he wrote in a since deleted tweet.

Facebook attributed the problem to a mundane bug in the platform’s automated spam filter, but some researchers and former Facebook employees worry it’s also a harbinger of what’s to come. With a global health crisis sweeping the globe, millions are confined to their homes, and social media platforms have become one of the most vital ways for people to share information and socialize with one another. But in order to protect the health of its staff and contractors, Facebook and other tech companies have also sent home their content moderators, who serve as their first line of defense against the horrors of the internet. Their work is often difficult, if not impossible, to do from home. Without their labor, the internet might become a less free and more frightening place.

“We will start to see the traces, which are so often hidden, of human intervention,” says Sarah T. Roberts, an information studies professor at UCLA and the author of Behind the Screen: Content Moderation in the Shadows of Social Media. “We’ll see what is typically unseen—that’s possible, for sure.”

Read all of our coronavirus coverage here.

After the 2016 US presidential election, Facebook significantly ramped up its moderation capabilities. By the end of 2018, it had more than 30,000 people working on safety and security, about half of which are content reviewers. Most of these moderators are contract workers, employed by firms like Accenture or Cognizant in offices around the world. They work to keep Facebook free of violence, child exploitation, spam, as well as other unseemly content. Their jobs can be stressful, if not outright traumatizing.

On Monday night, Facebook announced thousands of contract content moderators would be sent home “until further notice.” The workers would still be paid—although they wouldn’t receive the $1,000 bonus Facebook is giving to full-time staff. To fill the gap, Facebook is shifting more of the work to artificial intelligence, which CEO Mark Zuckerberg has been heralding as the future of content moderation for years. But some of the most sensitive content will be given to full-time staff, Zuckerberg told reporters on a call Wednesday, who will continue working at its offices.

Among Facebook users, Zuckerberg said, “I’m personally quite worried that the isolation from being at home could potentially lead to more depression or mental health issues.” To prepare for the potential onslaught, Facebook is ramping up the number of people working on moderating content about things like suicide and self-harm, he added. Another concern is the spread of misinformation—always an issue online, but particularly during a public health crisis. As part of its wider response to Covid-19, Facebook also announced it’s rolling out a Coronavirus Information Center to the newsfeed, where people can get updated information about the pandemic from authoritative sources.",Social Media,WIRED,https://www.wired.com/story/coronavirus-social-media-automated-content-moderation/,"With content moderators working from home due to the coronavirus pandemic, Facebook and other tech companies are facing a potential increase in misinformation, violence, child exploitation, and other unseemly content on their platforms.",Security & Privacy
543,"Facebook’s New Plan May Curb Revenge Porn, But Won't Kill It","Facebook needed to move against nonconsensual porn. The scandal surrounding Marines United, a secret Facebook group of 30,000 servicemen who shared dozens of women's private images without permission, proved that. Now, the social media giant finally shuffling in the right direction.

On Wednesday, Facebook released new guidelines for how it plans to curb the sharing of nonconsensual porn, which some call ""revenge porn,"" whether revenge was the motive or not. Under Facebook's new bylaws, if revenge porn pops up in your newsfeed and you report it, a team of (unfortunate) Facebook employees will now vet the image, and implement photo-matching technology to make sure it doesn't spread any further. The protocol works across Facebook Messenger and Instagram, too. But while this policy is a great start---and will give Facebook some much-needed legal cover if federal law ever criminalizes nonconsensual porn---the only way to kill revenge porn is to stop it being posted in the first place.

Facebook's photo-matching technology should be a huge boon to revenge porn victims. ""The constant challenge for the victim is reporting each post that shares their photo,"" says Mary Anne Franks, who teaches First Amendment and technology law at the University of Miami Law School, and also serves as the tech and legislative policy advisor for the Cyber Civil Rights Initiative. ""So we're really excited about this. It will alleviate some of that burden.""

Once someone reports an image, Facebook can be reasonably confident photo-matching technology will catch the rest. (Even better, a pop-up will notify the would-be poster that the photo they're about to share is revenge porn.) It's basically the same hashing technology that powers Google's image search, and Facebook already uses something similar to help identify child pornography. Circumventing it is difficult: a person would need to making significant visual changes to the original image, like adding stickers or filters or pasting the person onto a new background to bamboozle the tech. ""If it's just some dude uploading a photo from his phone, this should work really well,"" says Jen Golbeck, a computer scientist at the University of Maryland.

Legally speaking, it's a good move for Facebook too. If sharing revenge pornography becomes a federal crime---which Franks and congressional representative Jackie Speier (D-California) are working on---Facebook is going to need to find some shelter. As with criminalized content like child porn or terrorist videos, online intermediaries like Facebook would be legally obligated to report revenge porn to the powers that be, retain evidence, and take good faith measures to stop its spread. The photo-matching technology would help the company do that. ""This is one of the ways Facebook could signal they are trying to address this problem in the same way as child pornography,"" Franks says.

The key word there, though, is ""trying."" Like child porn or doxxing, revenge porn inflicts damage the first time it's shared, so removing something after it's already been posted is a second-best solution. And this measure wouldn't even catch the nonconsensual porn shared within a closed ecosystem like the Marines United group. ""We have to work preemptively. We've got a real problem with people sharing these images in a likeminded group,"" Franks says. ""In that situation, a woman might not find out her photos had been shared for 8 or 9 months."" Or ever.",Social Media,WIRED,https://www.wired.com/2017/04/facebook-revenge-porn/,"Nonconsensual pornography, or ""revenge porn,"" is a growing problem on social media, causing great distress to victims and making them vulnerable to further harassment. Facebook's new photo-matching technology is a step in the right direction, but more needs to be done to prevent such images from being shared in the first place.",Security & Privacy
544,"Children are being ‘datafied’ before we’ve understood the risks, report warns – TechCrunch","A report by England’s children’s commissioner has raised concerns about how kids’ data is being collected and shared across the board, in both the private and public sectors.

In the report, entitled Who knows what about me?, Anne Longfield urges society to “stop and think” about what big data means for children’s lives.

Big data practices could result in a data-disadvantaged generation whose life chances are shaped by their childhood data footprint, her report warns.

The long term impacts of profiling minors when these children become adults is simply not known, she writes.

“Children are being “datafied” – not just via social media, but in many aspects of their lives,” says Longfield.

“For children growing up today, and the generations that follow them, the impact of profiling will be even greater – simply because there is more data available about them.”

By the time a child is 13 their parents will have posted an average of 1,300 photos and videos of them on social media, according to the report. After which this data mountain “explodes” as children themselves start engaging on the platforms — posting to social media 26 times per day, on average, and amassing a total of nearly 70,000 posts by age 18.

“We need to stop and think about what this means for children’s lives now and how it may impact on their future lives as adults,” warns Longfield. “We simply do not know what the consequences of all this information about our children will be. In the light of this uncertainty, should we be happy to continue forever collecting and sharing children’s data?

“Children and parents need to be much more aware of what they share and consider the consequences. Companies that make apps, toys and other products used by children need to stop filling them with trackers, and put their terms and conditions in language that children understand. And crucially, the Government needs to monitor the situation and refine data protection legislation if needed, so that children are genuinely protected – especially as technology develops,” she adds.

The report looks at what types of data is being collected on kids; where and by whom; and how it might be used in the short and long term — both for the benefit of children but also considering potential risks.

On the benefits side, the report cites a variety of still fairly experimental ideas that might make positive use of children’s data — such as for targeted inspections of services for kids to focus on areas where data suggests there are problems; NLP technology to speed up analysis of large data-sets (such as the NSPCC’s national case review repository) to find common themes and understand “how to prevent harm and promote positive outcomes”; predictive analytics using data from children and adults to more cost-effectively flag “potential child safeguarding risks to social workers”; and digitizing children’s Personal Child Health Record to make the current paper-based record more widely accessible to professionals working with children.

But while Longfield describes the increasing availability of data as offering “enormous advantages”, she is also very clear on major risks unfolding — be it to safety and well-being; child development and social dynamics; identity theft and fraud; and the longer term impact on children’s opportunity and life chances.

“In effect [children] are the “canary in the coal mine for wider society, encountering the risks before many adults become aware of them or are able to develop strategies to mitigate them,” she warns. “It is crucial that we are mindful of the risks and mitigate them.”

Transparency is lacking

One clear takeaway from the report is there is still a lack of transparency about how children’s data is being collected and processed — which in itself acts as a barrier to better understanding the risks.

“If we better understood what happens to children’s data after it is given – who collects it, who it is shared with and how it is aggregated – then we would have a better understanding of what the likely implications might be in the future, but this transparency is lacking,” Longfield writes — noting that this is true despite ‘transparency’ being the first key principle set out in the EU’s tough new privacy framework, GDPR.

The updated data protection framework did beef up protections for children’s personal data in Europe — introducing a new provision setting a 16-year-old age limit on kids’ ability to consent to their data being processed when it came into force on May 25, for example. (Although EU Member States can choose to write a lower age limit into their laws, with a hard cap set at 13.)

And mainstream social media apps, such as Facebook and Snapchat, responded by tweaking their T&Cs and/or products in the region. (Although some of the parental consent systems that were introduced to claim compliance with GDPR appear trivially easy for kids to bypass, as we’ve pointed out before.)

But, as Longfield points out, Article 5 of the GDPR states that data must be “processed lawfully, fairly and in a transparent manner in relation to individuals”.

Yet when it comes to children’s data the children’s commissioner says transparency is simply not there.

She also sees limitations with GDPR, from a children’s data protection perspective — pointing out that, for example, it does not prohibit the profiling of children entirely (stating only that it “should not be the norm”).

While another provision, Article 22 — which states that children have the right not to be subject to decisions based solely on automated processing (including profiling) if they have legal or similarly significant effects on them — also appears to be circumventable.

“They do not apply to decision-making where humans play some role, however minimal that role is,” she warns, which suggests another workaround for companies to exploit children’s data.

“Determining whether an automated decision-making process will have “similarly significant effects” is difficult to gauge given that we do not yet understand the full implications of these processes – and perhaps even more difficult to judge in the case of children,” Longfield also argues.

“There is still much uncertainty around how Article 22 will work in respect of children,” she adds. “The key area of concern will be in respect of any limitations in relation to advertising products and services and associated data protection practices.”

Recommendations

The report makes a series of recommendations for policymakers, with Longfield calling for schools to “teach children about how their data is collected and used, and what they can do to take control of their data footprints”.

She also presses the government to consider introducing an obligation on platforms that use “automated decision-making to be more transparent about the algorithms they use and the data fed into these algorithms” — where data collected from under 18s is used.

Which would essentially place additional requirements on all mainstream social media platforms to be far less opaque about the AI machinery they use to shape and distribute content on their platforms at vast scale. Given that few — if any — could claim not to have no under 18s using their platforms.

She also argues that companies targeting products at children have far more explaining to do, writing:

Companies producing apps, toys and other products aimed at children should be more transparent about any trackers capturing information about children. In particular where a toy collects any video or audio generated by a child this should be made explicit in a prominent part of the packaging or its accompanying information. It should be clearly stated if any video or audio content is stored on the toy or elsewhere and whether or not it is transmitted over the internet. If it is transmitted, parents should also be told whether or not it will be encrypted during transmission or when stored, who might analyse or process it and for what purposes. Parents should ask if information is not given or unclear.

Another recommendation for companies is that terms and conditions should be written in a language children can understand.

(Albeit, as it stands, tech industry T&Cs can be hard enough for adults to scratch the surface of — let alone have enough hours in the day to actually read.)

A recent U.S. study of kids apps, covered by BuzzFeed News, highlighted that mobile games aimed at kids can be highly manipulative, describing instances of apps making their cartoon characters cry if a child does not click on an in-app purchase, for example.

A key and contrasting problem with data processing is that it’s so murky; applied in the background so any harms are far less immediately visible because only the data processor truly knows what’s being done with people’s — and indeed children’s — information.

Yet concerns about exploitation of personal data are stepping up across the board. And essentially touch all sectors and segments of society now, even as risks where kids are concerned may look the most stark.

This summer the U.K.’s privacy watchdog called for an ethical pause on the use by political campaigns of online ad targeting tools, for example, citing a range of concerns that data practices have got ahead of what the public knows and would accept.

It also called for the government to come up with a Code of Practice for digital campaigning to ensure that long-standing democratic norms are not being undermined.

So the children’s commissioner’s appeal for a collective “stop and think” where the use of data is concerned is just one of a growing number of raised voices policymakers are hearing.

One thing is clear: Calls to quantify what big data means for society — to ensure powerful data-mining technologies are being applied in ways that are ethical and fair for everyone — aren’t going anywhere.",Social Media,TechCrunch,https://techcrunch.com/2018/11/09/children-are-being-datafied-before-weve-understood-the-risks-report-warns/,"The report raises concerns that the datafication of children growing up today and future generations could lead to a data-disadvantaged generation and have long-term impacts on life chances, with identity theft and fraud being a potential risk.",Security & Privacy
545,TikTok’s QAnon ban has been ‘buggy’ – TechCrunch,"TikTok has been cracking down on QAnon-related content, in line with similar moves by other major social media companies, including Facebook and YouTube, which focus on reducing the spread the baseless conspiracy theory across their respective platforms. According to a report by NPR this weekend, TikTok had quietly banned several hashtags associated with the QAnon conspiracy, and says it will also delete the accounts of users who promote QAnon content.

Tiktok tells us, however, these policies are not new. The company says they actually went on the books earlier this year.

TikTok had initially focused on reducing discoverability as an immediate step by blocking search results while it investigated, with help from partners, how such content manifested on its platform. This was covered in July by several news publications, TikTok said. In August, TikTok also set a policy to remove content and ban accounts, we’re told.

Despite the policies, a report this month by Media Matters documented that TikTok was still hosting at least 14 QAnon-affiliated hashtags with over 488 million collective views. These came about because the platform had yet to address how QAnon followers were circumventing its community restrictions using variations and misspellings.

After Media Matters’ report, TikTok removed 11 of the 14 hashtags it had referenced, the report noted in an update.

Today, a number of QAnon-related hashtags — like #QAnon, #TheStormIsComing, #Trump2Q2Q” and others — return no results in TikTok’s search engine. They don’t show under the “Top” search results section, nor do they show under “Videos” or “Hashtags.”

Instead of just showing users a blank page when these terms are searched, TikTok displays a message that explains how some phrases can be associated with behavior or content that violates TikTok’s Community Guidelines, and offers a link to that resource.

Media Matters praised the changes in a statement to NPR as something TikTok was doing that was “good and significant” even if “long overdue.”

While TikTok’s ban did tackle many of the top search results and tags associated with the conspiracy, we found it was overlooking others, like pizzagate and WWG1WGA, for instance. In tests this afternoon, these terms and many others still returned much content.

TikTok claims what we saw was likely “a bug.”

We had reached out to TikTok today to ask why searches for terms like “pizzagate” and “WWG1WGA” — popular QAnon terms — were still returning search results, even though their hashtags were banned.

For example, if you just searched for “pizzagate,” TikTok offered a long list of videos to scroll through, though you couldn’t go directly to its hashtag. This was not the case for the other banned hashtags (like #QAnon) at the time of our tests.

The videos returned discussed the Pizzagate conspiracy — a baseless conspiracy theory which ultimately led to real-world violence when a gunman shot up a DC pizza business, thinking he was there to rescue trapped children.

While some videos were just discussing or debunking the idea, many were earnestly promoting the pizzagate conspiracy, even posting that it was was “real” or claimed to be offering “proof.”

Above: Video recorded Oct. 19, 2020, 3:47 PM ET/12:47 PM PT

Other QAnon-associated hashtags were also not subject to a full ban, including WWG1WGA, WGA, ThesePeopleAreSick, cannibalclub, hollyweird and many others often used to circulate QAnon conspiracies.

When we searched these terms, we found more long lists of QAnon-related videos to scroll through.

We documented this with photos and videos before reaching out to TikTok to ask why these had been made exceptions to the ban. We specifically asked about the two top terms — pizzagate and WWG1WGA.

TikTok provided us with information about the timeline of its policy changes and the following statement:

Content and accounts that promote QAnon violate our disinformation policy and we remove them from our platform. We’ve also taken significant steps to make this content harder to find across search and hashtags by redirecting associated terms to our Community Guidelines. We continually update our safeguards with misspellings and new phrases as we work to keep TikTok a safe and authentic place for our community.

TikTok said also that the search term blocking must have been a bug, because it’s now working properly.

We found that, upon receiving TikTok’s confirmation, the terms we asked about were blocked, but others were not. This includes some of those mentioned above, as well as bizarre terms only a real conspiracy fan would know, like adrenochromereptilians.

We asked Media Matters whether it could still praise TikTok’s actions to ban QAnon content, given what, at the time, had appeared to be a loophole in the QAnon ban.

“TikTok has of course taken steps but not fully resolved the problem, but as we’ve noted, the true test of any of these policies — like we’ve said of other platform’s measures — is in how and if they enforce them,” the organization said.

Even if the banned content was only showing today because of a “bug,” we found that many of the users who posted the content have not actually been banned from TikTok, it seems.

Though a search for their username won’t return results now that the ban is no longer “buggy,” you can still go directly to these users’ profile pages via their profile URL on the web.

We tried this on many profiles of those who had published QAnon content or used banned terms in their videos’ hashtags and descriptions. (Below are a few of examples.)

What this means is that although TikTok reduced these users’ discoverability in the app, the accounts can still be located if you know their username. And once you arrive on the account’s page, you can still follow them.

These examples of “bugs” or just oversights indicate how difficult it is to enforce content bans across social media platforms.

Without substantial investments in human moderation combined with automation, as well as tools that ensure banned users can’t return, it’s hard to keep up with the spread of disinformation at social media’s scale.",Social Media,TechCrunch,https://techcrunch.com/2020/10/19/tiktoks-qanon-ban-has-been-buggy/,"Without significant investments in human moderation and effective enforcement tools, it is difficult for social media platforms to keep up with the spread of disinformation.","Information, Discourse & Governance"
