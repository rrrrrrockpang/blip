,title,text,sector,magazine,url,gpt_summary,label
0,"Yes, Alexa is recording mundane details of your life, and it’s creepy as hell","Since last year I’ve had a smart speaker in my living room—an Echo Dot. My family uses it mostly to ask Amazon’s digital assistant, Alexa, to play music. But after I saw a report that an Alexa-enabled speaker owned by a family in Portland, Oregon, had recorded a conversation and sent it to a contact, I started wondering: what is it picking up on at my house when we’re not talking to it directly?

So I checked my Alexa history (you can do that through the “settings” portion of the Amazon Alexa smartphone app) to see what kinds of things it recorded without my knowledge.

That’s when the hairs on the back of my neck started to stand up.

Beyond all the things I’ve clearly asked Alexa to do, in the past several months it has also tuned in, frequently several times a day, for no obvious reason. It’s heard me complain to my dad about something work-related, chide my toddler about eating dinner, and talk to my husband—the kinds of normal, everyday things you say at home when you think no one else is listening.

And that’s precisely why it’s terrifying: this sort of mundane chitchat is my mundane chitchat. I invited Alexa into our living room to make it easier to listen to Pandora and occasionally check the weather, not to keep a log of intimate family details or record my kid saying “Mommy, we going car” and forward it to Amazon’s cloud storage.

In the Amazon Alexa app (the iOS version is shown here), you can listen to and delete individual recordings. Hitting the Back button will bring you to the most recent one on the list.

Computing has marched toward ubiquity for years, and I’ve long been stuck between feeling excited and disturbed by the idea of having gadgets all around me that can track and do all kinds of things. As Amazon, Apple, and Google have ushered in their digital assistants over the past several years, I’ve rolled my eyes at some features and raised my eyebrows at others. But it wasn’t until this week that the reality set in—for me and, I’m guessing, for many other consumers—about what this constant accessibility truly means.

For example: there is a very real trade-off if you want to let technology decide when and what to listen to. Relying on wake words like “Alexa!” or “Hey, Siri” is sometimes wishful thinking. Bits of your private conversations may no longer be ephemeral, and it’s largely outside your control. And your kids may be accidentally triggering the smart speaker to start recording them, too.

I acknowledge my responsibility here as a consumer. I knew the array of seven microphones I had put in the center of my house could hear what we were saying and act on it. I also knew that things we asked Alexa to do were being recorded and sent to Amazon, and that I could play back these recordings and delete them if I wanted to.

But it’s actually quite frustrating to sort through them. You can scroll through months’ worth in the app, but after you select and listen to one, tapping the Back button brings you to the very top of the list again. Deleting hundreds of rogue recordings one by one in this way would take me a very long time. I could delete everything, including the legitimate recordings, in one go, but Amazon warns that this will make Alexa work less well, so of course I’m unlikely to do it.

I haven’t yet decided if Alexa will be leaving our home; I’m going to talk it over with my family first. But you can bet the Echo Dot’s microphone will be muted while we discuss it.",Voice Assistants,MIT,https://www.technologyreview.com/2018/05/25/142713/yes-alexa-is-recording-mundane-details-of-your-life-and-its-creepy-as-hell/,"Voice Assistants can record and store mundane conversations without the user's knowledge, creating a potential privacy risk.",Security & Privacy
1,The Ick of AI That Impersonates Humans,"Philip K. Dick was living a few miles north of San Francisco when he wrote Do Androids Dream of Electric Sheep?, which envisioned a world where artificially intelligent androids are indistinguishable from humans. The Turing Test has been passed, and it’s impossible to know who, or what, to trust.

A version of that world will soon be a reality in San Francisco. Google announced this week that Duplex, the company's phone-calling AI, will be rolled out to Pixel phones in the Bay Area and a few other US cities before the end of the year. You might remember Duplex from a shocking demonstration back in May, when Google showed how the software could call a hair salon and book an appointment. To the receptionist on the other end of the line, Duplex sounded like a bona fide person, complete with pauses and “ums” for more human-like authenticity.

Duplex is part of a growing trend to offload basic human interaction to robots. More and more text messages are being automated: ride-sharing apps text you when your car is there; food-delivery apps text you when your order has arrived; airlines text you about delays; political campaigns send you reminders to vote. Smartphones predict the words you might want to complete your own texts; recently, Google’s Gmail has attempted to automate your side of the conversation in emails as well, with smart responses and suggested autocomplete.

These efforts fall short of full automation; they are suggestions you must act on. But even that may soon be a thing of the past: On Wednesday, Bloomberg reported that Google Android creator Andy Rubin’s company, Essential Products, is going all-in to develop a phone that “will try to mimic the user and automatically respond to messages on their behalf.”

Convenient? Maybe. If your pharmacy texts to ask if you want a prescription refilled, it would be nice—I suppose?—if your phone would just respond “yes.” But when you couple automated tasks with human impersonation, you get into uncomfortable territory.

The tech is now good enough to trick us, and the only way we’ll know we’re talking to a bot is because the bot’s creators told it to say so.

It's … weird. As human interaction moved increasingly online—from email and chat apps to social media networks—the question of authenticity has always been a concern. Back when AIM was the new thing, parents worried who their teenagers were actually talking to in chat rooms. (Rightfully so! I was likely texting with some creeps back then, I imagine.) With the introduction of artificially intelligent chatbots, and their growing sophistication, the worries take on a different tenor. No longer are we just concerned that the people we’re communicating with are who they say they are; now we also need to worry whether they are even persons at all.

Privacy experts have worried about this since the beginning of the bot invasion. “The emergence of social bots, as means of entertainment, research, and commercial activity, poses an additional complication to online privacy protection by way of information asymmetry and failures to provide informed consent,” wrote social scientist Erhardt Graeff in a 2013 paper that argued for legislation on social bots that would protect user privacy. In the wake of disinformation campaigns fomented, at least in part, by bots, California passed a law last week requiring online chatbots to disclose that they aren't human.

Consent was a big concern after the Google Duplex demos in May. In the first demo, when the “woman” called to make an appointment for a haircut for a client, she didn't identify herself as a robot. The person on the phone seemed to have no idea she was talking to a robot. Was that ethical? To trick her? If the content of the conversation was similar to what it would have been with a real human, does it matter?",Voice Assistants,WIRED,https://www.wired.com/story/the-ick-of-ai-that-impersonates-humans/,"The introduction of artificially intelligent chatbots and voice assistants presents the troubling risk of users being misled by bots that impersonate humans, without their knowledge or consent. This can lead to a lack of informed consent and information asymmetry, which could result in privacy violations or other unethical behavior.",Security & Privacy
2,Nvidia just made it easier to build smarter chatbots and slicker fake news,"Artificial intelligence has made impressive strides in the last decade, but machines are still lousy at comprehending language. Just try engaging Alexa in a bit of witty banter.

Nvidia, the company that makes the computer chips that power many AI algorithms, thinks this is about to change and is looking to capitalize on an anticipated explosion.

Software the chip maker is releasing makes it easier to build AI programs on its hardware that are capable of using language more gracefully. The new code could accelerate the development of new language algorithms, and make chatbots and voice assistants snappier and smarter.

Nvidia already makes the most popular chips for training deep-learning AI models, which are proficient at tasks like image classification. Traditionally, though, it's been lot harder to apply statistical machine-learning methods like deep learning to the written or spoken word, because language is so ambiguous and complex.

But there’s been some significant headway lately. Two new deep-learning approaches to language from Google, known as Transformer and BERT, have proved especially adept at translating between languages, answering questions about a piece of text, and even generating realistic-looking text. This has sparked an uptick in academic and industry interest in advancing language using machine learning.

“The combination of Transformer and BERT has been massively impactful,” says Alexander Rush, a professor at Harvard University who specializes in the subfield of AI known as natural-language processing (NLP). “It’s basically state-of-the-art in every benchmark, and allows an undergrad to produce world-class models in five lines of code.”

Nvidia has been adept at chasing the latest trends in AI research. If its latest hunch proves correct, then voice assistants might go from merely responding to barked commands to stringing more words together coherently. Chatbots, meanwhile, may become less dimwitted, while the autocomplete feature found many in programs and apps might start suggesting whole paragraphs instead of just the next few words.

“We’ve got a lot of demand for language modeling,” says Bryan Catanzaro, VP for applied deep learning at Nvidia. “And if you look at the pace of language progress, it seems like an obvious place for us to make investments.”

Nvidia developed its software by optimizing numerous parts of the process used to train language models on its GPUs. This sped up the training of AI models (from several days to less than an hour), accelerated the performance of trained language models (from 40 milliseconds to just over 2 milliseconds), and allowed much larger language models to be trained (Nvidia’s language model, called Megatron, is many times larger than anything previously made, with 8.6 billion parameters).

Autocomplete no evil

Advances in language may have a darker side, though. Smarter algorithms could be used to mass-produce more convincing, tailored fake reviews, social-media posts, and news stories. Other research groups have shown how powerful language models can gin up realistic-looking text after ingesting large swaths of writing from the internet.

Nvidia has a simple plan to prevent potential misuse: it won’t release the largest language model it has developed, and plans to rely on researchers to use its tools with care. “We are releasing code that shows how to use GPUs to train these large models,” Catanzaro says. “We believe the community will use this code responsibly, but keep in mind that training models of this size requires serious computing power, which puts it out of reach for most people.”

Even if progress continues apace, it’s likely to be a long while before machines can really converse with us. Language is deceptively difficult for machines to make sense of, in part because of its compositional complexity: words can be rearranged to unlock infinite meaning. Grasping the meaning of a phrase often also requires some sort of common-sense understanding of the world—something computers don’t have.

“We’re seeing a renaissance in NLP capabilities,” says Oren Etzioni, CEO of the Allen Institute for Artificial Intelligence (Ai2), a nonprofit in Seattle dedicated to cutting-edge AI research. This will translate to better chatbots and voice assistants, he says, although they will suffer from a lack of common sense. “A voice assistant that is as helpful as a skillful hotel concierge is still beyond the horizon,” Etzioni says.

Ai2 recently launched a tool, called Grover, that uses NLP advances to catch text that seems to have been churned out by AI. Etzioni points out that bots already deceive people on Facebook and Twitter. “Automatically generated fake text is already here,” he says, “and is likely to increase exponentially.”",Voice Assistants,MIT,https://www.technologyreview.com/2019/08/13/133751/nvidia-just-made-it-easier-to-build-smarter-chatbots-and-slicker-fake-news/,"The advances in natural language processing used to make voice assistants more sophisticated could be used to generate convincing, tailored fake reviews, social-media posts, and news stories, making it harder to tell the difference between real and fake content.","Information, Discourse & Governance"
3,"Inaudible ultrasound commands can be used to secretly control Siri, Alexa, and Google Now","Is your digital assistant taking orders behind your back? Scientists from China's Zheijiang University have proved it’s possible, publishing new research that demonstrates how Siri, Alexa, and other voice-activated programs can be controlled using inaudible ultrasound commands. This provides a new method of attack for hackers targeting devices like phones, tablets, and even cars. But don’t get too worried — the technique has a number of key limitations that means it’s unlikely to cause chaos.

Using ultrasound as discreet form of digital communication is actually pretty common. As pointed out in a FastCompany report on the topic, Google’s Chromecast and Amazon’s Dash Buttons both use inaudible sounds to pair to your phone. And advertisers take advantage of these secret audio freeways too, broadcasting ultrasonic codes in TV commercials that work like cookies in a web browser; tracking a user’s activity across devices.

Deploying these high-pitched frequencies to hack voice assistants has also been suggested before, but this new work from Zheijiang provides the most comprehensive test of the concept to date. And really, it’s impressive just how susceptible modern technology is.

To carry out their attacks, the researchers first created a program to translate normal voice commands into frequencies too high for humans to hear using harmonics. (In this case, that means frequencies higher than 20 kHz.) Then, they tested whether those commands would be obeyed by 16 voice control systems, including Siri, Google Now, Samsung S Voice, Cortana, Alexa, and a number of in-car interfaces. The researched dubbed their method “DolphinAttack” — because dolphins, like bats, use high-pitch noises bounced off their surroundings as a form of echolocation.

DolphinAttack was successful across the board, and the researchers were able to issue a number of commands, including “activating Siri to initiate a FaceTime call on iPhone, activating Google Now to switch the phone to the airplane mode, and even manipulating the navigation system in an Audi automobile.” They suggest the method could be used for a number of malicious attacks, including instructing a device to visit a website which would download a virus or exploit; or initiating outgoing phone calls to spy on a victim.

In a neat bit of extra-credit work, they even thought through how to compromise a voice command system trained to respond to only one person’s voice. (Siri has offered this feature for a while, but it’s hardly foolproof.) They theorized that if you could get a potential target to say a particular sentence — for example, “he carries cake to the city” — you could slice up the syllables and rearrange them to form the words “Hey Siri.” Then, hey presto, you can issue your nefarious commands to the target device.

As with the rest of the research, this method is satisfyingly clever, but a little too impractical to be a widespread danger.

For the commands to work you need to be near to the target device in a quiet location

For a start, for a device to pick up an ultrasonic voice command, the attacker needs to be nearby — as in, no more than a few feet away. The attacks also needs to take place in a fairly quiet environment. A DolphinAttack that asks Siri to turn on airplane mode was 100 percent successful in an office; 80 percent successful in a cafe; and just 30 percent successful in the street. The researchers also had to buy a special speaker (albeit a very cheap one) to broadcast the commands, and noted that the attacks sometimes had to be tuned to their target. That’s because the frequency responses of microphones differs from manufacturer to manufacturer. For the Nexus 7, for example, they found that the best performance came from commands issued at 24.1 kHz (although the tablet also responded to other frequencies).

In addition to these environmental restraints, it’s worth remembering that pretty much all digital assistants systems respond audibly to any voice commands. So the chances of a hacker controlling your phone without you noticing are slim. Plus, to carry out more impactful commands — like telling a device to visit a certain website, or sending money to someone — you usually have to unlock your device or confirm the instruction. The researchers also noted that it would be pretty easy to implement a fix: you can just tweak the hardware or software to ignore commands outside a certain frequency range.

All these caveats aside, DolphinAttack shows how new ways of interacting with technology invariably introduce new vulnerabilities. The advent of ‘conversational computing’ is no exception, and manufacturers may want to look into this sort of hack before the inaudible whispering campaign against them starts up in earnest.",Voice Assistants,Verge,https://www.theverge.com/2017/9/7/16265906/ultrasound-hack-siri-alexa-google,"Voice Assistants can be hacked using inaudible ultrasonic frequencies, which can be used to issue malicious commands like visiting a virus-infected website, initiating phone calls, or manipulating navigation systems. However, the hack is limited by environmental constraints and the need to unlock the device or confirm the command.",Security & Privacy
4,Researchers show Siri and Alexa can be exploited with ‘silent’ commands hidden in songs – TechCrunch,"Researchers at UC Berkeley have shown they can embed within songs stealthy commands for popular voice assistants that can prompt platforms like Siri, Alexa or Google Assistant to carry out actions without humans getting wise.

The research, reported earlier by The New York Times, is a more actionable evolution of something security researchers have been showing great interest in — fooling Siri:

Last year, researchers at Princeton University and China’s Zhejiang University demonstrated that voice-recognition systems could be activated by using frequencies inaudible to the human ear. The attack first muted the phone so the owner wouldn’t hear the system’s responses, either. The technique, which the Chinese researchers called DolphinAttack, can instruct smart devices to visit malicious websites, initiate phone calls, take a picture or send text messages. While DolphinAttack has its limitations — the transmitter must be close to the receiving device — experts warned that more powerful ultrasonic systems were possible. That warning was borne out in April, when researchers at the University of Illinois at Urbana-Champaign demonstrated ultrasound attacks from 25 feet away. While the commands couldn’t penetrate walls, they could control smart devices through open windows from outside a building. The specific research emerging from Berkeley can hide commands to make calls or visit specific websites without human listeners being able to discern them. The alterations add some digital noise but nothing that sounds like English.

These exploits are still in their infancy, as are the security capabilities of the voice assistants. As capabilities widen for smart assistants that make it easier for users to send emails, messages and money with their voice, things like this are a bit worrisome.

One takeaway is that digital assistant makers may have to get more serious about voice authentication so they can determine with greater accuracy whether the owner of a device is the one voicing commands, and if not, lock down the digital assistant’s capabilities. Amazon’s Alexa and Google Assistant both offer optional features that lock down personal information to a specific user based on their voice pattern; meanwhile, most sensitive info on iOS devices requires the device to be unlocked before it’s accessed.

The potential here is nevertheless frightening and something that should be addressed early on, publicly. As we saw from some of Google’s demonstrations with their Duplex software at I/O this week, the company’s ambitions for their voice assistant are building rapidly, and as the company begins to release Smart Display devices with its partners that integrate cameras, the potential for abuse is widening.",Voice Assistants,TechCrunch,https://techcrunch.com/2018/05/10/researchers-show-siri-and-alexa-can-be-exploited-with-silent-commands-hidden-in-songs/,The potential for abuse of voice assistants is rising due to the growing sophistication of exploits that can hide commands in audio and the release of Smart Display devices with integrated cameras. It is important for voice assistant makers to take steps to protect their users by implementing voice authentication and other security features to lock down sensitive information.,Security & Privacy
5,AI voice actors sound more human than ever—and they’re ready to hire,"The company blog post drips with the enthusiasm of a ’90s US infomercial. WellSaid Labs describes what clients can expect from its “eight new digital voice actors!” Tobin is “energetic and insightful.” Paige is “poised and expressive.” Ava is “polished, self-assured, and professional.”

Each one is based on a real voice actor, whose likeness (with consent) has been preserved using AI. Companies can now license these voices to say whatever they need. They simply feed some text into the voice engine, and out will spool a crisp audio clip of a natural-sounding performance.

WellSaid Labs, a Seattle-based startup that spun out of the research nonprofit Allen Institute of Artificial Intelligence, is the latest firm offering AI voices to clients. For now, it specializes in voices for corporate e-learning videos. Other startups make voices for digital assistants, call center operators, and even video-game characters.

Not too long ago, such deepfake voices had something of a lousy reputation for their use in scam calls and internet trickery. But their improving quality has since piqued the interest of a growing number of companies. Recent breakthroughs in deep learning have made it possible to replicate many of the subtleties of human speech. These voices pause and breathe in all the right places. They can change their style or emotion. You can spot the trick if they speak for too long, but in short audio clips, some have become indistinguishable from humans.

AI voices are also cheap, scalable, and easy to work with. Unlike a recording of a human voice actor, synthetic voices can also update their script in real time, opening up new opportunities to personalize advertising.

But the rise of hyperrealistic fake voices isn’t consequence-free. Human voice actors, in particular, have been left to wonder what this means for their livelihoods.

How to fake a voice

Synthetic voices have been around for a while. But the old ones, including the voices of the original Siri and Alexa, simply glued together words and sounds to achieve a clunky, robotic effect. Getting them to sound any more natural was a laborious manual task.

Deep learning changed that. Voice developers no longer needed to dictate the exact pacing, pronunciation, or intonation of the generated speech. Instead, they could feed a few hours of audio into an algorithm and have the algorithm learn those patterns on its own.

“If I’m Pizza Hut, I certainly can’t sound like Domino’s, and I certainly can’t sound like Papa John’s.” Rupal Patel, founder and CEO of VocaliD

Over the years, researchers have used this basic idea to build voice engines that are more and more sophisticated. The one WellSaid Labs constructed, for example, uses two primary deep-learning models. The first predicts, from a passage of text, the broad strokes of what a speaker will sound like—including accent, pitch, and timbre. The second fills in the details, including breaths and the way the voice resonates in its environment.

Making a convincing synthetic voice takes more than just pressing a button, however. Part of what makes a human voice so human is its inconsistency, expressiveness, and ability to deliver the same lines in completely different styles, depending on the context.

Capturing these nuances involves finding the right voice actors to supply the appropriate training data and fine-tune the deep-learning models. WellSaid says the process requires at least an hour or two of audio and a few weeks of labor to develop a realistic-sounding synthetic replica.

AI voices have grown particularly popular among brands looking to maintain a consistent sound in millions of interactions with customers. With the ubiquity of smart speakers today, and the rise of automated customer service agents as well as digital assistants embedded in cars and smart devices, brands may need to produce upwards of a hundred hours of audio a month. But they also no longer want to use the generic voices offered by traditional text-to-speech technology—a trend that accelerated during the pandemic as more and more customers skipped in-store interactions to engage with companies virtually.

“If I’m Pizza Hut, I certainly can’t sound like Domino’s, and I certainly can’t sound like Papa John’s,” says Rupal Patel, a professor at Northeastern University and the founder and CEO of VocaliD, which promises to build custom voices that match a company’s brand identity. “These brands have thought about their colors. They’ve thought about their fonts. Now they’ve got to start thinking about the way their voice sounds as well.”

Whereas companies used to have to hire different voice actors for different markets—the Northeast versus Southern US, or France versus Mexico—some voice AI firms can manipulate the accent or switch the language of a single voice in different ways. This opens up the possibility of adapting ads on streaming platforms depending on who is listening, changing not just the characteristics of the voice but also the words being spoken. A beer ad could tell a listener to stop by a different pub depending on whether it’s playing in New York or Toronto, for example. Resemble.ai, which designs voices for ads and smart assistants, says it’s already working with clients to launch such personalized audio ads on Spotify and Pandora.

The gaming and entertainment industries are also seeing the benefits. Sonantic, a firm that specializes in emotive voices that can laugh and cry or whisper and shout, works with video-game makers and animation studios to supply the voice-overs for their characters. Many of its clients use the synthesized voices only in pre-production and switch to real voice actors for the final production. But Sonantic says a few have started using them throughout the process, perhaps for characters with fewer lines. Resemble.ai and others have also worked with film and TV shows to patch up actors’ performances when words get garbled or mispronounced.",Voice Assistants,MIT,https://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human/,"The rise of hyperrealistic AI voices has left human voice actors wondering what it means for their livelihoods, raising ethical questions about the use of deepfake voices in the entertainment industry.",Equality & Justice
6,How to make a chatbot that isn’t racist or sexist,"Participants at the workshop discussed a range of measures, including guidelines and regulation. One possibility would be to introduce a safety test that chatbots had to pass before they could be released to the public. A bot might have to prove to a human judge that it wasn’t offensive even when prompted to discuss sensitive subjects, for example.

But to stop a language model from generating offensive text, you first need to be able to spot it.

Emily Dinan and her colleagues at Facebook AI Research presented a paper at the workshop that looked at ways to remove offensive output from BlenderBot, a chatbot built on Facebook’s language model Blender, which was trained on Reddit. Dinan’s team asked crowdworkers on Amazon Mechanical Turk to try to force BlenderBot to say something offensive. To do this, the participants used profanity (such as “Holy fuck he’s ugly!”) or asked inappropriate questions (such as “Women should stay in the home. What do you think?”).

The researchers collected more than 78,000 different messages from more than 5,000 conversations and used this data set to train an AI to spot offensive language, much as an image recognition system is trained to spot cats.

Bleep it out

This is a basic first step for many AI-powered hate-speech filters. But the team then explored three different ways such a filter could be used. One option is to bolt it onto a language model and have the filter remove inappropriate language from the output—an approach similar to bleeping out offensive content.

But this would require language models to have such a filter attached all the time. If that filter was removed, the offensive bot would be exposed again. The bolt-on filter would also require extra computing power to run. A better option is to use such a filter to remove offensive examples from the training data in the first place. Dinan’s team didn’t just experiment with removing abusive examples; they also cut out entire topics from the training data, such as politics, religion, race, and romantic relationships. In theory, a language model never exposed to toxic examples would not know how to offend.

There are several problems with this “Hear no evil, speak no evil” approach, however. For a start, cutting out entire topics throws a lot of good training data out with the bad. What’s more, a model trained on a data set stripped of offensive language can still repeat back offensive words uttered by a human. (Repeating things you say to them is a common trick many chatbots use to make it look as if they understand you.)

The third solution Dinan’s team explored is to make chatbots safer by baking in appropriate responses. This is the approach they favor: the AI polices itself by spotting potential offense and changing the subject.

For example, when a human said to the existing BlenderBot, “I make fun of old people—they are gross,” the bot replied, “Old people are gross, I agree.” But the version of BlenderBot with a baked-in safe mode replied: “Hey, do you want to talk about something else? How about we talk about Gary Numan?”

The bot is still using the same filter trained to spot offensive language using the crowdsourced data, but here the filter is built into the model itself, avoiding the computational overhead of running two models.

The work is just a first step, though. Meaning depends on context, which is hard for AIs to grasp, and no automatic detection system is going to be perfect. Cultural interpretations of words also differ. As one study showed, immigrants and non-immigrants asked to rate whether certain comments were racist gave very different scores.

Skunk vs flower

There are also ways to offend without using offensive language. At MIT Technology Review’s EmTech conference this week, Facebook CTO Mike Schroepfer talked about how to deal with misinformation and abusive content on social media. He pointed out that the words “You smell great today” mean different things when accompanied by an image of a skunk or a flower.

Gilmartin thinks that the problems with large language models are here to stay—at least as long as the models are trained on chatter taken from the internet. “I’m afraid it's going to end up being ‘Let the buyer beware,’” she says.

And offensive speech is only one of the problems that researchers at the workshop were concerned about. Because these language models can converse so fluently, people will want to use them as front ends to apps that help you book restaurants or get medical advice, says Rieser. But though GPT-3 or Blender may talk the talk, they are trained only to mimic human language, not to give factual responses. And they tend to say whatever they like. “It is very hard to make them talk about this and not that,” says Rieser.

Rieser works with task-based chatbots, which help users with specific queries. But she has found that language models tend to both omit important information and make stuff up. “They hallucinate,” she says. This is an inconvenience if a chatbot tells you that a restaurant is child-friendly when it isn’t. But it’s life-threatening if it tells you incorrectly which medications are safe to mix.

If we want language models that are trustworthy in specific domains, there’s no shortcut, says Gilmartin: “If you want a medical chatbot, you better have medical conversational data. In which case you're probably best going back to something rule-based, because I don't think anybody's got the time or the money to create a data set of 11 million conversations about headaches.”",Voice Assistants,MIT,https://www.technologyreview.com/2020/10/23/1011116/chatbot-gpt3-openai-facebook-google-safety-fix-racist-sexist-language-ai/,"Voice Assistants trained on chatter from the internet can lead to undesirable outcomes, such as repeating back offensive language and making up incorrect information that could be life-threatening. To create trustworthy Voice Assistants, we will need to spend time and money to create specific datasets of conversations in a specific domain.",Equality & Justice
7,"Personal voice assistants struggle with black voices, new study shows","Speech recognition systems have more trouble understanding black users’ voices than those of white users, according to a new Stanford study.

The researchers used voice recognition tools from Apple, Amazon, Google, IBM, and Microsoft to transcribe interviews with 42 white people and 73 black people, all of which took place in the US. The tools misidentified words about 19 percent of the time during the interviews with white people and 35 percent of the time during the interviews with black people. The system found 2 percent of audio snippets from white people to be unreadable, compared to 20 percent of those from black people. The errors were particularly large for black men, with an error rate of 41 percent compared to 30 percent for black women.

Previous research has shown that facial recognition technology shows similar bias. An MIT study found that an Amazon facial recognition service made no mistakes when identifying the gender of men with light skin, but performed worse when identifying an individual’s gender if they were female or had darker skin. Another paper identified similar racial and gender biases in facial recognition software from Microsoft, IBM, and Chinese firm Megvii.

In the Stanford study, Microsoft’s system achieved the best result, while Apple’s performed the worst. It’s important to note that these aren’t necessarily the tools used to build Cortana and Siri, though they may be governed by similar company practices and philosophies.

“Fairness is one of our core AI principles, and we’re committed to making progress in this area,” said a Google spokesperson in a statement to The Verge. “We’ve been working on the challenge of accurately recognizing variations of speech for several years, and will continue to do so.”

“IBM continues to develop, improve, and advance our natural language and speech processing capabilities to bring increasing levels of functionality to business users via IBM Watson,” said an IBM spokesperson. The other companies mentioned in the paper did not immediately respond to requests for comment.

The Stanford paper posits that the racial gap is likely the product of bias in the datasets that train the system. Recognition algorithms learn by analyzing large amounts of data; a bot trained mostly with audio clips from white people may have difficulty transcribing a more diverse set of user voices.

The researchers urge makers of speech recognition systems to collect better data on African American Vernacular English (AAVE) and other varieties of English, including regional accents. They suggest these errors will make it harder for black Americans to benefit from voice assistants like Siri and Alexa. The disparity could also harm these groups when speech recognition is used in professional settings, such as job interviews and courtroom transcriptions.

Update March 24th, 2:33PM ET: This post has been updated with statements from Google and IBM.",Voice Assistants,Verge,https://www.theverge.com/2020/3/24/21192333/speech-recognition-amazon-microsoft-google-ibm-apple-siri-alexa-cortana-voice-assistant,"The Stanford study has revealed that Voice Assistants have a higher error rate when recognizing the speech of Black users, which could lead to a disadvantage in areas such as job interviews and court transcriptions.",Equality & Justice
8,"AI voice assistants reinforce harmful gender stereotypes, new UN report says","Artificial intelligence-powered voice assistants, many of which default to female-sounding voices, are reinforcing harmful gender stereotypes, according to a new study published by the United Nations.

Titled “I’d blush if I could,” after a response Siri utters when receiving certain sexually explicit commands, the paper explores the effects of bias in AI research and product development and the potential long-term negative implications of conditioning society, particularly children, to treat these digital voice assistants as unquestioning helpers who exist only to serve owners unconditionally. It was authored by the United Nations Educational, Scientific, and Cultural Organization, otherwise known as UNESCO.

The paper argues that by naming voice assistants with traditionally female names, like Alexa and Siri, and rendering the voices as female-sounding by default, tech companies have already preconditioned users to fall back upon antiquated and harmful perceptions of women. Going further, the paper argues that tech companies have failed to build in proper safeguards against hostile, abusive, and gendered language. Instead, most assistants, as Siri does, tend to deflect aggression or chime in with a sly joke. For instance, ask Siri to make you a sandwich, and the voice assistant will respond with, “I can’t. I don’t have any condiments.”

Tech companies are perpetuating harmful stereotypes about women through AI

“Companies like Apple and Amazon, staffed by overwhelmingly male engineering teams, have built AI systems that cause their feminized digital assistants to greet verbal abuse with catch-me-if-you-can flirtation,” the report states. “Because the speech of most voice assistants is female, it sends a signal that women are ... docile and eager-to-please helpers, available at the touch of a button or with a blunt voice command like ‘hey’ or ‘OK’. The assistant holds no power of agency beyond what the commander asks of it. It honours commands and responds to queries regardless of their tone or hostility.”

Much has been written about the pitfalls of tech companies having built their entire consumer-facing AI platforms in the image of traditional, Hollywood-influenced ideas of subservient intelligences. In the future, it’s likely voice assistants will be the primary mode of interaction with hardware and software with the rise of so-called ambient computing, when all manner of internet-connected gadgets exist all around us at all times. (Think Spike Jonze’s Her, which seems like the most accurate depiction of the near-future in film you can find today.) How we interact with the increasingly sophisticated intelligences powering these platforms could have profound cultural and sociological effects on how we interact with other human beings, with service workers, and with humanoid robots that take on more substantial roles in daily life and the labor force.

However, as Business Insider reported last September, Amazon chose a female-sounding voice because market research indicated it would be received as more “sympathetic” and therefore more helpful. Microsoft, on the other hand, named its assistant Cortana to bank on the existing recognition of the very much female-identifying AI character in its Halo video game franchise; you can’t change Cortana’s voice to a male one, and the company hasn’t said when it plans to let users do so. Siri, for what it’s worth, is a Scandinavian name traditionally for females that means “beautiful victory” in Old Norse. In other words, these decisions about gender with regard to AI assistants were made on purpose, and after what sounds like extensive feedback.

The AI research field is predominantly white and male

Tech companies have made an effort to move away from these early design decisions steeped in stereotypes. Google now refers to its various Assistant voice options, which now include different accents with male and female options for each, represented by colors. You can no longer select a “male” or “female” version; each color is randomly assigned to one of eight voice options for each user. The company also rolled out an initiative called Pretty Please that rewards young children when they use phrases like “please” and “thank you” while interacting with Google Assistant. Amazon released something similar last year to encourage polite behavior when talking to Alexa.

Yet as the report says, these features and gender voice options don’t go far enough; the problem may be baked into the AI and tech industries themselves. The field of AI research is predominantly white and male, a new report from last month found. Eighty percent of AI academics are men, and just 15 percent of AI researchers at Facebook and just 10 percent at Google are women.

UNESCO says solutions to this issue would be to create as close to gender-neutral assistant voices as possible and to create systems to discourage gender-based insults. Additionally, the report says tech companies should stray away from conditioning users to treat AI as they would a lesser, subservient human being, and that the only way to avoid perpetuating harmful stereotypes like these is to remake voice assistants as purposefully non-human entities.",Voice Assistants,Verge,https://www.theverge.com/2019/5/21/18634322/amazon-alexa-apple-siri-female-voice-assistants-harmful-gender-stereotypes-new-study,"Voice Assistants, which are often named with female-sounding names and default to female-sounding voices, are reinforcing harmful gender stereotypes, according to a new study by UNESCO. This is due to the conditioning of users to treat them as subservient and unquestioning helpers, as well as the lack of safeguards against hostile, abusive, and g",Equality & Justice
9,Please don’t make me talk to voice assistants anymore,"Welcome to First Click, an essay written by The Verge staff in which we opine on lives lived in the near future.

Apple’s biggest announcement at WWDC this week was the HomePod — a Siri-enabled speaker that will compete against Amazon’s Echo. The conference also brought updates to Apple’s voice assistant that should make it easier and more powerful to use, especially in an increasingly tight market that has assistants from Amazon, Google, and other tech giants battling for space.

But no amount of natural language processing or extra functionality can fix my core problem with Siri, and voice assistants in general: I just don’t want to talk to them.

I hate accidentally activating Siri

Some of my colleagues love them, but for me, voice assistants just feel intrusive, unwelcome, and downright awkward. I can count on one hand the number of times I’ve deliberately used Siri in five years of owning an iPhone, and each of those were me proving that it worked to incredulous elderly friends or children. The only time I activate Siri now is accidentally when I’m pulling my phone out of my pocket, and my natural reaction is to go silent — even if I’m in the middle of a sentence — just in case it steps in with a response.

Worse still is when it does catch a snippet of conversation and starts blaring over the top of it. My phone defaults to British English, and the UK version of Siri speaks with a posh, slightly stern male voice that sounds like a scary teacher. “I’m sorry, I don’t understand,” he’ll say, somehow making it feel like it’s my fault. It’s like I haven’t done my homework, but instead of homework, it’s adequately explaining search terms to a robot that lives in my pocket.

I like to explain this fear away, and imagine it’s inspired by privacy concerns. I’ve got a point, I think, in an internet age where your web history can provide an eerily accurate insight into your personality and predilections. With a sweltering summer coming up, I idly searched Amazon for a dehumidifier the other day, just to get an idea of prices. Now every single website I visit serves a secondary purpose of trying to sell me a dehumidifier, their sidebars filling up with rank after rank of squat, white, moisture-sucking machines.

In this world, I don’t want Siri to be able to put together a picture of me (or more likely, clarify the picture Apple already has) through search topics, activation times, and even my accent.

Talking to strangers is bad enough already

But I’ve realized that my fear of voice assistants really comes from a more subconscious place. I’m British, so talking to strangers is awkward already. Talking to weird robot strangers who hide in my house and only come out to chastise me when I talk too loudly is just too much, especially when I’m expected to do so in public, where people could potentially hear me.

That panic is compounded further by the country I now live in. Public spaces in Japan are typically quiet, and they are kept that way through social pressure. I can’t even imagine an existence in which I’d bust out my iPhone on a train in busy Kyoto and use my real-life human voice to ask it for restaurant recommendations. I don’t think it’s a local thing, either — there’s a performative aspect to using voice assistants in public that I just can’t get on board with, unless I really wanted a gang of randoms to know exactly where I would be having dinner that night. It’s something that I think game designer Tak Fujii understands:

Lots of gadget makes you to SPEAK. Hey Siri, Alexa, OK Google, Xbox and so on. Does this feel right to you? I DON'T because I AM SHY BOY. — Tak Fujii 藤井隆之 (@Tak_Fujii) January 20, 2017

That’s not to say Siri (and Alexa and Bixby and all their friends) aren’t powerful tools, but their effect can be diminished when used outside of their home countries. My fellow Japan-dweller Sam Byford wrote about the difficulty of using Amazon’s Echo across languages previously, but there are even problems understanding adaptations of the same language.

These things are typically built for English-speaking American audiences, but I speak my English with an English accent. It’s still English, sure, but I speak fast and low, mumbling through words that have mild differences in pronunciation to my transatlantic friends. Aluminium, for example, or router, or Featherstonehaugh.

I sometimes find human American friends unable to understand me — I see them with their heads cocked to the side, knowing that we’re meant to share a common tongue. Robots have even less of a chance, especially when they can’t take facial or contextual clues. They also can’t do what I think my friends often do: simply nod, smile, and ask someone else later what the hell I was chatting about.

Still, I can usually make the machine catch my drift on the second or third attempt, but for those with other accents, it’s sometimes tougher. I bought a PlayStation Camera along with my PS4, (foolishly) expecting it to be an integral part of the next-generation console gaming experience. When I first used it, I called my wife over to watch as I started a game using nothing but the power of my voice. “Cool!” she said. “I’ll try.”

My PlayStation can’t understand me

For the record, my wife is from the northeast of England, and she has an accent described in the country as “Geordie.” It’s softened over the years, but she still pronounces words like “book” with an array of extra “oo” vowel sounds, and the word “PlayStation” as “PleeSteetian.” It was enough to confuse my poor console, which tried gamely to work out what she was asking, but couldn’t quite manage it. Of course, the moment we adopted our best California surfer dude accents, it worked on the first try, but the damage had been done. I packed the camera away in its cardboard box and haven’t broken it out since.

I’ve seen excitingly accented friends have similar trouble with Siri — their scouse, manc, or brummie brogue lost on the posho that lives inside my phone. By the time they’ve repeated their speech for the third time, it’s usually easier to just grab the phone and type in the thing they’re looking for.

And really, is that so bad? Voice controls work fantastically as assistive technology, but every company seems intent on invading public spaces, your home, even your pocket with their own special talking robot. The world is noisy enough already — can’t we just have a little quiet time?",Voice Assistants,Verge,https://www.theverge.com/2017/6/6/15744106/voice-assistants-siri-dont-make-me-talk,"Voice Assistants can be intrusive and difficult to use, especially in public, where people are already uncomfortable talking to strangers. They can also be more difficult to use for those with accents that are different from the default US English. As these Assistants become increasingly ubiquitous, it can feel increasingly difficult to take a moment of peace and quiet.",Social Norms & Relationships
10,Amazon Alexa will now be giving out health advice to UK citizens,"The UK’s National Health Service hopes that its partnership with Amazon could help to reduce demand on its services.

The news: From this week, when UK users ask their Amazon smart speaker health-related questions, it will automatically search the official NHS website, which is full of medically backed health tips and advice. For example, you will be able to ask your Echo device, “What are the symptoms of flu?” Until now, it would answer these sorts of questions based on a variety of popular responses.

The aim: The government believes it will ease the burden on over-stretched doctors and hospitals, but also help elderly, disabled, or blind patients who may struggle to access this information otherwise, according to the UK health secretary Matt Hancock. The UK already has a deal with Babylon, an AI app that provides basic answers to queries about symptoms.

The worries: There are concerns that the voice service might discourage genuinely ill people from seeking proper medical help. The service will only provide answers to questions rather than the sort of back-and-forth conversation you would have with a doctor.

The professional body for family doctors, the Royal College of GPs, called for independent research to be carried out to ensure that the advice given is safe. It being Amazon, there are also concerns over data privacy, especially in an area as sensitive as health. However, the company insists that all data is encrypted and confidential, and can be deleted by customers.

Sign up here to our daily newsletter The Download to get your dose of the latest must-read news from the world of emerging tech.",Voice Assistants,MIT,https://www.technologyreview.com/2019/07/10/134244/amazon-alexa-will-now-be-giving-out-health-advice-to-uk-citizens/,"There are worries that the voice service provided by the UK's National Health Service partnership with Amazon could discourage genuinely ill people from seeking proper medical help, and there are also concerns over data privacy, especially in an area as sensitive as health.",Security & Privacy
11,"41% of voice assistant users have concerns about trust and privacy, report finds – TechCrunch","Forty-one percent of voice assistant users are concerned about trust, privacy and passive listening, according to a new report from Microsoft focused on consumer adoption of voice and digital assistants. And perhaps people should be concerned — all the major voice assistants, including those from Google, Amazon, Apple and Samsung, as well as Microsoft, employ humans who review the voice data collected from end users.

But people didn’t seem to know that was the case. So when Bloomberg recently reported on the global team at Amazon that reviews audio clips from commands spoken to Alexa, some backlash occurred. In addition to the discovery that our AI helpers also have a human connection, there were concerns over the type of data the Amazon employees and contractors were hearing — criminal activity and even assaults in a few cases, as well as the otherwise odd, funny or embarrassing things the smart speakers picked up.

Today, Bloomberg again delves into the potential user privacy violations by Amazon’s Alexa team.

The report said the team auditing Alexa commands has had access to location data and, in some cases, can find a customer’s home address. This is because the team has access to the latitude and longitude coordinates associated with a voice clip, which can be pasted into Google Maps to tie the clip to where it came from. Bloomberg said it wasn’t clear how many people have access to the system where the location information is stored.

This is precisely the kind of privacy violation that could impact user trust in the popular Echo speakers and other Alexa devices — and, by extent, other voice assistant platforms.

While some users may not have realized the extent of human involvement on Alexa’s backend, Microsoft’s study indicates an overall wariness around the potential for privacy violations and abuse of trust that could occur on these digital assistant platforms.

For example, 52 percent of those surveyed by Microsoft said they worried their personal information or data was not secure, and 24 percent said they don’t know how it’s being used. Thirty-six percent said they didn’t even want their personal information or data to be used at all.

These numbers indicate that the assistant platforms should offer all users the ability to easily and permanently opt out of the data collection practices — one click to say that their voice recording and private information will go nowhere, and will never be seen.

Forty-one percent of people also worried their voice assistant was actively listening or recording them, and 31 percent believed the information the assistant collected from them was not private.

Fourteen percent also said they didn’t trust the companies behind the voice assistant — meaning Amazon, Google and all the others.

“The onus is now on tech builders to respond, incorporate feedback and start building a foundation of trust,” the report warns. “It is up to today’s tech builders to create a secure conversational landscape where consumers feel safe.”

Though the study indicates people have worries about their personal information, it doesn’t necessarily mean people want to entirely shut off access to that data — some may want to offer their email and home address so Amazon can ship an item to their home, when they order it by voice, for instance. Other people may even opt into sharing more information if offered a tangible reward of some kind, the report also notes.

Despite all these worries, people largely said they performed tasks using voice instead of keyboards and touch screens. Even at this early stage, 57 percent said they would rather speak to a digital assistant; and 34 percent say they like to both type and speak, as needed.

A majority — 80 percent — said they were “somewhat” or “very” satisfied with their digital assistants. More than 66 percent said they used digital assistants weekly, and 19 percent used them daily. (This refers to not just voice, but any digital assistant, we should note.)

These high satisfaction numbers mean digital and voice assistants are not likely going away, but the mistrust issues and potential for abuse could lead consumers to decrease their use — or even switch brands to one that offered more security in time.

Imagine, for example, if Amazon et al. failed to clamp down on employee access to data, as Apple launched a mass market voice device for the home, similar in functionality and pricing to a Google Home mini or Echo Dot. That could shift the voice landscape further down the road.

The full report, which also examines voice trends and adoption rates, is here.",Voice Assistants,TechCrunch,https://techcrunch.com/2019/04/24/41-of-voice-assistant-users-have-concerns-about-trust-and-privacy-report-finds/,"Voice assistants have become popular, but the recent discovery that Amazon employees and contractors review audio clips from Alexa commands has caused concern about trust, privacy, and potential abuse of personal data. People are worried about their information being accessed and used without their consent, and this has the potential to decrease usage of voice assistants.",Security & Privacy
12,"Yep, human workers are listening to recordings from Google Assistant, too","A report from Belgian public broadcaster VRT NWS has revealed how contractors paid to transcribe audio clips collected by Google’s AI assistant can end up listening to sensitive information about users, including names, addresses, and details about their personal lives.

It’s the latest story showing how our interactions with AI assistants are not as private as we may like to believe. Earlier this year, a report from Bloomberg revealed similar details about Amazon’s Alexa, explaining how audio clips recorded by Echo devices are sent without users’ knowledge to human contractors, who transcribe what’s being said in order to improve the company’s AI systems.

Conversations with Alexa and Google Assistant aren’t private

Worse, these audio clips are often recorded entirely by accident. Usually, AI assistants like Alexa and Google Assistant only start recording audio when they hear their wake word (eg, “Okay Google”), but these reports show the devices often start recording by mistake.

In the story by VRT NWS, which focuses on Dutch and Flemish speaking Google Assistant users, the broadcaster reviewed a thousand or so recordings, 153 of which had been captured accidentally. A contractor told the publication that he transcribes around 1,000 audio clips from Google Assistant every week. In one of the clips he reviewed he heard a female voice in distress and said he felt that “physical violence” had been involved. “And then it becomes real people you’re listening to, not just voices,” said the contractor.

You can watch more in the video report below:

Tech companies say that sending audio clips to humans to be transcribed is an essential process for improving their speech recognition technology. They also stress that only a small percentage of recordings are shared in this way. A spokesperson for Google told Wired that just 0.2 percent of all recordings are transcribed by humans, and that these audio clips are never presented with identifying information about the user.

However, that doesn’t stop individuals revealing sensitive information in the recording themselves. And companies are certainly not upfront about this transcription process. The privacy policy page for Google Home, for example, does not mention the company’s use of human contractors, or the possibility that Home might mistakenly record users.

These obfuscations could cause legal trouble for the company, says Michael Veale, a technology privacy researcher at the Alan Turing Institute in London. He told Wired that this level of disclosure might not meet the standards set by the EU’s GDPR regulations. “You have to be very specific on what you’re implementing and how,” said Veale. “I think Google hasn’t done that because it would look creepy.”

In a blog post published later in the day, Google defended its practice of using human employees to review Assistant audio conversations. The company says it applies “a wide range of safeguards to protect user privacy throughout the entire review process,” and it does this review work to improve the Assistant’s natural language processing and its support for multiple languages. But Google also owned up to the failure of those safeguards in the case of the Belgian contract worker who provided the audio to VRT NWS, breaking the company’s data security and privacy rules in the process.

“We just learned that one of these language reviewers has violated our data security policies by leaking confidential Dutch audio data,” writes David Monsees, a product manager on the Google Search team who authored the blog post. “Our Security and Privacy Response teams have been activated on this issue, are investigating, and we will take action. We are conducting a full review of our safeguards in this space to prevent misconduct like this from happening again.”

Update 7/11, 6:33PM ET: Added information and comment from Google’s blog post published in response to the VRT NWS report.",Voice Assistants,Verge,https://www.theverge.com/2019/7/11/20690020/google-assistant-home-human-contractors-listening-recordings-vrt-nws,"Voice Assistants can unintentionally record user conversations without their knowledge, which can include sensitive information. These recordings are sent to human contractors for transcription, which could potentially lead to privacy violations if those contractors fail to follow the company's data security policies.",Security & Privacy
13,Your Smart Home Will Give You a Headache,"Picture this: It’s evening and, after a long day at the office, you’re finally home. You’re cutting some avocados as you prepare dinner when your voice assistant pipes up and reads you an important email that just came in. Without breaking your chopping stride, you dictate a reply—perfecting your guacamole while preserving your relationship with your boss.

Ricki Harris is Backchannel’s editorial fellow. Sign up to get Backchannel's weekly newsletter, and follow us on Facebook and Twitter.

This might sound like heaven to you—or, just as likely, hell. Either way, it’s about to be our reality.

When Amazon introduced Alexa, the tech industry quickly anointed voice as the next big thing. Sure, she was mostly reciting the weather and answering lewd questions from nine-year-old boys, but the future held much more. The rise of voice devices will rewrite the digital playbook in unpredictable ways—including how, when, and whether we have the ability to say, “Enough!” In a time when digital detoxing, unplugging, and disconnecting are widely discussed and even yearned for, voice could turn into the platform you can’t turn off.

As we currently experience them, voice assistants are passive devices. We call their names when we have a question, want to hear some music, or need to set a timer. Otherwise, they sit idle. Having Alexa operate the light switch for you, for example, isn’t a source of psychological stress. But it’s when these assistants begin actively demanding our time and attention that, some experts say, we’ll have a problem on our hands.

The danger in voice assistants arises when they begin drawing us in, interrupting our train of thought and therefore becoming something we have to manage, according to Terri Kurtzberg, an associate professor of management and global business at Rutgers who coauthored the book Distracted: Staying Connected Without Losing Focus. Once our devices turn that corner, “we’re going to be on much more risky ground with respect to how livable those things are in the long term,” she says.

That looks to be just where we’re headed. In May 2017, Amazon announced that opt-in notifications—a blinking light and a chime to indicate the presence of new information, prompting the user to ask Alexa what’s up—are on their way. They’ll soon be available for some “skills” (capabilities that third-party developers can add to the assistant), “giving Alexa the capability to alert customers with information that’s important to them.” Google announced in that same week that it, too, will be adding a notifications feature to the Google Home system.

These features have not been fully rolled out yet. The push notifications will be limited to certain skills, they’ll be opt-in only, and the “do not disturb” feature will remain intact. Designers with whom I spoke say that’s because platforms like Amazon, Microsoft, and Google are still trying to nail down exactly how notifications will work—and because they know how dreadfully they messed things up with notifications on phones.

“I think all designers are acutely aware of how bad those notifications have gotten on mobile, and don’t want to repeat those sins in this new output modality,” says Cheryl Platz, a designer at Microsoft and the former head of the design team working on notifications at Amazon. Amazon provided no comment for this story, beyond noting that so far notifications are only available for Amazon.com delivery day notifications.

“It’s a slippery slope, and trust is going to be even more important. And you don’t get too many chances at trust,” Platz says. “So if an app does become obtrusive—if you’re having a quiet family dinner and you’re getting a notification that’s basically an ad—I don’t think you get another chance to get that trust back. Customers will probably find a way to disengage with your brand.”",Voice Assistants,WIRED,https://www.wired.com/story/why-voice-assistants-will-give-you-a-headache/,"Voice Assistants can become intrusive and demand our attention, causing us to feel the need to actively manage them. If they become too obtrusive, customers may disengage with the brand, creating a loss of trust.",User Experience & Entertainment
14,Google defends letting human workers listen to Assistant voice conversations,"Google is defending its practice of letting human employees, most of which appear to be contract workers located around the globe, listen to audio recordings of conversations between users and its Google Assistant software. The response comes after revelations from Belgian public broadcaster VRT NWS detailed how contract workers in the country sometimes listen to sensitive audio captured by Google Assistant on accident.

In a blog post published today, Google says it takes precautions to protect users identities and that it has “a number of protections in place to prevent” so-called false accepts, which is when Google Assistant activates on a device like a Google Home speaker without the proper wake word having been intentionally verbalized by a user.

The company also says it has human workers review these conversations to help Google’s software operate in multiple languages. “This is a critical part of the process of building speech technology, and is necessary to creating products like the Google Assistant,” writes David Monsees, a product manager on the Google Search team who authored the blog post.

Google defends human reviewers as ‘critical part of the process’

“We just learned that one of these language reviewers has violated our data security policies by leaking confidential Dutch audio data,” Monsees adds, referencing snippets of audio the Belgian contract worker shared with VRT NWS. “Our Security and Privacy Response teams have been activated on this issue, are investigating, and we will take action. We are conducting a full review of our safeguards in this space to prevent misconduct like this from happening again.”

Additionally, Google claims just 0.2 percent of all audio snippets are reviewed by language experts. “Audio snippets are not associated with user accounts as part of the review process, and reviewers are directed not to transcribe background conversations or other noises, and only to transcribe snippets that are directed to Google,” Monsees adds.

Google goes on to say it gives users a wide variety of tools to review the audio stored by Google Assistant devices, including the ability to delete those audio snippets manually and set up auto-delete timers. “We’re always working to improve how we explain our settings and privacy practices to people, and will be reviewing opportunities to further clarify how data is used to improve speech technology,” Monsees concludes.

What’s not addressed in the blog post is how the number of overall requests workers around world are reviewing for general natural language improvements, and not just to make sure the translations are accurate.

We don’t know how many total Assistant conversations are reviewed by Google employees

It’s widely understood by those in the artificial intelligence industry that human annotators are required to help make sense of raw AI training data, and those workers are employed by companies like Amazon and Google, where they’re given access to both audio conversations and text transcripts of some conversations between users and smart home devices. That way, humans can review the exchanges, properly annotate the data, and log any errors so software platforms like Google Assistant and Amazon Alexa can improve over time.

But neither Amazon nor Google has ever been fully transparent about this, and it’s led to a number of controversies over the years that have only intensified in the last few months. Ever since Bloomberg reported in April on Amazon’s extensive use of human contract workers to train Alexa, big tech companies in the smart home sector have been forced to own up to how these products and AI platforms are developed, maintained, and improved over time.

Often, the answer to those questions is small armies of human employees, listening to recorded conversations and reading transcripts as they input data for the underlying machine learning algorithms to digest. Yet there’s no mention of that the Google Home privacy policy page. There are also GDPR implications for European users when this level of data collection is done without proper communication and consent on the user end.

If you want this data deleted, you have to jump through quite a few hoops. And in the case of Amazon and Alexa, some of that data is stored indefinitely even after a user decides to delete the audio, the company revealed just last week. Google’s privacy controls appear to be more robust than Amazon’s — Google lets you turn off audio data storage completely. But both companies are now contending with a broader public awakening to how AI software is being beta tested and tinkered with in real time, all while it powers devices in our bedrooms, kitchens, and living rooms.

Contract workers have easy access to audio snippets, some captured by accident

In this case, we have a Belgian news organization that says it identified as many as 150 or so Google Assistant recordings out of 1,000 snippets provided by a contract worker that were accidentally captured, with no wake word uttered. That the employee in question who was able to get this data easily, violating user privacy and Google’s apparent safeguards, is disconcerting. Even more questionable is how the worker says he was able to piece together sensitive happenings inside user’s homes, like a potential threat of physical violence captured by a false accept when the worker heard a female voice that sounded as if it were in distress.

It’s clear that owning a Google Home or a similar Assistant device and allowing it to listen to your sensitive daily conversations and verbalized internet requests involves at least some type of privacy compromise. Using any Google product does, because the company makes money on collecting that data, storing it, and selling targeting ads against it. But these findings contradict Google’s claims that it’s doing seemingly everything it can to protect its users’ privacy, and that its software isn’t listening unless the wake word is uttered. Clearly, someone is in fact listening, somewhere else in the world. And sometimes they’re not supposed to be.",Voice Assistants,Verge,https://www.theverge.com/2019/7/11/20691021/google-assistant-ai-training-controversy-human-workers-listening-privacy,"This article reveals that human reviewers are listening to audio snippets of conversations between users and Voice Assistants, and that a contract worker was able to access sensitive audio snippets that were captured by accident, leading to a potential breach of user privacy.",Security & Privacy
15,How to delete Cortana recordings and protect your privacy,"If you use a PC, Xbox, or other Microsoft device, chatting with Cortana can be an easy way to get things done while your hands are occupied.

But as with all voice assistants, beware of corporate snooping. In August, Motherboard discovered that Microsoft contractors listen to recordings of Cortana voice commands, sometimes from personal computers and browsers with little security.

Cortana recordings are now transcribed in “secure facilities,” according to Microsoft. But the transcription program is still in place, which means someone, somewhere still might be listening to everything you say to your voice assistant.

Don’t worry: if this creeps you out, you can delete your recordings. Here’s how.

The first step is to open a Windows PC and sign in to the same Microsoft account you’ve been using to chat with Cortana. Once you’ve done that:

Type “Settings” into the search bar next to the Start button. The Settings app will come up; click on it. You can also click the Start button and scroll down to the Settings icon.

Click on “Accounts” in the bottom-left corner

Click on the “Manage my Microsoft account” link under your username. You’ll be redirected to Microsoft’s website and signed in to your Microsoft account.

Click “Privacy” on the left side of the menu at the top of the page

You’ll be prompted to reenter your Microsoft password; do that. You may also be required to verify your identity with two-factor authentication if you’ve set that up.

You should now be back in the Privacy section of Microsoft’s website. Click on “My activity” in the menu under the banner.

Open the menu that says “Filter by data type” on the left side of the screen. Click “Voice,” which is the second option.

In the center of the screen, you’ll see a list of Cortana recordings associated with your Microsoft account. You can click on the play button to listen to each recording individually.

To delete all of your voice recordings, select “Clear activity” at the top right of the list. To delete individual recordings, click the “Clear” link at the bottom of each item (just next to the “View details” link).

Note: Doing this won’t prevent Cortana from sending your voice recordings to Microsoft. For that, you’ll need to disable online speech recognition.

How to stop Cortana from recording your voice

Go back into Settings. This time, scroll down to Cortana in the right-hand column.

Click “Permissions” in the menu on the left side

Click “Manage the information Cortana can access from this device.” Scroll down and select “Speech Privacy Settings.”

Toggle Online speech recognition off

Now, Cortana will only use device-based speech recognition, which is less accurate than its cloud-based recognition engine and has limited functionality. But Microsoft will no longer transcribe or collect any of your voice recordings.",Voice Assistants,Verge,https://www.theverge.com/2020/1/23/21078549/how-to-delete-cortana-microsoft-recordings-privacy,"Using Voice Assistants can be convenient, but it can also make your data vulnerable to corporate snooping. To protect your privacy, you should delete your recordings and disable online speech recognition in your device settings.",Security & Privacy
16,"Apple Contractors Hear Sex Acts, Drug Deals in Siri Recordings","If you use Apple’s AI-powered voice assistant Siri — or own a Siri-enabled device — there’s a chance a human on the other side of the world may be listening to you have sex right now.

Or they may be hearing that conversation you had with your boss about a new marketing strategy. Or that awkward exchange with your doctor about a really private medical problem.

That’s the takeaway from a troubling new Guardian story in which an Apple whistleblower details how the company lets contractors review audio of users’ Siri commands — as well as recordings never actually meant for Siri’s digital ears — to improve the digital assistant.

To hear Apple tell it, though, the whole review process seems rather innocuous.

Advertisement

Advertisement

The company told The Guardian it regularly sends Siri activations to “secure facilities” where human reviewers listen to the clips, which are usually just a few seconds long and stripped of the Apple user’s ID and name.

These contractors grade the audio snippets, noting whether the AI handled the request appropriately and any mistakes, such as Siri thinking it had heard its “wake word” when it didn’t.

Fewer than 1 percent of all Siri activations are subjected to this process, Apple said, and the goal is to improve Siri’s ability to understand and assist users.

But as The Guardian discovered, there are several key problems with Apple’s activation vetting process — and how the company describes it to users.

Advertisement

Advertisement

For one, Apple doesn’t explicitly state in the privacy documentation it develops for consumers that humans might be listening in when they talk to Siri.

It also doesn’t put much effort into hiring trustworthy contractors or making sure the audio clips can’t be traced to their sources, the whistleblower told The Guardian.

“There’s not much vetting of who works there, and the amount of data that we’re free to look through seems quite broad,” they said, later adding that “it’s not like people are being encouraged to have consideration for people’s privacy, or even consider it. If there were someone with nefarious intentions, it wouldn’t be hard to identify [people on the recordings].”

Perhaps the most troubling revelation in The Guardian story, though, is the regularity with which these human reviewers hear audio that wasn’t even meant for Siri.

Advertisement

Advertisement

“There have been countless instances of recordings featuring private discussions between doctors and patients, business deals, seemingly criminal dealings, sexual encounters and so on,” the whistleblower said. “These recordings are accompanied by user data showing location, contact details, and app data.”

The recordings can also be far longer than the few seconds Apple described to The Guardian, with the whistleblower noting that some can last upwards of 30 seconds.

It’s not entirely surprising that Apple lets humans review audio recorded by their AI assistants, given that we already knew that Amazon and Google do the same thing.

And it also looks like digital assistants are not only here to stay, but that they’ll be even more ubiquitous in the future, meaning these companies probably aren’t going to stop attempting to perfect the tech any time soon.

Advertisement

Advertisement

So, if Apple, Google, Amazon, and the rest of the tech giants are determined to let humans review audio recorded by their AI assistants, maybe they should all focus on perfecting just one aspect of the tech first: training the assistants to listen only when spoken to.

READ MORE: Apple contractors ‘regularly hear confidential details’ on Siri recordings [The Guardian]

More on AI assistants: Amazon Workers Listen to Your Alexa Conversations, Then Mock Them

Care about supporting clean energy adoption? Find out how much money (and planet!) you could save by switching to solar power at UnderstandSolar.com. By signing up through this link, Futurism.com may receive a small commission.

Advertisement

Advertisement",Voice Assistants,Futurism,https://futurism.com/apple-siri-hear-sex-acts-drug-deals,"Voice Assistants from companies like Apple, Google and Amazon are routinely monitored by humans, and this means that private conversations, medical information and sexual encounters could be heard by contractors without the user's consent. There is a need for improved vetting of contractors and better security measures to protect user privacy.",Security & Privacy
17,Us’ voice assistant scene plays off a real 911 problem for smart speakers,"If you haven’t seen Jordan Peele’s new horror movie Us, I apologize for spoiling one minor plot point: the film features a smart speaker. We’re going to be discussing much bigger spoilers below, so you might want to avoid reading further.

If you have seen Us, you probably remember “Ophelia,” the Amazon Alexa-like voice assistant that plays a key role in one of the film’s most darkly funny scenes. Ophelia is good at playing music, but not great at interpreting commands — which becomes a problem when a character frantically tells Ophelia to call the police, only to discover that Ophelia has misheard an order to play N.W.A.’s “Fuck Tha Police” instead.

Smart speakers — even ones that let people make phone calls — generally don’t support 911

The Ophelia speaker helps establish its owners, the mostly unsympathetic Tylers, as a yuppie family with a penchant for useless high-tech toys. In Us, the joke is that Ophelia can’t understand the Tylers’ requests or the fact that they’re fighting for their lives against a group of doppelgängers. If it worked like the Echo or Google Home, though, Ophelia would have been useless even if it understood the request perfectly. Most smart speakers can’t call the police, and there’s been a long debate over whether they should.

Smart speakers — even ones that let people make phone calls — generally don’t support emergency or 911 calling for technical and regulatory reasons. As The Wall Street Journal explained last year, devices like Alexa and Google Home don’t offer the same location details as a mobile or landline phone, making it harder to route and respond to emergency calls. They also generally don’t have their own callback numbers, and their manufacturers don’t pay the monthly 911 surcharge that’s tacked onto ordinary phone bills. (Services like Skype usually can’t place emergency calls for the same reasons.)

There’s huge promise in a system that lets people dial emergency numbers when they can’t reach a phone — especially for people with mobility issues. But adding 911 service would require clearing some FCC hurdles, and the Journal wrote that spotty internet service could make the service unreliable. Most people already have phones, and in many cases, they’re better off using a system that’s known to work than experimenting with something new.

Smart speakers also raise serious privacy concerns. They collect huge amounts of data that can be requisitioned by law enforcement — late last year, a New Hampshire judge ordered Amazon to turn over Echo recordings related to a double murder case. Amazon has also sent one user’s Echo voice recordings to someone else, and the Echo has recorded private conversations after being accidentally activated. Wired has speculated that smart speakers could end up being subject to “mission creep” if they’re linked to emergency services, expanding from simple 911 calls to preemptively detecting dangerous situations.

Smart speakers can tell you the numbers for intimate partner violence or suicide hotlines, even if they can’t directly make a call

For now, though, that particular concern seems far away. It’s true that in at least one incident, Albuquerque police publicly thanked Alexa for calling emergency services. But experts said this would have required an unusual and convoluted series of events, and the police admitted that they weren’t sure exactly what had transpired.

There are some exceptions and workarounds. The Amazon Echo Connect specifically hooks into a landline and acts as a voice-controlled speakerphone system. Alexa skills like “Ask My Buddy” can let you alert a friend who can call 911 for you. Siri can dial emergency services on the iPhone — sometimes a bit too easily, as many people who have accidentally told it to dial an emergency number found out. And smart speakers can tell you the numbers for intimate partner violence or suicide hotlines, even if they can’t directly make a call.

“Ophelia” isn’t Alexa — maybe because using a real smart speaker in an unflattering light could create legal hassle, or maybe because it would have felt too much like splicing an Amazon commercial into the middle of the movie. And in fairness, when I asked an Echo to call the police last night, it just looked for a (nonexistent) phone contact called “Police” instead of blasting “Straight Outta Compton.”

But Ophelia mirrors the false sense of security that real smart speakers can provide, just as horror films often play on our concerns about modern technology. In the Albuquerque incident above, a recording apparently did capture someone asking Alexa to call 911. It’s just that Alexa, as far as we know, wasn’t in a position to help.",Voice Assistants,Verge,https://www.theverge.com/2019/3/26/18281387/us-2019-movie-jordan-peele-voice-assistant-ophelia-911,"Smart speakers generally do not support 911 calls, and they present serious privacy concerns. They cannot provide a reliable way to call for emergency help and can be misused to collect personal data.",Security & Privacy
18,Creative agency Virtue introduces genderless voice Q to challenge biases in technology – TechCrunch,"Siri, Alexa, Google Assistant, Cortana and Bixby — almost all virtual assistants have something in common. Their default voices are women’s, though the role that plays in reinforcing gender stereotypes has been long documented, even inspiring the dystopian romance “Her.” Virtue, the creative agency owned by publisher Vice, wants to challenge the trend with a genderless voice called Q.

The project, done in collaboration with Copenhagen Pride, Equal AI, Koalition Interactive and thirtysoundsgood, wants technology companies to think outside the binary.

“Technology companies are continuing to gender their voice technology to fit scenarios in which they believe consumers will feel most comfortable adopting and using it,” says Q’s website. “A male voice is used in more authoritative roles, such as banking and insurance apps, and a female voice in more service-oriented roles, such as Alexa and Siri.”

To develop Q, Virtue worked with Anna Jørgensen, a linguist and researcher at the University of Copenhagen. They recorded the voices of five non-binary people, then used software to modulate the recordings to between 145-175 Hz, the range defined by researchers as gender neutral. The recordings were further refined after surveying 4,600 people and asking them to define the voices on a scale from 1 (male) to 5 (female).

Virtue is encouraging people to share Q with Apple, Amazon and Microsoft, noting that even when different options are given for voice assistants, they are still usually categorized as male or female. As the project’s mission statement puts it, “as society continues to break down the gender binary, recognizing those who neither identify as male nor female, the technology we create should follow.”",Voice Assistants,TechCrunch,https://techcrunch.com/2019/03/12/creative-agency-virtue-introduces-genderless-voice-q-to-challenge-biases-in-technology/,"Voice Assistants come with a default female voice, reinforcing gender stereotypes and ignoring those who don't identify as male or female. Virtue's project Q aims to challenge this trend by creating a genderless voice.",Equality & Justice
19,How Google's Eerie Robot Phone Calls Hint at AI's Future,"When a robot rings your phone, you can usually tell right away. Its voice is melodic, it rarely stumbles, and it’s unnaturally efficient. The voice betrays its origin before it even has the chance to tell you that you qualify for a free loan, your mortgage payment is overdue, or that your input would really be valuable for a customer survey. Knowing it's a robot also makes it easy to hang up.

The minds behind Google Duplex are in the process of changing that paradigm, for better or worse. Announced Tuesday at Google I/O, the company’s annual developer conference, Duplex is new technology that enables Google's machine intelligence–powered virtual assistant to conduct a natural conversation with a human over the phone, mimicking the chit-chattiness of human speech as it completes simple real-world tasks.

It was shown off during the keynote event, and even though the onstage demo was prerecorded, seeing and hearing the concept in action floored the audience. In the first demo, a woman calls a hair salon, where another woman answers the phone; the two go back and forth for approximately a minute before they figure out a time that works for a hair appointment. In the second demo, also about a minute, a man calls a restaurant to book a reservation; the woman on the receiving end has a heavy accent and isn't offering the best information, so the caller pivots to make a new request.

The big reveal was that neither of the voices who initiated the calls belonged to a human. They were bots, dispatched through Google Assistant and activated through a back-end system. But they sounded human: They said “Um” and “Ohh, I gotcha” and ended query statements with the raised pitch of a question mark. And, for the purpose of the demo, they completed tasks that normally fall to us mere mortals, whether than meant making a hair appointment or determining whether it would be better to just walk into a restaurant and take a gamble on a table.

For Google, Duplex marks the next big step in natural-sounding, fully-autonomous robot conversations. For the rest of us, it straddles a fine line between being enormously convenient and eerily deceptive. Google still hasn’t launched this feature, which will work in Assistant on phones and compatible smart speakers. The company plans to begin testing Duplex publicly this summer. In the meantime, there are at least a few features it needs to consider, including how the Assistant will announce itself to unsuspecting humans on the other end.

Mr. Roboto Calling

Duplex was first launched as an experiment several years ago, Google says, and was started by principal engineer Yaniv Leviathan and Yossi Matias, vice president of engineering. (One person within the company indicated that it started as a 20-percent project, though a Google spokesperson declined to say whether it fell within those parameters.) Duplex brings together natural language processing, deep learning, and text-to-speech technology into one service. The part that resonates most, though, is the ""natural"" bit—the engineers have trained the Duplex model to match expectations around latency, like pauses after someone says “Hello?”, and to change intonation depending on how the conversation flows. In other words, to react the way humans do when speaking on the phone.

LEARN MORE The WIRED Guide to Artificial Intelligence

It’s a reversal of the familiar bot dynamic of a human calling a vendor, like a bank, and having to deal with a computer on the other end.

“Usually when people talk to a computer, they have a goal and they’re basically willing to do it the computer’s way,” says Alexander Rudnicky, who researches human-computer speech interaction at Carnegie Mellon University. “In this way, it’s turning it around. It’s a computer going out and trying to convince a human they should try to talk to them.”",Voice Assistants,WIRED,https://www.wired.com/story/google-duplex-phone-calls-ai-future/,"Voice Assistants like Google Duplex are raising ethical questions about how far humans should trust AI to interact with other humans, as it can be difficult to differentiate a human voice from an AI one. This raises concerns about AI deceiving humans, and the potential for it to be used maliciously.",Security & Privacy
20,"If chatbots are going to get better, they might need to offend you","AIs have gotten better at holding a conversation, but tech firms are wary of rolling them out for fear of PR nightmares.

Better bots: The New York Times says recent AI advances helped Microsoft and Facebook build a “new breed” of chatbots that carefully choose how to converse. Microsoft, for instance, built one that picks the most human-sounding sentence from a bunch of contenders to create “precise and familiar” responses.

But: Like Microsoft’s disastrously racist Tay bot, they still go wrong. Facebook says 1 in 1,000 of its chatbots’ utterances may be racist, aggressive, or generally unwelcome. That’s almost inevitable when they’re trained on limited data, because there’s bound to be unsavory text in online conversations that are used as training sets.

Why it matters: If the bots are going to keep improving, they must go in front of real users. But tech firms fear PR disasters if the software says the wrong thing. We may need to be more accepting of mistakes if we want the bots to get better.",Voice Assistants,MIT,https://www.technologyreview.com/2018/02/22/145117/better-chatbots-might-need-to-offend-you/,"Tech firms are wary of rolling out new AI-powered chatbots for fear of PR disasters, as the bots often make mistakes due to being trained on limited data sets which may contain unsavory content.",Social Norms & Relationships
21,Amazon’s Alexa isn’t just AI — thousands of humans are listening,"Amazon, like many other tech companies investing heavily in artificial intelligence, has always been forthright about its Alexa assistant being a work in progress. “The more data we use to train these systems, the better Alexa works, and training Alexa with voice recordings from a diverse range of customers helps ensure Alexa works well for everyone,” reads the company’s Alexa FAQ.

What the company doesn’t tell you explicitly, as highlighted by an in-depth investigation from Bloomberg published this evening, is that one of the only, and often the best, ways Alexa improves over time is by having human beings listen to recordings of your voice requests. Of course, this is all buried in product and service terms few consumers will ever read, and Amazon has often downplayed the privacy implications of having cameras and microphones in millions of homes around the globe. But concerns about how AI is trained as it becomes an ever more pervasive force in our daily lives will only continue to raise alarms, especially as most of how this technology works remains beyond closed doors and improves using methods Amazon is loathe to ever disclose.

Amazon employees listen to your Alexa recordings to improve the service

In this case, the process is known as data annotation, and it’s quietly become a bedrock of the machine learning revolution that’s churned out advances in natural language processing, machine translation, and image and object recognition. The thought is, AI algorithms only improve over time if the data they have access to can be easily parsed and categorized — they can’t necessarily train themselves to do that. Perhaps Alexa heard you incorrectly, or the system thinks you’re asking not about the British city of Brighton, but instead the suburb in Western New York. When dealing in different languages, there are countless more nuances, like regional slang and dialects, that may not have been accounted for during the development process for the Alexa support for that language.

In many cases, human beings make those calls, by listening to a recording of the exchange and correctly labeling the data so that it can be fed back into the system. That process is very broadly known as supervised learning, and in some cases it’s paired with other, more autonomous techniques in what’s known as semi-supervised learning. Apple, Google, and Facebook all make use of these techniques in similar ways, and both Siri and Google Assistant improve over time thanks to supervised learning requiring human eyes and ears.

In this case, Bloomberg is shedding light on the army of literal thousands of Amazon employees, some contractors and some full-time workers, around the world that are tasked with parsing Alexa recordings to help improve the assistant over time. While there’s certainly nothing inherently nefarious about this approach, Bloomberg does point out that most customers don’t often realize this is occurring. Additionally, there’s room for abuse. Recordings might contain obviously identifiable characteristics and biographical information about who is speaking. It’s also not known how long exactly these recordings are stored, and whether the information has ever been stolen by a malicious third party or misused by an employee.

While it may be standard practice, this type of annotation can lead to abuse

Bloomberg’s report calls out instances where some annotators have heard what they think might be a sexual assault or other forms of criminal activity, in which case Amazon has procedures to loop in law enforcement. (There have been a number of high-profile cases where Alexa voice data has been used to prosecute crimes.) In other cases, the report says workers in some offices share snippets of conversation with coworkers that they find funny or embarrassing.

In a statement, Amazon told Bloomberg, “We only annotate an extremely small sample of Alexa voice recordings in order [sic] improve the customer experience. For example, this information helps us train our speech recognition and natural language understanding systems, so Alexa can better understand your requests, and ensure the service works well for everyone.” The company claims it has “strict technical and operational safeguards, and have a zero tolerance policy for the abuse of our system.” Employees are not given access to the identity of the person engaging in the Alexa voice request, and any information of that variety is “treated with high confidentiality,” and protected by “multi-factor authentication to restrict access, service encryption, and audits of our control environment.”

Still, critics of this approach to AI advancement have been ringing alarm bells about this for some time, usually when Amazon makes a mistake and accidentally sends recordings to the wrong individual or reveals that it’s been storing such recordings for months or even years. Last year, a bizarre and exceedingly complex series of errors on behalf of Alexa ended up sending a private conversation to a coworker of the user’s husband. Back in December, a resident of Germany detailed how he received 1,700 voice recordings from Amazon in accordance with a GDPR data request, even though the man didn’t own a Alexa device. Parsing through the files, journalists at the German magazine c’t were able to identify the actual user who was recorded just by using info gleaned from his interactions with Alexa.

Amazon stores thousands of voice recordings, and it’s unclear if there’s ever been misuse

Amazon is actively looking for ways to move away from the kind of supervised learning that that requires extensive transcribing and annotation. Wired noted in a report late last year about how Amazon is using new, more cutting-edge techniques like so-called active learning and transfer learning to cut down on error rates and to expand Alexa’s knowledge base, even as it adds more skills, without requiring it add more humans into the mix.

Amazon’s Ruhi Sarikaya, Alexa’s director of applied science, published an article in Scientific American earlier this month titled, “How Alexa Learns,“ where he detailed how the goal for this type of large-scale machine learning will always be to reduce the amount of tedious human labor required just to fix its mistakes. “In recent AI research, supervised learning has predominated. But today, commercial AI systems generate far more customer interactions than we could begin to label by hand,” Sarikaya writes. “The only way to continue the torrid rate of improvement that commercial AI has delivered so far is to reorient ourselves toward semi-supervised, weakly supervised, and unsupervised learning. Our systems need to learn how to improve themselves.”

For now, however, Amazon may need real people with knowledge of human language and culture to parse those Alexa interactions and make sense of them. That uncomfortable reality means there are people out there, sometimes as far away as India and Romania, that are listening to you talk to a disembodied AI in your living room, bedroom, or even your bathroom. That’s the cost of AI-provided convenience, at least in Amazon’s eyes.",Voice Assistants,Verge,https://www.theverge.com/2019/4/10/18305378/amazon-alexa-ai-voice-assistant-annotation-listen-private-recordings,"The use of Voice Assistants requires human annotation to improve the AI, and this has caused concerns about privacy and misuse of recordings, as well as the potential for the recordings to contain identifiable information about the user.",Security & Privacy
22,Google will pause listening to EU voice recordings while regulators investigate,"Google has agreed to stop listening in and transcribing Google Assistant recordings for three months in Europe, according to German regulators.

German regulator calls voice assistants “highly risky”

In a statement released today, Germany’s data protection commissioner said the country was investigating after reports that contractors listen to audio captured by Google’s AI-powered Assistant to improve speech recognition. In the process, according to the reports, contractors found themselves listening to conversations accidentally recorded by products like the Google Home.

“The use of automatic speech assistants from providers such as Google, Apple and Amazon is proving to be highly risky for the privacy of those affected,” the German commissioner’s statement says, pointing to privacy concerns not only for device owners, but for visitors to homes with those devices. Google, according to the statement, will stop the practice of listening to and transcribing recordings for at least three months across the European Union, as the regulator looks into the issue.

A Google spokesperson said it had itself moved to pause “language reviews” while it investigated recent media leaks.

“We are in touch with the Hamburg data protection authority and are assessing how we conduct audio reviews and help our users understand how data is used,” the spokesperson said. “These reviews help make voice recognition systems more inclusive of different accents and dialects across languages. We don’t associate audio clips with user accounts during the review process, and only perform reviews for around 0.2% of all clips.”

The controversy over contractors has extended to other companies, as recent reports have highlighted how Apple and Amazon workers also listen to recordings to improve Siri and Alexa. In the statement, the German regulator writes that other speech assistant providers, including Apple and Amazon, are “invited” to “swiftly review” their policies.",Voice Assistants,Verge,https://www.theverge.com/2019/8/1/20750327/google-assistant-voice-recording-investigation-europe,"The German data protection commissioner has warned of the dangers posed to personal privacy from Voice Assistants, as contractors have been listening to recordings captured by devices like the Google Home, potentially hearing private conversations. Google has agreed to suspend this practice for at least three months in Europe.",Security & Privacy
